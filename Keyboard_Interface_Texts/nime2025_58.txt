 
 
Maximum Silence to Noise:  Sound synthesis for responsive gestural control    Andrew R. Brown Griffith University   226 Grey St.  South Brisbane, Australia, 4104 andrew.r.brown@griffith.edu.au  
    
ABSTRACT Modulation synthesis has been a foundational technique in the development of electronic musical instruments since their inception. This paper presents a novel approach to ring modulation synthesis, termed Maximum Silence to Noise (MSN), along with an associated method of gestural control facilitated by a pressure-sensitive multi-touch controller. The primary objective of this research is to develop an instrument capable of producing a broad and diverse range of audio spectra that can be expressively articulated through responsive touch-based interaction. Integrating the synthesis process with gestural parameter mapping is crucial for the performative capabilities of New Interfaces for Musical Expression (NIMEs). The technical development of an MSN-based instrument was subject to an iterative design process with mixed method evaluation. The usability and practical application of the MSN instrument was refined through performance experiences, which illustrate the effectiveness of the synthesis-gesture mappings in providing dynamic and expressive control over the diverse generated audio spectra. Author Keywords Synthesis, gesture, interaction, expression 1. INTRODUCTION The evolution of musical interfaces is ever-changing, with techniques being invented, adapted, and reimagined. This article explores part of that journey, blending ring modulation and multi-touch control with new innovations in a streamlined digital audio system.  It introduces a multi-touch musical instrument utilizing Maximum Silence to Noise (MSN) synthesis—a versatile ring modulation technique. Designed for expressive performance, the instrument employs subtle gestures to shape diverse audio spectra, from sine waves to noise, allowing rich timbral variation and dynamic control.  Following a historical and technical overview, the article details the MSN algorithm and its gestural mapping through a multitouch interface, enabling nuanced interaction with modulating sound spectra. It also discusses evaluation approaches, including experiences performing live with the MSN instrument worldwide.  A version of the MSN software as a Pure Data patch is available for download from GitHub: https://github.com/algomusic/Max-Silence-Noise 2. BACKGROUND Like most projects, the development of the MSN algorithm and instrument was influenced by various factors. The work draws inspiration from the rich history of ring modulation synthesis, particularly from implementations in the form of handheld DIY hardware. The gestural control of the synthesis algorithm utilises bespoke software running on commercial hardware and builds upon 
prior work in multi-touch gestural control of music systems. This section surveys prior work in each of these areas of influence. 2.1 Ring Modulation Ring modulation is a type of amplitude modulation where the audio signals are all within the audio range. In digital systems, it essentially involves multiplying the two signals. If one of the signals has a low frequency, it produces a tremolo effect. However, when both signals are within the audible range, more complex and captivating timbres emerge from the combination of sidebands generated around the signal frequencies. By making one or both waveforms unipolar instead of bipolar, the number of sidebands produced increases. In the current MSN implementation, waveforms other than maximum silence are bipolar.  Ring modulators were widely used in the early years of electronic music. A prominent user of the device was Karlheinz Stockhausen. His works that featured a ring modulator include Mikrophonie II (1965) for choir, Hammond organ and 4 ring modulators; Mixtur (1965) for orchestra, 4 sine-wave generators, and 4 ring modulators; and MANTRA (1970) for 2 ring modulated pianos.  Stockhausen was not the only composer to embrace ring modulation at this time. For example, Roger Smalley’s Monody (1972) was composed for piano, percussion and ring modulator [13].  Recognising the trend in ring modulation usage at that time, Simon Emmerson [4] reflected on its prominent use in contemporary music and other media contexts, such as creating the Dalek voice in Doctor Who.  Today, five decades after its inception, ring modulation has become an integral component of numerous synthesisers. Unlike its historical applications in composition or the inclusion of ring modulation as an optional feature in two-oscillator subtractive synthesisers, the development of the MSN instrument was more directly influenced by the performativity of related techniques as implemented on low-fidelity hardware electronic instruments that also emerged in the 1970s and maintained popularity until the early 2000s. 2.2 Hardware Inspirations DIY instruments using modulation synthesis are prized for their raw, dynamic sound and expressive touch interaction, keeping them popular. These low-fidelity, handheld devices often rely on simple analogue and digital electronics to generate square wave oscillators, which, when ring modulated, produce clangorous tones. This inharmonic character also shaped metallic percussion in iconic drum machines like the Roland TR series. While MSN embraces high-fidelity software synthesis, it integrates rich spectra ring modulation’s noisy energy while balancing it with gentler tones from less harmonically rich sources.  Influences behind the MSN instrument include the Crackle Box, BoardWeevil, and Atari Punk Console—each known for using ring modulation or related techniques to create dynamic, inharmonic spectra. 

 
 
 The Kraakdoos, or Crackle Box1, was designed by Michel Waisvisz at STEIM in the 1970s, it is a touch and pressure sensitive device that makes a wide range of noises when played with. It uses the human body via touchpoints on the device as a vital part of the electronics, with the skin’s resistance playing a role. The device is portable, including battery power and speaker [12].  Over the course of a decade in the early 21st century, BugBrand developed a diverse range of Weevil synthesisers2. The Weevils all stem from a single core circuit that utilises CMOS 4000 chips to generate simple square waves that are quasi-ring modulated together. Numerous touchpoints allowed for interactions that disrupt the circuit and change the sound. Weevils were always equipped with an onboard speaker to ensure their portability and allowing them to be treated as ‘acoustic’ instruments. BoardWeevils, specifically, were produced as raw PCB instruments without a case. The first was introduced in 2009 followed by different versions in 2012 and 2015.   The Atari Punk Console (APC) is a simple and popular DIY synthesiser circuit. It uses 555 timer ICs to generate square wave signals. Originally designed by Forrest Mims III as the “Stepped Tone Generator,”3 it was later named “Atari Punk Console” by Kaustic Machines due to its sonic resemblance to classic Atari console sounds from the 1980s.4 It features two controls: one for frequency and another for volume, which can be adjusted using potentiometers or other variable inputs like light, temperature, or pressure.  These traditions of modulation synthesis, particularly with a lo-fi emphasis, continue to thrive through the work of makers like BLEEPLABs5, Gijs Gieskes6, and Bastl Instruments7.  2.3 Interface Inspirations A second major source of inspiration for the MSN instrument is the use of multi-touch interfaces for gestural expression. One of the attractions of the low-fidelity ‘noise’ machines is the immediacy of sound making through touch control. It was important to emulate this in the interface design for the MSN instrument. Simple, yet expressive, gestures have always been at the heart of musical expression. As Ainger and Schroeder note; “gesture can be thought of as the animating force of music—the force that gives life to music” [1, p. 333].   Although gesture has been incorporated into electronic music composition at least since Iannis Xenakis’ UPIC system [9], there continues to be a strong focus on real-time control in electronic music practices. As Simon Emmerson [5] notes in more recent work than quoted earlier, in the latter decades of the 20th century there had been a ‘rupture’ of relationships between body and sound in various electroacoustic and generative music practices. He laments “the loss of appreciation of human agency within the sound world” [5, p. 206] and recognised that at the turn of the century “the need for human-computer interfaces more sensitive to the needs of performers is emerging as the most important new field of research [5, p. 209].” That the NIME conference itself emerged in the same year as Emmerson’s publication is a testament to how widespread that sentiment was, and recent decades have seen a revitalised interested in interaction and electronic musical expression.  2.3.1 Screen-based touch interfaces With the advent of smart phones and tablet computers, touch screen interfaces have become ubiquitous. Some examples of these applied to musical performance include TouchNoise [3], a system that generates and modulates noise spectra through multitouch interaction with a system of autonomous sound particles. It features on-screen  1 https://www.eam.se/kraakdoos/ 2 https://www.bugbrand.co.uk/product/weevils-2005-2015/ 3 https://en.wikipedia.org/wiki/Forrest_Mims 4 https://compiler.kaustic.net/machines/apc.html 5 https://bleeplabs.com 6 http://gijs.gieskes.nl 
buttons and dials for parameter adjustment, and a screen region where gestures influence a sonic particle system, resulting in a variety of musically intriguing sonic phenomena. Engeln et al. [6] developed a multi-touch screen interface for controlling audio morphing. Their interface visualised the internal DSP processes and allowed direct manipulation of the morphing parameters using multi-touch gestures. Gelineck et al. [7] exploit the advantages of multitouch interaction for music mixing with their 6to6Mappr system. McPherson and Kim [11] added multi-touch sensitivity to a piano style keyboard. Their design supports several single and multi-finger gestures.   There are many multi-touch music apps available for smart phones and tablets, some of the more imaginative ones using finger gestures for synthesis control, like the MSN instrument does, include TC-Performer and TC-11 by BitShape, Borderlands Granular by Chris Carlson, and VOSIS by Life Orange which is described in a NIME paper by Ryan McGee [10]. 2.3.2 Pressure Sensitivity Many of the multi-touch interfaces mentioned previously were screen based and so are limited to 2D control. One of the expressive features of the MSN instrument is its use of pressure sensitivity from the ROLI Lightpad Block. With the growing interest in the MIDI Polyphonic Expression (MPE) protocol the popularity of pressure sensitive multi-touch controllers such as the LinnStrument8 and Harken Continuum9 has increased. However, even these instruments build on some earlier pioneering efforts such as the Thunder from 1989 by renowned electronic musical instrument designer Don Buchla10.   The Thunder was a device equipped with multiple touch-sensitive keys and accompanying software (STORM) for MIDI control. Keys 1 to 9 respond to pressure, while keys 10 to 25 respond to both pressure and location. These keys can be assigned to play notes, control MIDI devices, and trigger pre-programmed “riffs.” More recently, an emulation of the Thunder was produced for the Sensil Morph11 using a rubber overlay resembling the Thunder interface.  ROLI have developed a series of musical controllers that employ multi-touch pressure sensitivity include the Lightpad Block M that is employed as the interface for the MSN instrument. The Lightpad Block is a small (94mm square) touch sensitive pad that transmits MIDI or OSC data over USB. It has an illuminated, touch-responsive surface, that can simultaneously recognise several touch points in three dimensions. For MSN, the Lightpad Block was programmed for two finger gestural tracking using the BLOCKS SDK and the LittleFoot programming language. 3. Instrument Design The MSN Instrument is a musical device that features novel ring modulation techniques thanks to its dual morphing oscillators, and multi-touch performance via control surfaces. In this section those features are expanded upon. 3.1 MSN Modulation At the heart of the MSN instrument is a novel implementation of ring modulation that features two identical oscillators that morph from silence to white noise, passing through sine and square waveforms along the way. These morphing oscillators enable a smooth transition from simple to complex audio spectra.   To achieve a staged and interesting transition from silence to white noise, the MSN oscillator morphs between several wave types as illustrated in Figure 1. The first is DC-offset silence, referred to here as maximum silence, and the second is a sine function. The pure sine 7 https://bastl-instruments.com 8 https://www.rogerlinndesign.com/linnstrument 9 https://www.hakenaudio.com/ 10 https://en.wikipedia.org/wiki/Buchla_Thunder 11 https://morph.sensel.com/ 
 
 
transitions to a more hollow-sounding square wave, then the pulse width of the square is adjusted for a more nasal timbre. From here the sound cross fades to noise waveforms, featuring a mixture of random cracking that produces a fire-like timbre and concluding with white noise.  
 Figure 1 – MSN Oscillator Waveforms The touch pad interface maps the transition from maximum silence to noise on the x axis and oscillator pitch on the y axis. Oscillator volume is assigned to the z, pressure, axis. Two of these mapped oscillators are each controlled by a finger on the pad, and multi-touch capability enables them to be independently performed. The two waveforms are ring modulated, and the combined 6 dimensions of control enables a rich variety of sonic expression.  The term ‘maximum silence’ refers to a solution to an issue that arises when either of the modulating oscillators is silent, no sound results from the ring modulator. To keep the modulation amount independent of amplitude envelope, the approach taken is to output a positive DC offset ‘silence’ signal that maintains the amplitude of the other modulating oscillator. To keep the gain of the modulated signal consistent, the DC offset is increased as the amplitude of the modulating oscillator is decreased. 3.2 Multi-touch Interactions The MSN instrument hardware integrates with the Pure Data (Pd) software, incorporating a ROLI Lightpad Block and an iPad running TouchOSC. The Pd patch receives MIDI data from the block via USB. In many performances with the MSN instrument, interaction data from the instrument is also transmitted to a Touch Designer network, which generates visual materials.  The ROLI Lightpad Block serves as the primary gestural controller for MSN. As shown in Figure 2, the oscillator’s timbral range is mapped from left to right across the Block’s surface, while the oscillator pitch is mapped from top to bottom. The pressure applied to the Block controls the instrument’s amplitude envelope. The Block is a multi-touch device, allowing for the tracking of two finger positions, one for each oscillator.  
 12 https://geformt.org/showcases/2018/10/25/gesture-editor.html 
 Figure 2. Parameter mappings on the Lightpad Block An iPad running TouchOSC software is also used and has a series of on-screen sliders and buttons, as shown in Figure 3, that control audio effects and, when present, visual software parameters. The effects that are controlled include volume, panning automation, delay and reverb parameters.  
 Figure 3. iPad TouchOSC interface 3.3 Gestural Heuristics Various gesture description languages (GDLs) have been proposed for multi-touch interfaces. These are reviewed by Kammar et al. [8], who also propose their own system, Gesture Formalism for Multi-Touch (GeForMT). This GDL has six key elements, but only three of these apply to the MSN interface: Performed Movement, Number of Touch Contacts, and Spatial Relations. Often, GDLs are used to describe gestures that invoke commands or outcomes, like swiping up, down or across a smartphone screen. By contrast, gestures on the MSN instrument, and on most other musical instruments, are not ‘commands’ to be recognised by the system, but rather methods of triggering sonic outcomes.   There are some basic gestural techniques used by the MSN interface. These techniques are: Tap – briefly pressing the pad, Hold – a sustained tap in one location, and Contour – a glide through pitch and/or timbral space. They can be executed by either of two fingers or by the two fingers in any combination (e.g. hold & contour). Some of these gestural possibilities are illustrated in Figure 4 using the visual representation adopted by the GeForMT editor12.  
12345678910012345678910
X Axis
Y Axis
12345678910012345678910
X Axis
Y Axis
12345678910012345678910
X Axis
Y Axis
12345678910012345678910
X Axis
Y Axis
12345678910012345678910
X Axis
Y Axis
100123456789
10
0123456789
X Axis
Y Axis
AmplitudeWaveformPitch
Touch 1
Touch 2

 
 
  Figure 4 – Example Pad gestures. Single Tap/Hold, single contour, hold & contour, double contour.  Music interfaces typically need gestures to be repeatable and easy to learn, and these basic techniques adhere to that requirement, but each performer can create their own set of combined gestures to give their performance a unique character.  4. TESTING AND EVALUATION The testing and evaluation of the MSN instrument employed two distinct stages, synthesis and usability. 4.1 Synthesis Testing Ensuring the MSN sound synthesis worked correctly involved a mixed-methods approach combining empirical results from oscilloscopes and spectrum plots with aesthetic judgments of the audible results. This approach guided the selection and refinement of the most appropriate DSP algorithms in each case.  Experiments were conducted to determine suitable transition solutions between waveforms for the MSN oscillator. Figure 5 shows some of the Pd patches employed to explore different transition options. Particular options were found to be appropriate for specific transitions. The noise and crackle waveforms were simply cross faded with adjacent waveforms. The transitions from sine to square to pulse were handled by modifying function parameters. Specifically, the sine amplitude was increased and clipped into the square waveform. Subsequently, the sine phase balance was distorted by frequency modulation to introduce a pulse width change.  
 Figure 5 – Transition alternatives explored in Pure Data The challenging transition was between two ring modulated sinewaves and a single sinewave. The maximum silence DC offset solution, described above, was used for this stage. 4.2 User Interface Testing Insights into usability for this project were largely informed by participant observation. An approach that acknowledges there is significance in what Karen Barad refers to as a “performative  13 https://www.explodingart.com/arb/2020/09/05/audio-visual-interiors/ and https://www.johnrobertferguson.com/avint/ 
understanding of discursive practices” [2, p. 802]. A perspective that takes account of embodied knowledge and seeks to evaluate the efficacy of the instrument’s expressivity as emerging from a marriage of human performativity and material agency that arises from the collection of design decisions and engineering implementations.  The participants in this evaluation, including the author, are practitioners and expert users whose judgement and insights about the performativity of the system are well informed. These participant reports can be considered scholarly and to hold justifiable interpretations based on multiple sources of comparison given the participant’s experience with many interactive music systems.   The live performance testing of the MSN instrument focused on several performances of a piece called Audio-visual Interiors, created by the author and his colleague John Ferguson. This work was performed variously as a solo and duet using the MSN instrument and, at times, other instruments developed by the performers. The work featured real-time visual projections created in Touch Designer that responded to performance data from the controllers. Performances took place at these venues or events: The Queensland Conservatorium, Griffith University in Brisbane, the Australasian Computer Music Conference in Melbourne, the Glasgow Electronic and Audiovisual Media (GLEAM) Festival, and an Electronic Music Concert at the University of Derby. Documentation of these performances is available for review online13.  The first version of MSN software ran directly on an iPad, using MobMuPlat as its host, integrating all controls, effects, and visuals as Pure Data interface objects. With each performance, the software evolved, adding features like audio signal and MIDI message indicators. Later enhancements included an audio-test toggle for sound checks and MIDI channel selection, enabling multiple MSN patch instances to respond independently to different performers. The underlying MSN oscillator implementation proved remarkably stable from the beginning. Adjustments were made to the sensitivity curves of the Lightpad Block software to enhance expression and responsiveness. However, over time, iPad OS updates seemed to diminish the reliability of running the Pd software as a touch interface on the iPad. Consequently, the parameter control elements were transferred to TouchOSC, and the MSN synthesis patch was executed on the same computer that managed the visuals.  4.3 Musical Affordances After spending significant time playing on the MSN instrument the following observations about its musical tendencies were observed.   The range of timbral variety was very extensive and following adjustments to pad sensitivity, a wide range of range of expressivity was enabled by articulations from short tapping to sustained contours and from gentle pressure-induced swells to dramatic transients. This enabled a range of aesthetic outcomes that were also commented upon by audience members following live performances.   The Lightpad Block is quite small and so movements across it need to be subtle and exact repeatability of touch location is unlikely. Mostly this can work as a positive for adding subtle variation and movement to the sound through gestural deviations. However, locating precise pitch is very difficult, so the instrument does not lend itself to the playing of melodies or aligning with conventional harmonic musical material.   Although the ‘pad’ interface visually suggests a more percussive performance, the amplitude envelope is directly controlled over time by sustained pressure. This means the articulation of events is also directly controlled, making the expressive experience more akin to a stringed instrument or a theremin.  Due the fact that each oscillator in ring modulation has an equal effect on the outcome, gestures can be inverted with similar outcomes. 
1 1
1
2
1
2

 
 
Appreciating this enables more flexibility in finger gesture combinations. Also, because gestures at either touch location can be used to vary the frequency or timbral relations, this may seem to run counter to knob-per-function instrumental experience where each control point is unique.   The Block can be played with either one or two hands, and typically more subtle or gentle expressions are easiest with two fingers on one hand and more assertive, dramatic or faster gestures are easier with one finger on each of two hands.  The setting for built-in effects, particularly delay, can greatly influence the approach to playing and exploration. To fully appreciate the instrument’s expressive potential, it’s necessary to experiment with different effects settings.   The TouchOSC interface is used less frequently during performances compared to the Block, nevertheless it was found to provide effective control over MSN effects and TouchDesigner scene changes. 5. CONCLUSION The development of the Maximum Silence to Noise (MSN) instrument marks a novel step in ring modulation synthesis and gestural music interaction. By combining waveform transitioning with pressure-sensitive multi-touch control, it provides a versatile platform for shaping diverse audio textures, building on the strengths of both ring modulation and touch-based interfaces.  Through iterative design and live performance testing, the MSN instrument has demonstrated dynamic and expressive control over generated spectra. Its responsive synthesis-gesture mapping enables performers to explore a broad timbral range, from subtle sine waves to complex noise environments. While monophonic, it thrives in ensemble settings.  Participant feedback has refined its usability, improving gestural mappings and interface responsiveness for more intuitive play. As MSN evolves, its potential to support musical expression in electronic music performance should grow.  6. ETHICAL STANDARDS The research has been carried out in line with the ethical standards and practices of my affiliated institution, and/or relevant regional/national guidelines.  
7. REFERENCES [1] Ainger, M. and Schroeder, B. 2014. Gesture in the Design of Interactive Sound Models. The Oxford Handbook of Interactive Audio. K. Collins, B. Kapralos, and H. Tessler, eds. Oxford University Press. 333–349. [2] Barad, K. 2003. Posthumanist Performativity: Toward an understanding of how matter comes to matter. Signs: Journal of women in culture and society. 28, 3 (2003), 801–831. [3] Berndt, A., Al-Kassab, N. and Dachselt, R. 2014. TouchNoise: A Particle-based Multitouch Noise Modulation Interface. Proceedings of the International Conference on New Interfaces for Musical Expression (London, UK, 2014), 323–326. [4] Emmerson, S. 1977. Ring modulation and structure. Contact: A Journal for Contemporary Music (1971-1990). 17 (1977). [5] Emmerson, S. 2000. “Losing Touch?”: The human performer and electronics. Music, Electronic Media and Culture. Ashgate. 194–216. [6] Engeln, L., Kammer, D., Brandt, L. and Groh, R. 2018. Multi-Touch Enhanced Visual Audio-Morphing. NIME Proceedings (Blacksburg, VA, 2018), 152–155. [7] Gelineck, S. and Serafin, S. 2012. Longitudinal evaluation of the integration of digital musical instruments into existing compositional work processes. Journal of New Music Research. 41, 3 (2012), 259–276. [8] Kammer, D., Henkens, D., Henzen, C. and Groh, R. 2015. Gesture Formalization for Multitouch. Software: Practice and Experience. 45, 4 (2015), 527–548.  [9] Marino, G., Serra, M.-H. and Raczinski, J.-M. 1993. The New UPIC System: Origins and Innovations. Perspectives of New Music. 31, 2 (1993), 258–269. [10] McGee, R. 2013. VOSIS: a Multi-touch Image Soniﬁcation Interface. Proceedings of the New Interfaces for Musical Expression Conference (Duejeon, Korea, 2013). [11] McPherson, A. and Kim, Y. 2011. Design and applications of a multi-touch musical keyboard. Proceedings of SMC (Padova, Italy, 2011). [12] Raes, G.-W. 2005. The Crackle box. Experimental Music: Book part 2: Live electronics. [13] Vickery, L. 2016. The role of ring modulation in the formal structure of Roger Smalley’s Mondy for piano with live electronic modulation. Sound Scripts. 5, 1 (2016), 17.   