Playsound.space: Inclusive Free Music Improvisations
Using Audio Commons
Ariane de Souza Stolﬁ
∗1
arianestolﬁ@gmail.com
Miguel Ceriani 2
m.ceriani@qmul.ac.uk
Luca Turchet 2
luca.turchet@qmul.ac.uk
Mathieu Barthet 2
m.barthet@qmul.ac.uk
1University of São Paulo 2Centre for Digital Music
School of Communication and Arts Queen Mary University of London
Av. Prof. Lúcio M. Rodrigues, 443 - Mile End Road, London
CEP: 05508-900 São Paulo, SP , Brasil United Kingdom
ABSTRACT
Playsound.space is a web-based tool to search for and play
Creative Commons licensed-sounds which can be applied
to free improvisation, experimental music production and
soundscape composition. It provides a fast access to about
400k non-musical and musical sounds provided by Freesound,
and allows users to play/loop single or multiple sounds re-
trieved through text based search. Sound discovery is fa-
cilitated by use of semantic searches and sound visual rep-
resentations (spectrograms). Guided by the motivation to
create an intuitive tool to support music practice that could
suit both novice and trained musicians, we developed and
improved the system in a continuous process, gathering fre-
quent feedback from a range of users with various skills. We
assessed the prototype with 18 non musician and musician
participants during free music improvisation sessions. Re-
sults indicate that the system was found easy to use and
supports creative collaboration and expressiveness irrespec-
tive of musical ability. We identiﬁed further design chal-
lenges linked to creative identiﬁcation, control and content
quality.
Author Keywords
Web Audio, Inclusive Design, Music Improvisation, Cre-
ative Commons
CCS Concepts
•Applied computing →Sound and music comput-
ing; Performing arts; •Information systems →Music
retrieval;
1. INTRODUCTION
Until recently, music production depended on the techni-
cal ability to play musical instruments [18]. This repre-
sents a barrier to music making since musical instruments
∗corresponding author
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
can be expensive if not cumbersome, are not widely ac-
cessible, and/or require very speciﬁc knowledge to be con-
trolled. The spread of personal computers and smartphones
has improved access to music making technologies, however
computer-based music making software are generally com-
plex with steep learning curves and mostly target musical
experts and professionals. On the other end of the spec-
trum, several tools appear to be too simple, lacking expres-
sivity, or acting as toys [24].
This work is part of a larger project aiming at developing
easy to use web-based tools for music making for ubiquitous
modern devices without requiring extra software installa-
tion [29]. The interface presented in this paper, Playsound,
seeks to support musical creativity and be engaging in an
inclusive way to address “the needs of the widest possible
audience, irrespective of age or ability” [10].
Our major domain of application is free music improvisa-
tion which is deﬁned as an autonomous musical activity [9]
that usually leads to pluralist situations, with the emphasis
on the playing process, and the interaction between musi-
cians in the moment [4]. In opposition to idiomatic impro-
visation, such as those practiced in some forms of jazz or
hip hop, free improvisation can lead to non-metric forms
without predeﬁned key or structure but where variations of
timbre [3] prevail. The idea of playing with “an expanded
sound palette” has been explored in music since Luigi Rus-
solo [25], but the advances of web technologies and access to
online audio content now allow composers to access a more
diverse array of sounds then ever. Such musical form lends
itself well to the type of sonic material used in soundscape
composition [27] such as ﬁeld recordings or synthetic tex-
tures. With regards to experimental music, Cage welcomed
“dissonances and noises” as any other musical sounds [8].
These are likely to occur in free improvisations due to the
layering of sounds moving away from tonal compositions.
Amongst online audio content resources, a wide range of
non musical and musical sounds are made publicly available
through the Audio Commons Ecosystem [14] 1. The idea of
Playsound started from the the ﬁrst author’s will to make
use of such broad online Creative Commons sound mate-
rial in practical musical contexts, without having to rely on
personal local audio collections.
1The Audio Commons initiative aims to bring Creative
Commons audio content to artists and the creative indus-
tries. Creative Commons copyright licenses provide a stan-
dardized way to give the public permission to share and use
creative work on conditions deﬁned by the content creators.
228
2. RELATED WORKS
We considered related works in the three following research
areas related to NIME.
(i) Technology-mediated group music improvisa-
tion. Within the NIME context, a lot of diﬀerent ap-
proaches have been developed to use the computer as in-
strument in free improvisation. Examples include collabo-
rative live coding [15] and laptop orchestras [2]. Collabo-
rative live coding often includes development of technology
for synchronization between devices [30], which is here not
necessary since the aesthetic choice is to leave the rhythmic
structure unconstrained.
(ii) Web-based music making tools. Over the past
few years, especially with the development of the Web Au-
dio API, a lot of research has been conducted to build
platforms to make music online. Some of them are devel-
oped based on previous types of digital music instruments
(DMIs) such as digital audio workstation emulators [19] or
sequencers [12], while others leverage web connectivity for
participatory experiences [21, 31, 29]. Such a diversity of
works shows the potential of web technologies to support
new interfaces for musical expression, but most of the cur-
rently developed instruments either require expert music
knowledge or are simple to use but restricted in terms of
musical expressiveness [11].
(iii) Re-purposing of sounds. Recorded sound sam-
ples are widely employed in several aesthetic music tradi-
tions such as Hip Hop, Plunderphonics, Electronic Music,
Musique Concr` ete, Soundscape Composition. Online au-
dio content collection such as Freesound.org, Redpanal.org,
Sampleswap.org and others, are used by composers and pro-
ducers for various types of multimedia applications, such
as motion picture, advertisement, video games and music
compositions [28]. The APICultor [26] uses machine learn-
ing techniques to provide an environment for re-purposing
sound samples from online databases. Lee et al. proposed a
live coding tool with the YouTube API for free improvisa-
tion [20]. By providing database access trough a REST API
[1], Freesound.org enables musicians and designers to cre-
ate applications exploiting its audio content in live applica-
tions. Freesound Explorer [13] organizes sounds in a spatial
conﬁguration related to sound similarity and uses colors to
represent timbral aspects. However, this tool primarily tar-
gets navigation and exploration rather than music making
and it does not allow users to select sounds from multiple
semantic queries, as in Playsound (see Section 3.1). Beat-
Push [12] is a simple sound sequencer with special audio
eﬀects which can be used to produce metric music.
3. DESIGN
Following a practice-based research approach, the ﬁrst au-
thor initially developed Playsound for her own use, as a
tool to support her practice in free music improvisation as
a solo performer and in ensembles. This objective was then
expanded by opening the tool to other users and collecting
feedback in a series of formal evaluations.
3.1 Motivations and requirements
As a musician non familiar with melodic/harmonic instru-
ment practice and traditional music notation, but famil-
iar with music technology and web development, the ﬁrst
author wanted to build a platform where she could select
sounds from the Freesound database visually to play a large
number of sounds during live performances. In this sense,
the tool was primarily aimed at providing a rich sound
palette without the necessity of instrumental and technical
virtuosity.
Some diﬃculties were identiﬁed in current practice when
using sound samples during live performances: musicians
need to know how given sonic materials sound before play-
back; when browsing sound databases, users generally need
to listen to a large amount of sounds to choose some that
can satisfy their needs; also, complex sounds are diﬃcult
to represent through conventional music notation (e.g., the
same note or chord played from diﬀerent sources can sound
very diﬀerent), and traditional music notation is not ca-
pable of representing a whole range of non-musical sounds
(e.g., nature-related sound, speech) typically available in
databases such as Freesound [28]. We chose to use spec-
trograms2 that can be directly sourced from the Freesound
API as visual representation for the sounds, as they let users
get some cues about the sound properties before playback.
3.2 Design choices and methodology
For computer users, typing text is arguably more accessi-
ble than controlling a musical instrument [22]. Our mecha-
nism of sound selection exploits the idea of semantic queries
which are opened to any users without requiring music knowl-
edge. We followed a minimalist design approach to develop
a simple and intuitive interface providing fast responses and
a large number of search results, and we emphasized the
display of sound spectrograms as, after training, they could
become a quick way to characterise the sonic aspects of the
sounds returned by the system.
During the ﬁrst phase of development, Playsound.space
was developed following a LeanUx [16] design methodol-
ogy by starting with a very basic working model of the sys-
tem and sequentially adding features. During this stage,
that lasted over four months, user interface testing was
conducted by the main designer, playing in solo sessions,
and was also informed from feedback collected with other
users. These were non musicians and musicians from dif-
ferent ﬁelds such as cinema, performance, music technology
and media and arts technology, and were consulted in face-
to-face interactions or by chat.
The development started by building a search engine which
was gradually improved through the following steps: devel-
opment of a URL-based system to store and recall selected
sounds; adaptation of the user interface for smartphone; in-
tegration of a WAV sound recorder directly from the web
interface; integration of the loop function for expressive-
ness; enhancement of the audio player by adding individual
volume controls for each sound and a button to delete un-
wanted sounds from the list of selected sounds.
Since the start of the project, functional versions of the
software were maintained online and released as open source
software on Github 3. After the system reached a certain
level of maturity, a formal evaluation process was under-
taken in the context of live performances (see Section 4).
3.3 Implementation
Playsound was coded in JavaScript with the Angular.js frame-
work, as a single page application and a node.js server han-
dling the authentication process with the Freesound REST
API. The website is accessible at http://playsound.space
and works with any browser compatible with HTML5 and
the Web Audio API. Figure 1 shows Playsound’s client/server
architecture, and Figure 2 displays a screenshot of the UI.
By using mostly client-side processing, the tool provides fast
query responses and audio playback and does not require
too much processing power from the server.
2Spectrograms are visual representations of the distribution
of energy of sound frequencies over time.
3The source code of Playsound.Space is available at: https:
229
/user /users
/hdd /hdd
/hdd
/laptop
playsound
user
freesound 
community
sounds, 
spectrograms 
and metadata
freesound 
server
playsound 
server
authentication 
server 
querieshtml/
javascript 
templates
Figure 1: Playsound client/server architecture
3.4 User interaction
Users can search for sounds by entering textual descriptions
(keywords) in a text input ﬁeld. While the user is typing, a
range of corresponding sounds appears on the right side of
the UI4, as shown on Figure 2. The nature of the retrieved
sounds depend on the metadata provided during uploads
from Freesound community users, such as tags, descriptions
or ﬁle names. Users can then select retrieved sounds by
clicking on their spectrogram image. This triggers sound
playback and generates a player object which is displayed
on the left side of the UI. While sounds are being played,
it is still possible to search for other sounds or to select
more sounds from the same search query. Users can play
simultaneously any number of sounds returned from multi-
ple searches (given computing limitations). When sounds
are selected, their identiﬁers (ID) are appended to the URL
of the website. This allows users to retrieve their selections
by loading the same URL again. A video demonstrating an
example of creative practice with Playsound is available at:
https://youtu.be/yv8T70rawzs.
Sound controls include play, pause, loop and volume. The
UI also allows the user to remove sounds from the selec-
tions. The audio player relies on a standard HTML audio
object, so depending on the browser, the controls can be
slightly diﬀerent. Users can also save individual sounds lo-
cally through the player. A plus sign allows users to open
an additional tab in the browser displaying an empty form
that can be populated with concurrent queries and resulting
playing sounds, and a record button allows users to record
the audio streaming into a WAV sound ﬁle.
4. EV ALUATION
We used Playsound as a technology probe in order to“collect
information about the use and the users of the technology in
a real-world setting, the engineering goal of ﬁeld-testing the
technology, and the design goal of inspiring users and de-
signers to think of new kinds of technology to support their
needs and desires” [17]. We assessed our system in two dif-
ferent music making contexts with a total of 18 participants
//github.com/arianestolfi/audioquery-server
4The Sound samples that are ready to be played upon con-
nection are the ones returned by searching “undeﬁned” on
Freesound.
Table 1: Performers in music improvisation mixed
ensemble sessions: (M): musician; (N): non-
musician.
Session Performers
1 P1 (M), P2 (M), P3 (N), P4 (M)
2 P1 (M), P4 (M), P5 (M)
3 P1 (M), P3 (N), P4 (M), P5 (M), P6 (M)
having various musical skills: three participants belonged
to an ensemble mixing participants using Playsound and
other musicians; 15 participants used Playsound in trios.
Our evaluation was centered on HCI frameworks related to
usability [6], engagement [7] and creativity support. As in
[31], we used a mixed methods approach combining quanti-
tative and qualitative self-reports as well as behavioral data
measured from log activity.
All the evaluation sessions were conducted in a perfor-
mance room of about 80 m 2 with dedicated acoustic treat-
ments and PA system. We documented the sessions using
audio and video recordings. In both ensembles, participants
were ﬁrst introduced to the concept of free music improvi-
sation based on mutual listening and the freedom to play
spontaneously without pre-conceived arrangement, musical
structure, key or meter.
4.1 Music improvisation mixed ensemble
We established a small free music improvisation ensemble
including Playsound users and other performers to test the
tool in a real use case situation, as it was designed for the
free improvisation practice.
4.1.1 Participants and procedure
To date three rehearsal sessions each lasting one hour were
held involving a total of six musician participants:
P1 (Playsound and vocal techniques), P2 (SuperCollider
and Playsound), P3 (Playsound), P4 (guitar with eﬀects),
P5 (Playsound), P6 (smartphone and percussion); 3 fe-
males, 3 males (mean age = 33); see arrangement in Ta-
ble 1. One participant did not have prior experience as a
performer. In each session participants were invited to play
three pieces of about 10 minutes and to discuss their expe-
rience after each piece. Audio recordings of seven 10 min
improvisation pieces are available at the link below 5.
4.1.2 Results
With this process, we tested how the tool could be used as
an “instrument” to improvise, how expressive it was, and
how other musicians responded to the music produced with
it. Discussions held with the musicians after the sessions
revealed that both the Playsound users and co-performers
were satisﬁed with the musical improvisations. The out-
comes were well received given that none of the players had
previously played together and that the form was left free.
The tool was enjoyed for the richness of the sounds it pro-
vided (“I like the fact that every idea of sound I have is in
my hands”). Feedback from participants also helped to im-
prove functional aspects such as volume control. The fact
that a non musician who used Playsound was able to play
several live improvisations with trained musicians can be
seen as a positive sign of inclusive design, and we tested
this further by gathering musicians and non musicians in
trio ensembles.
4.2 Playsound trio ensembles
5http://finetanks.com/records/puppets/
230
Figure 2: Screenshot of the Playsound web interface in Google’s Chrome browser.
The second use case consisted in ﬁve trios playing music
improvisations using Playsound as sole instrument. We
wanted to investigate if users new to the system could use it
to play collaboratively, how they engaged with it and how
it supported their creativity.
4.2.1 Participants
15 participants were recruited (5 females, 10 males, age =
32.7±5.4 years). 8 of them considered themselves as musi-
cians (4 intermediate and 4 experienced), while 7 did not.
Figure 3 shows three of the groups in a playing situation.
Figure 3: Trios playing for Playsound user tests.
4.2.2 Procedure
Participants were instructed to use Playsound on their own
laptops. They were ﬁrst invited to explore the interface
during 5 minutes for familiarization, phase during which
they could ask questions to the experimenters. After this,
they were asked to play three free musical improvisations
each lasting about 5 minutes. They were encouraged to
listen to each other to establish a musical dialogue and
to develop sound ideas using the web platform, searching
keywords that were representative of the sound ideas they
had. Participants were free to propose any sound idea they
wanted with the audio content available through the plat-
form. After each session, participants were invited to dis-
cuss together their experience in using the interface and
improvising music with others. After the music sessions,
they had to complete an online survey. All sessions were
ﬁlmed and recorded.
4.2.3 Survey and analysis methods
The survey included questions related to demographics (age,
gender, musical experience), usability (SUS usability scale
[6]) and overall feedback on engagement and creative learn-
ing. The SUS questionnaire investigates dimensions related
to interest, complexity, ease of use, simplicity, integration,
consistency, diﬃculty through 5-point Likert items. We also
included 10-point Likert items to assess levels of engage-
ment, learning, novelty, relevance and quality of retrieval,
spectrogram familiarity and usefulness. Answers to Likert
items were subjected to statistical analyses using the Mann-
Whitney-Wilcoxon (MWW) test to compare non musicians
and musicians. Browser console logs were analysed to char-
acterise the creative musical interactions from participants.
We also analysed group discussions held after each piece
using an inductive thematic analysis [5].
4.3 Playsound trio results
4.3.1 Usability and Engagement
No signiﬁcant diﬀerences were found between non musicians
and musicians for all the questionnaire items (MWW test).
Figure 4 illustrates the results of the SUS questions. Par-
ticipants strongly agreed that the interface was easy to use
and to learn, was not complex, and reported being conﬁdent
in using the system. Participants were more neutral about
whether they would use the system frequently, perhaps due
to the novel exposure to the free musical improvisation style.
Figure 5 indicates that on average participants felt highly
engaged while playing with others. Some participants found
the system to provide an innovative way to compose music
and also found the spectrograms to be useful to ﬁnd sounds.
Participants felt more neutral about whether they learned
something about sound and music making while using the
system, which can be expected given the short exposure
time (about 15 mins).
4.3.2 Log data
231
use system frequently
system unnecessarily complex
easy to use
need technical supportfunctions well integratedtoo much inconsistency
learn to use quickl
y
inconvenient to use
confidence using system
need to learn a lot before use
Strongly disagree
 
Neutral
 Strongly agree
Figure 4: Mean and standard error of the results of
the SUS questionnaire items.
Level of engagement
New learning
Novel way of composing
Sound retrieval
Spectrogram familiaritySpectrogram usefulness
Not at all
Neutral
Very Much
Figure 5: Mean and standard error of the results of
the questionnaire items.
Out of 45 trio pieces, we were able to collect 27 logs from
10 participants due to technical issues. Three-way analy-
ses of variance (type II) were conducted to test the main
and interaction eﬀects of musical experience (non musician,
musician), participant, and piece on the number of queries
and number of sounds played during a piece. No main nor
interaction eﬀects were found which indicates that the num-
ber of queries and sound played were unaﬀected by expe-
rience, participants and pieces. There were on average 8
queries (SD=2) and 24 sounds played (MIN=9, MAX=101,
SD=18) per piece showing creative engagement with the
system by all participants. The higher variance on the
number of sounds played may be related to diﬀerent playing
strategies tested at various times. Sounds were either played
once or multiple times and the number of repetitions varied
also a lot from 1 to 66 (mean = 2.3, SD = 3.8) showing
cases were the content is judged relevant, being repeated.
4.3.3 Thematic analyses
We conducted an inductive thematic analysis by generat-
ing codes from the group discussion transcripts. The codes
were further organised into themes that reﬂected patterns,
as described below.
Expressiveness. Three participants expressed strong
satisfaction to be able to retrieve any type of sounds (e.g.,“I
like the fact of being able to get immediately whatever type
of sound comes to my mind and use it for composing in real
time!”).
Monitoring. Recurring comments by nine participants
reﬂected that selected samples could not be auditioned prior
to being played and that samples had to be initially played
at maximum volume. However, participants reported to
have found a workaround for these issues by using gradual
fade ins.
Relevance and surprise . Five participants reported
that the retrieved sounds did not fully correspond to the
keyword they had typed in and highlighted the importance
to have better tags in Freesound (e.g., “Some sounds were
diﬀerent from what I expected. I had to try diﬀerent sounds
before ﬁnding the sound that I wanted”). Interestingly, three
participants valued the surprise element that could be the
source of new ideas (e.g., “It is a very interesting method to
compose because there is a surprise factor” , “The surprise
of having sounds that I did not expect gave me new ideas,
and I used them”).
Expressive control. Six musician participants felt the
need of having more expressive controls as the interface al-
lowed only volume modulations of the triggered sounds.
They suggested to add controls for a master volume and
the possibilities of synchronizing the beat of samples trig-
gered by diﬀerent users, to associate computer keyboard
keys to samples, to be able to rate the sounds that they
liked the most to identify them faster, and to decide which
portions of the samples to loop. Two participants reported
that the impossibility of being synchronized with the beat
of other participants led them to adopt other compositional
choices (e.g., “I avoided sounds with rhythm and selected
non-musical sounds”).
Identiﬁcation. Five participants reported diﬃculties in
recognising which sounds they played and which were played
by the others. This led one participant to suggest to “build
a collaborative interface which also displays what the other
musicians searched and are playing.”.
Creativity support and narrative. Four participants
reported that they tried to create a narrative in relation
to what other musicians were playing (e.g., “I searched the
keywords to adapt to the context. An idea from the other
musicians triggered another idea from me, so we can create
a narrative all together” , “I tried to respond to what the
others did, for instance I heard him playing the birds so I
tried to ﬁnd sounds of the cats.” ).
Spectrogram usefulness . Five participants, who had
a music technology background, reported to have found the
spectrograms useful (e.g.,“The spectrogram really helped me
to read the sounds and I based my decisions on that.” ). One
participant commented “It is a diﬀerent way of playing: I
am using my eyes to play music.” . However, ﬁve other par-
ticipants, who were not able to decode spectrograms, re-
ported to have relied on the displayed name and duration.
4.4 Critical analysis
Results from the ensemble and trio performances indicate
that it was easy for ﬁrst time users to play live with oth-
ers using the tool. The semantic sound search functionality
facilitated interaction between musicians and led to inter-
esting musical situations through the use of similar or con-
trasting materials at diﬀerent moments, and rich variation
of timbres and rhythms. It also allowed users to express
sound ideas and emotions even without technical expertise
and musical technique. As stated by Magnusson [23], “the
design of a musical instrument or a composition is a design
decision conditioned by the properties found in the source
material”. In the case of Playsound, constraints are linked
to the type of audio material available in the Freesound
database and its crowd-sourced descriptive metadata which
are typically noisy. Other constraints result from the design
choices, such as adoption of a minimalist approach, and lim-
itations of the technologies adopted. Users could respond
creatively to this constraint. For example, the impossibility
to synchronize loops of diﬀerent durations and with others,
232
and the uncertainty of how samples will sound, generated
polyrhythmic and layered timbre patterns that are desirable
in free improvisation contexts and in experimental music
practices.
5. CONCLUSIONS AND FUTURE WORK
In this paper we presented the design and evaluation of
Playsound, a web interface for sample-based music making.
The system proved successful in supporting the initial de-
sign goal from the ﬁrst author to be able to re-purpose Cre-
ative Commons samples in free music improvisation prac-
tice. Results also showed that the query mechanism and
user interface make the tool inclusive and accessible even to
non musicians.
Throughout the evaluation, we observed diﬀerent expec-
tations from users, some who liked the simplicity and lim-
ited controls, others who desired more expressive controls.
We have since then improved the player to include some of
the features mentioned by them. These include the possibil-
ities to control the sample playback rate, to select the sam-
ple starting point from the spectrogram, to select sounds
without triggering playback, and to access the original con-
tent on Freesound. We will continue to include new features
in the next releases aiming at improving the loop control
(start and end points) and at providing ﬁlters to enable
more complex sound transformations. Pursuing the Cre-
ative Commons philosophy, we wish to contribute uploading
to Freesound new content, which may in turn be accessed
through the tool. We also envision to create a collaborative
platform that will let users share the the same environment
for musical practice and participatory performances.
6. ACKNOWLEDGMENTS
We acknowledge support from University of S˜ ao Paulo’s Nu-
Som Research group and the CAPES PDSE grant awarded
to Ariane Stolﬁ. This work is also supported by the EU
H2020 grants Audio Commons (No. 688382) and Internet
of Musical Things (No. 749561). We would like to thank
Adan Benito, Thomas Vassalo and Alessia Milo for their
help in the development and we also thank all the partici-
pants in the evaluations.
7. REFERENCES
[1] V. Akkermans, F. Font, J. Funollet, B. de Jong,
G. Roma, S. Togias, and X. Serra. Freesound 2: An
improved platform for sharing audio clips. Proc.
ISMIR, 2011.
[2] J. Albert. Improvisation as Tool and Intention.
Critical Studies in Improvisation , 8(1), may 2012.
[3] M. Barthet, P. Depalle, R. Kronland-Martinet, and
S. Ystad. Analysis-by-synthesis of timbre, timing, and
dynamics in expressive clarinet performance. Music
Perception, 28(3):265–278, 2011.
[4] C. Bergstroem-Nielsen. Keywords in Musical Free
Improvisation. Music and Arts in Action , 5(1), 2016.
[5] V. Braun and V. Clarke. Using thematic analysis in
psychology. Qualitative Research in Psychology,
3(2):77–101, 2006.
[6] J. Brooke. Sus: A quick and dirty usability scale,
1996.
[7] N. Bryan-Kinns, P. G. T. Healey, and J. Leach.
Exploring mutual engagement in creative
collaborations. In Proc. C&C ’07, pages 223–232, New
York, NY, USA, 2007. ACM.
[8] J. Cage. Silence : lectures and writings . Calder and
Boyars, 1968.
[9] C. Canonne. Du concept d’improvisation ` a la pratique
de l’improvisation libre, 2016.
[10] J. Clarkson. Inclusive design: design for the whole
population. Springer, 2003.
[11] C. Dobrian and D. Koppelman. The ’E’ in NIME:
musical expression with new computer interfaces.
Proc. NIME, pages 277–282, 2006.
[12] E. Feenstra. BeatPush. Proc. WAC, 2016.
[13] F. Font and G. Bandiera. Freesound Explorer: Make
Music While Discovering Freesound! Proc. WAC,
2016.
[14] F. Font and X. Serra. The Audio Commons Initiative.
Proc. ISMIR, pages 3–4, 2015.
[15] J. Freeman and A. Van Troyer. Collaborative textual
improvisation in a laptop ensemble. Computer Music
Journal, 35(2):8–21, 2011.
[16] J. Gothelf. Lean UX: Applying lean principles to
improve user experience. O’Reilly Media, Inc., 2013.
[17] H. Hutchinson, W. Mackay, B. Westerlund, B. B.
Bederson, A. Druin, C. Plaisant,
M. Beaudouin-Lafon, S. Conversy, H. Evans,
H. Hansen, N. Roussel, B. Eiderb ¨ack, S. Lindquist,
and Y. Sundblad. Technology probes: Inspiring design
for and with families. In Proc. CHI ’03, New York,
NY, USA, 2003. ACM.
[18] F. Iazzetta. A M´ usica, o Corpo e as M´ aquinas.
Revista Opus, 4:1–20, 1997.
[19] N. Jillings and R. Stables. An Intelligent audio
workstation in the browser. In WAC 2017, aug 2017.
[20] S. W. Lee, J. Bang, and G. Essl. Live Coding
YouTube: Organizing Streaming Media for an
Audiovisual Performance. Proc. NIME, pages
261–266, 2017.
[21] S. W. Lee, A. D. J. de Carvalho, and G. Essl. Crowd
in C[loud]. In Proc. WAC, 2016.
[22] P. Levinson. Digital McLuhan : a guide to the
information millennium . Routledge, 2001.
[23] T. Magnusson. Designing Constraints: Composing
and Performing with Digital Musical Systems.
Computer Music Journal , 34(4):62–73, dec 2010.
[24] J. McDermott, T. Giﬀord, A. Bouwer, and M. Wagy.
Should Music Interaction Be Easy? 2013.
[25] E. X. Merz. Composing with All Sound Using the
FreeSound and Wordnik APIs. Musical Metacreation:
AIIDE Workshop, pages 83–90, 2013.
[26] H. Ordiales and M. L. Bruno. Sound recycling from
public databases. In Proc. Audio Mostly, 2017.
[27] J. Pigrem and M. Barthet. Datascaping: Data
Soniﬁcation as a Narrative Device in Soundscape
Composition. In Proc. Audio Mostly, 2017.
[28] G. Roma and P. Herrera. Representing Music as
Work in Progress. In Structuring Music through
Markup Language, pages 119–134. IGI Global,
Hershey, PA, 2013.
[29] A. Stolﬁ, M. Barthet, F. Gorodscy, and A. D.
Carvalho Jr. Open band: A platform for collective
sound dialogues. In Proc. Audio Mostly, 2017.
[30] S. Wilson, N. Lorway, R. Coull, K. Vasilakos, and
T. Moyers. Free as in BEER: Some Explorations into
Structured Improvisation Using Networked
Live-Coding Systems. Computer Music Journal ,
38(1):54–64, mar 2014.
[31] Y. Wu, L. Zhang, N. Bryan-Kinns, and M. Barthet.
Open symphony: Creative participation for audiences
of live music performances. IEEE MultiMedia,
24(1):48–62, 2017.
233