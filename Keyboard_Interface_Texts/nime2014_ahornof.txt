The Prospects For Eye-Controlled Musical Performance 
 
Anthony J. Hornof  
Department of Computer and Information Science 
University of Oregon 
  Eugene, OR 97403-1202 USA   
hornof@cs.uoregon.edu 
 
 
ABSTRACT 
Although new sensor devices and data streams are increasingly used 
for musical expression, and although eye -tracking devices have 
become increasingly cost-effective and prevalent in research and as a 
means of communication for people with severe motor impairments, 
eye-controlled musical expression nonetheless remains somewhat 
elusive and minimally explored. This paper (a) identifies a number of 
fundamental human eye movement  capabilities and constraints 
which determine in part what can and cannot be musically expressed 
with eye movements, (b) reviews prior work on eye -controlled 
musical expression, and (c) analyzes and provides a taxonomy of 
what has been done, and what will need to be addressed in future 
eye-controlled musical instruments. The fundamental human 
constraints and processes that govern eye movements create a 
challenge for eye-controlled music in that the instrument needs to be 
designed to motivate or at least permit specific unique visual goals, 
each of which when accomplished must then be mapped, using the 
eye tracker and some sort of sound generator, to different musical 
outcomes. The control of the musical instrument is less direct than if 
it were played with muscles that can be controlled in a more direct 
manner, such as the muscles in the hands.  
Keywords 
Eye-controlled interfaces, eye tracking, human performance, music, 
musical expression, sensor interfaces. 
1. INTRODUCTION 
Despite a somewhat dramatic increase over the last few years in the 
use of new sensor technologies for musical expression (such as with 
the Kinect motion detector and the Wii Remote inertial measurement 
unit), as evidenced by the nature of papers presented in recent years 
at this conference, and even though eye tracking devices are getting 
cheaper and better, there remains relatively little work done to 
perform music with eye movements. There in fact appear to be only 
six successful projects to date. This paper attempts to understand why 
there has been such relat ively little work in this area.  The paper 
discusses the human processes that are involved in moving the eyes, 
summarizes the six prior eye-music projects, and characterizes the 
fundamental constraints of eye-controlled musical expression. 
1.1 Why the Eyes Move 
The eyes are used primarily to take in visual information— to see the 
world— and hence the muscular control that is required to rotate the 
eyeballs in their sockets is primarily used to bring visual items that 
are in the periphery into the high-resolution vision at the center of 
vision, the “point of gaze.” A person’s introspective experience of his 
or her eye movements is not that the eyeballs are rotating in their 
sockets but rather that new high- resolution visual information 
becomes available to visual perception. 
 Conscious deliberate control of the point-of-gaze is perhaps only 
possible to the extent that a person can consciously and deliberately 
pursue different specific visual-perceptual goals such as to bring a 
white dot into high resolution vision or, in everyday terms, to simply 
“look at” the dot. It is difficult, without practice, to not look at a dot 
that consistently appears on a computer screen just to one side of the 
point of gaze; there is a tendency to try to look at the dot, which 
causes it to move, and it gets chased off the screen [3]. Eye 
movements can only be controlled indirectly, by pursuing visual 
goals. To build an eye -controlled musical system, the instrument 
designer must create a set of visual-perceptual goals each of which 
can be accomplished by the performer and uniquely detected and 
mapped to a different musical outcome. The specific kinds of eye 
movements that people can make to achieve visual perceptual goals 
are discussed next. 
1.2 How the Eyes Move 
There are a limited number of distinct classes of eye movements that 
people can make, and thus that could be mapped to direct musical 
outcomes. These classes are summarized here, drawing from 
Rosenbaum [8]. Each class of movements has evolved in humans to 
support a different supporting role in visual perception. 
 
Single point-of-gaze.  Though we have two eyes, they typically point 
together at the same location, producing a single point of gaze. This 
assists visual perception by providing additional and redundant 
sensory information as well as distance information. 
 
Saccades and fixations.  The most typical eye movements are quick 
saccades that last on the order of 30 ms and which jump the point-of-
gaze from one location to another. The gaze then typically stays at 
that new location for a fixation that lasts roughly 100 to 400 ms. The 
fixations permit high-resolution visual information to be collected for 
an extended period and for new gaze destinations to be considered. 
 
Smooth pursuits.  In a smooth pursuit the eyeballs rotate at a steady 
rate. This type of movement is typically produced only when there is 
a steadily moving object to lock onto. This permits vision to gather 
visual information from objects that are moving, or that are stationary 
when the head is turning. 
 
Blinks.  Blinks occur to moisten the surface of the eye and to protect 
the eye from approaching objects. Blinks have been used in some 
eye-controlled interfaces but are not typically used in such interfaces 
intended for people with severe motor impairments because many 
such users cannot control their blinks. 
 
Jitter.  During a fixation, the eyes do not hold perfectly still but 
instead make very small random movements on the order of 0.1° of 
visual angle. The jitter prevents receptor cells from becoming over-
saturated and the image from fading. 
 
From among these classes of eye movements, fixations are the 
primary behavioral phenomenon that are used to issue direct 
commands to a computer because a fixation on or near an object is a 
visual-perceptual goal that can be easily motivated on a visual display 
and used to motivate an eye movement that can be detected by an eye 
tracker in a relatively straightforward manner. However, all of the 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 
NIME‘14, New Interfaces for Musical Expression, June 30 – July 03, 2014, 
Goldsmiths, University of London, UK. Copyright remains with the author(s). 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
461
(x, y) data that are reported by the eye tracker can potentially be 
mapped to musical outcomes, including the path of the eyes during 
saccades, and even the jitter. Even though there is little evidence that 
a performer has any direct or indirect control over these aspects of the 
data stream, all of the data can of course be used in a composition. 
1.3 Eye Tracking Technology 
Most eye trackers work by using specialized computer vision 
algorithms to convert a video image of the eye into the (x, y) 
coordinates of where a person is looking on a computer screen. The 
(x, y) location is typically reported between 30 and 1,000 times per 
second (30 to 1,000 Hz). In order to report the coordinates accurately, 
the algorithms are necessarily complex and , for commercial eye 
tracking companies, closely-guarded trade secrets. 
 One challenge when working with eye tracking technology is to 
get accurate data [12]. Reasonably accurate commercial eye trackers, 
such as by SensoMotoric Instruments (smivision.com) or L.C. 
Technologies (eyegaze.com), can report where a person is looking 
with reasonable accuracy, within roughly 1° of visual angle (which is 
roughly half the width of your thumb at arm’s length). But these 
commercial systems also tend to be somewhat expensive, on the 
order of US$10,000. This presents a problem for computer musicians 
who would like to explore eye -controlled music but do not have 
access to accurate commercial devices. Cheaper commercial systems 
that cost on the order of US$200 are currently being introduced to the 
market but it remains to be seen how accurate and reliable these 
devices will be. Some open source eye trackers are available, such as 
the ITU Gaze Tracker (gazegroup.org), EyeWriter (eyewriter.org), 
and openEyes (thirtysixthspan.com/openEyes/), with the ITU Gaze 
Tracker perhaps the most advanced of the three. But these systems do 
not yet appear to have the accuracy of a good commercial system. 
The accuracy and reliability of an eye tracking system will directly 
affect the extent to which it could be used to control a computer, such 
as for musical performance. 
 The idiomatic interaction technique used in eye -controlled 
interfaces is to present a user with oversized buttons that the user can 
decisively select by looking at buttons for short periods of time. The 
buttons need to be oversized to compensate for the jitter in the eye 
movements and the error in the eye tracker. This interaction 
technique is relatively easy to implement because all that needs to be 
done to determine which buttons are pressed is to count the number 
of consecutive gaze samples that occur within each button region. 
Smooth pursuits are not typically used in eye-controlled interfaces 
because it is not as straightforward to move objects around a display 
such that the eyes could lock onto those distinct objects to issue 
distinct commands, but this would be an interesting area to explore in 
part because it would permit a range of aesthetically interesting visual 
components in an eye-controlled interface. 
1.4 An Upper Limit on Eye-Controlled 
       Performance 
Given the idiomatic interaction technique of using fixations to control 
an interface with the eyes, we can establish an upper limit on the 
number of discrete eye commands that a person can issue in a given 
amount of time. In terms of how many fixations can be made per 
second, there is strong evidence that, even with practice and in the 
most optimal circumstances, such as simply moving the eyes back 
and forth between two buttons, people can make at most four 
specific, deliberate eye-controlled commands per second. Figure 1 
shows the results of an experiment conducted in our lab in which 
participants used their eyes (and three different eye-controlled trigger 
methods) to tap along to a beat (alternating between eight beats of 
taps and eight beats of rest), and we found that 50% accuracy is only 
achievable for inter-beat intervals of 250 ms and higher, and that the 
best performance requires inter-beat intervals of 300 ms and higher. 
When the eyes are not simply bouncing back and forth between two 
points but instead moving to new intended locations with each 
saccade, such as when eye -typing, it appears as if eye -controlled 
buttons can be selected no faster than one button every 600 ms [6]. 
Working within the human and technology constraints discussed 
here, researchers and musicians have successfully developed a 
number of eye-controlled systems for musical expression, which will 
be discussed next. 
2. A REVIEW OF EYE-CONTROLLED 
     MUSICAL PERFORMANCE 
To date, there appear to be just six published or publicized projects 
that have successfully developed eye-controlled musical instruments, 
compositions, or performances. This review captures the author’s 
understanding of all such work based on reviewing the literature, 
searching the web, and communicating with researchers and 
practitioners. The projects are presented in an order that shows the 
natural progression of the development of ey e-controlled musical 
expression. 
2.1 Intuitive Ocusonics – Polli (1999) 
The earliest work on eye-controlled music is the Intuitive Ocusonics 
system, and compositions created using this system [7]. Figure 2 
shows some of the software components, and Figure 3 shows a 
performance using the system at SIGGRAPH in 2001. Intuitive 
Ocusonics used Steim’s BigEye software as a low resolution video 
analyzer to monitor the position of the dark pupil and iris in 
comparison to the white of the eye, giving the composer roughly nine 
unique pupil-and-iris positions to work with, each of which was used 
to trigger different synthesized or sampled sounds. It appears that the 
system was somewhat prone to error, and that the performer did not 
have tightly-coupled control of the instrument and thus the sounds 
that were played, but also that the compositions were designed to use 
this error as a creative element. This is the only system discussed here 
that did not work in some way with the x and y positions of the gaze 
on the display, but instead directly interpreted the video stream of the 
eye itself. Though this would understandably be more error-prone 
and thus introduce more artifacts into the performance, Intuitive 
Ocusonics is arguably the first attempt to create a DIY (do -it-
yourself) eye tracker and evidently the first attempt to use the eyes to 
control a musical performance. 
 
Figure 1. The percentage of successful tapping trials as a 
function of the inter-onset interval between the beats, a 
measure of an ability to tap a rhythm using eye movements.  
 
IOI
Percentage of Successful Trials
150 175 200 250 300 400
0 10 20 30 40 50 60 70 80 90 100
FIXATION
EXIT_BOX
PEAK_VELOCITY
FINGER
Second Run Data
Percentage of Successful Trials 
Interonset (Beat) Interval (ms)
100%
150
0%
50%
250200 300 400
Finger tapping
Eye tapping
Eye trigger method
Proceedings of the International Conference on New Interfaces for Musical Expression
462
 
Figure 2. Andrea Polli’s Intuitive Ocusonics system used 
Steim’s BigEye to extract data from video images of the 
eyes and Max/MSP to convert these data into sound.  
(Image from www.andreapolli.com. Used with permission.)  
 
 
Figure 3. A performance using the Intuitive 
Ocusonics system in 2 001. (Image from 
www.andreapolli.com. Used with permission.)  
 
2.2 Oculog – Kim et al. (2007) 
Oculog [5] is another DIY eye-image tracker that, like Ocusonics, 
works directly with the video image of the eye, though Oculog uses 
computer vision algorithms to compute the relative horizontal and 
vertical position of the pupil in the camera frame (with no calibration 
to the computer’s display). Because the change in the position of the 
pupil is relative to the camera, and the camera is fixed to goggles 
worn by the performer, the performer must move his or her eyes 
based on visual-perceptual goals that can be generated from the 
performer’s immediate visual environment. This arrangement does 
mean that smooth pursuit movements can be evoked quite easily by 
simply fixating an object and then moving the head ( though this 
effectively relegates the eye tracker to a reverse head tracker). In a 
composition entitled Saccadic Variations that was created using  
Miller Puckette’s Pure Data software, the pupil coordinates are 
mapped to a tone generator, with the note number mapped to the x 
coordinate and the key velocity to the y coordinate. A progression 
through segments of the composition is controlled based on which of 
four quadrants (top left, top right, bottom left, or bottom right) the 
pupil is reported in after a blink is detected. 
2.3 EyeMusic v1.0 – Hornof et al. (2006) 
The EyeMusic system [2] connected a commercial eye tracker to 
computer music software, specifically a 60 Hz L.C. Technologies 
Eyegaze system to Max/MSP. Custom Max objects were created 
using C/C++ to make the eye tracking data available to Max/MSP in 
real time. After a number of informal studies, the multimedia 
composition EyeMusic v1.0 was produced [4]. Figure 4 shows the 
stage setup for EyeMusic v1.0 and Figure 5 highlights the various 
display components used in a performance. The multimedia 
composition is designed so that the performer could pursue visual-
perceptual goals that are translated into a visual experience that is 
controlled by the performer and shared with the audience. The 
performer controls a small white dot, a gaze cursor, that corresponds 
to where he or she is looking. The cursor is repositioned on the 
display based on every (x, y) coordinate that is reported by the eye 
tracker (60 times per s) and this data is also used to create a granular 
synthesis of click-sounding samples that are spatialized to play near 
the gazepoint on the display by using stereo panning. A secondary 
display shows a video of the eye, permitting the audience to see and 
hear that the eyes are moving the gaze cursor. Visual events, such as 
the gaze cursor colliding with moving red bal ls, are mapped to 
musical elements such that the audience can see how the eyes are 
being used to play the instrument. The performer moves through 
segments of the composition with eye blinks, each of which is 
sonified as a sudden, loud, slamming sound. EyeMusic v1.0 was 
performed at SEAMUS 2006 and NIME 2007. 
 
Figure 4. The staging for EyeMusic v1.0. The projection 
(inset) is the same image that the performer viewed  
to play the piece (though flipped horizontally).  
 
Figure 5. The display components used in EyeMusic v1.0. 
The audience and the performer each see a large view of the 
control surface, and a small video feed of the eye image.  
 
Proceedings of the International Conference on New Interfaces for Musical Expression
463
2.4 The EyeHarp – Vamvakousis (2011) 
EyeHarp [10] was developed using the EyeWriter open source eye 
tracker (www.eyewriter.org) and then transitioned to use the ITU 
Gaze Tracker (gazegroup.org) open source eye tracker [11], each of 
which appears to report with reasonable accuracy the ( x, y) 
coordinate on the display where a person is looking. EyeHarp 
presents the user with a complex interface with extensive 
functionality, though numerous human factors considerations serve 
to tame the complexity. For example, Figure 6 shows how rather than 
laying out the buttons in the configuration of a piano keyboard, 
which was demonstrated to be somewhat problematic for the eyes, 
the buttons are arranged in a pie menu [1] with the center of the pie 
the standby gaze location, and hence the eye-image status display is 
at that location (though much smaller than the status display in the 
center of the display for Duet for Eyes ). Small white dots are 
positioned in fixed locations across the visual control surface to help 
create the visual-perceptual goals that are needed to move the eyes to 
these locations, similar to [3]. Though EyeHarp was not designed in 
collaboration with or specifically for people with disabilities, the 
author did conduct an informal evaluation of EyeHarp with people 
with cerebral palsy, as shown in Figure 7. 
2.5 Duet for Eyes – Donegan et al. (2007) 
Perhaps the most exciting application of eye tracking for real-time 
input to a computer is when eye tracking can be used to  provide 
people with severe motor impairments a means of operating a 
computer and communicating with other people. In this role the 
technology is referred to as assistive technology or augmentative 
communication. Duet for Eyes  is one of several musical 
performances made possible by a collaboration of musicians, 
performers with severe motor impairments, augmentative 
communication experts, and others. The team use d off -the-shelf 
augmentative communication equipment and software (Tobii eye 
trackers running The Grid by Sensory Software) to configure eye-
controlled buttons so that the performers could use these buttons to 
play sound files that the performers selected in advance from large 
collections of sound files. Figure 8 shows a screenshot of the gaze-
controlled musical palette that was used by the performers. The 
arrows on the eye-controlled buttons serve as memory aides and as 
visual targets for eye movements made from one side of the screen to 
the other. Figure 9 shows Duet for Eyes being performed in 2008 at 
Trinity College in Dublin. Performers included James Brosnan, Katie 
Gillian, Colm O’Snodaigh, Eoin O’Brien, Robbie Perry, and others. 
Mick Donegan and Lizbeth Goodman directed the project, with 
Donegan providing eye tracking expertise [9]. 
 
It is noteworthy that Duet for Eyes was developed in collaboration 
with its intended end-users. In the screenshot shown in Figure 8, the 
black rectangle with the two white dots shows the user where the two 
eyes are currently positioned in the camera frame, and whether each 
eye is currently being successfully tracked (the white dot for each eye 
disappears if that eye is not being tracked). Providing tracking status 
in a prominent part of the display is an example of how collaborating 
directly with people with disabilities in the design of assistive 
technology will likely lead to a more user-friendly design for that 
population. This clear salient visual feedback permits self-monitoring 
and assists with the of making compensatory voluntary head 
movements to keep the eyes in range of the cameras, as is needed for 
users with motor impairments who have involuntary head 
movements that periodically move their eyes out of range. 
Figure 7. EyeHarp tested by users with cerebral palsy. 
(Images from [10]. Used with permission.)  
Figure 6. One of the two control surfaces in the  
EyeHarp interface [10]. The ey es are currently 
playing a note on the bottom right of the wheel.  
 
Figure 8. The gaze-controlled musical palette constructed 
for Duet with Eyes by Donegan and collaborators using The 
Grid from Sensory Software running on a Tobii eye 
tracker. (Screenshot courtesy of Mick Donegan.)  
Figure 9. Duet for Eyes being performed in 2008. (Image 
from illustriouscompany.co.uk. Used with permission.)  
Proceedings of the International Conference on New Interfaces for Musical Expression
464
2.6 Ableton-Live-adapted –  
Margulies and Anderson (2012) 
Eye-controlled deejaying was successfully implemented by adapting 
the music production software Ableton Live so that it could be used 
via the eye-controlled mouse emulation that is built into the Tobii 
CEye assistive technology eye tracker (discussed at 
1blinkequalsyes.com). This integration was developed 
collaboratively by Jon Margulies and David Anderson after David 
became disabled by amyotrophic lateral sclerosis (ALS). Figure 10 
shows a screenshot with the eye-controlled mouse emulation palette 
overlaid on top of Ableton Live. Figure 11 shows David using the 
system to deejay a New Year’s Eve party at the end of 2011. Ableton 
Live was adapted by identifying all of the musical tasks that a deejay 
needs to do, such as launching a song or fading a song across eight 
bars, and by creating a separate button for each task in Ableton 
Live’s Session View, which can be populated with a grid of buttons, 
with each button triggering a separate musical task. All actions were 
quantized to start on the next bar. The key technical insight that made 
this adaptation possible was that, although some of the tasks such as 
launching a song were already implemented within Ableton Live, 
others such as fading across eight bars could be accomplished by 
launching MIDI events (that appeared to Ableton Live to be external 
MIDI events) and which then sent the appropriate commands to 
Ableton Live. This permitted all of the interactions such as adjusting 
on-screen faders with a click-drag-click (which is difficult to do with 
eye movements) to be converted into button presses which could be 
issued with eye movements. (Jon Margulies, personal 
communication.)  
 
 
Figure 10. The Tobii eye-controlled mouse emulation 
buttons overlaid on top of Ableton Live.  
(Image from http://1blinkequalsyes.com/)  
 
Figure 11. A deejay diagnosed with ALS uses an eye tracker 
connected to Ableton Li ve to mix the music at a party.  
(© 2013 Nityia Photography. Used with permission.)  
3. ANALYSIS 
This paper will now  provide a taxonomy of the work that has 
been done to date to create eye -controlled musical instruments 
and musical expression , and  briefly ponder likely future 
developments in this performance and design space.  
3.1 A Taxonomy of Eye-Controlled 
Musical Performance 
Eye-controlled musical instruments and compositions  can be 
classified by asking a number of questions about each project. 
 
Dimension 1: What is the motivation for the project?  
The projects discussed here had three motivations: 
(a) To create avant garde music and multimedia, as in Intuitive 
Ocusonics, EyeMusic v1.0, and EyeHarp. 
(b) To explore the technical and practical possibilities with eye -
controlled music, as in Oculog and EyeHarp.  
(c) To create and deliver a tested working interface to people 
with disabilities, as in Duet for Eyes  and Ableton -Live-
adapted, the only two systems that were developed in 
partnership with such target users. 
 
Dimension 2: What are the visual objects o n the control 
surface that are used to motivate the visual-perceptual goals 
that a performer will use to play the instrument ?  
The “control surface ” is the screen that the performer  looks at 
to control the instrument. Recall that the eyes cannot be 
controlled as directly as the hands, but are instead moved by 
pursuing visual-perceptual goals, such as to look at specific objects. 
These projects have three types of control surfaces: 
(a) No control surface, as in  Intuitive Ocusonics and Oculog. 
(b) A screen of large stationary buttons that primarily serve a 
functional role, as in EyeHarp, Duet for Eyes , and Ableton -
Live-adapted. Each button has a visual object  at its center, 
which creates a visual-perceptual goal . The targets are 
words, symbols, pictures, or small white dots.  
(c) Large visual objects  that move on the screen in the context 
of a time -based multimedia composition, as in EyeMusic 
v1.0, in which t he visual -perceptual goals are to move the 
gaze cursor to explore a black screen, collide with red 
circles, and create an audio -visual narrative. 
 
Dimension 3: Is a gaze cursor displayed on the surface? 
The gaze cursor is a small visual object that appears where the 
eye tracker reports that a user is looking. It helps to show the 
performer and possibly the audience  how the eye tracking data 
are driving the performance . Some implications for showing  or 
now showing  a gaze cursor are discussed in Dimension 6(e). 
Do these compositions use them?  
(a) Yes. EyeMusic v1.0 and EyeHarp show a gaze cursor.  
(b) No. Duet for Eyes and Ableton-Live-adapted do not appear to 
show a gaze cursor. 
 
Dimension 4: Can the control surfaces be shown to  the 
audience to help them understand that the eyes are 
controlling the instrument?  
In all projects, simply projecting a video of the performer’s 
eyes will help to emphasize that the eyes are playing the 
instrument. Intuitive Ocusonics and EyeMusic v1.0  both did 
this. For the compositions that use control surfaces, could it 
also be helpful to project these surfaces for the audience to see?  
The answer is three shades of “yes”: 
(a) Definitely yes. For EyeMusic v1.0 , the control su rface is an 
integral component, and the gaze cursor  shows the 
performer playing the instrument.  
(b) Probably yes. For EyeHarp, skillful playing on its 
minimalist visual layout is compelling and captivating to 
watch, especially when the gaze cursor is visible and 
annotated with explanations of what each eye- command is 
Proceedings of the International Conference on New Interfaces for Musical Expression
465
doing. For Ableton -Live-adapted, it m akes sense to show  
the control surface but only in the vicinity of the deejay 
booth, consistent with how deejay activities can typically be 
observed in a nightclub but only near the deejay booth.  
(c) Possibly yes. As Duet for Eyes  was staged, the p erformer’s 
faces were projected. A  second projection of the control 
surfaces alongside the ir faces could possibly make the 
mappings from eye movements to sounds more clear to the 
audience (especially if the control surfaces  are flipped 
horizontally to maintain left-to-right consistency be tween 
the eyes and the surfaces  from the audience’s perspective).  
 
Dimension 5 : How does each composition deal with the 
timing limitations  of eye -controlled interfaces  and yet still 
create an interesting composition  and performance?  
Recall that the eyes can issue at most two to four decisive eye-
controlled commands per second. 
(a) All of the eye -controlled compositions address  this in part 
by incorporating substantial pre -recorded or pre -
programmed material  into the composition  such that these 
musical elements could continue playing for a period of 
time after each eye-command. 
(b) EyeMusic v1.0  includes audio -visual components that 
continue somewhat inde pendently of the eye movements.  
(c) The EyeHarp incorporates an eye -programmable sequencer 
such that, over the course of the performance,  the performer 
can first program the sequencer to play a background 
harmony, and then play the foreground melody  directly. 
(d) Duet for Eyes  is played in concert with conventional 
musicians who continue to play between eye-commands. 
(e) Two of the compositions , Oculog and EyeMusic v1.0 , use 
the continuous flow of  all of eye data, not just the data from 
the performer-initiated eye commands, to generate sounds. 
 
Dimension 6: How does the system manage the error and 
jitter that are inherent in eye tracking data?  
Eye-controlled interfaces need to accommodate the error in eye 
trackers, and the jitter in fixations . These compositions 
accommodate it in five different ways:  
(a) By moving the composition forward somewhat 
independently of the e ye movements . Intuitive Ocusonics  
does this to the greatest extent, using the eye data to just 
nudge the audio. 
(b) By using the relative rather than the absolute position of the 
eyes. Oculog does this. 
(c) By creating oversized visual targets. All four of the systems 
with control surfaces do this , as does Oculog. 
(d) By sonifying all of the raw data such that the jitter creates a 
musical texture of its own. Oculog and EyeMusic v1.0 both 
do this.  EyeMusic v1.0 uses the error and jitter as both an 
audio and visual element, with the jitter bringing the gaze 
cursor to audible and visible life, and the error creating 
visible-narrative tension as the  audience can see the 
performer working to compensate for error . 
(e) By showing the gaze cursor on the display . EyeMusic v1.0 
and EyeHarp both do this. When a gaze cursor is displayed, 
it is typically a short distance from the actual point of gaze 
due to error in the eye tracking device. This permits a 
performer to  learn how to compensate for  the error  in real 
time, such as by learning to look slightly next to a target . 
That Duet for Eyes and Ableton-Live-adapted do not s how a 
gaze cursor makes the systems easier to user for novices who 
would be distracted by the cursor, but may also prevent expert 
users from learning how to anticipate and work with the error and 
master an eye-controlled instrument. 
None of the systems appear to reduce error by using  smoothing, 
which is to report, rather than the raw  (x, y) data, a running 
average of the last n samples, such as the last 0.25 s of samples . 
To not smooth the data is a sensible design decision given that 
smoothing introduces a control delay , and in general a musician 
wants his or her instrument to respond as quickly as possible.  
3.2 The Future of Eye-Controlled Music 
Eye-controlled musical expression remains relatively unexplored 
because of the numerous constraints imposed by how and why the 
eyes move and because of the limits of eye tracking technology. It is 
hoped that articulating these constraints and challenges, in the context 
of the few successful projects that have been completed, will make it 
easier to plan for the challenges in the design of future eye-controlled 
musical instruments and compositions. As the cost of eye trackers 
continues to drop and as consideration of eye tracker accuracy 
continues to rise [12], it is becoming easier to explore the potential 
for eye-controlled musical expression, and more exciting work will 
certainly be done. We will likely see the development of software 
libraries for connecting eye trackers to multimedia authoring toolkits, 
for creating the visual-perceptual objects needed in eye-controlled 
interfaces, and for automatically detecting when the performer 
pursues these visual-perceptual goals to issue commands in the 
context of eye-controlled musical expression. 
4. ACKNOWLEDGMENTS 
The author thanks Jason Freeman for motivating this paper. 
5. REFERENCES 
1.  Callahan, J., Hopkins, D., Weiser, M., & Shneiderman, B. 
(1988). An empirical comparison of pie vs. linear menus. In CHI 
‘88: Proceedings of the SIGCHI Conference on Human Factors 
in Computing Systems, New York: ACM, 95-100. 
2.  Hornof, A., & Sato, L. (2004). EyeMusic: Making music with 
the eyes. In Proceedings of the 2004 Conference on New 
Interfaces for Musical Expression (NIME  2004), Shizuoka, 
Japan, June 3-5, 185-188. 
3.  Hornof, A., Cavender, A., & Hoselton, R. (2004). EyeDraw: A 
system for drawing pictures with eye movements. In 
Proceedings of ASSETS 2004: The Six th International ACM 
SIGCAPH Conference on Assistive Technologies, Atlanta, 
Georgia: Atlanta, Georgia, October 18-20, 86-93. 
4.  Hornof, A. J., Rogers, T., & Halverson, T. (2007). EyeMusic: 
Performing live music and multimedia compositions with eye 
movements. In Proceedings of the 7th international conference 
on New interfaces for musical expression (NIME 2007), New 
York: ACM, 299-300. 
5.  Kim, J., Schiemer, G., & Narushima, T. (2007). Oculog: Playing 
with eye movements. In Proceedings of the 7th International 
Conference on New Interfaces for Musical Expression (NIME 
2007), New York: ACM, 50-55. 
6.  Majaranta, P., Ahola, U.-K., & Špakov, O. (2009). Fast gaze 
typing with an adjustable dwell time. In Proceedings of ACM 
CHI 2009: Conference on Human Factors in Computing 
Systems, New York: ACM, 357-360. 
7.  Polli, A. (1999). Active vision: Controlling sound with eye 
movements. Leonardo, 32(5), 405-411. 
8.  Rosenbaum, D. A. (1991). Human Motor Control. New York: 
Academic Press. 
9.  SMARTlab Digital Media Institute. (2008). World premiere: 
DUET for EYES: Eyejamming & eyebodyweaving. Retrieved 
from smartlab-ie.com/flyer/dublin_mytobii09.pdf in April, 2014. 
10. Vamvakousis, Z. (2011). The EyeH arp: A gaze -controlled 
musical instrument. Master’s Thesis, Universitat Pompeu Fabra. 
11. Vamvakousis, Z., & Ramirez, R. (2012). Temporal control in the 
EyeHarp gaze-controlled musical interface. Proceedings of the 
International Conference on New Interfaces for Musical 
Expression (NIME 2012), six pages. 
12. Zhang, Y. & Hornof, A. (2014). Easy post-hoc spatial recali-
bration of eye tracking data. In Proceedings of the Eye Tracking 
Research and Applications Symposium (ETRA 2014), 95-98. 
Proceedings of the International Conference on New Interfaces for Musical Expression
466