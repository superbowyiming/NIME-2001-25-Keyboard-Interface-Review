Towards a Virtual Assistant
for Performers and Stage Directors
Alain Bonardi
MSH-Paris Nord
UMS 2553
4, rue de la Croix-Faron
93210 La Plaine Saint-Denis
alain.bonardi@wanadoo.fr
Isis Truck
MSH-Paris Nord
UMS 2553
4, rue de la Croix-Faron
93210 La Plaine Saint-Denis
truck@ai.univ-paris8.fr
Herman Akdag
LIP6 - UPMC
UMR 7606
8 ,r u ed uC a p i t a i n eS c o t t
75015 Paris
Herman.akdag@lip6.fr
ABSTRACT
In this article, we present theﬁrst step of our research work to
design a Virtual Assistant for Performers and Stage Directors,
able to give a feedback from performances. We use a
methodology to automatically construct fuzzy rules in a Fuzzy
Rule-Based System that detects contextual emotions from an
actor's performance during a show.
We collect video data from a lot of performances of the same
show from which it should be possible to visualize all the
emotions and intents or more precisely “intent graphs”. To
perform this, the collected data deﬁning low-level descriptors
are aggregated and converted into high-level characterizations.
Then, depending on the retrieved data and on their distribution
on the axis, we partition the universes into classes. The last step
is the building of the fuzzy rules that are obtained from the
classes and that permit to give conclusions to label the detected
emotions.
Keywords
Virtual Assistant, Intents, Emotion detector, Fuzzy Classes,
Stage Director, Performance.
1.INTRODUCTION
Directing is a complex task notably when productions contain a
part of improvisation. However the performers of these
productions need to be directed since the improvisation always
follows some rules.
Thus, computers may probably be of great help in assisting the
stage director and/or the performer. Indeed, we think it is
important to conceive several tools to help the stage director in
his task of actors’ performance supervision. The tool we
propose here is a kind of assistant that gives a visual
representation – through a graph – of a set of complex data for
the exploration and observation of a show. It also permits to
understand better the creation and execution of the show. One
important point is that, as every computer program, the assistant
must be deterministic and systematic, i.e. it should always give
the same results for a given entry and it must look over the
whole data. Like high-level sportsmen that have tools to
analyze, correct and improve their gesture, we want to propose a
tool to assist the creative artists, to let them better understand
the phases of a performance and to help them in their creation
process.
For our practical experiments we have chosen to work on a
digital opera called Alma Sola written by Bonardi and
Zeppenfeld where a performer plays (sings and moves) different
blocks from different universes (such as Prologue, Love,
Pleasure, etc.). Alma Sola is an open form of opera [1]. The
performer embodies a feminine Faust andwanders through the
various universes splitinto blocks. She therefore interprets an
opera playlist that she selects during the show itself. For
instance, a performance can be : Love-3, then Wealth-5, then
Pleasure-3, etc. Thanks to Hidden Markov Models, the computer
offers continuations to the performer and suggests the next block
to be performed (cf. Figure 1).
Figure 1. Navigation in Alma Sola digital opera.
We retrieve the actor’s performance on video files and look for
emotions in the data. A distinction must be made between intent
and emotion: indeed intent corresponds to the conscious part of
the emotion. Thus the assistant enables the comparison between
the performer’s intent and the rendered emotions in the context
of Alma Sola. It is widely known that fuzzy logic offers good
tools to deal with such subjective concepts [10], emotions here,
this is why we shall use a fuzzy rule-based system (FRBS) to
detect the performer’s emotions.
The article is organized as follows: first we give an overview of
the existing research about assistants in art, then we describe our
assistant and notably the way we retrieve emotion descriptors. In
the third section we show how the descriptors are partitioned in
classes. This step is necessary to build correctly the fuzzy rules
that form the inference system. Finally, section 4 concludes this
study.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
NIME 06,J u n e4 - 8 ,2 0 0 6 ,P a r i s ,F r a n c e .
Copyright remains with the author(s).
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
326
2.Research about performer's assistants
In a way, research about performer’s assistants has existed for
centuries. One immediately thinks of the use of mirrors in
dance. At the time when mirrors become common place in
Europe (Renaissance), the first treatise about dance is released
by Thoinot Arbeau in 1589.
The issues raised are still the same :
 first point, to be able to state an ideal prescription of the
performance. This generally starts from scores and
notations, which have been developped for centuries in
music and dance. They include both implicit (you have to
play an F, but the score does not tell how to do it) and
explicit (cross hands when playing the piano, for instance)
gestures to be achieved. From these structured indications, a
dancer or a musician tries to infer some of the author’s
intentions[5]. His/her representation of the author's
intentions become the ideal prescription of the performance.
 second point, to be able to measure a kind of difference
between the realized performance and its ideal prescription.
For instance, in his approach of “Virtual scores”, Manoury
has considered [7] in his pieces for solo instrument and live
electronics the computation of this difference ( Jupiter for
solo flute and live electronics, 1987,En Echo for voice and
live electronics, 1991) as a basis to generate electronic
sounds.
 third point, which is correlated to the second one, is the
approach to measure this difference. The first approach
consists in directly measuring various aspects of the
performance, using captors in a broad meaning [11]. V arious
captors are nowadays available: video camera, wireless
microphone, ultrasound device, carpet detectors, digital
compass, etc. The second approach consists in collecting
human appreciations of this difference between the
prescription and the realization. Composer Roger Reynolds
has for instance imagined psychological testing [9] with
listeners for this pieceThe Angel of Death.
From a technical point of view, dedicated software platforms
have been developped. Recently, Camurri and his team have
developed the first robust platform for the analysis of gestures
and consequently of performer’s emotions. It is named EyesWeb
[3], [6]. It is not based on captors implemented on the
performer’s body (with heavy batteries and radio transmission),
but on video capture with a static shot. It is based on a graphical
language that implements many descriptors of gestures: quantity
of motion, stability, etc. EyesWeb has become a worldwide
standard for performance analysis.
Roughly ten years before, the Ircam institute (and Cycling 74
company) had developed the real-time digital sound analysis
and synthesis platform to complete Max software. It is named
MSP (Methods for Sound Processing) and is now included in
Max software (Max/MSP). This software is a worldwide
standard in real-time sound analysis. At the present time the
state-of-the-art consists mainly in inferring a few emotional
states from the raw data delivered by EyesWeb and/or
Max/MSP.
For instance, a project relatively similar to ours is developed by
Friberg and his team. They have conceived a real-time algorithm
to analyze emotional expression in musical performance and
body movement [4]. In the framework of a game named “Ghost
in the cave”, the player has to express di•erent emotions using
his/her body or his/her voice, and these emotions are the input
values of the software. They use EyesWeb to recover body
movements and sound descriptors (sound level, instant tempo,
articulation, attack rate, high-frequency content). But in this
game, there is an immediate feedback, and the player has to
move constantly and talk until the software reacts according to
his/her wishes. Our aim is not the same, i.e. the assistant adapts
to the performer and not the contrary.
3.Project Description
As explained above, we have chosen to work on the interactive
operaAlma Sola written by A. Bonardi and C. Zeppenfeld. Two
dissimilar scenes have been extracted to be used as “sample
scenes”: they are the chanted Prologue (which is improvised,
unwritten) and the Love Universe (which is “strictly written”).
The performer (a singer/dancer, here) is filmed by a camera in
wide and static shot (cf. figure 2, left) and his/her voice is
recorded in a separate file, in order to handle both sources of
data separately. The project is centered on two main phases: the
acquired scene processing (with the performer) and the sound
capture processing. In this article we focus on the first phase,
where EyesWeb has been used (through a dedicated patch we
have written) to analyze accurately, understand and exploit non-
verbal expressive gestures. Several parameters can be extracted
from the video file: quantity of motion, stability, motion
duration, pause duration in a scene, contraction index and
surface of the performer in the image, convex hull of the body
silhouette, velocity, acceleration, etc. This patch construction
step is not trivial and represents hours of tests with sample
videos and it implies a concertation with the stage directors.
Figure 2. Left, an image taken form the video file; right, the
corresponding convex of the body silhouette
(performer : Claire Maupetit).
First of all, the background of the image must be removed —
using the difference between two frames — in order to keep
only the performer’s movement, since the camera is static. Next,
the parameters can be easily extracted because we are sure they
only concern the performer, and not the background. Here is the
description of some of the most interesting parameters for our
problem (Table 1 shows some of them). The quantity of motion
is computed by the number of pixels changing position between
two instants (thewhite pixels in figure 2); the convex hull of the
body silhouette is the bounding rectangle of thewhite pixels (cf.
figure 2); the stability is the ratio of the height of the
silhouette’s center of gravity on the length of the segment
connecting the lower points of the silhouette; the contraction
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
327
index is the ratio of the silhouette’s surface over the surface of
the convex hull. The stability is an important descriptor since it
gives good insight on whether the performer is near the ground
or not, i.e. whether the performer puts himself at risk or not. The
contraction index is also very useful and reflects whether the
performer is effusive or not.
Table 1. Descriptor definitions.
Choosing these parameters judiciously (called video descriptors)
allows us to compute various aggregations of each set of values
for each descriptor. The chosen aggregators are: partial and
general means, standard deviation, covariance, etc. Then, to a
meta-level, we characterize and categorize the sequences of each
scene thanks to an FRBS. The number of categories used in this
opera is five (Sleepy, Angry, Happy, LoveBeliever (could be
also called Effusive), LittleEffusive). Figure 3 sums the whole
process up. (We capture 25 frames per second, i.e., 25 values per
second for each gesture descriptor.)
Figure 3. Our system components.
As can be seen in Figure 3, once the descriptors are extracted
from the video files, they are aggregated in order to give a
description of each emotion we want to recognize. Figure 4
shows graphically the results for the Universe of Love.
The next step is to classify in partitions the aggregation results.
For example, when the performer expresses an emotion such as
happiness, he/she moves a lot. But this is not always easy to
guess even if the emotions share some general patterns, event in
the restricted context ofAlma Sola.
Figure 4. Aggregated descriptors for each emotion
i nt h eU n i v e r s eo fL o v e .
The fuzzy partitioning allowing the construction of the rules is
now described.
4.Building the Fuzzy Rules
Fuzzy partitioning needs to spread the various values taken by
the aggregation results suitably. However a simple uniform
distribution of the classes on the axis is not appropriate since
sometimes small, other times average or in other cases big
variations lead to an emotion change, depending on the
aggregated vectors. Like Martinez & al. in [8], we propose a
categorization depending on the data distribution. Five classes
are considered: V ery Low, Low, Average, High and V ery High
values (denoted VL, L, A, H, VH).
Table 2. Classification for the Universe of Love.
In the software we propose (see section 5), we also offer the
possibility to use L-R fuzzy numbers exclusively (cf. figure 5)
or L-R fuzzy intervals exclusively.
Figure 5. The L-R fuzzy numbers used in the software.
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
328
Finally, it is easy to establish the fuzzy rules according to Table
2, one rule per line. Here is an example for the Love Universe:
5.Application
The application we have developed implements the concepts
explained above. We have worked on two universes: Love and
Prologue. The performer has played several times each universe
and the software has tried each time to detect the emotions
perceived. The video files obtained for the performances are
split into smaller files that are given to the software. After
having chosen the fuzzy subsets that will be used for the
partitioning (cf. Section 4), the values of the aggregated
descriptors are displayed and a graphical result is also proposed.
Figure 6 shows that both Love Universe and Prologue have
rather been performed with Sleepy emotion.
The results are better (i.e., the results for the emotions are more
clearcut) using partitioning with both L-R fuzzy numbers and L-
R fuzzy intervals because they adapt to the level of precision,
depending on the values retrieved from the video file.
Figure 6. A screenshot of the software.
6.Conclusion
In this paper we have presented a system that is able to give
clues to the stage director in order to evaluate a performer’s
rendition. This is done thanks to a fuzzy rule-based system that
detects the actor’s emotions during a performance show. One
originality is the way we construct the fuzzy classes when
partitioning the universes before the rule construction. They are
dynamically built according to the values characterizing the
performance.
As a future work, we will try our assistant on other test sets, i.e.
with more records fromAlma Sola (with the same performer or
not) but also with records from other shows. Moreover, it would
be very interesting to include a back propagation in the
software: when an unexpected emotion is detected, the assistant
should suggest modifications of his behaviour to the performer
in order to obtain best results during the next detection.
Concerning Alma Sola opera itself, we can also imagine in the
future that the assistant could be used to classify the blocks
performed according to the detected emotions and then
contribute to the design of the open form.
7.ACKNOWLEDGMENTS
We thank Adrien Revault d’Allonnes and Murat Goksedef for
their collaboration and help on this work. This project was
funded by the Maison des Sciences de l’Homme Paris Nord.
8.REFERENCES
[1] A. Bonardi & F. Rousseaux. New Approaches of Theatre
and Opera Directly Inspired by Interactive Data-Mining. In
Proceedings of the Int. Conf. Sound & Music Computing
(SMC’04), pages 1-4, Paris, France, 2004.
[2] B. Bouchon–Meunier. La logique floue et ses applications.
Addison–Wesley, 1995.
[3] A. Camurri, M. Ricchetti & R. Trocca. EyesWeb - toward
gesture and affect recognition in dance/music interactive
systems. In Proceedings of the Int. Conf. IEEE Multimedia
Systems, Firenze, Italy, 1999.
[4] A. Friberg. A fuzzy analyzer of emotional expression in
music performance and body motion. In Proceedings of
Music and Music Science, Stockholm, 2004.
[5] K. Kahol, P. Tripathi & S. Panchanathan. Automated
Gesture Segmentation From Dance Sequences. In
Proceedingsof the Sixth IEEE International Conference on
Automatic Face and Gesture Recognition (FGR 2004) ,
pages 883- 888, Korea, 2004.
[6] E. Lindström, A. Camurri, A. Friberg, G. V olpe & M.-L.
Rinman. Affect, attitude and evaluation of multi-sensory
performances. In Journal of New Music Research, 34(1):
69-86, Taylor & Francis, 2005.
[7] P. Manoury & M. Battier. Les partitions virtuelles. InIrcam
documentation, Paris, 1987.
[8] L. Martinez, J. Liu, Da Ruan & J.B. Yang. Fuzzy Tools to
Deal with Heterogeneous Information in Engineering
Evaluation Processes. In Information Sciences ,I nP r e s s ,
2006.
[9] R. Reynolds. Epilog:Reflections on psychological testing
with The Angel of Death. InMusic Perception n ° 22, pages
351-355, 2004.
[10] L.A. Zadeh. Toward a generalized theory of uncertainty
(GTU) — an outline. InInformation Sciences, Elsevier, 172
(1-2), pages 1-40, 2005.
[11] C. Zeppenfeld. L’acteur face aux technologies, Master
dissertation, University Paris 3, 2004.
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
329