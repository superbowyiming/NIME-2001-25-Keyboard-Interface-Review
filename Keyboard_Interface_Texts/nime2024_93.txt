Resonant Object Interface: Implementation and Initial Exploration of a Tactile Acoustic Interface   Sasha Leitman School of Engineering and Computer Science Victoria University of Wellington, NZ sleitman@gmail.com 
 Dale A. Carnegie School of Engineering and Computer Science Victoria University of Wellington, NZ dale.carnegie@vuw.ac.nz  
 Jim Murphy New Zealand School of Music Victoria University of Wellington, NZ jim.murphy@vuw.ac.nz       ABSTRACT The Resonant Object Interface (ROI) is an acoustic input system that provides nuanced, embodied, and expressive engagement with control signals in audio software. The ROI’s unique sensing methodology is built from resonant objects, vibration exciters and contact microphones. As users touch the resonant object, their hands dampen the signals from the vibration exciters. The resultant signals are analysed and turned into control data for audio software. This produces a music controller that is deeply tactile, capable of repeated gestures and uniquely tangible.   This paper gives an overview of the design and implementation of the ROI and then describes a series of user engagements with the device.  Author Keywords NIME, new music controller, acoustic input device, tactile  CCS Concepts • Applied computing → Sound and music computing; Performing arts; Human-centered computing → Interface design prototyping.  1. INTRODUCTION The Resonant Object Interface is a musical interface inspired by the primary author’s work with feedback resonance created using vibration exciters and contact microphones in interactive sound art pieces such as The Barrels of the Beast [10]. Users’ tactile engagement with resonating surfaces such as the tops of large metal barrels and their ability to modify the amplitude of individual resonances on the barrel surfaces prompted an investigation into the possibility of turning this means of interaction into a nuanced musical input system.    We offer a brief discussion of work related to feedback, resonance and sound sensor interfaces, discuss the technical implementation of the ROI and give an overview of our explorations of the device within our own creative practice and in three iterative explorations with other users. 2. BACKGROUND The use of sound sensors (air and contact microphones) can add gestural flexibility and nuanced control to DMIs [9]. Sound sensors are uniquely both broad and sensitive – when attached to objects, they pick up all of the excitations of that object, both intended and unintended. The broad sensing of the object allows users to individualize their playing styles and quickly modify their actions while focusing more on the physical object than the particulars of the sensor [2].   
 When sound sensors are coupled with the use of auditory feedback, new musical interactions are created that provide unique and exciting musical and gestural possibilities. As the AHRC Feedback Musicianship Network [1] note, “There are large gaps in knowledge of luthiery of these hybrid acoustic/electromechanical/digital instruments, which also demand new composition, notation and performance techniques, and new understandings of virtuosity.”   Seth Cluett’s “the interior of objects” [6] and Øyvind Brandtsegg’s “Finger mounted piezos - exploration by touch” [5] explore the sonic possibilities in the combination of vibration exciters and contact microphones on a resonant surface. Armitage [3] describes an Audio-Tactile Interface that uses these elements in a resonant plate which is dampened by clay sculptures to modify digital resonance models on a tuned percussion DMI. Research by Armitage et al. detail the use of this system to explore Digital Lutherie [4] and note that systems such as these provide gesturally flexible and personalised interactions.  The ROI explores the use of these elements to create a nuanced and tactile means of engaging control parameters within audio software. It builds on the affordances that these other works highlight and seeks to use those parameters to create an acoustic input device for digital music. 3. DESIGN AND IMPLEMENTATION As shown in Figure 1, the Resonant Object Interface works by exciting a resonant object with square wave at a known audio frequency using a vibration exciter.  The user touches the resonant object and alters the resonances of that object.  A contact microphone on the resonator picks up the altered signal.  The resultant signal is spectrally analysed and the magnitude of the higher harmonic frequencies of the square wave are measured.  That magnitude is scaled and converted into a MIDI CC number which can then be mapped in audio software or hardware 
Figure 1. Signal Flow Diagram of the ROI   
NIME’24, September 4–6, 2024, Utrecht, The Netherlands
Licensed  under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Copyright remains with the author(s).
to appropriate parameters such as audio effects, filters, or synthesis parameters.  One of the appealing ideas of the Resonant Object Interface is that it is a sensing methodology that could theoretically be used on any resonant object – from 55-gallon drums to a cigar box to a tabletop.  The ROI incorporates a separate Resonator and Processor, shown in Figure 2. The Processor is a dedicated hardware device that handles the required signal processing and outputs MIDI over USB.  
 A vibration exciter and piezoelectric contact microphone are mounted inside of the resonator on the underside of the top plate.  A Dayton DAEX13CT-4 Coin Type 13mm 3W 4W exciter is employed. The contact microphone is a 27 mm piezoelectric bender disc that is commonly used for musical instruments. Both electronic elements are wired to 3.5 mm audio jacks on one of the longer edges of the resonator.  The contact microphone and the vibration exciter are placed several centimetres in opposite directions from the centre of the plate to avoid activating any powerful nodes on the resonant top plate.  The PJRC Teensy 4.0 Microcontroller [12] combined with an associated PJRC Audio Adapter Board [13] provides DSP analysis and MIDI output. The Teensy Audio Adapter line out level is amplified using the Adafruit Stereo 3.7W Class D Audio Amplifier, built around the MAX98306 Class D amplifier chip. This provides sufficient amplification for the 3W vibration exciters. The Teensy Audio Library [14] object AudioAnalyzeToneDetect  [11] implements the Goertzel Algorithm [8] to measure the frequency magnitude of specific known frequencies.   A TFT touchscreen and rotary encoder are used for setting the various gain and frequency parameters of this highly acoustic system – the preamplification of the contact microphone signal, the volume level for the vibration transducer and the frequencies and waveform shapes of the signals sent to the vibration transducer.    The ROI is initially imagined as a bimanual system with one hand selecting the notes and the other hand modifying timbre and expression, as shown in Figure 3. 
 
2. USERS We explored the use of the ROI through performances by the primary author, open ended explorations with professional musicians who have prior experience with music technology, amateur singers, and informal sharing of the device with music technologists. Each iteration of this sharing process has transformed and matured how we share the ROI. 2.1 Performances by the Primary Author During the development process, the primary author used her own musical explorations as the initial test case for the device. She developed a style of physical interaction that favoured keeping a varying but significant portion of her hand on the face of the resonator while applying significant pressure, similar to “kneading” bread. In the studio and in two live performances, the author found the ROI to be capable of nuanced and predictable control.  2.2  Experienced Music Technologists We next engaged professional music practitioners who were comfortable with music technology to explore the ROI. We left the structure and direction of these interactions intentionally open ended to broadly explore how users might approach this new device and to determine how much guidance was required to help users form a bridge between the acoustic phenomena of the resonating plate and the musical control possibilities that the device might afford. One practitioner, a composer and professional violist, was enthusiastic about the sonic possibilities and the acoustic grounding of the device exclaiming, “This is an instrument!” Other practitioners found the ROI confusing and felt that it did not produce repeatable results.  We observed that users who had prior knowledge about the acoustics of resonant plates, such as having seen a demonstration of Chladni patterns or learned about the acoustics of string instruments, had a more intuitive understanding about how the ROI both functioned and could be used. Additionally, we found that users wanted a more clearly developed initial musical state from which to begin their exploration. 2.3 Task-Based User Study of Singers Building on the findings from our work with experienced practitioners, our next step was to develop and then test the use of prescribed musical presets and a more developed onboarding process. To rapidly test these modifications and examine if we could more quickly convey the use and function of the ROI, we chose to focus on a single musical use case - singers. Our goal was to determine the effectiveness of presets on early familiarization with the device and singers would hopefully have the easiest time getting acquainted with the device because they have two free hands. This group was not screened for their experience with music technology, and we made no assumptions about prior experience or knowledge in the fields of acoustics and resonance.  2.3.1 Onboarding Each singer received the same introduction and onboard process.  • A five-minute tutorial video was shown that gave a simple explanation of frequencies, waveforms, and resonance. • The user was given an opportunity to feel and hear the timbre of the resonator changing as they touched it without the sound being routed to any software parameters. • A visualization of the two higher partials values mapped to MIDI values of 0 to 127 was shown so that they could visually see how their touch was changing the readings. 2.3.2 Presets Three presets were tested: a reverb, a resonant filter, and a harmonizer. Mappings for each preset were limited to two MIDI CC values generated from the third and fifth harmonic of the square wave.  Each preset was designed to sound pleasing on a vocal signal.  The audio effects chosen were ones that enhance rather than distort the vocal signal and the parameters that were modified within each preset were chosen to work well together. 
Figure 2. Resonator and Processor  
Figure 3. Bimanual Operation of the ROI  
2.3.3 Observations We shared the ROI with ten singers using this onboarding process. This user engagement produced more consistently positive feedback than our prior work.    We observed: • It is important to clarify to users that the device works best when it is thought of as a tool for moving from one state to another as opposed to keeping the value in one specific position. • People have a strong inclination to focus on position as though it were a touchscreen.  This has become a deeply engrained interaction model and was difficult to overcome on an intuitive level. • Most users felt that they were able to make small changes in the sound and could repeat some sounds and timbres, particularly if they enjoyed those sounds. • Many of the users used words to describe an enjoyable, meditative, and embodied experience. 2.4 Keyboard Presets Building on our work with singers, we developed a bimanual implementation of the ROI with a keyboard controller, shown in Figure 4. These musical presets employed many of the same techniques used for the singers: two frequency magnitudes were mapped to two musical parameters that were kept within a sonically pleasing range. 
  This implementation of the ROI was shared with music technologists in informal demonstrations.  Having simplified and tuned the system, the return of bimanual control felt comfortable and not cognitively challenging for users.  Within this well-tuned system, different touch styles emerged, including gentle patterned strokes as well as the firmer pressure described in previous sections. The stability of the system made it possible for users to develop a more tacit understanding of the ROI.  3. CONCLUSION We have described the Resonant Object Interface, a novel means of interacting with computer music software that provides nuanced and embodied control using vibration exciters, contact microphones and resonant objects. We have discussed four explorations of the ROI’s use – as a tool used by the primary author, initial sharing with professional musicians, exploration of presets and onboarding with singers and a keyboard implementation shared with fellow music technologists. We have the articulated observations made and lessons learned from these initial explorations, and we have described the evolution of our communication with new users.   
4. ETHICAL STANDARDS This research has been approved by the Victoria University Wellington Human Ethics Committee. [Application #ber 0000031006 and 0000028501]. 5. REFERENCES [1] AHRC. 2022, “AHRC Feedback Musicianship Network.” Retrieved February 29, 2024 from https://feedback-musicianship.pubpub.org/ [2] Jack Armitage and Andrew McPherson. 2018. Crafting digital musical instruments: an exploratory workshop study. In Proceedings of the International Conference on New Interfaces for Musical Expression, Blacksburg, Virginia, USA, 2018.  [3] Jack Armitage. 2020. An experimental audio-tactile interface for sculpting digital resonance models using modelling clay. In Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming (Programming '20). Association for Computing Machinery, New York, NY, USA, 225–226. https://doi.org/10.1145/3397537.3398482 [4] Jack Armitage, Thor Magnusson, and Andrew McPherson. 2023. Studying Subtle and Detailed Digital Lutherie: Motivational Contexts and Technical Needs. Proceedings of the International Conference on New Interfaces for Musical Expression. [5] Øyvind Brandtsegg. 2022. “Finger mounted piezos - exploration by touch.” Retrieved October 03, 2023 from https://feedback-musicianship.pubpub.org/pub/events/release/40 [6] Ernst F. F. Chladni. Acoustics: Historical and Philosophical Development, ed. R. B. Lindsay p.156 (Dowden, Hutchinson & Ross, 1972) [7] Seth Cluett. 2019 “the interior of objects.” Retrieved February 29, 2024 from https://sethcluett.com/works/the-interior-of-objects/ [8] Gerald Goertzel. 1958), "An Algorithm for the Evaluation of Finite Trigonometric Series", American Mathematical Monthly, 65 (1): 34–35, doi:10.2307/2310304 [9] Sasha Leitman. 2020. Sound Based Sensors for NIMEs. Proceedings of the International Conference on New Interfaces for Musical Expression.  [10] Sasha Leitman. 2013 Barrels of the Beast. Retrieved February 5, 2024 from http://sashaleitman.com/BarrelsOfTheBeast [11] PJRC, “Audio Analyze Tone Detect.” Retrieved February 5, 2024 from https://www.pjrc.com/teensy/gui/?info=AudioAnalyzeToneDetect [12] PJRC, “Teensy 4.0.” Retrieved February 5, 2024 from https://www.pjrc.com/store/teensy40.html [13] PJRC, “Teensy Audio Adapter Board.” Retrieved February 5, 2024 from https://www.pjrc.com/store/teensy3_audio.html [14] PJRC, “Teensy Audio Library.” Retrieved February 5, 2024 from https://www.pjrc.com/teensy/gui/   
Figure 4. Keyboard Presets and Bimanual Control 