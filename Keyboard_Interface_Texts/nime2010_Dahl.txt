Sound Bounce: Physical Metaphors in Designing Mobile 
Music Performance  
 
Luke Dahl 
Center for Computer Research in Music and 
Acoustics (CCRMA) 
Stanford University 
660 Lomita Drive, Stanford CA, USA  
lukedahl@ccrma.stanford.edu 
 
 
Ge Wang 
Center for Computer Research in Music and 
Acoustics (CCRMA) 
Stanford University 
660 Lomita Drive, Stanford CA, USA  
ge@ccrma.stanford.edu 
 
 
ABSTRACT 
The use of m etaphor has  a prominent role in HCI, both as a 
device to help users understand unfamiliar technologies, and as 
a tool to guide the  design  process . Creators of new computer -
based instruments f ace similar design challenges as  those in 
HCI. In the course of creating a new piece for Mobile Phone 
Orchestra we propose  the metaphor of a sound as a ball  and 
explore the interactions and sound mappings it suggests.  These 
lead to the design of a gesture -controlled instrument that allow s 
players to “bounce” sounds,  “throw” them t o other players, and 
compete in a game to “knock out ” others’ sounds. We 
composed the piece SoundBounce based on these interactions, 
and note that audiences s eem to find performances of the  piece 
accessible and engaging, perhaps due to the visibility of th e 
metaphor. 
 
Keywords 
Mobile music, design, metaphor , performance, gameplay. 
1. INTRODUCTION 
Designers and composers of computer-based musical 
interactions are faced with a great number of choices and 
opportunities. Computer hardware affords numerous input 
capabilities such as  mice, keyboards, accelerometers, and touch 
screens, and software allows us  to map these to sound 
generating processes in ways that are not constrained by physics 
or material considerations in the way that acoustic instruments 
are. Furthermore, performance practices in technology -
mediated music are not standardized. The composer must 
design not only the  instruments, including the actions by which 
they are performed  and the resulting sounds , but also the 
interactions between conductor, pe rformers, and score, all the 
while taking into account how these affect the audience’s 
understanding of the performance . 
The Stanford Mobile Phone Orchestra (MoPhO ) is a performing 
ensemble in which small hand -held computing devices such as 
iPhones and iPod  Touches  are u sed as musical instruments  [3, 
11]. Composers write custom  software applications to enable 
the musical interactions they desire, and in this process confront 
the aforementioned design challenges.  
Researchers and designers  in human -computer interaction 
(HCI) often use metaphor  to help users quickly gain an intuitive 
understanding of a device’s operation,  as well as to guide the 
process of designing new inter actions [1]. 
This paper describes the role of metaphor in HCI desig n, and its 
use in the creation and performance of the piece SoundBounce, 
for MoPhO. We used the metaphor of a sound as a ball  to 
explore possible interactions, to create the sound bounce 
instrument and its sound control mappings, and to structure the 
performance of the piece. The paper concludes with an  
evaluating the expressive  capabilities of the instrument and the 
success of its performance from the audience’s perspective.   
2. METAPHOR IN DESIGN 
2.1 History 
A metaphor is a figure of speech or a literary device  in which 
one object is described as if it were another. The use of 
metaphor in human computer interaction has a long and rich 
history, as described in detail by Blackwell  [1]. Often an entity 
on the computer, such as a file directory, is visually represen ted 
as a  physical object, a folder in the case of the desktop 
metaphor, and the operations on this entity are described as the 
physical actions typically performed on the object, e.g. opening, 
adding and removing documents.   
The metaphor allow s the user t o understand abstract or 
complicated operations of the computer in terms of physical 
objects or interactions with which they are already familiar . A 
metaphor may refer to a physical reality, or to a common 
cultural understanding (what Fels  calls literature  [4] ). The 
work of Lakoff and Johnson suggests that all abstract 
understanding occurs through metaphors that reference our 
embodied spatial experience [9,10]. 
The desktop metaphor is probably the most well- known 
application of metaphor in HCI, but other e xamples include 
navigation metaphors in file and web browsing, cockpit 
metaphors (a sub -class of navigation) in games and in  
discussions of technologically -augmented power [1 ], and cave 
metaphors in virtual reality.  
A metaphor  can be used not only to struc ture a user’s 
experience but als o to guide the design process. Fels describes 
the use of metaphor in designing new  musical instruments in 
general and its application to four unique instruments  [4] . In 
Verplank’s Framework for In teraction Design, which is  used in 
a course on music interacti on design at Stanford [ 5], metaphor 
is used during brainstorming and design  to create meaning s for 
 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided t hat copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME2010, June 15-18, 2010, Sydney, Australia 
Copyright remains with the author(s).  
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
178
both designer and the user. 
2.2 The Bouncing Ball 
Typically a new design does no t emerge fully -formed in a flash 
of inspirati on. Rather  t hrough exploration and prototyping an 
initial insight may lead to subsequent discoveries and 
refinements.  
SoundBounce began with the desire to create a gesture-
controlled instrument that enabled interesting interactions 
between performers. The idea of a “bounc e” gesture came from 
holding an  iPod  Touch  in hand and playfully exploring 
movement possibilities. Unlike the Wiimote, which one tends to 
hold with the thumb side of the hand up , the iP od is typi cally 
held with the screen up. The flatness of the screen suggests a 
paddle, and an upward flick of the wrist creates a motion as if 
one were bouncing a ball upward (Fig. 1). The idea of bouncing 
a sound led to the metaphor of a sound as a ba ll. Bouncing a 
ball then becomes the act of “playing with a s ound” and 
“keeping it going.” We will discuss in section 3 .1 how this 
metaphor led to a particular sound synthesis and mapping.  
 
Figure 1: Holding the iPhone suggests a “hit” gesture 
Notice that this metaphor makes us think of a sound not as the 
result of some process , but as a distinct object, separate from 
other sounds, amenable to manipulation, and able to be 
transferred from one person to another. Blackwell points out 
that a metaphor frames not just the interaction between a  user 
and the computer, b ut also how the designer sees the user. The 
metaphor of a sound as a ba ll frames the performer as someone 
playing with a ball. When more than one player is present this 
leads naturally to the int eraction of passing a sound  from one 
performer to another . Thus we added to the possible 
interactions the ability to take aim at another performer and 
throw a sound to them. 
 
Figure 2: Throwing a sound  
It is only a small step from passing a ball around to playing a 
ball game. As one possible mode of interaction bet ween players 
we created a game using only the actions of bouncing and 
throwing sounds . Players attempt to knock out others players’ 
sounds by throwing their own sound at them. Details are given 
in section 3.2.  
3. FROM METAPHOR TO MUSIC 
3.1 Metaphor-Based Mappings 
In computer -based instruments the mapping from  physical  
movement to sound generation has been decoupled from 
physical necessity, giving the designer a staggering number of 
options.  Luckily we can use our metaphor to structure not only 
the interac tions with the instrument and between performers, 
but also the sounds that these interactions create.  
In the sound bounce instrument the ball is not just a metaphor. 
It is implemented in code  as a virtual bouncing ball object with 
its own simulated physic s. When a  player makes a hitt ing 
gesture an upward velocity is imparted onto this virtual ball  
which then rises and falls ballistically under the accel eration of 
constant gravity. The movement of the  ball is  sonified, creating 
an instrument that maps gestu re to sound via a simulated 
physics.  
The sound of the ball is created wit h f requency modulation  
(FM) synthesis, where a  single carrier oscillator’s audio -rate 
frequency is modulated by the output of a slower modulation 
oscillator.  In order to choose a ma pping from the virtual ball’s 
behavior to synthesis parameter we can again refer to  metaphor. 
People often employ  spatial and movement metaphors when 
discussing music and its e ffects [ 7]. We say that a melody 
“rises”, “falls”, or “comes to rest . One common  association is 
between spatial height and melodic pitch height.  In the sound  
bounce instrument the height of the virtual ball controls both 
the frequency of the carrier oscillator and the amplitude of the 
modulating oscillator.  This leads to a sound that  rises in  pitch 
and gets spectrally brighter as the ball rises, thus taking 
advantage of these metaphorical associations. 
Some occurrences suggest discrete sonic events. A  sharp impact 
sound occurs whenever a performer bounces a ball . If the player 
fails to hit the ball before it falls below a certa in height the ball 
is dropped,  triggerin g a crashing sound and making the ball  
unavailable to further interaction  
A performer can take aim by holding the  iPhone horizontal and 
pointing it at another p layer. This triggers two short pitched 
sounds which are not metaphorically motivated. R ather they are 
designed to allow the performers to hear whom they are aiming  
at and who is aiming at them. The first sound is played by 
performer’s own instrument at a pitch associa ted with the 
person aimed at. The second occurs a fraction of a second later 
on the instrument of the person aimed at, at a pitch asso ciated 
with the person aiming. These sounds  create an audible link 
between the players  and become part of the soundscape  of the 
piece. 
Once a performer has taken aim they can pass their sound to 
another player by making a n over-handed throwing gesture  (Fig 
2). The sound of a throw is similar to a bounce in that the 
virtual ball is launched upwards. However the sound gradual ly 
cross-fades from the thrower’s instrument to the receiver’s, 
creating the illusion of a moving sonic object.  
3.2 The Game 
The last section of the SoundBounce piece is structured as a 
game. The soundscape of the game is designed so that all 
actions and chang es of state have audible correlates, allowing 
both players and audience to perceive what is happening 
without any explicit visual information.  
When a player ’s ball is knocked out by  another player’s throw, 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
179
the targeted player’s instrument plays a loud clan ging sound. 
The targeted player then loses health , thereby coming closer to 
dying. This change of state  is conveyed by the addition of a 
noisy distortion to all of the player’s sounds. As the player loses 
more health  the distortion gets progressively  greater.  This can 
be seen as a metaphorical association of clear sounds with good 
health and distortion with poor health .  Once a player dies they 
can no longer make sound  and are ejected from the game . 
 
3.3 Implementation 
The sound bounce instrument is implemented as an iPhone 3.0 
application, and uses the MoMu API [2 ] for audio output, 
accelerometer and compass data, and network communication 
using Open Sound Control  (OSC) . All computation, including 
gesture detection and so und synthesis, is performed by this 
application. 
The code is structured into three objects . Gesture  Control uses 
information from the iPhone’s accelerometers and compass to 
detect when the user perform s a hit, throw, or aim gesture.  The 
Bounce Synth object computes the virtual ball physics and 
performs all sound processing . Hit and throw messages from 
Gesture Control are used to impart velocity to the virtual ball  
whose height is used to control  the FM synthesizer. Bou nce 
Synth also contains  a wavetable s ynth which plays the sounds 
associated with the hit, ball drop, aim, and knockout even ts.  
Audio from the FM and w avetable synths is passed through a 
distortion which adds noise based on the performer’s game 
health. The Game Control object keeps track of all game states, 
and sen ds and receives OSC messages for aiming , throwing , 
and receiving sounds. 
3.4 Performance History 
SoundBounce was originally created as a piece for the Stanford 
Laptop Orchestra (Slork) in the spring of 2009. Performers 
were seated at laptop stations, and ma de gestures with iPod 
touches. We used the iPhoneOS application TouchOSC  [6]  to 
send accelerometer data from the iPods to laptop computers via 
OSC. Gesture detection was performed o n the laptop by a 
Max/MSP patch that th en sent OSC messages to a Chu cK 
program, also on the laptop, for sound synthesis. The premiere 
performance on June 4, 2009 was composed and coordinated by 
Luke Dahl, Diana Siwiak, Leah Reid, and Lauchlan Casey. 
In autumn of 2009 SoundBounce was modified fo r performance 
by the S tanford Mobile Phone Orchestra , and the sound bounce 
iPhone application was developed . All details presented in this 
paper pertain to the MoPhO version of the piece. 
SoundBounce for Mo PhO was premiered on December 3 , 2009 
at the Center for Computer Research in Music and Acoustics 
(CCRMA) at Stanford University .  The five performers stood in 
a circle facing each other  in the center of the performance 
space, with audience  members both surrounding and within this  
circle. Sound was projected from powered speakers mounted on 
gloves worn by the performers.  The structure of the piece wa s 
based on  the  three principle interactions: bouncing, throwing, 
and game play.   
Before the piece begins performers check that sound and 
networking are working properly by aiming at each other and 
listening to the pitched aim sounds . This  creates a polyphonic 
texture akin to the sound of an orchestra warming up. The piece 
begins with each performer in turn generating a new sound, 
bouncing it a f ew times, and letting it drop. All performers then 
bounce their sound s in synchrony and suddenly let them drop . 
In the second section players throw  a sound from one player to 
the next , first in a circle, and then in more complex patterns 
with multiple sounds at onc e. The piece ends with the game, 
during which the soundscape becomes progressively more 
distorted as players lose health, and then sparser as players drop 
out. When the duel between the last two remaining players is 
resolved, the winner performs a few victory bounces  and the 
piece ends.   A video of this performance can be seen at 
http://ccrma.stanford.edu/~lukedahl/soundbounce/ 
4. REFLECTIONS 
4.1 Expressivity and Visibility 
Since design is an iterative process, i t can be useful to reflect on 
the instrument we designed and the music we composed for it. 
We can measure an instrument’s ex pressivity in a number of 
ways. Does it allow for a wide range of sonic possibilities? 
Does it enable precise control of subtle variations in sound? 
According to these metrics the sound bounce instrument is not 
especially expressive. Performers can c hoose how high to 
bounce a ball  and can be somewhat expressive in choosing the 
timing of their gestures, but the sonic palette of ball -controlled 
FM synthesis and pre-recorded triggered sounds is rather 
limited. These limits may be a function of the metap hor: 
Gesture Control
Game Control
Bounce Synth
Ball 
Physics FM Synth
Wavetable 
Synth
Distortion
drop
ball 
height
hit,
throw
aim, throw
- hit
- drop
- aim
- knockout
knockout, aim
health
catch
Accelerometers Compass
Open Sound 
Control
A u d i o
Out
Figure 3: Sound Bounce instrument objects  
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
180
bouncing a ball is not  expressive in the way we associate with 
musical instruments. 
Fels claims that expressivity is a function of transparency – that 
is, whether the mapping from ac tion to sound is understandable 
by both the perfo rmer and the audience – and that  transparency 
is improv ed through the use of metaphor [4] . Indeed, the 
metaphor used in SoundBounce help s performers quickly 
understand the instrument and make s it easy to communicate 
about the  structure of the piece, e.g. “L et’s go to the passing -
the-ball-around section.” More importantly,  the use of metaphor 
allows the audience to understand both the relationship between 
a performer’s gesture and the resultant sound, and the  
interactions occurring between performers. In other words, 
Metaphor increases what Klemmer calls visibility [8]. 
Bouncing a ball may not be  expressive, but it is playful.  Rather 
than enabling individual expressivity, p erforming SoundBounce 
brings out a sort of group expressivity.  Players use a number of 
non-verbal cues such as body orientation and eye contact  to 
communicate intentions during performance, and t hese non -
audible actions become part of the performance. During the 
December 2009 Mo PhO performance the audience seemed to 
pick up on these signals , responding  with symp athetic 
vocalizations when a throw mistakenly went to the wrong 
recipient. The g ame section of the piece allows for the most 
diversity of behaviors, with impromptu allianc es being made 
and broken during the course of the game.  Again , the audience 
became more engaged as the rules of the game became apparent  
and the competition intensified, signaling so through laughs and 
vocalizations. 
4.2 Aesthetics 
We use the verb “play” to describe both what we do with a 
musical ins trument and what we do in a game.  However it is 
unclear whether the use of gameplay in a musical performance 
leads to an aesthetic and mus ical experience by the audience. 
Do they perceiv e the performance as music or as art,  or do they 
watch the performance in the same way they might regard a 
sports event?  
Using a metaphor in an artistic endeavor comes with the danger 
that the result may be overly simplistic. It is possible that our 
rather straightforward application of a metaphor imposes too 
much structu re on the audience’s experience,  leaving too little 
room for the active interpretation and multiplicity of 
understandings that good art allows.   
5. CONCLUSION 
Future work on SoundBounce might procee d in two different 
directions. We can explore the metaphor further in order to 
enrich the possible interactions with the instrument. For 
example w hat would it mean to catch a sound from the 
environment, or  to intercept or steal someone else’s  sound?  
How might  the metaphor help us modify the  instrument, 
interactions, and composition to work  for a large number of 
performers? Or we could use the instruments and in teraction 
possibilities defined so far in new compositions whose stru cture 
is less restrict ed by metaphor and thus open to more diverse 
interpretations. 
In summary, following the lead of HCI, we have used  a 
physically based metaphor, a sound as a ball , to explore 
possible performer -instrument and performer- performer 
interactions. This led to the design of an instrument and a 
musical piece, SoundBounce, whose sound mappings and 
structure are informed by the same metaphor. Upon reflection 
we found that t he resulting instrument, while not being 
expressive on its own, elicits interactions that are 
understandable and engaging to an audience.  
6. ACKNOWLEDGMENTS 
Our appreciation and  thanks to the members of the 2009 
Stanford Laptop Orchestra, especially Diana Si wiak and Leah 
Reid, and to the members of the 2009 Stanford Mobile Phone 
Orchestra. 
We hope that this paper will inspire other researchers in 
technology-mediated music interactions to reflect on and 
publish their own design processes. 
 
7. REFERENCES 
[1] Blackwell, A.F. The reification of metaphor as a design 
tool. ACM Trans. Comput.-Hum. Interact. 13, 4 (2006), 
490-530.   
[2] Bryan, N., Herrera, J., Oh, J., Wang, G.  The mobile music 
(momu) api. Proceedings of the International  Conference 
on New Interfaces for Musical Expression (NIME), under 
review, Sydney, Australia, 2010.   
[3] Oh,  Herrera, Bryan, Dahl, Wang . Evolving the Mobile 
Phone Orchestra. Proceedings of the International  
Conference on New Interfaces for Musical Expression  
(NIME), under review, Sydney, Australia, 2 010.  
[4] Fels, S., Gadd, A., and Mulder, A. Mapping transparency 
through metaphor: towards more expressive musical 
instruments. Org. Sound 7, 2 (2002), 109-126.   
[5] Gurevich, M., Verplank, B., and Wilson, S.  Physical 
Interaction Design for Music. Proceedings of  the 
International Computer Music Conference  (2003). 
[6] Hexler TouchOSC iPhone application. 
http://hexler.net/software/touchosc 
[7] Johnson, L.S. "Something in the Way She Moves" -
Metaphors of Musical Motion . Metaphor and Symbol, 18 , 
2 (2003), 63 - 84.   
[8] Klemmer, S.R., Hartmann, B., and Takayama, L. How 
bodies matter: five themes for interaction design. ACM 
(2006), 140-149. 
[9] Lakoff, G. and Johnson, M. Metaphors We Live By . 
University Of Chicago Press, 1980.   
[10] Lakoff, G. and Johnson, M. Philosophy in the Flesh : The  
Embodied Mind and Its Challenge to Western Thought. 
Basic Books, 1999.   
[11] Wang, G., G. Essl, and H. Pentinnen. Do Mobile Phones 
Dreams of Electric Orchestras? Proceedings of the 
International Computer Music Conference . (2008) Belfast. 
 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
181