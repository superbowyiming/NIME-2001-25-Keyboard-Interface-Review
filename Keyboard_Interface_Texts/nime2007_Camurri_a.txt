Active listening to a virtual orchestra through an 
expressive gestural interface: The Orchestra Explor er 
 
Antonio Camurri 
InfoMus Lab – Casa Paganini 
University of Genova 
Piazza Santa Maria in Passione, 34 
16123 Genova, Italy 
+39 010 2758252 
antonio.camurri@unige.it 
 
Corrado Canepa 
InfoMus Lab – Casa Paganini 
University of Genova 
Piazza Santa Maria in Passione, 34 
16123 Genova, Italy 
+39 010 2758252 
corrado@infomus.org 
 
Gualtiero Volpe 
InfoMus Lab – Casa Paganini 
University of Genova 
Piazza Santa Maria in Passione, 34 
16123 Genova, Italy 
+39 010 2758252 
gualtiero.volpe@unige.it 
 
 
ABSTRACT  
In this paper, we present a new system, the Orchest ra Explorer, 
enabling a novel paradigm for active fruition of so und and music 
content. The Orchestra Explorer allows users to phy sically 
navigate inside a virtual orchestra, to actively ex plore the music 
piece the orchestra is playing, to modify and mold the sound and 
music content in real-time through their expressive  full-body 
movement and gesture. An implementation of the Orch estra 
Explorer was developed and presented in the framewo rk of the 
science exhibition “Cimenti di Invenzione e Armonia ”, held at 
Casa Paganini, Genova, from October 2006 to January  2007. 
Keywords  
Active listening of music, expressive interfaces, f ull-body motion 
analysis and expressive gesture processing, multimo dal interactive 
systems for music and performing arts applications.  
1. INTRODUCTION 
Nowadays, listening to sound and music is usually s till a passive, 
non-interactive experience. Quoting John Sloboda “I n highly 
industrialized societies, we listen to more music, but we make 
less” [1]. Even modern devices do not allow for int eractive user 
participation. This can be considered a degradation  of traditional 
listening experience, in which the public can inter act in many 
ways with performers to modify the expressive featu res of a piece. 
In this paper, we introduce a new system, the Orche stra Explorer, 
enabling a novel paradigm for active experience of sound and 
music content. With active experience  and active listening  we 
mean that listeners are enabled to interactively op erate on music 
content, by modifying and molding it in real-time w hile listening. 
Active listening is the basic concept for a novel g eneration of 
interactive music systems [2], which are particular ly addressed to 
a general public of beginners, naïve and inexperien ced users, 
rather than to professional musicians and composers . 
The Orchestra Explorer allows users to physically n avigate inside 
a virtual orchestra, to actively explore the music piece the 
orchestra is playing, to modify and mold in real-ti me the music 
performance through expressive full-body movement a nd gesture. 
Concretely, the virtual orchestra is spread over a physical surface. 
By walking and moving on the surface, the user disc overs each 
single instrument and can operate through her expre ssive gestures 
on the music piece the instrument is playing. 
The Orchestra Explorer is not a simple reproduction  or remixing 
of multiple audio tracks, nor a full automatic cond ucting system. 
It is something in between these two extremes. On t he one hand, it 
provides the active listener with means for operati ng on the sound 
and music content that are not available in simple (passive) 
reproduction or remixing of multiple audio tracks. On the other 
hand, it does not provide the full control of the p erformance as in 
traditional conducting systems (e.g., see [3]). Thi s approach is 
motivated by our aim of developing a new paradigm w hich, while 
actively engaging listeners, is at the same time di fferent from 
traditional metaphors such as conducting, and enabl ed by recent 
research on expressive multimodal interfaces. The u ser really 
becomes an explorer of sound and music, i.e., she d iscovers the 
content step by step; she gradually understands how  music 
performance works; she learns how to operate on the  content.  
Section 2 introduces the overall system architectur e; Section 3 
describes the model we designed for interacting wit h space; 
Section 4 and 5 illustrate the implementation of an  Orchestra 
Explorer installation developed for the science exh ibit “Cimenti di 
Invenzione e Armonia”. The conclusions summarize so me issues 
that emerged from such installation.   
2. SYSTEM ARCHITECTURE 
Figure 1 shows the overall architecture of the Orch estra Explorer. 
Since in this paper we wish to put into evidence ho w localization 
and expressive gesture features operate in controll ing real-time 
processing of the input audio stream, we distinguis hed two major 
components of the system: the audio component in th e lower part 
and the control component in the upper part of the figure. The 
system also operates on a video stream and provides  a video 
output, but this is secondary with respect to the a udio output. 
 
Permission to make digital or hard copies of all or  part of this work for 
personal or classroom use is granted without fee pr ovided that copies are 
not made or distributed for profit or commercial ad vantage and that 
copies bear this notice and the full citation on th e first page. To copy 
otherwise, or republish, to post on servers or to r edistribute to lists, 
requires prior specific permission and/or a fee. 
NIME07 , June 7-9, 2007, New York, NY 
Copyright remains with the author(s). 
 
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
56
 
 
 
The input of the audio component is multi-channel a udio. In a 
typical Orchestra Explorer configuration two audio channels 
(stereo recordings) are associated to each instrume nt of the virtual 
orchestra. Recordings of whole sections of an orche stra can also 
be used. Initially, each audio channel is processed  separately, 
keeping however synchronization among tracks. One o r more 
sound processing modules operate on every single au dio channel. 
The sound processing modules receive values of para meters from 
the control component. The processed sound tracks a re then 
mixed in real-time. The mixing process is also cont rolled by the 
control component. Finally, further effects and sou nd processing 
techniques can be applied to the mixed output, agai n controlled in 
real-time by the control component. Typical output is stereo or 
quadrifonic audio, reproduced in the space the user  is navigating. 
Inputs of the control component are images from a v ideocamera 
and possibly data from sensors (e.g., accelerometer s). Images are 
processed by means of computer vision techniques in  order to 
segment the body of the user from the background an d to track in 
real-time her ( x , y ) position on the surface she is walking on (body 
segmentation, localization, and tracking module). T he module for 
interaction with space (models for interaction with  space) receives 
as input such ( x , y ) position, whereas the expressive gesture 
analysis module receives the user position, process ed images (e.g., 
the body silhouette), and possible filtered data fr om sensors. 
The expressive gesture analysis module extracts exp ressive 
features from the incoming images and data. Express ive features 
provide a high-level qualitative description of the  gestures a user 
is performing. For example, Quantity of Motion (QoM ) measures 
the amount of detected motion (related to the energ y of a gesture), 
Contraction Index (CI) measures how much the body i s contracted 
or expanded, Directness Index (DI) measures how muc h a gesture 
trajectory is straight, Stability Index (SI) measur es how much a 
posture is stable. A detailed description of the ex pressive features 
we extracted and used in several applications can b e found in 
[4][5]. Starting from such expressive features, hig her-level aspects 
of gesture can be investigated. For example, in a r ecent study [6] 
we used a set of expressive features of gesture tra jectories for 
classifying expressive gesture according to the Tim e and Space 
dimensions of Rudolf Laban’s Theory of Effort [7][8 ] (i.e., as 
Direct or Flexible, Quick or Sustained). In the sam e way, the 
movement of an active listener in the Orchestra Exp lorer space 
can be characterized as Direct or Flexible, Quick o r Sustained. 
Starting from that, further high-level characteriza tion of user’s 
behavior is possible. For example, a flexible, non- fluid, sustained 
movement can indicate hesitation, whereas direct, q uick, and fluid 
movement can indicate decision and determination.    
The expressive gesture analysis module is organized  according to 
the multi-layered framework for expressive gesture processing [9] 
we developed during the EU-IST Project MEGA (Multis ensory 
Expressive Gesture Applications) and further refine d in the EU-
IST Tai-Chi Project (Tangible Acoustic Interfaces f or Computer –
Human Interaction). 
The module that models interaction with space (disc ussed in 
details in Section 3) computes and superimposes ont o the physical 
space a collection of 2D potential functions. At le ast one potential 
function is associated to each input audio channel for controlling 
real-time mixing. However, further potential functi ons can be 
created for controlling effects and sound processin g modules. 
While the listener navigates the physical space, th e values of such 
potential functions in correspondence of her ( x , y ) position are 
computed. The current values of the potentials asso ciated with 
real-time mixing are used as weights for the sound level of the 
corresponding channel. The current values of the po tentials 
associated with parameters of effects or other soun d processing 
modules operate on the corresponding modules. The e xpressive 
gesture analysis module can dynamically modify the current value 
of the parameters of the potential functions, so th at their profile 
can be molded by the expressive gestures the user p erforms while 
exploring the virtual orchestra.   
The control component can therefore operate on the audio 
component in two different ways (see the different kinds of arrows 
in Figure 1):  
Figure 1. The overall system architecture of the Or chestra Explorer 
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
57
(i) Through a direct mapping of expressive gesture features onto 
sound processing parameters (dotted arrows in Figur e 1). For 
example, a higher value of CI can be associated to a larger 
bandwidth of a band-pass filter, a higher value of QoM to a 
stronger reverb component in the sound. 
(ii) Through the mapping of the current values of t he potential 
functions onto sound processing parameters (dashed arrows in 
Figure 1). E.g., a specific effect can be activated  in a certain 
area of the space only, or hesitating movement can cause the 
instruments to be more spread over the orchestra sp ace. 
3. MODELS FOR INTERACTION WITH 
SPACE 
This Section describes in more details the 2D poten tial functions 
we developed for modeling interaction with space. L et us consider 
for example the 2D potential functions used for con trolling real-
time mixing (remember however that a similar potent ial function 
can also be used for real-time control of audio eff ects). Typically, 
a potential function is associated to each instrume nt of the virtual 
orchestra. The potential function associated to the  k -th instrument 
P k ( x, y ) is given by the weighted sum of two components, a n 
exponential one E k ( x, y ) and a logarithmic one L k ( x, y ): 
 ... N k yxLwyxEwyxP kkkkk 1     ),(),(),( 21 =+=   
where N  is the number of instruments.  
3.1 Exponential component 
The exponential component is computed as a non-norm alized bi-
dimensional Gaussian function, that is: 



















 −+−−−




 −
−
−=
22
2 2
)1 ( 2
1exp),(
yk
yk
yk
yk
xk
xk
k
xk
xk
k
Mkk
yyxxEyxE σ
µ
σ
µ
σ
µρσ
µ
ρ
for k  = 1… N.  
The Gaussian is placed on the 2D surface through th e µ xk  and µ yk 
parameters. The point with coordinates ( µ xk , µ yk ) corresponds to 
the center of the Gaussian, i.e., to its peak value . Note that such 
parameters can be changed in real-time, i.e., the i nstruments can 
be potentially moved around while the user is inter acting with the 
virtual orchestra. For example, it is possible to m ake an 
instrument move according to user’s gestures (e.g.,  by indicating 
toward the right, the user could move toward the ri ght the 
instrument she is listening to). 
Parameters σ xk  and σ yk  control how much the Gaussian is spread 
over the space along the x  and y  directions. Small and similar 
values for σ xk  and σ yk  produce a Gaussian having a peak situated 
in a very localized area of the space, that is the single instruments 
are very separated and do not overlap too much (a k ind of spatial 
staccato). Large and similar values make instead th e single 
instruments to be very spread over the space and ov erlapping each 
other (a kind of spatial legato). Very different va lues of σ xk  and 
σ yk  cause the Gaussian to be elongated along the x  or the y  
direction. 
For example, Figure 2 shows the projections on the ( x , y ) plane of 
two Gaussian potentials localized along a diagonal of the surface 
(dark red color corresponding to high values of the  potential 
function). In case (a) σ xk  and σ yk  are both set to 0.5, in case (b) 
they are both set to 0.1, in case (c) they are both  set to 0.25. As for 
music listening, in case (a) the user will listen t o the two 
instruments separately and in the space in between the two 
instruments no sound will be produced. In case (b) the two 
instruments are highly overlapped, i.e., there is a  wide area in 
between the two peaks where the user will listen to  both 
instruments. In case (c), there is only a small are a where the 
instruments are overlapping, that is, while moving from one 
instrument to the other the user will listen to the  first one, then to 
both of them for a short time, and finally to the s econd one. 
  
 
 
σ xk  and σ yk  can also be modified in real-time according to use r’s 
gestures. For example, the way of approaching an in strument can 
change the way the instrument is spread over the sp ace (e.g., a 
direct and decided gesture may be associated to a n arrow area, 
whereas a flexible, hesitant movement may produce a  wider one). 
Parameter ρ k  controls the orientation of the 2D Gaussian. By 
jointly setting σ xk , σ yk , and ρ k  we can control the elongation of the 
Gaussian along a given direction. Figure 3 shows fo r example a 
Gaussian, localized in the centre of the space, obt ained with σ xk  = 
0.7, σ yk = 0.2, and ρ k  = -0.9. In such a way, it is possible to stretch 
an instrument along space, i.e., by walking along t he stretching 
direction, a user will listen to increasing levels for the instrument, 
until she will reach the peak, and then decreasing levels after the 
peak. Stretching can be the result of an expressive  gesture of the 
user. For example, a repetitive run forth and back along a given 
direction can cause an instrument to be stretched a long that 
direction. 
 
Finally, E Mk  controls the amplitude of the Gaussian, which is 
related to the level of the associated instrument. Usually, E Mk  is 
set so that the maximum value of the overall potent ial function is 
1. However, it could also dynamically change in rea l-time, i.e., 
some instruments could gradually fade out, while ot hers fade in. 
3.2 Logarithmic component 
The logarithmic component is computed as follows: 
( )[ ]


 ≤−++=
 otherwise                                                               0
),( if      ),(1log)1log(),( MkkkMkk
Mkk
Mk
k
dyxdyxddada
L
yxL
 
for k  = 1… N . 
 
(a) (b) (c) 
Figure 2. Effect of different values of parameters σσ σσ xk  and 
σσ σσ yk  on 2D Gaussian potential functions 
Figure 3. Joint effect of 
σσ σσ xk , σσ σσ yk , and ρρ ρρ k  on the 
elongation and 
orientation of 2D 
Gaussian potential 
functions 
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
58
d k ( x , y ) is the (Euclidean) distance of the current positi on of the 
user from the point ( µ xk , µ yk ) where the logarithmic component is 
centered. That is: 
( ) ( )  ... N k yxyxd ykxkk 1     ),(
22
=−+−= µµ  
The point ( µ xk , µ yk ), where the logarithmic component is localized, 
is typically the same point where the Gaussian comp onent is also 
localized. 
d Mk  is the maximum allowed distance from the point ( µ xk , µ yk ). For 
distances larger than d Mk  the logarithmic component is set to 0. By 
default d Mk  is set to the maximum distance that a user can rea ch 
while interacting with the virtual orchestra. That is, if S  is the set 
of points ( x , y ) that can be reached by the user during interactio n, 
d Mk  is computed as: 
),(max
),(
yxdd kSyxMk ∈
=  
In the very usual case of a rectangular surface, d Mk  can be easily 
computed as the maximum distance of the central poi nt ( µ xk , µ yk ) 
form the four corners of the rectangle. 
However, the default choice for d Mk  often needs to be modified 
because of the overall musical result of the Orches tra Explorer. 
Taking d Mk  as the maximum reachable distance would cause an 
instrument to be more or less perceivable in almost  the whole 
space. This is usually not the purpose of the insta llation. Smaller 
values for d Mk  are thus often selected in a way, which is analogo us 
to what we already discussed for σ kx  and σ ky , i.e., small values for 
d Mk  generate localized and separated instruments, whil e high 
values produce spread and overlapping instruments. 
a
k  controls the slope of the logarithmic component. T he slope is 
related to how much gradual the fade in and the fad e out of an 
instrument is while a user enters the area occupied  by the 2D 
potential function. A small value for  a k  produces a smooth slope, 
i.e., a more gradual fade in/fade out of the instru ment. The higher 
is the value of a k , the more sudden is the attack of the instrument. 
Similarly to the other parameters, a k  can also be changed in real 
time, so that the suddenness of the attack of an in strument can be 
related to the user’s expressive motion and gesture . For example, 
repeated impulsive movements can cause an instrumen t to become 
more sudden in its attack 1 . 
Finally, L Mk  controls the amplitude of the logarithmic componen t, 
which is related to the level of the associated ins trument. As for 
the Gaussian component, L Mk  is usually set so that the maximum 
value of the overall potential function is 1. 
Figure 4a shows the overall potential function P
k ( x, y ). The profile 
of P k ( x, y ), obtained for y  = 0.5 is shown in Figure 4b (where the 
continuous line corresponds to the overall potentia l function, the 
dashed line to the Gaussian component, and the dott ed line to the 
logarithmic component). The overall potential funct ion is given 
by the weighted sum of the Gaussian and the logarit hmic 
components. The overall potential function depicted  in Figure 4 is 
                                                                 
1  Note that here we are referring to a kind of “spat ial” attack, i.e., 
the suddenness of the fade in/out while entering in  the area of 
an instrument, which is of course different from th e usual 
“time” attack of sound. 
obtained by setting the two weights to 0.5. The two  components 
are both centered in the middle of the space. 
 
  
(a) (b) 
  
 
The logarithmic component provides a smooth fade in /out while 
the user enters or exits the area covered by the po tential function. 
The Gaussian component produces a strong peak in pr oximity of 
the central point of the potential. In such a way t he user will 
gradually listen to an increasing level of the inst rument while 
entering the area and moving toward the center. She  will then 
listen to it playing loudly, once she reaches the c enter of the area. 
This mechanism provides a stronger perceptual aware ness of what 
is happening. 
4. THE INSTALLATION AT THE 
SCIENCE EXHIBITION “CIMENTI DI 
INVENZIONE E ARMONIA” 
We implemented several instances of the Orchestra E xplorer. The 
main one was developed for the science exhibition “ Cimenti di 
Invenzione e Armonia”, held at Casa Paganini, Genov a, Italy, 
from October 2006 to January 2007. The exhibition w as part of 
“Festival della Scienza”, a huge international scie nce festival held 
in Genova every year. 
The Orchestra Explorer was installed on the stage o f the 250-seats 
auditorium at Casa Paganini, an international cente r of excellence 
for research on sound, music, and new media, where InfoMus Lab 
has its main site. The installation covered a surfa ce of about 9 m ×  
3.5 m. A single vidocamera observed the whole surfa ce from the 
top, about 7 m high, and at a distance of about 10 m from the 
stage (we did not use sensors). Four loudspeakers w ere placed at 
the four corners of the stage for audio output. A w hite screen 
covered the back of the stage for the whole 9 m wid th. A 
videoprojector projected on such screen the video f eedback. 
Lights were set in order to enhance the feeling of immersion for 
the users and to have a homogenous lighting of the stage. 
The music piece “Borderline”, by M. Canepa, L. Cres ta, and A. 
Sacco, was selected for the installation. “Borderli ne” is an original 
piece of film music, which was never performed in p ublic. It does 
not have a strong characterization and it is well s uited as sound 
track for an exploration task. Since the users do n ot know it and it 
does not easily recall other pieces of music, “Bord erline” helps 
the users in paying attention to what they are list ening to. 
“Borderline” consists of 13 stereo audio tracks and  includes the 
following music instruments: harp, cello, horn, flu te, double bass, 
oboe, bassoon, percussions, piano, violins, and alt o pizzicato. The 
instruments were placed over the stage and were ass ociated to 13 
Gaussian/logarithmic potential functions. 
Figure 4. The overall potential function P k ( x, y ) (a) and its 
profile for y = 0.5 (b)  
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
59
A music stand was placed at the center of the stage  where the 
conductor usually takes place. On the music stand a  handmade 
map of the 13 instruments was placed. This helps th e user in 
finding her way while exploring the virtual orchest ra. 
When the user steps on the stage, nothing happens u ntil she goes 
in front of the music stand. Only at that point, th e user can start 
interacting with the orchestra. Contraction Index ( CI) is computed 
separately for the right and left side of the user’ s body. Expanded 
movement on a side makes the instruments on that si de audible. 
For example, if the user stretches her right arm sh e can listen to 
the instruments on her right. At this stage we cont rol separately all 
the 26 mono audio channels. The 26 audio channels w ere pre-
processed so that by just summing them, the final m ix can be 
obtained without further operations. CI on the righ t proportionally 
controls the level of the right channel of the inst ruments on the 
right, while CI on the left proportionally controls  the level of the 
left channel of the instruments on the left. When t he right part of 
the body is fully expanded the user listen to the m ono version of 
the instruments on the right coming from the loudsp eakers on the 
right. Only when the whole body is completely expan ded, the user 
listens to the stereo reproduction of the whole orc hestra. This 
mechanism together with the map the user has in fro nt of her 
makes the user aware of the instruments she can lis ten to and of 
the position where the instruments are approximatel y located. 
The user can now start moving around. While she mov es, real-
time mixing is performed and the level of each inst rument in the 
mixed output is controlled by the associated potent ial function. 
Moreover, expressive movement features are extracte d and used 
for direct control of parameters of sound processin g blocks. For 
example, Quantity of Motion and Contraction Index c an control 
reverberation, so that contracted and slow movement s make the 
sound drier. 
Visual feedback is also related to expressive featu res. Quantity of 
Motion controls the length and thickness of bright spikes in the 
output image and the length of a kind of shadow tha t follows the 
user’s silhouette. However, visual feedback is only  a secondary 
aspect of this installation and it was included mai nly for creating a 
more pleasant environment for the user. 
Figure 5 shows the running installation. Figure 5a shows a visitor 
stretching his arms in the position of the conducto r. Figure 5b 
shows a visitor exploring the orchestra. This shot is taken from a 
position near to the location of the videocamera, t herefore the 
point of view is similar to the point of view of th e videocamera. 
Over 2500 visitors visited “Cimenti di Invenzione e  Armonia”. 
Feedback was in general very positive. For example,  visitors often 
interacted with the system for a time longer than t he duration of 
the music piece. A brief training session, includin g explanations 
and demonstration by the staff, usually helps users  in experiencing 
the installation. In order to have a reliable asses sment, we are 
currently carrying out formal evaluation of usabili ty and 
pleasantness with a sample of subjects. We are also  considering 
analyzing trajectories of subjects e.g., for indivi duating possible 
frequently recurrent patterns in the exploration of  the space.  
The system demonstrated to be highly flexible. It p roved to be 
robust for people of different sizes (e.g., childre n and adults) and 
dressed with different clothes. Some repositioning of the potential 
functions and retuning is needed in case of porting  to another 
space of considerably different size.   
5.  IMPLEMENTATION: THE EYESWEB 
OPEN PLATFORM AND THE EYESWEB 
EXPRESSIVE GESTURE PROCESSING 
LIBRARY 
The instance of Orchestra Explorer we developed for  the exhibit 
“Cimenti di Invenzione e Armonia” was implemented u sing a new 
version of our EyesWeb open platform [9][10]: EyesW eb XMI 
(for eXtended Multimodal Interaction). The EyesWeb open 
platform and related libraries are available for fr ee on the 
EyesWeb website www.eyesweb.org. 
With respect to its predecessors, EyesWeb XMI stron gly enhances 
support to analysis and processing of synchronized streams at 
different sampling rates (e.g., audio, video, data from sensors). 
We exploited such support for the synchronized proc essing and 
reproduction of the 13 audio tracks in “Borderline” . The whole 
Orchestra Explorer installation was implemented as a single 
EyesWeb application (patch), managing audio process ing, video 
processing, extraction of expressive features from movement and 
gestures, real-time audio mixing and control of aud io effects, and 
generation of visual feedback. Every single compone nt of the 
application was implemented as an EyesWeb sub-patch . The 
whole application ran on a single workstation (Dell  Precision 380, 
equipped with two CPUs Pentium 4 3.20 GHz, 1 GB RAM , 
Windows XP Professional). 
Potential functions were implement as EyesWeb modul es (blocks) 
in a new version of the EyesWeb Expressive Gesture Processing 
Library. This library also includes modules for ext raction and 
analysis of expressive features from human full-bod y movement 
and gesture. 
 
(a) 
 
(b) 
Figure 5. The Orchestra Explorer at the science exh ibit 
“Cimenti di Invenzione e Armonia”, Casa Paganini, 
Genova, Italy, October 2006 – January 2007. 
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
60
6. CONCLUSIONS 
From our experience with the Orchestra Explorer, es pecially at the 
science exhibit “Cimenti di Invenzione e Armonia”, several issues 
emerged that need to be taken into account in futur e work. 
The Orchestra Explorer deeply changes the way music  is 
experienced. The exploration of a music piece along  the space 
dimension causes a radical change in the way the sa me music 
piece is experienced along the time dimension. E.g. , whereas the 
duration of “Borderline” is about 3’ 30’’, users us ually spent 
much more time in interacting with the Orchestra Ex plorer. In 
another instance of the Orchestra Explorer we notic ed that an 
excerpt of 30’’ duration, was fruited for more than  10’ without 
becoming annoying. 
The Orchestra Explorer may have a strong impact in applications 
to music education. It provides a unique way to go deeply inside a 
complex music piece, by allowing the user to experi ence at the 
same time the whole piece and the single instrument  (that can 
usually be accessed only by professionals). The ine xperienced 
user can learn, for example, how to distinguish the  timber of 
single instruments and how the single instruments c ontribute to 
the whole music piece. He can also learn that music  instruments 
do not play all at the same time (in the Orchestra Explorer when 
an instrument does not play, entering its area does  not produce 
any sound). 
The Orchestra Explorer involves technical and artis tic aspects. In 
fact, positioning the potential functions on the su rface, setting 
their parameters, choosing the mapping between gest ure features 
and parameters of the potential functions encompass es artistic 
choices that concur in the overall musical result o f the Orchestra 
Explorer. Designing an instance of the Orchestra Ex plorer is 
somewhat similar to creating a completely new music  piece. The 
original music piece is disassembled and recombined  in another 
way, along another dimension (space instead of time ). In this 
perspective, the Orchestra Explored can be consider ed as a kind of 
new composition tool or meta-instrument that allows  beginners to 
access to music composition. As such, it shares sim ilar problems 
with other existing Digital Music Instruments and s ound synthesis 
techniques. For example, the Orchestra Explorer can  be controlled 
through a huge number of low-level parameters. Each  of the 
potential functions we discussed has 9 parameters t hat can be 
modified in real-time. This means that the Orchestr a Explorer at 
“Cimenti di Invenzione e Armonia” had 13 × 9 = 117 parameters 
that could potentially be controlled in real-time. Finding suitable 
and meaningful (e.g., from a perceptual point of vi ew) high-level 
metaphors for managing such a complexity is a topic  which is still 
in the research agenda of the field of gestural con trol of digital 
musical instruments (see for example [11][12]). 
Finally, the Orchestra Explorer provides an example  of the 
concept of active listening of music that could cha racterize future 
devices and paradigms for music experience. We are currently 
developing an Orchestra Explorer that can be experi enced through 
a hand held mobile device. Besides the obviously ne eded changes 
in the interaction metaphors, such a mobile Orchest ra Explorer 
would allow active listening of music at home or in  other personal 
environments without the need of large surfaces or complex video 
tracking systems.   
7. ACKNOWLEDGMENTS 
We thank composer Marco Canepa for providing the mu sic piece 
“Borderline”, for his precious contribution in prep aring the audio 
material, and for the useful discussions about the design and the 
development of the Orchestra Explorer. We also than k our 
colleagues at DIST – InfoMus Lab for their concrete  support to 
this work. Finally, we thank Festival della Scienza  and the visitors 
of the science exhibition “Cimenti di Invenzione e Armonia” 
whose often enthusiast feedback strongly encouraged  us in going 
on with this research. 
8. REFERENCES 
[1] Else, L. The Power of Music - Show Me Emotion. New 
Scientist magazine , 29 November 2003, 40-42. 
[2] Rowe, R. Interactive music systems: Machine listening and 
composition . MIT Press, Cambridge MA, 1993. 
[3] Lee, E., Karrer, T., and Borchers, J. Toward a Framework for 
Interactive Systems to Conduct Digital Audio and Vi deo 
Streams. Computer Music Journal , 30,1  (Spring 2006), 21-
36. 
[4] Camurri A., Mazzarino B., and Volpe G. Expressi ve 
interfaces. Cognition, Technology & Work , 6,1 (Feb. 2004), 
15-22. 
[5] Camurri A., Lagerlöf I., and Volpe G. Recognizi ng Emotion 
from Dance Movement: Comparison of Spectator 
Recognition and Automated Techniques. International 
Journal of Human-Computer Studies , 59,(1-2) (July 2003), 
213-225. 
[6] Camurri A.. and Volpe G. Multimodal and cross-m odal 
analysis of expressive gesture in tangible acoustic  interfaces. 
In Proceedings of the 15th IEEE International Symposiu m 
on Robot and Human Interactive Communication (RO-
MAN2006) (University of Hertfordshire, Hatfield, United 
Kingdom, September 2006), IEEE Press. 
[7] Laban R., and Lawrence F.C. Effort . Macdonald & Evans 
Ltd., London, 1947. 
[8] Laban R., Modern Educational Dance . Macdonald & Evans 
Ltd., London, 1963. 
[9] Camurri A., De Poli G., Leman M., and Volpe G. Toward 
Communicating Expressiveness and Affect in Multimod al 
Interactive Systems for Performing Art and Cultural  
Applications, IEEE Multimedia Magazine , 12,1  (Jan. 2005), 
43-53. 
[10] Camurri A., Coletta P., Massari A., Mazzarino B., Peri M., 
Ricchetti M., Ricci A., and Volpe G. Toward real-ti me 
multimodal processing: EyesWeb 4.0. In Proc. AISB 2004 
Convention: Motion, Emotion and Cognition  (Univeristy of 
Leeds, 2004). 
[11] Hunt, A.D., Paradis, M., and Wanderley, M., Th e importance 
of parameter mapping in electronic instrument desig n. 
Journal of New Music Research,  32, 4 , (December 2003), 
429-440. 
[12] Wanderley, M.M., and Battier, M., (Eds.), Trends in 
Gestural Control of Music.  IRCAM - Centre Georges 
Pompidou, Paris, 2000
Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07), New York, NY, USA
61