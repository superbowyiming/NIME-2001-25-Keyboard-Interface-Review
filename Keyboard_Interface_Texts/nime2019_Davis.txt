IllumiWear: A Fiber-Optic eTextile for MultiMedia Interactions   Josh Urban Davis Department of Computer Science Dartmouth Hanover, NH joshurbandavis@gmail.com 
 
ABSTRACT We present IllumiWear, a novel eTextile prototype that uses fiber optics as interactive input and visual output. Fiber optic cables are separated into bundles and then woven like a basket into a bendable glowing fabric. By equipping light emitting diodes to one side of these bundles and photodiode light intensity sensors to the other, loss of light intensity can be measured when the fabric is bent. The sensing technique of IllumiWear is not only able to discriminate between discreet touch, slight bends, and harsh bends, but also recover the location of deformation. In this way, our computational fabric prototype uses its intrinsic means of visual output (light) as a tool for interactive input. We provide design and implementation details for our prototype as well as a technical evaluation of its effectiveness and limitations as an interactive computational textile. In addition, we examine the potential of this prototype’s interactive capabilities by extending our eTextile to create a tangible user interface for audio and visual manipulation.   Author Keywords Multimodal, fabric sensors, eTextiles, tangible interface, NIME CCS Concepts • Applied computing → Sound and music computing; • Human-centered computing → Haptic Devices; • Hardware → Tactile and hand-based interfaces;   1. INTRODUCTION Electronic fabrics, as opposed to traditional wearable electronics, are deformable and conformable, providing affordances not available to traditional rigid electronics. In this way, eTextiles present the potential to be truly ubiquitous computing platforms in that fabrics are omnipresent in our environment and are capable of deforming into a variety of shapes. Investigations into eTextiles usually refer to two complementary concepts: systems such as Lilypad [5] that are typically used for augmenting existing materials with electronic components, and efforts exist that try to combine the affordances of the textile and the functions of digital circuitry into hybrid eTextiles [26, 31].   
The motivations for working with eTextiles are varied. A recurring theme in the literature is the aesthetic and sensual qualities of fabrics, explored in various artistic installations and research prototypes [25, 30]. The popularization of conductive threads and wearable electronics including Google’s Project Jacquard [1] have renewed the interest in textiles and everyday clothing as an interaction platform. So far, the expressive capabilities of smart fabric have been primarily enabled by onboard electronics and development in sensing systems. More recent contributions have sought to eliminate the intrinsic onboard computing system by leveraging magnetic thread [10]. Others have focused on creating fabrics specifically for expressive purposes. Donneaud et al. focusing on the aesthetic of embroidering conductive thread on a multitouch system [9]. ZStretch presented a multimodal interface that leveraged stretching and multitouch for musical expression [7].   
  Figure 2: (left) IllumiWear prototype implementation. (right) IllumiWear prototype in the dark.  However, most of the works in this field have concentrated on eTextiles as a means of collecting and computing input from users through conductive and piezoresistive sensing techniques. In addition, while previous work has examined the potential of textile-embedded fiber optics as an input technique, little work has been done to examine a textile composed solely of interwoven fiber-optics, nor developed this interest within the domain of musical expression and multi-media interaction [43,44]. The development of such an input modality would be 
Figure 1: Overview of IllumiWear interaction techniques; (A) Multitouch input similar to a midi-keyboard; (B) Varying-pressure touch input for broadening tangible expressiveness; (C) Tangible bending and deformation . 
449
advantageous because the inherent visual properties of light could serve simultaneously as an input as well as an output.  Given the inherent visual properties of such a fabric, extending its functionality to the domain of sound presents an opportunity for rich multimodal interactions. In this paper, we will explore the use of a novel woven fiber-optic eFabric as a means of dual input and output. Our system leverages cost-effective fiber optics equipped with light sensing photodiodes to measure light-loss as the fabric is bent. We then extend this interactive fabric to be used as an interface for musical expression capable audio and visual output. We present IllumiWear, a prototype light sensing fabric as a music and visual input device. While our device is built on a tradition of music input devices, we offer a series of contributions over previous work. These include:  • Description of IllumiWear hardware implementation and prototype design  • Development of a light sensing technique for fiber optic woven eTextiles • Technical evaluation of the micro-bend touch, macro-bend touch, and locality of touch on our prototype eTextile  2. RELATED WORK Optical fibers are cost efficient and easy to manufacture and previous work in the literature indicates that they may serve as an adequate means of gesture input. Early work by Zimmerman, Lanier, Blanchard, Bryson and Harvill used thin fiber optics running down the back of each hand, each with a small crack in it [21]. This project, called “Dataglove”, measured the light shone down the cable so when the fingers are bent light leaks out through the cracks. Measuring light loss gives an accurate reading of hand pose. The Dataglove could measure each joint bend to an accuracy of 5 to 10 degrees, but not the sideways movement of the fingers (finger abduction) [22]. However, the CyberGlove developed by Kramer et al. uses strain gauges placed between the fingers to measure abduction as well as more accurate bend sensing [22]. Since the development of the Dataglove and CyberGlove many other glove based input devices have appeared as described by Sturman and Zeltzer [23]. Additional work has been conducted in application to the other fields, such as robotic motion control [24]. A considerable investigation into the use of interactive fiber optics has been conducted by Hannah Perner-Wilson and Mika Sitomi who have compiled a considerable catalogue of eTextile projects [45]. Given the demonstrated potential of fiber-optics as a material for eTextiles, we wove an eTextile exclusively out of fiber optic threads. In doing so, we increase the potential visual output of the system as well as leverage the means of input as a means of output.   The affordances of textiles or deformable instruments have been well explored within the literature. For example, Gomes, et al [33] explored flex-input for modulation and effect control while Troiano et al. explored what type of input musicians preferred for what type of control [6], suggesting tapping or pushing for sound generation and deformations for sound modulation. Recent interactive fabric-based computing projects have investigated developing new fabrics with additional functionality [3, 11, 15, 16]. Project Jacquard, for example, is a conductive yarn that can be weaved into touch-sensitive fabrics [1]. Ambikraf uses thermo-chromic fabrics to create displays that change color [14,17]. Chen et al. describe a technique for fabricating solar cells using polymer fibers to harvest energy from sunlight and mechanical motion [18].  Other textile-based projects use electronic components to build new interaction modalities on fabric [12,20].  Additional investigations explore 
applications of eTextiles to expand the tools available to fashion designers in manufacturing garments that have additional functional elements [1, 31, 34]. The literature also describes the technical and conceptual accessibility of eTextiles by novice users, making computational fabrics a promising tool to enable makers to prototype their own interactive garments [27, 36, 37]. Significant effort within the research community has described many novel applications, including physical media interactions [1-4]. Numerous works have also examined the effectiveness of eTextiles as interfaces for performance and musical expression [6]. Post et al. initially explored the concept of e-broidery to develop several fabric-based musical interfaces [3]. Other works have developed interaction techniques for deformable fabric musical interfaces such as Zstretch, a bendable fabric musical controller [7], and the Fabric Piezoresistive Multitouch Pad [8]. The usability of such interfaces has also been the topic of investigation. Giovanni et al. conducted user studies on how musicians interact with deformable interfaces [6]. The main results of the studies showed that most of the musician used these deformable interfaces for expressive purposes, particularly to manipulate and filter sound, rather than playing discrete notes. Finally, various methods exist to facilitate fast prototyping of multitouch input devices such as computational fabrics. The most common are capacitive touch sensors. Others have used infrared [39], resistive [35], or even skin-conductance based sensors [28]. In combination with fabric, stretch sensing approaches have also been popular [7, 40]. Methods used in flexible or fabric devices include wire resistive touch [38], two wire resistive touch [41] and a variety of other approaches [30]. Jordà explored the use of visual feedback for tangible instruments in Reactable, and found that certain forms of visual output provided useful information to the artist [46]. Similar to our interests, Karma Chameleon is a smart fabric that changes color in response to ambient light [19]. The sensing technique discussed in this paper is most closely related to our interests, however our system uses fiber optics exclusively as a means of input. We expand on these devices by showing how the data can be used for input to musical devices and as a source of visual feedback.   3. DESIGN AND IMPLEMENTATION To explore this concept of using light as a dual means of input and output, we designed and developed a prototype using off-the-shelf sensors and fiber-optic cables. In order to ensure the feasibility of our design for as many potential user’s as possible, we focused several design decisions on ease of implementation and economical efficiency.   
 Figure 3: Light loss resulting from deformation of woven fiber optic fibers [48].  As a mode of input, we focused on developing detection methods for 3 types of deformation. Micro-bending on fiber optics describes gentle touch input and is best utilized for discrete input gestures such as triggering a button [21]. Macro-bending describes larger deformation gestures such as fiber bends 90 degrees or greater. Since the cables are woven on top of each 
450
other, intersection detection allows our sensor to recover the severity of bend as well as the location of deformation. Our design uses woven fiber optics with a light emitter positioned on one end of the cable and a light sensor positioned on the other. As the fiber optic cable bends, the amount of light registered by the receiver should also subsequently be affected by the cable deformation. By weaving these fiber optics together, we can create a fabric capable of extracting not only the severity of deformation, but also the location. The weaving pattern used in baskets creates a grid-like lattice. Applying pressure to any element of this lattice would not only affect the light-loss of the deformed fiber-optic cable, but also the cables above and beneath the target cable (Figure 3). We leverage this property of fiber optics as an input mechanism to our fabric.   3.1 Hardware Implementation To implement our prototype, we divide all the optic fibers evenly into twenty bundles which are then woven like a basket. This fabrication process forms a ten by ten mesh. Each fiber optic cable is less than a single mm in diameter and each bundle measures approximately 5mm in diameter. Our design requires that propagation light be emitted from one end of each bundle and a light intensity sensor be equipped to the other end of the bundle. Our sensors, therefore must be able to capture light intensity within a modest sampling frequency. To simplify our design, our photodiodes must have built-in transimpedance amplifiers. We chose the TI OPT101J because it not only satisfies all the needs mentioned above but is also sensitive enough to describe optic fiber micro and macro bending. We then assemble all the sensors using two Arduino Unos and breadboards. Two Arduino Unos were used because of equipment constraints and the limited number of ADCs on a single board. Several alternative boards exist which include 16 or more ADCs including Arduino Mega, and Teensy, which may be better suited for replicating this system from scratch. We use dark insulating tapes to wrap the sensor to ensure they won’t be affected by ambient environmental light (Figure 2).  3.2 Sensing Technique The OPT101 is a monolithic photodiode with on-chip transimpedance amplifier (Figure 4). The integrated combination of photodiode and transimpedance amplifier on a single chip eliminates many common design problems such as leakage current errors, noise pick-up, and gain peaking as a result of stray capacitance. Output voltage increases linearly with light intensity. The amplifier is designed for single or dual power-supply operations [42].  
 Figure 4: Schematic for TI OPT101 photodiode sensor with embedded amplifier.  To better understand our expected results from a technical evaluation of our smart fabric, we first examined and evaluated the sensitivity of our sensors. Given 7V power, the sensor could output 5.348V in the indoor light environment and output 
151.8mV when covered with hand. We also connected the sensor to an optic fiber bundle to evaluate sensitivity when enclosed with the cable.  4. TECHNICAL EVALUATION We performed a technical evaluation of our sensing technique in order to understand the following capabilities of our eTextile prototype. First, we wanted to ensure the feasibility of our prototype as an input device. In order to do this, it is necessary to evaluate our sensing technique’s ability to detect micro-bend, macro-bend, and location extraction. Furthermore, it is necessary to examine to what degree micro-bend and macro-bend can be differentiated. Specifically, macro-bend should cause a larger drop in intensity than a micro-bend. We examine not only if our system can detect deformation, but also the severity of the deformation. This is necessary since less severe gestures such as micro-bending can be used as mappings to discrete button inputs and macro-bending can be mapped to larger gestures such as bending or deformation. It is also necessary to investigate if the location of deformation from an optic fiber bend can be detected by examining the intersection of two affected bundles. With this test, we can show that our system is able to output 2D positional coordinates of a bend on the mesh. Our prototype is constituted of a 10 by 10 bundle mesh, so the output is a two dimensional vector from {1, 2, …, 10} x {1, 2, …, 10}.  4.1 Results We conducted four experiments based on the motivation mentioned above. In each experiment, we connected the sensor output channel to multiple I2C routed Arduinos. It was necessary to route the Arduinos using I2C because of the limited analogue input pins of standard Arduino Uno boards. We visualize the data using the Arduino IDE plotting tool and stored the data as a CSV.   4.1.1 Sensing Micro-bend In this experiment, we slightly bend one of the optic fiber bundles in the mesh and measure the values changes. As you see from Figure 5, we have a 7 mV drops when the bend occurs. So even if the bend is slight, the system is able to capture it. The voltage drop depends on the amount of bending induced in the mesh or in the particular strand for which the sensor value drops.  
 Figure 5: Results of light-loss from micro-bending touch on a single fiber optic bundle.  4.1.2 Sensing Macro-bend In this experiment, we bend one of the optic fiber bundles in the mesh harder and measure the values changes. As you see from the Figure 6, we have a 15 mV drops when the bend occurs which is a larger drop than micro-bend. This indicates that the system is able to distinguish between different level of bend on the 
451
mesh.   
 Figure 6: Results of light-loss from macro-bending touch on a single fiber optic bundle.  4.1.3 Intersections  In this experiment, we bend the intersection of two optic fibers in the mesh and measure the values changes for both optic fibers. As you see from the Figure 7, the sensing values of two optic fibers drop simultaneously by approximately the same percentage, indicates the 2D location of the bend. The variance between the two values is significantly different for some intersecting bundles, which is the result of varying number of fiber optics collected within each bundle. In future implementations of this system, ensuring consistent division of fiber optics between these bundles could be helpful for normalizing these results.   
 Figure 7: Results of locality recovery from touch on two intersecting fiber optic bundles with little variance in baseline light output.  5. MAPPING FOR MULTIMEDIA INTERACTIONS Based on the above technical evaluation, we have determined 5 modes of interaction that are possible with IllumiWear. From these 5 gesture inputs, we have developed a system for musical expression by mapping each gesture to trigger or modulate an OSC sample. In this section we will explore these mapping procedures of interaction modalities to multimedia outputs. It should also be noted that these various interaction techniques can be used in isolation or in cooperation with each other. For sound generation and midi-mapping, we used Ableton Live.   
 Figure 8: In-Situ musical applications; (A) Pressure-sensitive touch input similar to a midi-keyboard; (B) Bendable deformation input; (C) Single direction sliding input; (D) Tangible deformation.  5.1 Variable Pressure Touch  The most basic functionality of our system is a mapping of samples to specific fiber locations (Figure 8A). Since intersection locality of pressed bundles is easily distinguishable, a single touch input could trigger a sample on or off, as well as alter the color of the interface itself. This data is mapped to midi by outputting the corresponding values of the lightloss sensors for intersecting bundles X, Y. When the intersection experiences significant lightloss, the note state is altered from on to off an vice versa. This is converted to a note number which outputs the desired audio sample. In practice, this basic technique resembles a keyboard midi-controller or other sample-based instrumentation such as a Launchpad. As demonstrated by our technical evaluation, the system is sensitive to variable pressure. As such, a touch-sensitive midi mapping is also feasible with IllumiWear. This touch sensitivity extends the above single touch input from a simple on/off midi mapping to allow for greater control of sample onset. Similar to the above methodology for midi mapping a single touch input, variable pressure touch recovers the location of the input interaction through an X, Y coordinate. However, unlike single touch interaction, the Boolean on/off note state is replaced with an integer or float which indicates the channel pressure at that given location. This pressure is converted to the desired parametric input for our synthesized sound. We implemented this variable pressure parameter to modulate the onset and attack of the given mapped note to further reinforce the familiarity of a traditional midi keyboard.    5.2 Sliding Gesture  Since the light-loss of each bundle is measured continuously, its localization can be extracted from a given vertical or horizontal set of woven intersections. This allows for a sliding gesture to be recoverable from the mesh (Figure 8C). In addition, a sliding gesture could indicate an alteration of fiber color or intensity. The data is mapped to midi by measuring the lightloss of a series of sensors along a single X or Y row or column. In the horizontal case, a single X lightloss sensor and the corresponding 10 intersecting Y sensors are monitored. In practice, we converted this input into a pitch-bend for a given note.    5.3 Deformation The bending of whole bundles is distinguishable from individual intersection touch by the detection of simultaneous light-loss 
452
across multiple intersecting bundles. As such, deformation-based interactions are a key component of IllumiWear. In addition, the differentiation of various bend angles allows for a degree of variability to be controlled by the deformation gesture. These deformation gestures can take the form of bending gestures, where a large section of the woven fiber is bent (Figure 8B), as well as more complicated deformations, such as grasping a handful of the fabric (Figure 8D). For our purposes, we mapped the bending of the left and right sides of the prototype to panning from the left and right speaker. Full tangible deformations such as slapping or grasping the bundle were left without and specific mapping. This was done to produce an element of playful tangible exploration and discovery through the development of a practice with the instrument.   Table 1: Examples of MIDI message mappings.  Sensing Modality Data No of Sensors Message Type Converted Data Single Touch Bool Int Int 1 x 1 intersection Note On/Off at location X, Y Note State X index Y index Variable Pressure Int/ Float Int Int  
1 x 1 intersection Note Off/On Channel Pressure at Location X, Y 
Channel Pressure X index Y index( Sliding Gesture Int Int 1 x 10 intersection Pitchbend across single horizontal strip  X index Y index(es) Tangible Deformation Bool/Int Int Int 10 x 10 intersection Pitchbend Note Value X index(es) Y index (es) Color Variation Int/Float 1 Note X index  5.4 Color Variation as Input Since the system is highly susceptible to variations in light intensity, alterations of fiber-bundle color can easily be used as an additional input modality. Altering the color of the fiber bundles, say from green to red, will noticeably affect the input parameter of the sin wave, since the light intensity of various colors is distinguishably different. In this way, various colors and their corresponding intensity values could be mapped to various output frequencies. This data is collected as a single integer or float from a single sensor measuring the intensity changes of a light and mapped to the corresponding parameter. For our purposes, we leveraged this input method in two ways. First, we delineated specific intersections as “mode” buttons, altering the state of the midi controller to different sample collections and different colors when pressed. This allowed the visual output of the device to reflect the current sample set. In addition, we also delineated a specific intersection to trigger the cycling of colors, and processed the raw light input as pitch modulation. This second color variation input allowed soundscapes to be aleatorily composed from the light itself until the intersection was triggered again, returning the system to its initial state.   6. CONCLUSION AND FUTURE WORK Computational Fabrics is an ever evolving research area that is rapidly growing. There has been extensive research on usage of such fibers but little research has gone into visible light and how the properties of visible light can be used simultaneously for gathering input from its surroundings and also provide visible output. Electronic fibers provide several properties that can be leveraged for various applications. IllumiWear employed light sensors to detect and recover the location of deformation by leveraging light-loss. This sensing technique was used to create an interactive eTextile made from woven fiber optics. We make use of the light loss principle to pinpoint coordinates on the mesh 
that underwent bending. We finally extend this fabric as a means of musical expression capable audio and visual output. The results of our technical evaluation demonstrated that we can accurately measure significant light loss when gentle or harsh presses are made against the fabric on intersection points of the mesh. We first looked at individual bundles and their corresponding light loss due to bending and then combined these bundle values across the x-y plane to recover deformation location. We also performed additional experiments to determine if bending the optic fibers upwards and downwards produced distinguishable patterns between the two deformations. Unfortunately, we were unable to detect an obvious difference between these two deformations. Both of these depend on the bending of the fiber independent of the direction they are bent. Although prior investigations have made progress in this domain, this remains a significant limitation of the system worthy of further investigation [47]. A major contribution of our research is the development of a system leveraging fibers as both a means of input and output. However, this interaction modality could have applications beyond aesthetic expression. For example, the visual feedback mechanism could be leveraged to ensure proper bow-hold for string musicians such as violinists. Our prototype presents a promising sensing technique for future investigations into deformable electronics and expressive tangible interfaces.  7. REFERENCES [1] I. Poupyrev, N.W. Gong, S. Fukuhara, M.E. Karagozler, C. Schwesig and K.E. Robinson, 2016, May. Project Jacquard: Interactive Digital Textiles at Scale. In Proc. CHI (pp. 4216-4227). ACM. 2.Berzowska, J., 2005.  [2] J. Berzowska, 2005. E-textiles: Wearable computers, reactive fashion, and soft computation. Textile, 3(1), pp.58-75.  [3] E.R. Post, M. Orth, P.R. Russo and N. Gershenfeld, 2000. E-broidery: Design and fabrication of textile-based computing. IBM Sys, 39(3.4), pp.840-860. [4] H. Perner-Wilson, L. Buechley and M. Satomi, 2011, January. Handcrafting textile interfaces from a kit-of-noparts. In Proc. 5th TEI (pp. 61-68). ACM. [5] L. Buechley, M. Eisenberg, J. Catchen, and A. Crockett. The LilyPad Arduino: using computational textiles to investigate engagement, aesthetics, and diversity in computer science education. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08).  [6] G.M. Troiano, E.W. Pedersen and K. Hornbæk. 2015. Deformable Interfaces for Performing Music. Proc. CHI ’15. (2015), 377–386. [7] A. Chang and H. Ishii, 2007, June. Zstretch: a stretchy fabric music controller. In Proc. 7th NIME (pp. 46-49). ACM.  [8] J.S. Roh, Y. Mann, A. Freed and D. Wessel, 2011, June. Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers. In NIME (pp. 393-398).  [9] M. Donneaud, C. Honnet, P. Strohmeier. 2017. Designing a Multi-Touch eTextile for Music Performances [10] J. Chan and S. Gollakota. 2017. Data Storage and Interaction using Magnetized Fabric [11] J. Leong, P. Parzer, F. Perteneder, T. Babic, C. Rendl, A. Vogl, H. Egger, A. Olwal, and M. Haller. 2016. proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 335–346.  [12] M. Mauriello, M. Gubbels, and J. E. Froehlich. Social fabric ftness: the design and evaluation of wearable E-textile displays to support group running. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’14).  
453
[13] M. Orth, R. Post, and E. Cooper. Fabric computing interfaces. In CHI 98 Conference Summary on Human Factors in Computing Systems.  [14] R. L. Peiris, M. J. Tharakan, N. Fernando, and A. D. Chrok. Ambikraf: A nonemissive fabric display for fast changing textile animation. In IFIP 9th International Conference on Embedded and Ubiquitous Computing (EUC ’11).  [15] T. S. Saponas, C. Harrison, and H. Benko. 2011. PocketTouch: through-fabric capacitive touch input. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST ’11).  [16] A. Wang, V. Iyer, V. Talla, J. R. Smith, and S. Gollakota. FM Backscatter: Enabling Connected Cities and Smart Fabrics. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17).  [17] L. Devendorf, J. Lo, N.Howell, J. L. Lee, N. Gong, M. E. Karagozler, S. Fukuhara, I. Poupyrev, E. Paulos, and K. Ryokai. I don’t Want to Wear a Screen: Probing Perceptions of and Possibilities for Dynamic Displays on Clothing. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’16).  [18] J. Chen, Y. Huang, N. Zhang, H. Zou, R. Liu, C. Tao, X. Fan, and Z. L. Wang. Micro-cable structured textile for simultaneously harvesting solar and mechanical energy (Nature Energy ’16).  [19] J. Berzowska and M. Skorobogatiy. Karma Chameleon: Bragg Fiber Jacquard-Woven Photonic Textiles. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’10).  [20] S. Kim, E. Paulos, and M. D Gross. WearAir: expressive t-shirts for air quality sensing. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI ’10).  [21] M. Billinghurst. Chapter 14: GESTURE BASED INTERACTION [22] K. G. Drew, H. Larry, W. Neff. Evaluation of the CyberGlove as a whole-hand input device. ACM Transactions on Computer-Human Interaction. 1995;2:263–283. [23] P. K. Sharma, S. Sharma. Evolution of Hand Gesture Recognition : A Review, 2015 [24] E. Fujiwara, D. Y. Miyatake, M. F. M. dos Santos, C. K. Suzuki. Development of a Glove-Based Optical Fiber Sensor for Applications in Human-Robot Interaction, 2013 [25] B. Bongers and C. Heffer 2015. Pattern Stations - Extending Textile Materials through Tangible Interaction. Proc. TEI ’14. (2015), 405–406. [26] L. Buechley and M. Eisenberg 2009. Fabric PCBs, electronic sequins, and socket buttons: techniques for etextile craft. Personal and Ubiquitous Computing. 13, 2 (Feb. 2009), 133–150.  [27] L. Buechley, N. Elumeze, and M. Eisenberg, 2006. Electronic/computational textiles and children’s crafts. Proc. Interaction Design and Children ’06 (New York, New York, USA, 2006), 49.  [28] J. Burstyn, N. Fellion, P. Strohmeier and R. Vertegaal, 2015. Printput: Resistive and capacitive input widgets for interactive 3D prints. Proc. INTERACT’15 (2015),  [29] F. Davis and Felecia. 2015. The Textility of Emotion. Proc. Creativity and Cognition ’15 (New York, New York, USA, 2015), 23–32.  [30] A. Freed. 2009. Novel and Forgotten Current-steering Techniques for Resistive Multitouch, Duotouch, and Polytouch Position Sensing with Pressure. Proc. NIME ’09. (2009), 230–235.  [31] R. Freire, C. Honnet and P. Strohmeier. 2017. Second Skin : An Exploration of eTextile Stretch Circuits on the Body. TEI ’17 (2017).  
[32] A. Gomes, L. Priyadarshana, J.P. Carrascal and R. Vertegaal. 2016. WhammyPhone: Exploring Tangible Audio Manipulation Using Bend Input on a Flexible Smartphone. Proc UIST ’16 Adjunct (New York, New York, USA, 2016), 159–161. [33] N.A. Hamdan, F. Heller, C. Wacharamanotham, J. Thar and J. Borchers. 2016. Grabrics: A Foldable TwoDimensional Textile Input Controller. Proc. CHI EA ’16 (New York, New York, USA, 2016), 2497–2503.  [34] K. Hartman, J. McConnell, B. Kourtoukov, H. Predko and I. Colpitts-Campbell. 2015. Monarch: Self-Expression Through Wearable Kinetic Textiles. Proc. TEI ’16. (2015),  [35] D. Holman, N. Fellion and R. Vertegaal. 2014. Sensing touch using resistive graphs. Proc. DIS ’14 (New York, New York, USA, Jun. 2014), 195–198.  [36] Y.B. Kafai, E. Lee, K. Searle and D.  Fields. 2014. A Crafts-Oriented Approach to Computing in High School: Introducing Computational Concepts, Practices, and Perspectives with Electronic Textiles. ACM Transactions on Computing Education. 14, 1 (2014), 1–20.  [37] Y.B. Kafai, K. Searle, E. Kaplan, D. Fields, E. Lee and D. Lui. 2013. Cupcake cushions, scooby doo shirts, and soft boomboxes: e-textiles in high school to promote computational concepts, practices, and perceptions. Proc. SIGCSE ’13. (2013), 311–316. [38] A. Schmeder and A. Freed. 2010. Support Vector Machine Learning for Gesture Signal Estimation with a Piezo-Resistive Fabric Touch Surface. Proc. NIME ’10. (2010), 244–249.  [39] P. Strohmeier. 2015. DIY IR Sensors for Augmenting Objects and Human Skin. Proc. AH ’15 (2015), 181–182.  [40] P. Strohmeier, R. Vertegaal and A. Girouard. 2012. With a flick of the wrist: stretch sensors as lightweight input for mobile devices. Proc. TEI ’12 (2012), 307–308.  [41] XY fabric interface | Hackaday: http://hackaday.com/2008/05/30/xy-fabric-interface/. Accessed: 2017-01-27. [42] TEXAS INSTRUMENT: OPT101 Monolithic Photodiode and Single-Supply Transimpedance Amplifier. [43] S. Hashimoto, R. Suzuki, Y. Kamiyama, M. Inami, T. Igarashi. 2013. LightCloth: sensible illuminating optical fiber cloth for creating interactive interfaces. Proc. CHI’13. (2013), 603-606. [44] G. Kakehi, Y. Suguira, A. Withana, C. Lee, N. Nagaya, D. Sakamoto, M. Sugimoto, M. Inami, T. Igarashi. 2011. FuwaFuwa: detecting shape deformation of soft objects using directionally photoreflectivity measurement. Proc. SIGGRAPH’11 (2011) Emerging Technologies. Article No 5. [45] H. Perner-Wilson, M Satomi. Fiber Optic Poetry. How to Get What You Want: eTextile documentation. Kobakant.at [46] S. Jordà. 2008. On stage: The reactable and other music tangibles go real. International Journal on Arts and Technology 1(3). January 2008. DOI: 10.1504/IJART.2008.022363 [47] P. Strohmeier, V. Håkansson, C. Honnet, D. Ashbrook, K. Hornbæk. 2019. Optimizing Pressure Matrices: Interdigitation and Interpolation Methods for Continuous Position Input. Proc. ACM International Conference on Tangible, Embedded, and Embodied Interaction (TEI’19). March 2019. Tempe, AZ [48] W. Wang, W. Ledoux, B. Sangeorzan, P. Reinhall. 2005. A shear and plantar pressure sensor based on fiber-optic bend loss. Journal of Rehabilitation Research & Development. Volume 42, Number 3. 315-326.   
454