Listening to Your Brain: Implicit Interaction in
Collaborative Music Performances
Sebastián Mealla
Music Technology Group
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain
sebastian.mealla@upf.edu
Aleksander Väljamäe
∗
Laboratory of Brain-Computer
Interfaces
Graz University of Technology
Krenngasse 37
8010 Graz, Austria
aleksander.valjamae@tugraz.at
Mathieu Bosi
Music Technology Group
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain
mbosi@gmail.com
Sergi Jordà
Music Technology Group
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain
sergi.jorda@upf.edu
ABSTRACT
The use of physiological signals in Human Computer Inter-
action (HCI) is becoming popular and widespread, mostly
due to sensors miniaturization and advances in real-time
processing. However, most of the studies that use physiology-
based interaction focus on single-user paradigms, and its
usage in collaborative scenarios is still in its beginning. In
this paper we explore how interactive soniﬁcation of brain
and heart signals, and its representation through physical
objects (physiopucks) in a tabletop interface may enhance
motivational and controlling aspects of music collaboration.
A multimodal system is presented, based on an electro-
physiology sensor system and the Reactable, a musical table-
top interface. Performance and motivation variables were
assessed in an experiment involving a test “Physio” group
(N=22) and a control “Placebo” group (N=10). Pairs of
participants used two methods for sound creation: implicit
interaction through physiological signals, and explicit in-
teraction by means of gestural manipulation. The results
showed that pairs in the Physio Group declared less diﬃ-
culty, higher conﬁdence and more symmetric control than
the Placebo Group, where no real-time soniﬁcation was pro-
vided as subjects were using pre-recorded physiological sig-
nal being unaware of it. These results support the feasibility
of introducing physiology-based interaction in multimodal
interfaces for collaborative music generation.
Keywords
Music, Tabletops, Physiopucks, Physiological Computing,
BCI, HCI, Collaboration, CSCW, Multimodal Interfaces.
∗Also at SPECS Laboratory, Universitat Pompeu Fabra.
Roc Boronat 138, 08018 Barcelona, Spain.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’11,30 May–1 June 2011, Oslo, Norway.
Copyright remains with the author(s).
1. INTRODUCTION
In recent years, physiology-based systems have led to im-
plicit models of interaction where the user’s physiological
signals, such as brain waves, electro-dermal activity (EDA)
or heart rate, are monitored, mapped and transformed in
commands to control devices and applications [23]. This in-
teraction paradigm is based on internal states of the human
body and has been explored by diﬀerent disciplines such as
cognitive psychology, neuroscience, physiological comput-
ing, enactive media and HCI. However, most of these stud-
ies are focused on single-user scenarios, either in clinical re-
habilitation [27], or in communication and control applica-
tions [4]. At the same time, the use of electro-physiological
systems in collaborative scenarios and Computer-Supported
Collaborative Work (CSCW) is still scarce.
In this paper we present a collaborative music system
that combines implicit interaction based on physiology sens-
ing and explicit interaction based on a tangible interface
for real-time sound generation and control (Reactable) [13].
This multimodal system displays physiological signals
through sound, graphics and physical objects (physiopucks)
which can be manipulated by physiology emitters and their
partners. We hypothesize that such use of physiological sig-
nals via (physiopucks) will enhance motivational and con-
trolling aspects of music creation in collaborative scenarios.
The study of HCI systems based on the combination of
physiological signals and tabletops has not been widely ex-
plored. We are only aware of two similar studies using phys-
iological signals and tabletops [28] [9], which nonetheless
lack the collaborative and musical aspects that this paper
aims to analyze in order to contribute to the understanding
of such a paradigm.
To assess the eﬀect of physiology-based interaction in
music collaboration using the aforementioned system, task-
oriented experiments between pairs of participants were car-
ried on. Performance and motivational aspects of music
collaboration were assessed using self-report methods.
2. STATE OF THE ART
2.1 Physiology-based Music
In the process of designing a physiology-based interface, spe-
ciﬁc body states are mapped to an explicit display technique
[1]. For example, this can be achieved through interactive
soniﬁcation, which allows the exploration of physiological
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
149
signals by their adaptive transformation into sound [10].
Research on sound and music computing pioneered the
use of bioelectrical signals in interactive systems. Rosen-
boom’s implementations of physiological measures for mu-
sic generation are among the ﬁrst outstanding works in the
ﬁeld. His musical systems presented parameters and tex-
tures driven by electroencephalography (EEG) and heart
rate, among other physiological techniques [25].
More recent research associates EEG-acquired data with
musical imagination [20], leading to new techniques and de-
vices, such as Miranda’s Brain-Computer Music Interface
(BCMI)Piano System that trains the computer to identify
EEG patterns associated with cognitive musical tasks, or
generative systems for music mixing [21]. Finally, neuro-
feedback training systems have been developed in the eﬀort
to enhance music and creative performance [8].
2.2 Electro-physiology Sensor Systems
Conventional electro-physiology systems use electrical con-
ductors to measure electrical signal derived from brain and
body activity. For instance, Brain Computer Interfaces
(BCI) use electrodes placed in the scalp to measure brain
electrical activity (EEG) and transform it into commands
that allow control of devices and applications [23]. There-
fore, it provides a non-muscular communication channel
that has been widely used in clinical rehabilitation. Physio-
logical interfaces may also include the measurement of other
biopotentials diﬀerent to brainwaves, such as electrocardio-
graphy (ECG) or electrooculography (EOG), using a single
device.
2.3 Musical Tabletop Interfaces
There has been a proliferation of musical tabletops in the
past decade. Projects such as the Audiopad [22], the Mu-
sicTable [2] or the Reactable [13], started showing the possi-
bilities and aﬀordances of tangible tabletop musical instru-
ments. Some of these devices are more oriented towards
sound synthesis (e.g. Reactable), some towards composi-
tion (e.g. Xenakis [3]) or sequencing (e.g. Scrapple [18]).
Some are meant for professional or experienced musicians,
while others are more oriented towards education or enter-
tainment (e.g. Zen Waves [7]).
Independently of the many diﬀerences that can exist be-
tween all these systems, scholars tend to agree in the bene-
ﬁts resulting from interacting with these large-scale tangible
and multi-touch devices. Their vast screens make them ex-
cellent candidates for collaborative interaction and shared
control [6], while supporting real-time, multidimensional as
well as explorative interaction. These characteristics also
make tabletops especially suited for both novice and ex-
pert users. Additionally, we think that the visual feedback
possibilities of this type of interfaces, makes them ideal for
understanding and monitoring complex mechanisms, such
as the several simultaneous processes that can take place in
a digital system for music performance [13].
3. SYSTEM ARCHITECTURE
In this paper we present a ﬁrst working prototype of a
multimodal system for collaborative sound generation and
control, combining physiological computing and a tabletop
interface1. This section describes the extraction and pro-
cessing of the physiological signal, the mappings applied for
physiology-based soniﬁcation, its parameters for sound gen-
eration and control, ﬁnishing with the integration with the
Reactable framework.
1Video available on http://www.vimeo.com/14675468
Figure 1: Multimodal Music System. Physiological
signals (red dotted arrows) are wirelessly streamed
to a server that applies a signal processing and soni-
ﬁcation. EEG-based sound synthesis and tempo
control through heart rate are integrated in the Re-
actable framework, and presented to performers as
physiopucks (blue dotted arrows).
3.1 Signal Extraction and Processing
The proposed system uses Starlab’s Enobio for physiologi-
cal signal extraction. Enobio is a wearable, wireless electro-
physiology sensor system that captures three biopotentials:
EEG, ECG and EOG. It features 4 channels connected to
dry active electrodes with a sample rate of 250hz, a reso-
lution of 0.589µv, maximum Signal-to-Noise Ratio of 83db,
a 16-bit Successive-Approximation Register (SAR) Analog-
to-Digital Converter, and an automatic oﬀset compensation
for each channel [26].
Figure 1 describes the system’s design. A dry electrode
is placed on the frontal midline (Fz) lobe of participants
for EEG recording [16]. The electrode for heart rate detec-
tion is placed in the wrist of subjects using a wristband.
Physiological signals are acquired, ampliﬁed and streamed
wirelessly to a server application for processing and soniﬁca-
tion. There, the synchronization is managed by the Enobio
software suite, that applies a digital ﬁlter to reduce noise
(centered between 50 and 60hz) and sends the EEG and
heart rate data to the sound engine. At this stage, a EEG-
based sound synthesis and a tempo control based on heart
rate are computed and streamed to the Reactable frame-
work via a TCP/IP port.
3.2 Sound Engine
In this study, the selection of physiology soniﬁcation meth-
ods had two motivations. First, we wanted to provide feed-
back with minimal delay about changes from diﬀerent fre-
quency bands of EEG. Second, we aimed at easily recog-
nizable soniﬁcation that would stand out from other sounds
generated using a musical tabletop interface.
The system’s sound engine uses a direct mapping between
EEG alpha-theta bands (4-12Hz) and the audible sound fre-
quency spectrum. This mapping was motivated by alpha-
band neurofeedback designs [8]. This EEG processing unit
appears as a sound generator puck (brain-labeled physiop-
uck) on the Reactable. On the other hand, the heart rate
is mapped to another puck to control tempo or beats per
minute (BPM) on the Reactable (heart-labeled physiopuck)
(see Figure 1).
The Pure Data (Pd) computer music system [24] per-
forms the real-time signal analysis and sound synthesis. It
has been chosen due to its openness and suitability for per-
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
150
forming such tasks, and for its ﬂexibility when deﬁning the
mappings. This software also favors a robust integration
with the Reactable framework, whose sound engine has been
built with Pd.
3.2.1 EEG and Heart Rate Signal Processing
The computed magnitude spectrum for each EEG frame is
used to shape the spectrum of a white noise signal. Each
frequency bin is then used to weight the ﬁrst 128 frequency
bins of a 256 bins white noise FFT. Working at 44.1 kHz
for audio synthesis, a frequency range going from 0 Hz to
11025 Hz is covered, with each frequency bin taking about
86 Hz. The spectral magnitudes are equalized by weighting
the chosen curve to emphasize the weaker higher frequen-
cies. The sound resynthesis stage consists of an overlap-add
of the inverse FFT of the weighted and equalized magnitude
spectrum of each consecutive processed EEG signal block
and is entirely handled by the Pd synthesis engine. The
resynthesized audio signal is ﬁnally streamed over a TCP-
IP/LAN connection to a server running the Reactable soft-
ware, where the EEG-based sound synthesis and the heart
rate tempo control are ﬁnally mapped to thephysiopucks.
The heart rate signal is processed by ﬁrst applying an
adaptive rescaling of the system. A two-seconds sliding win-
dow (500 samples) checks for the minimum and maximum
values. Therefore, the signal is normalized depending on
that range. This adaptive approach compensates for the
signal without losing heart rate peak resolution. Peaks in
the heart rate are detected by applying a simple threshold
function. A heartbeat is detected if the normalized signal
is above the 40% of the normalized range. A new heartbeat
is then detected only if this signal falls below 30%.
3.3 Integration into the Reactable
The Reactable’s sound synthesis and control methods fol-
low a modular approach, a prevalent model in electronic
music, which is based on the interconnection of sound gen-
erators and sound processors units. In the Reactable this
is achieved by relating pucks on the surface of the table,
where each puck has a dedicated function for the genera-
tion, modiﬁcation or control of sound. Reactable’s objects
can be categorized into several functional groups such as
audio generators, audio ﬁlters, controllers (which provide
additional control variables to any other object) or global
objects (which aﬀect the behavior of all objects within their
area of inﬂuence) [13]. Each of these families is associated
with a diﬀerent puck shape and can have many diﬀerent
members, each with a distinct and human-readable symbol
on the surface. Because of this modular approach, the inte-
gration of a physiological subsystem into the standard Re-
actable was straighforward. Two new pucks (physiopucks)
were created, allowing the performers to use their physio-
logical signals to generate and control sound, in the same
manner as using standard Reactable objects (see Figure 1).
4. EXPERIMENT
To assess the eﬀect of physiology-based interaction on col-
laborative music experiences, and to evaluate the perfor-
mance of the proposed multimodal system, we designed a
task-oriented experiment of music creation involving two
participants. Each experiment took around 45 minutes and
was designed to measureperformance and motivation using
self-reported ratings. The experiment was conducted in ac-
cordance with the ethical standards laid down in the 1964
Declaration of Helsinki.
4.1 Experimental Setup and Task design
The experiment involved a pair of participants with two
distinct roles: one termed user who operated the Reactable
pucks with her hands, and one termedemitter who manipu-
lated the standard pucks but also provided the physiological
signals for thephysiopucks.
These user-emitter pairs worked with a set of six stan-
dard Reactable pucks plus the two physiopucks. After a
ﬁrst explorative phase, two ﬁve-minute tasks were to be
completed. Each task consisted in replicating 15-seconds
prerecorded music excerpt created with the same pucks that
were available to the participants during the test. Alluser-
emitter pairs listened to the same music reference. Once
the excerpt was played, the user-emitter pair had up to 5
minutes to mimic the sound. The participants were able to
replay the reference at any time by asking the experiment
leader. This task-oriented design was applied to encourage
the user-emitter pair in a music composition process.
During the task, the user manipulated the pucks in the
surface of the Reactable (gestural interaction) whereas the
emitter performed both gesturally and through her own
physiological signals mapped to the physiopucks (implicit
interaction). Physiopucks were available for both emitters
and users to be combined with any of the standard Re-
actable objects.
4.2 Sample and Groups
A total of 32 participants, age mean of 28.09 years old
(SD=3.5), 15 females and 17 males, with no experience us-
ing the Reactable, took part in the experiment. They were
distributed in two groups: a Physio Group (N = 22) where
signals from theemitter were mapped in real-time to the
physiopucks; and a Placebo Group (control group, N = 10)
where physiopucks were driven by pre-recorded EEG and
heart rate signals, thus providing no real feedback to the
user-emitterpairs. Participants were unaware of this eﬀect
and emitters in both groups were told they were control-
ling the physiopucks. The physiological signals used by the
Placebo Group were recorded from a person who composed
the reference music excerpt and were similar to the ones in
Physio Group.
4.3 Measures
Measures were taken using three self-reported tools: (1)
Pre-test questionnaire: demographics, general music knowl-
edge, electronic music skills and Reactable
knowledge; (2) Post-test questionnaire with 10 measures
representing motivation and performance, based on a 5-
points Likert scale ranging from “strongly agre” to “strongly
disagree”, except where noted or implied; (3) Self-Assessment
Manikin (SAM) using 9-points pictorial scale for emotional
valence and arousal [17].
Each measure in the post-test questionnaire contained
from 2 to 5 questions. The measures concerning collabo-
rative performance were based on [12] and involve Feed-
back (M1), Distribution of Control (M2), Social Aﬃnity
(M3) and Nature of the Task (M4). The motivation mea-
sures were based on [11], which describes Curiousity (M5),
Diﬃculty (M6), Conﬁdence (M7) (10-points Likert scale),
Control of the Interface (M8), Motivation (M9) and Satis-
faction (M10). The detailed description of these factors and
questionnaires can be found in [19].
5. RESULTS & DISCUSSION
The ratings from the abovementioned questionnaires were
collected and analyzed for 4 types of participants: physio-
emitters (subjects manipulating pucks and providing physi-
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
151
ological signals for the physiopucks); physio-users (subjects
manipulating pucks and interacting with physio-emitters);
placebo-emitters (subjects manipulating pucks, believing
they were providing physiological signals for thephysiopucks
when those were actually pre-recorded); and placebo-users
(subjects manipulating pucks and interacting with placebo-
emitters). The data was collected through computer-based
questionnaires, and mean was taken over the questions cor-
responding to each measure.
Two analyses were done. First, t-tests were applied to
compare the means between participants within each ex-
perimental group (subsection 5.1) and between them (sub-
section 5.2). Second, the variation of all responses within
each tested pair (emitter and user) was evaluated by ap-
plying a Pearson correlation analysis (subsection 5.3). No
signiﬁcant diﬀerences were found for the demographic data
collected in the pre-test questionnaires.
5.1 Emitters vs. Users Analysis
In these analyses, we compared the diﬀerences between
emitters and users within each experimental group.
5.1.1 Physio Group: Emitters vs. Users
In this analysis only Motivation ratings (M9) was close to
signiﬁcant, t(21) = −1.90,p = .071, with physio-emitters
being more motivated than physio-users. Both types of par-
ticipants reached similar levels ofDiﬃculty (M6), and the
Distribution of Control (M2) did not show signiﬁcant diﬀer-
ence between physio-emitters and physio-users (see Figure
2, left quadrants).
The lack of signiﬁcant diﬀerence for all measures could
be an indicator that both emitters and users within the
Physio Group had a similar experience during collaboration.
Importantly, these factors diﬀered from Placebo Group, as
shown in the next subsection.
5.1.2 Placebo Group: Emitters vs. Users
The analysis showed two results. Diﬃculty ratings (M6)
was signiﬁcant,t(9) = −3.57,p<.01, with placebo-emitters
declaring higher challenge (M = 2.46,SD = 0.18) than
placebo-users ( M = 1.93,SD = 0.27) (see Figure 2, right
quadrants). This may show that placebo-emitters could
perceive that the feedback was not working properly. Sec-
ondly, the analysis unveiled signiﬁcant diﬀerences forDis-
tribution of Control (M2) (t(9) = −2.35,p<.05) as shown
in Figure 2. Placebo Group showed an asymmetric ten-
dency, with placebo-emitters declaring higher Control (M =
2.80,SD = 0.44) than placebo-users ( M = 1.80,SD =
0.83).
A high perception of Diﬃculty from the placebo-emitters
would potentially force them to take a more active role in
“making system work”, forcing placebo-users to give up a
more active role in the control distribution.
5.2 Between Group Analyses
In these analyses, we compared ratings of emitters and users
from diﬀerent experimental groups.
5.2.1 Physio-Users vs. Placebo-Users
Physio-users declared higher Conﬁdence (M7) (M = 5.06,
SD = 1.45) in the task as compared to placebo-users ( M =
3.55,SD = 1.19), t(15) = 2.03,p<. 05. Importantly, while
the settings were identical forusers in both groups, the con-
ﬁdence of placebo-users could be aﬀected by the lack of clear
feedback perceived by placebo-emitters. In a similar man-
ner, the diﬀerence in Distribution of Control (M2) was sig-
niﬁcant between users in both groups, t(15) = 2.6,p<.05.
Operating under the same conditions, physio-users reported
Emitters
Physio Group Placebo Group
Difficulty
Distribution of
Control
Users
Figure 2: Ratings for Diﬃculty (M6) and Distribu-
tion of Control (M2) measures in four participant
types (scale from 0 to 5). Error bars show standard
deviation. See sections 5.1 and 5.2 for the details.
higher Control (M = 3.00,SD = 0.89) than placebo-users
(M = 1.80,SD = 0.83). Figure 2 (lower quadrants) clearly
shows this eﬀect. As mentioned in section 5.1.1, physio-
users did not show an asymmetric Distribution of Control
compared to their physio-emitters. However, this measure
is signiﬁcantly lower for placebo-users compared to placebo-
emitters.
5.2.2 Physio-Emitters vs. Placebo-Emitters
Physio-emitters showed higher Conﬁdence levels (M7),
(M = 4.90,SD = 1.06) compared to placebo-emitters (M =
3.65,SD = 0.96) at t(15) = 2.24,p<.05. Second, placebo-
emitters reported greater Diﬃculty (M6)(M = 2.46; SD =
0.18) than physio-emitters ( M = 1.96,SD = 0.64) t(15) =
−2.37,p < .05. The introduction of a sham pre-recorded
signal for placebo-emitter had a clear eﬀect not only in the
performance and motivation of these participants, but also
in the role of their partners (i.e. placebo-users).
5.3 Correlation analysis
To study in depth the synchronization between user and
emitter in participant’s pair, we applied a correlation anal-
ysis to evaluate the consistency between their responses to
each questions. When all measures were combined together,
both Physio and Placebo groups show high level of response
consistency between user-emitter pairs. Interestingly, when
correlations were analyzed measure by measure, a diﬀerent
picture emerged (see Table 1).
The Feedbackmeasure (M1) showed higher correlation for
Physio pairs ( r = 0.51) than for Placebo pairs (r = 0.25).
In the case of the former, the correlation level shows the
importance that both participants assigned to the audiovi-
sual feedback coming from the system during the collabora-
tive tasks. Placebo pairs responses are almost uncorrelated,
which indicates that placebo-emitters were not able to rec-
ognize the feedback coming from the Reactable, and such a
factor also aﬀected placebo-users collaborating with them.
The correlation analysis of Collaborative nature of the
tasks (M4) showed diﬀerences between Physio and Placebo
user-emitter pairs. Whereas the Physio pairs showed mod-
erate and signiﬁcant correlation between participants (i.e.
there was an agreement on considering the tasks as collabo-
rative), the Placebo pairs’ ratings were not correlated. This
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
152
Table 1: Pearson correlation coeﬃcients of user-
emitter pairs responses for Physio and Placebo
Groups (*signiﬁcance at 0.05, **at 0.01, ***at 0.005
level)
Measures Physio Placebo
All measures 0.80*** 0.68***
Feedback (M1) 0.51** 0.25
Distribution of Control (M2) 0.51 0.53
Social Aﬃnity (M3) 0.52** 0.64
Nature of Task (M4) 0.41** 0.21
Curiousity (M5) 0.49*** 0.63*
Diﬃculty (M6) 0.43* 0.72*
Conﬁdence (M7) 0.81*** 0.51***
Control of the Interface (M8) 0.11 0.42
Motivation (M9) 0.34 0.03
Satisfaction (M10) 0.62*** 0.62**
Arousal 0.25 0.6
Valence 0.13 -0.17
result supports the feasibility of physiology-based interac-
tion for music collaboration.
For Diﬃculty measure (M6), ratings from user-emitter
pairs were highly correlated in Placebo but not for Physio
group (see Figure 1). Interestingly, valence-arousal rat-
ings were not highly correlated except arousal ratings for
Placebo group, which corroborates the results for diﬃculty
measure.
Measure of Control of the Interface (M8) showed moder-
ate correlation for Placebo, but not for Physio Group. To-
gether with a signiﬁcant asymmetry between emitters and
users in the Placebo Group when running the t-test, this
shows that this asymmetry was consistent among its user-
emitter pairs.
Finally, Motivation measure (M9) were almost uncorre-
lated between user-emitter pairs in both groups. This is
especially interesting for the Placebo Group, as it shows a
tendency to lose interest in the performance during collab-
oration.
6. GENERAL DISCUSSION
The presented results highlight speciﬁc aspects of a sys-
tem that combines implicit, physiology-based and explicit,
tabletop-based interaction in music collaboration. Similar
levels of rated diﬃculty and strong correlation of conﬁdence
ratings for user-emitter pairs in Physiology group show that
this new multimodal system do not impose major diﬃcul-
ties for music collaboration. On the contrary, the similar
ratings of distribution of control - a fundamental factor for
assessing the symmetry of music collaboration - that were
given by the Physio Group (but not Placebo) show that the
proposed implicit interaction model encouraged symmetric
music collaboration between the participants.
The results also show that placebo-emitters expressed
higher levels of diﬃculty and lower levels of conﬁdence.
While such experiences were expected for participants who
where provided with a fake biofeedback, it is notable how
these aﬀected the experience of their partners, placebo-
users. As an example, we can mention the signiﬁcantly
lower level of conﬁdence in placebo-users as compared to
physio-users, regardless them both operating the system in
the same conditions. This reciprocity eﬀect in the perfor-
mance of participants has to be taken into account in the
design of multimodal interfaces for music collaboration.
The experiment also helps to understand the perceptual
aspects of display techniques based on physiological signals.
The scores corresponding to audiovisual feedback reached a
high correlation in the Physio Group, but not in the Placebo
Group. This indicates that the participants were able to
perceive whether the feedback from the soniﬁcation engine
and the Reactable graphical interface was linked to their
physiological signals or not. This factor is particularly in-
teresting for collaborative music performances, as it shows
that a direct mapping between EEG spectral bands and the
audible sound frequency spectrum is eﬀective as an identiﬁ-
able auditory display. It also unveils that both emitters and
users were able to recognize the sound processes driven by
physiological signals, within a multimodal musical interface
that included other control paradigms (e.g. gestural input).
However, the musical expressivity arising from such design
has to be further explored, as discussed in the next section.
6.1 Future Work
Several potential upgrades for the system are foreseen. First,
alternative EEG sensing devices can be used in order to im-
prove signal acquisition and cover other regions of the brain.
Second, regardless the fact that subjects did not perceive
signiﬁcant latency when running the experiment, a better
communication protocol can be applied to improve the con-
nectivity between modules and reduce latency, for instance
by using Open Sound Control (OSC). Finally, other soniﬁ-
cation mappings can be applied in order to achieve higher
musical expressiveness and intuitiveness. Designs based on
adaptive systems can be envisioned, where physiological sig-
nals are monitored only covertly, in absence of user’s inten-
tional control. Such collaborative system could then pas-
sively monitor performers’ perceptual, cognitive and emo-
tional states and use real-time machine learning methods
for adaptive multisensory feedback. [5] [15] [14].
Future experiments can be complemented with time mea-
sures (e.g., how long does it take to complete a task us-
ing the system), physiological measures (recording of EEG,
ECG and EDA) that characterize psychophysiological states,
visual recording for behavioral observation (gestures, fa-
cial expressions), qualitative data from the participants and
similarity metrics between the sound references and recorded
trials. Importantly, future studies will involve pairs ofemit-
ters performing together, instead of a user-emitter design.
This will allow to study physiological synchronization be-
tween performers. Finally, to assess the musical possibilities
of the multimodal system, experiments with professional
musicians can be carried on, given their previous training.
7. CONCLUSIONS
Physiological computing in collaborative HCI applications
is a rapidly developing ﬁeld of research that require new
experimental paradigms and methodologies. This paper
presents a multimodal system for music collaboration, and
a methodology for assessing participants’ performance and
motivation. The analysis has shown that the combination
of implicit, physiology-based and explicit, tangible interac-
tion is (a) feasible for participants collaborating in music
composition, and (b) that it preserves a balanced distribu-
tion of control between collaborators. These results strongly
support the use of physiological interfaces for music collabo-
ration, as they can lead to meaningful and novel experiences
in the ﬁeld of CSCW and music creation. Together with the
creation and control of sounds, brain and body signals may
be powerful indicators of performer’s emotional and cogni-
tive states during collaboration, guiding music anticipation
and interpersonal synchronization.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
153
8. ACKNOWLEDGMENTS
This work was supported by TEC2010-11599-E and MAEC-
AECID. We want to thank Arnau Espinoza, Carles F. Juli` a,
Daniel Gallardo and Eliza-Nefeli Tsaoussi for their advice.
9. REFERENCES
[1] J. Allanson and S. Fairclough. A research agenda for
physiological computing. Interacting with Computers ,
16(5):857–878, october 2004.
[2] R. Berry, M. Makino, N. Hikawa, and M. Suzuki. The
augmented composer project: The music table. In
Proceedings of the 2nd IEEE/ACM International
Symposium on Mixed and Augmented Reality , page
338. IEEE Computer Society, 2003.
[3] M. Bischof, B. Conradi, P. Lachenmaier, K. Linde,
M. Meier, P. P¨otzl, and E. Andr´ e. Combining tangible
interaction with probability-based musical
composition. In Proceedings of the 2nd international
conference on Tangible and embedded interaction,
pages 121–124. ACM, 2008.
[4] G. Edlinger, C. Holzner, C. Groenegress, C. Guger,
and M. Slater. Goal-oriented control with
brain-computer interface. In D. Schmorrow,
I. Estabrooke, and M. Grootjen, editors, Foundations
of Augmented Cognition. Neuroergonomics and
Operational Neuroscience, pages 732–740. Springer
Berlin / Heidelberg, 2009.
[5] S. H. Fairclough, K. M. Gilleade, L. E. Nacke, and
R. L. Mandryk. Brain and body interfaces : Designing
for meaningful interaction. InCHI 2011, pages 1–4,
2011.
[6] Y. Fernaeus, J. Tholander, and M. Jonsson. Beyond
representations: towards an action-centric perspective
on tangible interaction.International Journal of Arts
and Technology, 1(3):249–267, 2008.
[7] E. Glinert, N. Kakikuchi, J. Furtado, T. Wang, and
B. Howel. Zen waves. Tangible Media Group, MIT
Media Lab, Boston, 2008.
[8] J. Gruzelier. A theory of alpha/theta neurofeedback,
creative performance enhancement, long distance
functional connectivity and psychological integration.
Journal of Neurophysiology, 10:101–9, 2009.
[9] T. Hermann, T. Bovermann, E. Riedenklau, and
H. Ritter. Tangible computing for interactive
soniﬁcation of multivariate data. In 2nd International
Workshop on Interactive Soniﬁcation, pages 1–5,
York, UK, 2007.
[10] T. Hermann and A. Hunt. The discipline of interactive
soniﬁcation. Proceedings of the Int. Workshop on
Interactive Soniﬁcation, pages 1–9, 2004.
[11] K. Issroﬀ and T. del Soldato. Incorporating
motivation into computer-supported collaborative
learning. In Proceedings of European conference on
artiﬁcial intelligence in education , 2006.
[12] A. Jones and K. Issroﬀ. Learning technologies:
Aﬀective and social issues in computer-supported
collaborative learning.Computers & Education ,
44(4):395–408, 2005.
[13] S. Jord` a. On stage: the reactable and other musical
tangibles go real. International Journal of Arts and
Technology, 1(3/4):268–287, 2008.
[14] M. Kaipainen, N. Ravaja, P. Tikka, R. Vuori,
R. Pugliese, and M. Rapino. Enactive Systems and
Enactive Media. Embodied human - machine coupling
beyond interfaces.Leonardo, 5(44), 2011.
[15] A. Y. Kaplan, J.-G. Byeon, J.-J. Lim, K.-S. Jin, and
B.-W. Park. Unconscious Operant Conditioning in
the Paradigm of Brain-Computer Interface Based on
Color Perception.Intern. J. Neuroscience,
115(1):781–802, 2005.
[16] J. D. Kropotov. Quantitative EEG, Event Related
Potentials and Neurotherapy. Academic Press, San
Diego, CA, 2009.
[17] P. J. Lang. Behavioral treatment and bio-behavioral
assessment: computer applications. In J. J.
J.B Sidowski and T.A.Williams, editors,Technology
in Mental Health Care Delivery Systems , pages
119–137. 2005.
[18] G. Levini. The table is the score: An
augmented-reality interface for real-time, tangible,
spectrographic performance. InProceedings of ICMC.
School of Art, Carnegie Mellon University, 2006.
[19] S. Mealla. Eﬀects of physiology-based interaction in
collaborative experiences, 2010.
[20] E. Miranda, S. Roberts, and M. Stokes. On
generating eeg for controlling musical systems.
Biomedizinische Technik, 49(1):75–76, 2004.
[21] E. R. Miranda and V. Soucaret. Mix-it-yourself with
a brain-computer music interface. In ICDVRAT,
pages 1–7, Maia, Portugal, 2008.
[22] J. Patten, B. Recht, and H. Ishii. Audiopad: a
tag-based interface for musical performance. In
Proceedings of the 2002 conference on New interfaces
for musical expression, pages 1–6. National University
of Singapore, 2002.
[23] G. Pfurtscheller, B. Z. Allison, C. Brunner,
G. Bauernfeind, T. Solis-Escalante, R. Scherer, T. O.
Zander, G. Mueller-Putz, C. Neuper, and
N. Birbaumer. The Hybrid BCI. Frontiers in
neuroscience, 4(April):42, 2010.
[24] M. Puckette. Max at seventeen. Computer Music
Journal, 24(4):31–43, Winter 2002.
[25] D. Rosenboom. Extended musical interface with the
human nervous system. Leonardo Monograph Series.
International Society for the Arts, Sciences and
Technology (ISAST), (1), 1997.
[26] Starlab. Enobio User Guide. Starlab, Barcelona, 2010.
[27] J. Wolpaw, N. Birbaumer, D. McFarland,
G. Pfurtscheller, and T. Vaughan. Brain-computer
interfaces for communication and control. Clinical
neurophysiology, 113(6):767–791, 2002.
[28] B. F. Yuksel, M. Donnerer, J. Tompkin, and A. Steed.
A novel brain-computer interface using a multi-touch
surface. Proceedings of the 28th international
conference on Human factors in Computing Systems -
CHI ’10, page 855, 2010.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
154