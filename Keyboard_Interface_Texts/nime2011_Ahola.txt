Raja - A Multidisciplinary Artistic Performance
Tom Ahola
Nokia Research Center
Itämerenkau 11-13
00180 Helsinki, Finland
tom.m.ahola@nokia.com
Koray Tahiro˘glu
Aalto University,
School of Art and Design,
Department of Media
PO box 31000 00076 Aalto,
Finland
koray.tahiroglu@aalto.ﬁ
Teemu Ahmaniemi
Nokia Research Center
Itämerenkau 11-13
00180 Helsinki, Finland
teemu.ahmaniemi@nokia.com
Fabio Belloni
Nokia Research Center
Otakaari 5, Espoo, Finland
fabio.belloni@nokia.com
Ville Ranki
Nokia Research Center
Otakaari 5, Espoo, Finland
ville.v.ranki@nokia.com
ABSTRACT
Motion-based interactive systems have long been utilized
in contemporary dance performances. These performances
bring new insight to sound-action experiences in multidisci-
plinary art forms. This paper discusses the related technol-
ogy within the framework of the dance piece,Raja. The per-
formance set up of Raja gives a possibility to use two com-
plementary tracking systems and two alternative choices for
motion sensors in real-time audio-visual synthesis.
Keywords
raja, performance, dance, motion sensor, accelerometer, gyro,
positioning, soniﬁcation, pure data, visualization, Qt
1. INTRODUCTION
Raja is a Finnish word and means border. We wanted to
cross the border and tear down walls between interdisci-
plinary teams by joining technology and diﬀerent art forms
together. Raja is a new fusion example of technology, dance,
sound design and computer graphics. It has been so far per-
formed three times, in Tampere, Helsinki and London.
Human motion has been used to control sound in many
diﬀerent ways before. Commonly used technologies for de-
tecting motion, position, proximity and gestures include
camera based systems[3], light sensors, ultrasonic sensors[6],
capacitive proximity sensors, piezo triggers, gyros[1] and ac-
celerometers. The scope can be anything from tracking one
solo performer to tracking large audiences[4]. The level of
motion tracking varies from bistable acceleration triggers
to accurate gesture recognition. The novelty in the Raja
performance compared to other systems is the simultane-
ous use of two diﬀerent tracking systems: motion and posi-
tion. Also, there is a possibility to use both accelerometer
and gyro for motion tracking. In the following sections the
overview of the dance, the technical system, motion track-
ing and positioning technologies are presented. Strategies
for sound synthesis and visualization are described in detail.
The paper concludes with a presentation of outcomes and
discussion about the future development of Raja.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’11,30 May–1 June 2011, Oslo, Norway.
Copyright remains with the author(s).
2. THE CHOREOGRAPHY
The choreography of the Raja dance performance for three
dancers (Figure 1) was sketched by a professional choreog-
rapher in an iterative process. Initially, the choreographer
was briefed about the possibilities of the technologies to
be used. At this stage, the choreographer created the ﬁrst
version of the choreography, which was used as a basis for
further development.
Figure 1: The Raja dancers.
In the second phase, the sensor conﬁguration was selected:
the dancers would each wear one motion tracker on their
right wrist and one positioning tag in their hair. This selec-
tion had an impact into some details of the choreography.
For example, the hand positions and movements were em-
phasized in order to provide a better signal for the sound
synthesis. In addition, the movements of the dancers were
modiﬁed so that their position on the dance ﬂoor would
have a greater spread and variety. The ﬁrst version of the
sound design was also introduced.
The third version of the performance was modiﬁed from
the second version based on feedback from the audience.
The performance was divided into three parts where diﬀer-
ent sets of sounds were used to create an interesting contrast
between the parts. The middle part was slow and peace-
ful with some improvisation. The performance ended with
a short aggressive section where the dancers were dancing
very intensively.
3. THE TECHNICAL SYSTEM
The technical conﬁguration (Figure 2) of the system in-
cludes three laptop computers, networked together via an
Ethernet switch. The indoor positioning, motion sensing
and sound synthesis systems were each using their own lap-
top. In theory, everything could run in a single computer.
However, to be able to monitor and control the diﬀerent
technologies at the same time, it was more convenient that
each team had their own laptop.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
433
Figure 2: The performance system setup.
3.1 Indoor Positioning
High Accuracy Indoor Positioning (HAIP) technology de-
veloped in Nokia Research Center Helsinki Laboratory was
used in the system[2]. HAIP provides position estimation
of small battery powered transmitter tags by measuring the
direction of the received UHF radio signals received by a re-
ceiver. The tags are worn by the dancers and the receiver is
mounted above the dance ﬂoor. Several tags can be tracked
with one receiver system. The accuracy of the position-
ing system is limited by the fact that the radio signal is
blocked by the human body. Thus, the tags should prefer-
ably be mounted in the head area and the receiver should
be mounted high up to have the best possible connection.
In practice, the usable area is directly below the receiver so
that the radius of the area is approximately 1.2 times the
diﬀerence between the receiver and the tag height.
3.2 Motion Sensors
Ariane sensor-box motion sensors were used in the system.
These are wireless sensor devices designed in the Advanced
Systems Engineering department of Nokia Research Cen-
ter. The sensor-box features accelerometer, gyro and mag-
netometer sensors, each with 3 axes. In addition they have
two buttons, a light sensor, a barometer, a RGB LED, a
vibro-tactile actuator and Bluetooth wireless connectivity.
In Raja a 4g range is used for the accelerometer and the
range of the gyro is 600 ◦/s.
For the reception of sensor data from the motion sensors,
an application called SensorPerformer was implemented.
This application includes the sensor processing algorithms,
a network interface to stream motion sensor data out, a
network interface to receive position data, a visualization
engine and a sound engine. The application was coded in
C++ using the Qt multi-platform application development
framework.
4. SENSOR SIGNAL PROCESSING
4.1 Accelerometer
4.1.1 Effect of gravity
The accelerometer was chosen as the motion sensor for the
ﬁrst version of the system. The challenge with an accelerom-
eter for motion sensing is that it is also sensitive to the grav-
itational force. An algorithm was developed to remove the
gravitation component from the sensor signal so that linear
motion of the device could be used as the input to sound
generation and visual presentation.
A device (Figure 3(a)) is aﬀected by gravitational accel-
eration ag. Motion related linear acceleration am sums to
the ag to give a total acceleration a, which is sensed by the
accelerometer. We are only interested in motion and thus,
if we know the gravitation we can remove it from the sensor
signal and we getam = a −ag.
(a)
 (b)
Figure 3: Total acceleration vector during linear
movement (a) is the sum of gravitational acceler-
ation and acceleration of the movement. Rotation
(b) causes a change in the acceleration vector.
However, rotation of the device (Figure 3(b)) causes the
gravity vector to change. In the initial orientation the grav-
itational acceleration is ag0 and after rotation of angle ϕ
the acceleration is ag1 . Thus, if we at some point assume
gravity is ag0 and remove it from the sensor signal to de-
tect linear motion, after a rotation of ϕ the diﬀerence ac-
celeration ar = ag1 −ag0 , is erroneously detected as mo-
tion. The magnitude of this error can be calculated as
|ar|2 = 2g2(1 −cos ϕ), where g = |ag0 |= |ag1 |. A signiﬁ-
cant visible movement has an acceleration of ∆ |ar|= 0.2g,
or more. An almost unnoticeable rotation of only ∆ ϕ =
cos−1(1 −∆|ar|2/(2g2)) ≈11◦ results in such a change in
the movement acceleration vector magnitude.
4.1.2 Removal of gravity
The red trace in Figure 4 shows the magnitude of the total
acceleration vector measured from the wrist during a short
segment of dance. In the beginning there are slow moves
which build up to an energetic section. One can see that
the acceleration approaches the 1g gravity in still parts.
Figure 4: Accelerometer signal.
The removal of the gravity component is described in the
diagram in Figure 5. Each orthogonal axis x, yand z of the
acceleration vector is processed identically according to this
algorithm. First the estimated gravity is subtracted from
the signal so that the acceleration caused by movement re-
mains. This signal is low-pass ﬁltered to reduce noise before
it is diﬀerentiated for detection of change. A comparator
checks if the change is above a ﬁxed threshold.
In moments of stillness the change signal is below the
threshold and a switch enables integration of the movement
acceleration. This integrator output is the estimate of the
gravity component as the closed loop with negative feedback
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
434
Figure 5: Gravity removal algorithm and velocity
computation.
will settle so that the integrator input will be nulled. The
blue trace in Figure 4 shows the motion acceleration, which
is the result of gravity removal. It can be noticed that in
most still moments the acceleration signal is now close to
zero.
4.1.3 Computation of velocity
A velocity value is a more useful parameter because in most,
if not all, natural sound generation processes the sound is
related to the velocity of some mechanical part of the in-
strument, or air ﬂow velocity in case of wind instruments.
Acceleration happens mostly at the beginning of a move
and at the end of a move, in the opposite direction. This
would mean two separate signals for one sound gesture. By
integrating the acceleration a velocity signal is generated
with a smooth attack, sustain and decay.
During active movement the change signal (Figure 5) is
above the threshold and a switch connects the motion ac-
celeration signal to an integrator, which outputs a velocity
value. In moments of stillness a switch leaks the integrator
to reduce velocity oﬀsets caused by asymmetry in the accel-
eration signal. Figure 6 shows the linear velocity computed
from the acceleration shown in Figure 4.
Figure 6: Velocity signal.
4.2 Gyro
We decided to use the gyro sensor instead of the accelerom-
eter in the second performance set up as it is not inﬂuenced
by gravity and the output is a velocity. We believed this
would result in a more responsive and predictable sound
representation of movement. The angular velocity trace in
Figure 6 shows the gyro signal vector magnitude from the
same wrist and performance as the accelerometer signals.
Using the gyro signal was indeed found to be more respon-
sive. A signiﬁcant diﬀerence was observed, however, in the
dynamic range. The gyro signal appears to be only lit-
tle stronger during the energetic sections compared to the
calm moments. We believe this is due to human physiology
and motorics. Energetic movement results in only little in-
crease in the rotational velocity of the wrist although linear
acceleration can increase signiﬁcantly.
5. AUDIO SYNTHESIS
The audio synthesis was implemented with Pure Data (PD).
Sounds can be controlled by either angular or linear velocity,
chosen independently between sensors. Three synthesized
instruments were created, one for each dancer. The ampli-
tude of each instrument is controlled by the vector magni-
tude of the velocity data so that the sound level correlates
to intensity of movement.
The ﬁrst instrument, Frequency Synthesis Module, maps
musical textures with glassy, oscillating sounds. Eight digi-
tal oscillators’ frequencies are controlled by the three veloc-
ity values from the 3-axis motion sensor. The 3-to-8 diver-
gent mapping of the control signals is implemented so that
each velocity controls each frequency with a certain weight.
These weights were experimentally and artistically selected.
The second instrument, Wave Module is a polyphonic
sampler with eight sample-voices. The magnitude of the
velocity data applies parameter changes to the transforma-
tions of sampled sounds, controlling the playback rate of
the polyphonic sampler. Resulted output implies dry, me-
chanical sounds with ﬁltered pitch tonality.
The third instrument, Sin Module, is a frequency modu-
lated oscillator. It generates a cosine wave with the ampli-
tude controlled by an envelope generator. The magnitude
of the velocity data is mapped to frequency and the envelop
generator values. The output is streamed to a cosine wave-
shaper and ﬁltered by a voltage controlled bandpass. The
instrument generates brassy, sharp sounds with the dancer’s
movements.
In the third version of the Raja performance, a second
set of instruments was introduced. The aim was to make
this set towards more easy listening using classical instru-
ment sounds. Piano samples were used for this set, which
was used in the slower and more relaxed middle part of the
performance, while keeping the electronic sound set in the
beginning and end parts. The magnitude of the velocity
data is scaled to integer numbers between 0 and 12. The
generated values are mapped to MIDI note values in har-
monic C minor scale.
Two separate background instruments were designed to
support each section in the Raja performance. Both back-
ground instruments are sample based and create a dynamic
rhythmic pattern. While the ﬁrst background instrument
creates certain tones continuously including the fundamen-
tal frequency in the piano section, the second one creates
sounds containing inharmonic clusters of partials.
The positions of the dancers are mapped to the spatial-
ization of each instrument. The system supports stereo or
multichannel speaker systems. VBAP technology is used
to control the direction of the audio stream in the perfor-
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
435
mance[5].
6. VISUALIZATION
An early version of the visualization was based on the idea
that it would become a painting of the dance performance.
Every movement and position of each of the dancers would
be recorded in the painting and in theory the performance
could be reconstructed from the painting afterwards. How-
ever, the visualization in this way was believed to be too
chaotic for the audience to understand the correlation be-
tween dancers’ actions and what was happening on the
screen. To move closer to being a real-time illustration of
position and movement the image was made to continuously
fade away so that the more current graphical draw would
stand out.
The ﬁnal visualization design is implemented as follows.
Three graphical objects of diﬀerent color represent the three
dancers. The positions of the objects on the screen are a
direct mapping of the positions of the dancers on the dance
ﬂoor. The objects can morph from a small dot to a large
8-bladed star when the dancers go from motionless to full
motion. The star also rotates, with rotation direction and
speed correlating with the sensed motion velocity. As a
result each dancer is drawing a colored trace on the drawing
canvas with size and texture matching the energy of the
moves (Figure 7). And when the dancers stop, the traces
fade away and only dots remain to indicate their position on
the ﬂoor. The visualization was implemented using polygon
drawing methods of Qt.
Figure 7: Visualization screenshot
7. THE EXPERIENCE OF THE DANCERS
We wanted to investigate how the dancers experienced the
interactive sound production by asking them to ﬁll in a
questionnaire with three aspects: 1) implementation of the
choreography when compared to a typical contemporary
dance performance, 2) sensibility of the improvisation sec-
tion and 3) the predictability of the sounds and implications
of it on the dance.
From the dancers’ perspective the main diﬀerence to a
typical contemporary dance was the lack of the support
from the music. Because it was missing, more communi-
cation between the dancers was required. Performing the
movements simultaneously was more diﬃcult because the
synchronization had to be based only on visual communi-
cation between the dancers. On the other hand, missing
time line of the music gave more temporal freedom in im-
plementing the choreography, for example, the length of a
pause.
The improvisation was considered to be the most pleasant
part of the dance, especially because it used the classical
piano sounds. After learning how the sounds respond to
the movements, it was easier to modify the improvisation
to produce certain type of sound patterns.
All the dancers mentioned that initially the sounds were
somewhat irresponsive, predictability was weak and in some
parts the sounds were not in harmony with the movement.
These concerns, however, were dispelled when the project
proceeded.
8. CONCLUSIONS
Comparing gyro and accelerometer sensors it was found in
practice that the gyro sensor was more responsive and pre-
dictable. However, the dynamics of the gyro was ﬂatter.
The accelerometer gave a better match between the energy
of movement and energy of the sound. Gravity removal and
velocity computation from the acceleration signal is approx-
imate and the cause of the drawbacks of the accelerometer.
Human physiology, on the other hand, is the probable cause
of reduced dynamics of using the gyro. We are currently
working on an improved algorithm using both gyro and ac-
celerometer to achieve a solution with beneﬁts from both
sensors.
The approach to produce sound by movement was inter-
esting for the dancers. They especially liked the improvi-
sation part and experienced it as an additional channel to
express movements. When the dancers got freedom to plan
the movements together with the sounds, the overall expe-
rience became more harmonious. Thus, the choreography
and the sounds should be designed in parallel.
One of the future directions to improve Raja is to enhance
the sound-action strategies. By recognizing diﬀerent types
of movements and mapping that to a sound synthesis engine
with a richer set of timbral and temporal parameters a more
intriguing experience for both performers and audience can
be created.
9. ACKNOWLEDGMENTS
The authors would like to warmly thank the support from
Tekes (HEI project) and the Academy of Finland (pr. 137646).
Without this support this multidisciplinary collaboration
work would not have been possible. We also thank chore-
ographer Jari Saarelainen and dancers Kiia Elonen, Nora
Laitinen, Mia-Mari Sinkkonen and Nina Kivisilta for their
great creative co-operation.
10. REFERENCES
[1] R. Aylward and J. A. Paradiso. Sensemble: A wireless,
compact, multi-user sensor system for interactive
dance. Proceedings of the 2006 International
Conference on New Interfaces for Musical Expression
(NIME06), Paris, France, pages 134–139, 2006.
[2] F. Belloni, V. Ranki, A. Kainulainen, and A. Richter.
Angle–based indoor positioning system for open indoor
environments.Proceeding of Workshop on Positioning,
Navigation and Communication (WPNC), Hannover,
Germany, 2009.
[3] M. Ciglar. A full-body gesture recognition system and
its integration in the composition ”3rd. pole”. ICMC,
2008.
[4] M. Feldmeier and J. A. Paradiso. An interactive music
environment for large groups with giveaway wireless
motion sensors.Computer Music Journal, 31(1):50–67,
Spring 2007.
[5] V. Pulkki and M. Karjalainen. Multichannel audio
rendering using amplitude panning. Signal Processing
Magazine, 25(3):118–122, 2008.
[6] F. Vogt, G. McCaig, M. A. Ali, and S. Fels. Tongue ’n’
groove: An ultrasound based controller. Proceedings of
New Instruments for Musical Expression (NIME),
Dublin, Ireland, 2002.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
436