SMuSIM: a Prototype of Multichannel Spatialization
System with Multimodal Interaction Interface
Matteo Bozzolan
Department of Electronic Music
Conservatory of Music G.Verdi
Como, Italy
matteo.bozzolan@alice.it
Giovanni Cospito
Department of Electronic Music
Conservatory of Music G.Verdi
Como, Italy
giovanni.cospito@fastwebnet.it
ABSTRACT
The continuous evolutions in the human-computer inter-
faces ﬁeld have allowed the development of control devices
that let have a more and more intuitive, gestural and non-
invasive interaction.
Such devices ﬁnd a natural employment also in the music
applied informatics and in particular in the electronic music,
always searching for new expressive means.
This paper presents a prototype of a system for the real-
time control of sound spatialization in a multichannel con-
ﬁguration with a multimodal interaction interface. The spa-
tializer, called SMuSIM, employs interaction devices that
range from the simple and well-established mouse and key-
board to a classical gaming used joystick (gamepad), ﬁnally
exploiting more advanced and innovative typologies based
on image analysis (as a webcam).
Keywords
Sound spatialization, multimodal interaction, interaction
interfaces, EyesWeb, Pure data.
1. INTRODUCTION
Technology and music have always had a particular re-
lationship and aﬃnity. In particular, the researches and
experimentations in the ﬁelds of electricity ﬁrst and then
of the electronics and informatics have allowed, in the last
two centuries, the birth of a series of instruments for a new
musical expressivity.
Besides, thanks to a more and more available computa-
tional power associated with the development of new tech-
nics and technologies for the human gestuality acquisition
and analysis, new ways have opened in the ﬁeld of the
human-computer interaction, allowing so the birth of a new
generation of interfaces that ﬁnd a natural employment even
in music applications.
As reported in [11], the most widespread interaction de-
vices currently used are (in an increasing order of complex-
ity): PC keyboard, mouse, joystick, MIDI keyboard, video
camera, touchpad, touchscreen, 3D input devices (data glo-
ves, electromagnetic trakers) or haptic devices.
This paper shows the results of the experimentation of
some of these interfaces for the realization of a system for
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME08, Genova, Italy
Copyright 2008 Copyright remains with the author(s).
the multichannel spatialization of sound sources. In par-
ticular the devices explored in this work are:mouse and
keyboard (very simple and primitive), agamepad (classical
gaming joystick) and awebcam (low cost USB camera, that
allows, through image analysis techniques, a totally non-
invasive and free-hand interaction).
In respect of the sound spatialization, the proposed proto-
type provides a quadriphonic sound diﬀusion and allows to
control up to four independent sound sources. The spatial-
ization technique implemented is the well-known Amplitude
Panning extended to the multichannel case. This choice of
simplicity ﬁnd its motivation in the fact that the primary
aim of this work is the investigation on the interaction in-
terfaces rather than the implementation of advanced spa-
tialization algorithms. The sound projection space can be
artiﬁcially altered by controlling the direct to reverberated
signal ratio.
2. RELATED WORKS
Although the use of spatial sound is present since from
the origins of the music and it appears many times in clas-
sical western music, it becomes a fundamental practice and
a key aesthetical element mainly from the second half of the
past century (ﬁrst thanks to the development of sound dif-
fusion electrical devices and then because of the revolution
of electronic and digital sound systems). For brevity, in this
section are presented only some of the most recent works in
the ﬁeld of real-time sound spatialization digital systems.
A ﬁrst example isMidiSpace [6], a system for the spatial-
ization of MIDI sound ﬁles in virtual environments realized
at the end of the ’90s at the Sony Computer Science Lab
in Paris. It is one of the earliest sound spatialization ex-
periments in 3D worlds and it gives the user two distinct
graphic interfaces to control the application: the ﬁrst one
(bidimensional) allows to displace the various sound sources
(identiﬁed by a set of musical instruments) in the projection
space, while the second one (three-dimensional and realized
in VRML) controls the movements of an avatar in the vir-
tual world. The spatialization technique is the two-channel
Amplitude Panning and the interaction devices are mouse
and keyboard.
A more recent work is represented byViMiC [3], a real-
time system for the gestural control of spatialization for
small ensamble of players. It belongs to the wider project
Gesture Control of Spatializationstarted in the 2005 at the
McGill University IDMIL Lab (Montreal, Canada). It’s
very interesting because it allows the user to control the dis-
placement of the sound sources simply by moving his hands
in the air (thanks to a complex apparatus for movements
interpretation and codiﬁcation called Gesture Description
Interchange Format). A set of 8 sensors (connected with
an electromagnetic tracking system) is applied to the two
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
24
hand of the player.
Zirkonium [9] is a software implemented to control the
spatialization within the Klangdom system at the ZKM
(Germany). The Klangdom is formed by 39 speaker and it
can be controlled byZirkonium through mouse and joystick.
It implements various spatialization algorithms (Wave Field
Synthesis, Ambisonics, Vector Base Amplitude Panning e
Sound Surface panning) and it allows the user to deﬁne an
arbitrary number of resources1 to spatialize in the concert
hall. The system is controlled through a simple graphic
interface.
Challenging Bodies[4] is a complex multidisciplinary pro-
ject for live-performances of disabled people realized at the
Informatics and Music Department of the Regina Univer-
sity (Canada). Within this wide project, theRITZ system,
through various techniques, allows to frontally spatialize up
to 10 input signals coming from musical instruments with 7
loudspeaker placed in front of the players. Its control inter-
face is made up by two windows: the ﬁrst one, implemented
in GEM2, supplies a graphical feedback of the loudspeakers
conﬁguration and it allows to modify the position of the
sound sources in the space, while the second one, the main
control patch implemented in Pure Data, gives the user the
possibility to set the relative and absolute sound levels. The
system is hardly oriented to scalability and usability.
The last example is the work recently proposed by Scha-
cher [10] at the ICMST of the Zurich university (Switzer-
land). It consists of a design methodology and of a set of
tools for the gestural control of sound sources in surround
environments. The spatialization is made through a struc-
tured and formalized analysis that allows to map the player
gestures on the sources movements by applying various ty-
pologies of geometric transformations. From the point of
view of the input devices, the system does not have a con-
solidated structure, but the interfaces used up to now spaces
from data gloves equipped with multiple sensors (pressure,
position, bending) to haptic arms and graphic and multi-
touch tablets. The spatialization algorithms used are the
Ambisonics and the Vector Based Amplitude Panning.
3. IMPLEMENTATION: SMUSIM
SMuSIM is a multichannel sound spatialization system
with a multiple and multimodal interaction interface. It
is designed for real-time applications in musical expressive
contexts (electronic music spatialization, distributed and
collaborative network performances).
Figure 1: The system’s architecture.
In this ﬁrst implementation, the speaker are supposed to
be arranged in the spatialization room in a typical quadri-
1A resource is a set of one or more audio sources coming
from an audio ﬁle, a network stream or any audio device.
2http://gem.iem.at
phonic conﬁguration with the 4 loudspeaker placed at the
corners of the room. The projection space can be artiﬁ-
cially extended and modiﬁed by controlling the direct to
reverbereted signal ratio (for the creation of illusory acous-
tic spaces).
The spatializer allows the player to control up to four
simultaneous sound sources and a graphical feedback gives
the instantaneous state of the system.
The system oﬀers a set of functionalities that allows a
complete and eﬃcient control of the spatialization and in
particular: a punctual and precise placement of the sound
sources in the space, the control of relative and absolute
volume levels, the automatization of the movements, a non-
linear interpolation of the position of the sources in time and
the possibility to load pre-recorded sound ﬁles or to acquire
signals coming from a microphone or any audio device.
As shown in Figure 1, the system has been implemented
in Pure Data3 (and its graphical interface GrIPD4) and
EyesWeb5 (with the creation ofad hoc additional blocks)
communicating through theOSC6 protocol, making soSMu-
SIM a native network distributed application (both with
one ore more instances on several machines, allowing mul-
tiple distributed conﬁgurations).
3.1 Interaction interfaces
The prototype oﬀers three diﬀerent typologies of human-
computer interaction devices for the spatialization’s con-
trol. Keyboard and mouseare the simplest and the most
widespread ones. The user controls the di ﬀusion of the
sound sources in the space through a combination of actions
and commands coming from the PC keyboard and from the
mouse. In this case the system provides (in addition to
the visual feedback window) a bidimensional graphic envi-
ronment where the player can put and move some graphic
objects representing the diﬀerent sound sources.
Figure 2: Input devices used forSMuSIM.
The second device is agamepad, a classical gaming con-
troller with two axis and ten buttons freely conﬁgurable.
The very compact dimensions and the ergonomicity make
the devices very usable and allows a great playability.
The last interface is a standard low-cost USB webcam
that acquires the movements of a set of colored objects.
Each physical object (through a color-based tracking algo-
rithm) is associated to a sound object in the sound projec-
tion space.
The player can use one ore more devices at the same
time (allowing a collaborative and multi-user performance).
The proposed interfaces are deliberately simple, cheap and
3http://www.puredata.org
4http://www.eyesweb.org
5http://crca.ucsd.edu/˜jsarlo/gripd/
6http://opensoundcontrol.org
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
25
widely available on the market in order to let the system
easily usable and accessible to any user level.
3.2 Software components structure
As shown in Figure 3, the application is composed by
some functional units that perform the various needed tasks.
Data coming from the input devices are acquired, for-
matted and analyzed by theDevice controller unit that is
constituted by other 4 sub-units, one for each input device.
In particular Mouse/Keyboard controller supplies a gra-
phic window (interaction environment) where the user can
displace the four objects representing the sound sources
with the mouse. A set of keyboard key combinations allows
to perform a set of predeﬁned actions (shifting of single or
groups of sources, maintaining or not their topological con-
ﬁguration, loading/saving default conﬁgurations, etc.).
Figure 3: Diagram of the software functionalities
implemented in SMuSIM.
Joystick controllerallows to control the spatializer with a
standard 2-axis and 12-buttons gamepad. The interface be-
tween the gamepad and the spatializer is managed through
GrIPD, that provides all the needed functionalities. But-
tons are used to select the sources to be controlled, while
the two analog mini-sticks determine the changes in their
position and volume. With this device it is easy to control
more than one source at the same time7. Webcam con-
troller manages data coming from the video acquisition de-
vice. The interaction paradigm in this case is the following:
the webcam ﬁlms a plane and neutral colored surface on
which are placed the objects to be tracked; the webcam’s
ﬁeld of view correspond to the diﬀusion space and the po-
sition of the colored objects determines the displacement of
the sound sources on the sound projection space. The unit
provides a set of tools for the real-time selection of the de-
sired color to track (simply by picking it out on a window
showing the webcam video stream) and for the extraction of
centroids and bounding boxes of the color blobs. Bounding
boxes are used to set the volume levels of each source (a
vertical position stands for maximum volume, a horizontal
one for mute). The MIDI controller block processes data
coming from an optional MIDI device (both hardware and
software).
The source movements can be automatized thanks to the
Automatization unit, while the position’s changes of sound
sources are made not instantaneous throughInterpolator,
that generates a smoothed and decelerated motion by a non-
linear interpolation of subsequent positional data.
Spatializer is the unit that performs the computation
7Thanks to the compact size and to the ergonomicity of
the gamepad, that allows the contemporaneous pressure of
more than one button at the same time.
of the attenuation levels to apply to the audio signals on
each channel. The spatialization technique is the Ampli-
tude Panning extended to the multichannel case. On the
basis of the positional data of the virtual sources coming
from the input devices, a monophonic signal (considering a
single source) is applied to the various channels with a gain
factor as follows:
xi(t)= gix(t),i =1 , ..., N
where xi(t) is the signal to apply to the loudspeakeri, gi
the gain factor of the correspondent channel,N the cardi-
nality of the loudspeaker andt the time. The gain factorgi
has a non-linear proportionality with the position (x, y) of
a single sound source in the space. To overcome the 6dB at-
tenuation at the center of the projection space, a quadratic
sinusoidal compensation curve is applied along the two di-
mensions. By considering all the sound sources involved,
the resulting signalX(t) can ﬁnally be deﬁned as:
X(t)=
K∑
j=1
xi(t)
where K is the maximum number of sound sources in-
volved in the spatialization (K = 4 on the speciﬁc case of
SMuSIM ).
The graphic and audio feedback production is managed
respectively by theGraphics display and Sound production
units. The last one prepares the audio stream to send to
the loudspeakers. It essentially manages the reverberation
algorithm by applying it to the resulting signal coming from
the combination of the original audio stream (furnished by
the Audio streaming/playback unit) and the spatialization
data, allowing in this way the creation of illusory acous-
tic spaces. By controlling the balance between the direct
and reverberated signal independently for each channel, it
is possible, besides increasing the overall distances percep-
tion, to deform the sound projection environment (by act-
ing along one ore more dimensions of the room). Currently
the functionalities of this unit are extremely limited in view
of a future integration of a sound synthesis engine for the
real-time generation of sounds.
4. FUTURE WORK
The system developed is still in a prototypal phase and
has some limitations that can be easily improved. First
it can be interesting to test some other interaction inter-
faces (to enlarge the multimodality issue) such as more
performative cameras (higher frame-rate, infrared lighting)
or other technologies for the exploitation of the gestural
control of the instrument (electro-magnetic or ultra-sound
tracking systems, data gloves). A study is currently ac-
tive for the exploration of touch-sensible interfaces (graphic
tablets, multitouch and painterly interfaces).
A second improvement refers to the spatialization tech-
nique, given that in this ﬁrst phase of the project it has
not been the crucial aspect of the work. The simple Ampli-
tude Panning technique can be replaced by more complex
and eﬃcient algorithm such as the Vector Based Amplitude
Panning, Ambisonics extended to a multichannel conﬁgu-
ration and Wave Field Synthesis.
Another key issue is represented by the performances of
the system that are the main requisite of the application in
contexts of real-time musical performances. In fact there are
actually some latency problems in the conﬁguration with
the webcam running particularly on not high performances
machines or notebooks. This could be resolved by improv-
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
26
ing and optimizing both the tracking algorithm and the vi-
sual feedback production (in case abandoning the EyesWeb
and Pd platforms and realizing an integrated, stand-alone
and dedicated software application).
From the point of view of the automatization, it does not
provide any way of interaction with the player, but it is
an autonomous and isolated modality. It could be inter-
esting the implementation of rules for pattern learning and
reproduction in order to let the system able to imitate and
continue a performance initially guided by a human user.
Other possible developments could refer to the diﬀusion
system (increasing the number of loudspeakers and their
conﬁguration) and to the integration of a sound synthesis
engine within the application.
During this ﬁrst phase of the work there was not enough
space for an intensive and structured test session on a large
and heterogeneous set of users. However a hypothetical
evaluation experiment has been predisposed for a future
use.
The experiment has a total duration of about 45 minutes
and it is composed by six sections:
1) free trial of the instrument (10 min) without any expla-
nation about the working principles of the system (the user
has previously read a short user manual)
2) supervised test (10 min) in which the user has to execute
some tasks evaluated by the operators
3) explanation of the working principles (5 min) by an op-
erator in order to increase the consciousness of control of
the spatialization instrument and to accelerate the learning
process
4) repetition of the test (10 min) after the explanations of
the operator
5) questionnaire (5 min) of evaluation compiled by the user
6) interview (5 min) in which the operators deepen some
aspects appeared during the test.
The two proposed tests contains list a of 21 tasks (for each
test) that the user has to execute. Each task receives a
mark according to a ﬁve point Likert-scale (1: not exe-
cuted, 5: executed at the ﬁrst trial). The tasks are sorted
by the increasing level of diﬃculty and they are intended
to test most of the functionalities of the instrument and its
expressive possibilities. The questionnaire presents 22 ques-
tions divided into 5 categories: usability of the system (8),
learnability (3), audio feedback (3), visual feedback (4) and
overall opinion (4). Also in the questionnaire the players
has to give a mark according to a ﬁve point Likert-scale (1:
bad, 5: very good).
5. CONCLUSIONS
A real-time sound sources spatialization system with a
multimodal interaction interface has been developed.
The interaction interfaces have been realized with very
simple and inexpensive technologies and devices, that have
nevertheless shown satisfactory expressive and interaction
possibilities. In particular the best results came out, as
expected, with the gamepad and the webcam, devices that
allow more freedom in movements and a more intuitive and
natural interaction. Moreover the webcam let the user move
independently each sound source (action impossible with
both the mouse and the gamepad). On the other hand, the
performances are one of the key aspects associated with this
last kind of device, because of the computational load of the
image analysis techniques that make the real-time issue a
crucial aspect of the application.
In general even all the graphic rendering operations for
the creation of the visual feedback are particularly oner-
ous for the overall performances of the system. Under this
consideration, the graphic feedback proposed to the user is
quite simple and thin, but it results very eﬃcient and let
have the actual state of the sound sources in the diﬀusion
space always under control.
From the point of view of the sound spatialization, the
Amplitude Panning technique produces the expected re-
sults. It is very eﬃcient, it does not have problems of com-
putational complexity and it is easily conﬁgurable to the
various executive and technical contexts (customization of
the panning curves and of the number of diﬀusion channels).
Even if an intensive and large scale test session has still to
be conducted,SMuSIM has shown good results in terms of
learnability, intuitivity and expressiveness. There are vari-
ous possible developments of this work and they refer both
to software and hardware issues (input devices, diﬀusion
system) and applicative and musical aspects.
6. REFERENCES
[1] A. Camurri et al. Toward real-time multimodal
processing: EyesWeb 4. InProceedings of the
Convention on Motion, Emotion and Cognition
(AISB04), Leeds, UK, 2004.
[2] J. M. Chowning. The simulation of moving sound
sources. InJournal of the Audio Engineering Society,
volume 19, pages 2–6, 1971.
[3] M. Marshall, Wanderley, et al. On the development of
a system for the gesture control of spatialization. In
Proceedings of the 2006 International Computer
Music Conference (ICMC06), pages 360–366, New
Orleans, USA, 2006.
[4] J. Nixdorf and D. Gerhard. Real-time sound source
spatialization as used in Challenging Bodies:
implementation and performance. InProceedings of
the 2006 International Conference on New Interfaces
for Musical Expression (NIME06), pages 318–321,
Paris, France, 2006.
[5] N. Orio, N. Schnell, and M. M. Wanderlay. Input
devices for musical expression: borrowing tools from
HCI. InProceedings of the 2001 International
Conference on New Interfaces for Musical Expression
(NIME01), 2001.
[6] F. Pachet and O. Delerue. A mixed 2D/3D interface
for music spatialization . InProceedings of the First
International Conference on Virtual Worlds, pages
298–307, Paris, France, 1998.
[7] M. Puckette. Pure Data: another integrated computer
music environment. InProceedings of the 1996
International Computer Music Conference (ICMC96),
pages 269–272, Hong Kong, China, 1996.
[8] V. Pulkki. Spatial sound generation and perception
by amplitude panning techniques. Graduation thesis,
Helsinki University of Technology, Laboratory of
Acoustics and Audio Signal Processing, 2001.
[9] C. Ramakrishnan, J. Gossmann, and L. Brummer.
The ZKM Klangdom. InProceedings of the 2006
International Conference on New Interfaces for
Musical Expression (NIME06), pages 140–143, Paris,
France, 2006.
[10] J. C. Schacher. Gesture control of sounds in 3D space.
In Proceedings of the 2007 International Conference
on New Interfaces for Musical Expression (NIME07),
pages 358–361, New York, USA, 2007.
[11] L. Schomaker, A. Camurri, et al. A taxonomy of
multimodal interaction in the human information
processing system. Technical report, Nijmegen
University, 1995.
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
27