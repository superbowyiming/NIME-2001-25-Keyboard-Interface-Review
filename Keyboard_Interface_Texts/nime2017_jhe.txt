Towards Related-Dedicated Input Devices for 
Parametrically Rich Mechatronic Musical Instruments 
 
 
Jingyin He  
Division of Humanities 
Yale-NUS College 
Singapore 
jon.he@yale-nus.edu.sg 
 
Jim Murphy  
New Zealand School of 
Music 
Victoria University of 
Wellington 
Wellington, New Zealand 
jim.murphy@vuw.ac.nz 
 
Dale A. Carnegie  
School of Engineering and 
Computer Science 
Victoria University of 
Wellington 
Wellington, New Zealand  
dale.carnegie@affiliation.org 
 
Ajay Kapur  
School of Engineering and 
Computer Science 
Victoria University of 
Wellington 
Wellington, New Zealand  
ajay@karmetik.edu
   
ABSTRACT 
In the recent years, mechatronic musical instruments (MMI) have 
become increasingly parametrically rich . Researchers have 
developed different interaction strategies to negotiate the challenge of 
interfacing with each of the MMI’s high-resolution parameters in real 
time. While mapping strategies hold an important aspect of the 
musical interaction paradigm for MMI, attention on dedicated input 
devices to perform these instruments live should not be neglected. 
This paper presents the findings of a user study conducted with 
participants possessing specialized musicianship skills for MMI 
music performance and composition. Study participants are given 
three musical tasks to complete using a mechatronic chordophone 
with high dimensionality of control via different musical input 
interfaces (one input device at a time). This representative user study 
reveals the features of related-dedicated input controllers, how they 
compare against the typical MIDI keyboard/sequencer paradigm in 
human-MMI interaction, and provide an indication of the musical 
function that expert users prefer for each input interface. 
 
Author Keywords 
human-robot interaction; wearable sensor interface; musical robotics; 
mechatronic musical instruments; user study 
 
ACM Classification 
H.5.2 [Information Interfaces and Presentation] User Interfaces— 
Evaluation/methodology, H.5.2 [Information Interfaces and 
Presentation] User Interfaces --- Input devices and strategies  
1. INTRODUCTION 
In recent years, mechatronic musical ins truments (MMI) have 
become increasingly parametrically rich [16]. This increase in the 
number of user accessible parameters presents the potential of greater 
amounts of musical expression to composers and performers [18]. 
While many early musical robots presented users with a small 
number of discrete parameters, some current systems allow for many 
continuous parameters to be affected [6]. Consequently, this increase 
in parameters comes with an increase in the level of difficulty of 
interfacing with each of the high-resolution parameters in real time, 
in the context that the MMIs behaves like a musical instrument rather 
than as an autonomous agent. This is one of the major challenges 
facing users and creators of parametrically dense mechatronic 
musical instruments. To address this problem, new control systems, 
which include new custom input devices as well as mapping 
strategies between a human performer and a parametrically-dense 
MMI, must be explored. While mapping strategies hold an important 
aspect of the musical interaction paradigm for MMI, attention on 
related-dedicated input devices to perform these instruments live 
should not be neglected. A r ecent user study on using new 
mechatronic musical instruments conducted by Murphy et al. expert 
users revealed that there exists a need for related-dedicated human-
to-mechatronic input interfaces [10]. Two questions emerged when 
designing these new specialized musical interfaces:  
1. How does new custom input devices, specifically gestural 
controllers, compare to the conventional control paradigm of 
MIDI keyboard and sequencer in this human -mechatronic 
musical interaction? 
2. What are the key factors that contribute towards the 
compelling musical interactions, mediated by dedicated input 
devices, between the composer/performer and the new 
mechatronic musical instruments? 
 To answer these questions, a user study is conducted. The main 
goal of the study is to gather information from users pertaining to 
their experiences in using different musical interfaces to control 
parametrically dense mechatronic musical instruments. An enhanced 
understanding of users’ experiences will provide an indication to 
which device p erforms best in compositio nal and performative 
contexts, together with insights into the key factors that afford 
intuitive, idiomatic, and highly embodied live interactions between 
human and MMIs  in a digital musical instrument model . 
Consequently, this will facilitate research in the design, development, 
and evaluation of new musical interfaces for new mechatronic 
musical instruments in composition and live performance. 
 This paper begins with a brief introduction to recent trends  in 
MMIs and interaction strategies. Thereafter, the user study design, 
protocol, procedure, and results are described in the fol lowing 
sections. Finally, t his paper concludes with a discussion and 
describes future works that may arise. 
2. INTERACTING WITH MECHATRONIC 
MUSICAL INSTRUMENTS 
The majority of existent research in human-robot musical interaction 
remains limited to mapping schemes rather than the input devices 
[15, 18]. While researchers in the field have created specialized input 
devices to control mechatronic musical instruments [6, 7], most of 
these devices are developed for use with mechatronic instruments 
that are parametrically simple and are designed from an idiosyncratic 
perspective [4]. To address this issue, one can turn to the literature of 
design guidelines and princ iples for digital musical instruments 
(DMIs) as described in [2, 5, 12]. While some o f the design 
guidelines for the input module of digital musical instruments may be 
transferable, parametrically rich mechatronic musical instruments 
pose a set of new considerations. The output module of DMIs is 
typically computer-based sound synthesis and thus does not have an 
embodied relation to its input [9]. As such, researchers have proposed 
to design controllers of DMIs with “sufficiently convincing gestural 
control affordances to overcome any concern about authenticity in 
performance whilst providing the potential for highly nuanced, 
expressive, embodied music performances” [11]. 
210
 In the case of MMIs, the sound producing mechanisms are 
constrained by the mechanical and physical limits of their responsible 
actuators. Furthermore, there is a physical and visual causality 
between the movements of the actuators and their sonic outcomes. 
Consequently, two main mapping directions can be derived when 
considering the features of MMIs: 
1. Since the sound-producing mechanisms of MMIs are obvious 
and visible, there is freedom to utilize more abstract mapping 
schemes. This results in interactive performance systems, in 
which the MMIs may seem to be autonomous or “intelligent” 
as described in [4]. 
2. Because of the physical and visual causality that is apparent to 
the user, more explicit mapping schemes that correlate the 
user’s actions to the actuators are required to achieve an 
intuitive and expressive control of the MMIs. 
3. USER STUDY DESIGN 
This user study is designed to gather information from users 
pertaining to their experiences in using different musical 
interfaces to control a parametrically dense mechatronic 
musical instrument.  As d escribed later in the following 
sections, study participants are given three musical tasks to 
complete using a mechatronic chordophone with high 
dimensionality of control via different musical input interfaces 
(one input device at a time). Thereafter, a se ries of questions 
are posed to gain insights into their interaction experience 
afforded by the musical input interfaces. 
 This study was conducted with approval from the Standing 
Committee of the Human Ethics Committee at Victoria 
University of Wellington. 
3.1 Participants 
Due to the specialized nature of the study, participants would require 
specific musicianship skills to provide insightful feedback. Drawing 
upon Murphy et al. ’s user study on using new MMIs [10], 
participants with the following musicianship are gathered via email 
correspondence: 
1. Familiarity with electronic music composition tools such as 
digital audio workstation (DAW) software and musical 
interfaces that may be classified as new interfaces for musical 
expression; 
2. Familiarity with electroacoustic and synthetic composition 
techniques; 
3. Experience in working with musical mechatronic instruments. 
 In total, six participants (four males and two females, aged between 
25 to 34) took part in this user study. 
3.2 Study Protocol 
Four criteria are identified from Shackel’s criteria for usability of 
HCI systems [14] and Pressing’s cybernetics of the control interface 
[13] to be key characteristics of a related-dedicated input controller 
for parametrically rich mechatronic musical instruments, and are 
used to facilitate the evaluation of users’ experience in controlling a 
parametrically-rich MMI with different input controllers. The four 
criteria are: 
1. Ease of use: the efficiency and effectiveness of how one can 
accomplish musical tasks; 
2. Immediacy: the extent of translation from intention to the 
execution; 
3. Access to control multiplicity: the ability to simultaneously 
access and modify a range of sound -shaping parameters to 
alter the sonic output; 
4. Precision: the extent to which one’s control inputs affect the 
sonic outcome within just-noticeable-difference threshold. 
 Similar to the dimension space used to evaluate digital musical 
instruments [1] and collaborative musical performance systems [3] 
proposed by Birnbaum et al. and Hattwick and Wanderley 
respectively, this study utilizes a dimension space representation, 
with the proposed four criteria as axes, to visualize user experience in 
controlling parametrically rich MMI s with different musical 
interfaces. While previous dimension space representations utilize a 
more subjective and qualitative measurement of each axis, this study 
extends the representation model and utilizes a five-point Likert scale 
[8] to measure users’ attitudes towards each criterion: one denotes the 
least and five denotes the most. A glossary of the definitions for each 
criterion is provided to the study participants, and an explanation of 
each term is provided at the beginning of the study. 
3.3 Study Procedure 
In this user study, the participants utilize three different musical 
interfaces to interact with Swivel 2, a six -stringed mechatronic 
chordophone as shown in Figure 1. The tools used in this study are: 
1) a ubiquitous MIDI keyboard controller, 2) MIDI sequencer in 
Ableton Live 9, and 3) g.qin, a custom gestural controller that 
measure the three-dimensional orientation and physical dynamics of 
the fingers and wrist of the left hand. No other gestural controllers 
were included because the goal of this study is a comparison between 
the gestural control paradigm and the typical MIDI 
keyboard/sequencer paradigm. 
 
 
Figure 1. Swivel 2, a parametrically rich mechatronic 
chordophone. 
 
There are three sections to this study. In the first section, the 
participants answer questions about their background in using new 
musical interfaces and MMI. In the second section, participants are 
asked to perform several musical tasks with the MMI using one of 
the musical interfaces presented to them in a random order to 
minimize the learning effect. There are three tasks in total: the first 
task requires the participants to “jam” with the presented interface to 
familiarize themselves and perform as if in a live performance 
scenario; the second task is to perform a musical phrase that contains 
a slide from one position to another, picking the string, and damping 
the string; the third task is to perform a specified musical phrase as 
shown in Figure 2. Thereafter, they are asked to provide short 
answers and rating (one to five, least to most) with regard to their 
experience with the musical interface used to complete the requested 
musical tasks. This section is repeated for the remaining two musical 
interfaces. In the final section of the study, participants are asked to 
compare and contrast their experience between the three musical 
interfaces presented and provide a ranking for each device in each 
criterion, along with explanations for their evaluation. 
211
 
 
Figure 2. Task 3: Pick the string on the first pick, slide into the 
target note on the second beat (no pick), and pick on the 
following third, fourth and fifth beat. 
3.4 Setup 
This study was conducted in a studio at Victoria University of 
Wellington. A MacBook Pro laptop running Ableton Live 9 DAW 
and custom software was used to transmit MIDI data to Swivel 2. 
Table 1 shows the mapping scheme employed by each device in this 
user study. In Ableton Live 9 DAW, MIDI-clips in session view are 
used as the MIDI sequencer: study participants control Swivel 2’s 
parameters using the envelope tracks (via mouse and keyboard) of a 
MIDI clip as shown in Figure 3. 
 
 
Figure 3. Envelope tracks of a MIDI clip controlling Swivel 
2’s picking. Study participants access a drop-down menu, 
highlighted in green, to select the envelope track corresponding 
to Swivel 2’s parameters. 
 
Table 1: Mapping schemes of the three musical interfaces 
used in user study. 
Input Output 
Keyboard g.qin Ableton Swivel 2 
Note on Single-tap on 
trackpad 
Envelope track of 
controller 7 (trigger 
when value vt−1 ̸= vt) 
String 
picker 
Assignable 
button #1 No assignment 
Envelope track of 
controller 9 (127 = 
not damped, 0 = 
damped) 
String 
damper 
Note 
(discrete) + 
pitchbend 
(offset note, 
bipolar, 
continuous) 
Yaw 
orientation of 
left hand 
Envelope track of 
pitchbend 
Fretting 
Position 
Modulation 
Wheel 
Hand posture 
(fist = fully 
clamped, open 
= not clamped) 
Envelope track of 
controller 8 
Fretting 
Strength 
  
 A ubiquitous keyboard (M-Audio Axiom 252) is connected to the 
laptop with a custom Max/MSP patch that maps the controls of the 
keyboard to the parameters of Swivel 2. g.qin, as illustrated in Figure 
4, sends physical gesture data of the left hand wirelessly via 
Bluetooth. The data is mapped in another custom Max/MSP patch to 
control the actuators of Swivel 2. The range for each parameter was 
restricted such that the values resulted in musically sensible and 
mechanically safe output. 
 
 
Figure 4.  g.qin, a gestural controller that measures the 
metacarpophalangeal joint’s range of motion (ROM) of all fingers 
(except the little finger) and the wrist, together with linear 
acceleration of the instrumentalist’s left hand. 
4. USER STUDY FINDINGS 
This section presents the findings gathered from users’ feedback of 
the four criteria for each of the musical interfaces provided and  
concludes with the findings from the final section of the survey 
questionnaire that addresses participants’ comparison of the three 
musical controllers as a dedicated controller for parametrically-rich 
mechatronic musical instruments. 
4.1 MIDI Sequencer in Ableton Live 9 
Figure 5 shows the dimension space representation of study 
participants’ evaluation of MIDI sequencer in Ableton Live 9 DAW 
as a related-dedicated input controller for Swivel 2. The dimension 
space representation reveals that users find the device’s ease of use, 
immediacy of control, and access to control multiplicity to be low 
and precision to be high. Findings on each criterion are described 
below. 
 
Participant 1
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 2
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 3
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 4
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 5
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 6
Ease of use
Immediacy
Access to multiplicity
Precision
 
Figure 5. Dimension space representation of users’ responses on 
the four criteria of MIDI sequencer in Ableton Live 9 dedicated 
input controller for Swivel 2. 
  
 With regards to ease of use, most users attribute the low ease of use 
to the cumbersomeness to perform a musical gestu re. Three 
participants, #3, #5, and #6, share similar sentiments on the efficiency 
and effectiveness of completing musical tasks, commenting that 
“many parameters must be programmed to generate a simple musical 
phrase”; “having to control all parameters th at lead to a single 
musical action separately is very difficult”; and “each parameter has 
to be controlled individually and you cannot view the other 
parameters...”. 
 On the immediacy to translate intention to execution, study 
participants were asked: “... did this interface afford immediate 
control over Swivel 2, requiring relatively little translation of your 
intended action between your execution of the action and its output?” 
212
While most participants (N = 5) gave a rating of ≤ 3 for the 
immediacy of control, participant #4 gave a rating response of 4. 
Upon examining participant #4’s comments, it appears that 
participant #4 may have misunderstood what was being asked: “the 
programmed controls were executed in time”. Most of the 
participants felt that a substantial amount of time was required to 
generate the desired output, commenting that “the control takes a lot 
of time to enact” and “it felt like it took a long time to get the 
(musical) gestures going...”. 
 In terms of the accessibility to control multiple  parameters 
simultaneously to modify the sonic output of the mechatronic 
instrument, all participants gave a rating of ≤ 3. Participant #2 (rating 
= 2) commented that “it did give me access to the parameter, but it 
would not work well for live performance without extensive pre-
programming”, and participant #4 (rating = 3) noted that the 
separation of controls made the interface non-intuitive. 
 All the users felt that the interface afforded precision (rating > 3). 
Notably, “very precise, and repeatable control over all parameters” 
and “...found it very precise for pitch control, but less for plucking 
and dampening” were some of the feedback received. 
 From the users’ feedback, a potential role of the MIDI sequencer in 
the creative musical process of MMIs seems more likely to be suited 
for offline composition and controlling live performance on a meso 
musical timescale, rather than performing it live as one would with 
an acoustic instrument. Therefore, it is deemed unsuitable in the 
context of this study, which goal is to find effective live performance 
interface schemes. 
4.2 Ubiquitous MIDI Keyboard 
The dimension space representation of users’ evaluation with regard 
to the ubiquitous MIDI keyboard as a dedicated input controller, as 
seen in Figure 6, reveals that most users (N = 5) find the keyboard 
effective and efficient in executing musical tasks, provides a direct 
translation of intention to execution, provides sufficient access to 
multiplicity of control, but does not perform well in precision. 
Further details of each criterion are described below. 
 
Participant 1
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 2
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 3
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 4
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 5
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 6
Ease of use
Immediacy
Access to multiplicity
Precision
 
Figure 6. Dimension space representation of users’ responses on 
the four criteria of ubiquitous MIDI keyboard controller as 
dedicated input controller for Swivel 2. 
 
 On using the MIDI keyboard controller, with the exception of 
participant #4, users generally find it efficient and effective to 
complete musical tasks. While participant #3 (rating = 3) found that 
even though the interaction afforded by having a physical controller 
was easy, the unrelated design of the keyboard to Swivel 2 (a 
chordophone) affected the experience to be less straight-forward: 
participant #4 commented that “it is difficult to control the slide 
consistently”. Difficult experiences from other users included 
“...parameters on the modulation wheel and pitch bend were 
difficult” and “damping and lifting the fretter take some thought”. 
 With immediacy in translating intention to execution, as expected, 
most users (N = 5) gave a response rating of > 3, and provided 
similar explanations such as “musical phrases can be played easily” 
and “the translation is easy”. Participant #6 (rating = 3) commented, 
“the translation is immediate but the nature of keyboard controllers 
dictates some mappings that are not very intuitive”. Participant #4 
found that the execution of the intention is more complex than the 
mechanical action being produced. 
 Most users agreed that the keyboard provided access (rating > 3, N 
= 5) to a range of output parameters for shaping the sonic outcome 
and allowed for fair precision (response rating > 3, N = 5) to repeat 
musical tasks within recognition threshold for Swivel 2. One user felt 
that the “small space of the pitch and mod wheels makes them less 
precise, thus repeatability would be an issue”, while another user 
found the keyboard controller to provide “less accuracy for 
controlling bends, and lifting the fretter”. 
 Although most users gave a generally favorable response about the 
device, participant #4 did not. The overall response of participant #4 
suggests that participant #4 may not be keyboard-trained. From the 
participants’ feedback, a potential role of the MIDI keyboard 
controller in the creative musical process of MMIs seems more likely 
to be suited for more conventional musical outcomes such as playing 
melodies that fit within Western musical scales and tunings. The 
favorable responses towards MIDI keyboard controller may be due 
to its ubiquity in the ele ctronic music community, despite its 
differences in playing schema when compared with Swivel 2: MIDI 
keyboard’s vertical movement for pitch bend controller to slide 
fretter’s position, versus Swivel 2’s horizontal movement. Hence, due 
to its non -intuitiveness, unrelated design to Swivel 2, and fair 
precision, the MIDI keyboard controller may not be suitable in a live 
performance scenario. 
4.3 Custom Gestural Controller — g.qin 
As illustrated in Figure 7, all users felt that g.qin can be a 
related dedicated input controller for Swivel 2. Their feedback 
on how g.qin measures with the four crite ria is  discussed 
below. 
 
Participant 1
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 2
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 3
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 4
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 5
Ease of use
Immediacy
Access to multiplicity
Precision
Participant 6
Ease of use
Immediacy
Access to multiplicity
Precision
 
Figure 7. Dimension space representation of users’ 
responses on the four criteria of the new physical gesture 
acquisition system, g.qin, as dedicated input controller for 
Swivel 2. 
 
 All study participants found g.qin to be efficient and effective 
in completing the three musical tasks (N = 5, response rating > 
4), and that its embodied relation to the actuators of Swivel 2 
affords immediate translation of intention to execution (N = 5, 
response rating > 4) . Almost all users (N = 5) described this 
ease of use and immediacy as “intuitive” and attribute this 
experience to the “highly correlated” motions of Swivel 2 ’s 
actuators and their actions. One user mentioned that the “visual 
213
and intuitive connection betw een hand movements and fretter 
positioning is very helpful”. Another user commented that the 
system consisting of Swivel 2 and g.qin is “quick to learn”. 
 In the domain of accessing multiple controls, it is reflected in 
the users’ response ratings (N = 5, > 4) that the compromise to 
exclude damping was negligible and did not hinder their ability 
to produce their desired sonic outcomes with Swivel 2. One user 
commented that “the mapping area (physical space) translated 
well to the performance space”, while another participant 
commented that g.qin provided “very good control over pitch 
slides and quick gestural behavior”. 
 With regard to its precision in produc ing outcomes within 
just-noticeable-difference threshold, all users responded that the 
control of pitch sliding was “very precise”, with one user 
mentioning that this was so even with the “the speed of 
transition and the curve of pitch slide”, as well as t he fretter’s 
final position. On the other hand, two users shared the 
sentiment that finding exact pitch location was “a little harder”. 
On this account, g.qin received an overall response rating of > 
4 from all study participants. 
 Similar to the ubiquitou s MIDI keyboard, the new physical 
gesture acquisition system measures very close to the four 
criteria of a related -dedicated input controller for new 
mechatronic musical instruments with a high number of output 
parameters. Preliminary speculation of this m ay be attributed 
towards the device ’s ability to transduce the performer -
instrument relationship. 
5. INTERDEVICE COMPARISON 
The previous section presented the study participants’ experience 
when utilizing the MIDI Sequencer in Ableton Live 9, a ubiquitous 
keyboard controller, and g.qin to control Swivel 2. In this section, a 
comparison of the different devices is presented based on the final 
section of the survey questionnaire that serves to gather users’ 
feedback on how one device compares to the other, their preferred 
way of interfacing with Swivel 2, and general questions about 
customized controllers, which are presented below. A dimension 
space representation summary is shown in Figure 8. 
 
 
Figure 8. Dimension space representation of users’ responses on 
the four criteria by input controllers. Clockwise: EoU represents 
ease of use, P represents precision, A2M represents access to 
multiplicity of control, and I represents immediacy. 
 
 All of the study participants concur that g.qin is their preferred way 
of interacting with Swivel 2, with many users attributing their choice 
mainly to the intuitive and embodied relation of input gestures and 
sonic outcome. One user mentioned that the interaction between the 
performer and Swivel 2 “is visually interesting for an audience”. The 
users’ comments on their preference among the three devices 
presented support the intuition that the four criteria established are 
qualities of a related-dedicated input controller for parametrically-rich 
MMIs. These comments include: “... to create a more interactive 
performance and embodied performance...”; “access is not as 
important as ease of use and immediacy in live performance”; “the 
immediacy and precision of g.qin make it best suited for live 
performance”; and “I felt a strong connection between my gestures 
and the movements of Swivel 2. These were largely lost with both 
other interfaces.”. 
 When asked “what would be a more intuitive musical interface for 
controlling Swivel 2”, most participants suggested similar gestural 
interfaces that directly capture body movements or position in space 
such as Wii Remote1 and the Laser Harp (described in [17]). Finally, 
users were asked if “a controller interface customized to be used with 
Swivel 2 would be more intuitive than a general-purpose interface”. 
All study participants agreed that it would, with one participant 
stating that “to explore the idiosyncrasies of Swivel 2, the controller 
needs to be well suited at controlling the movement of the fretting 
arm... this means that a grid based mapping of pitch is often not 
suited to controlling and exploring the musical capabilities of Swivel 
2”. 
6. CONCLUSION 
The findings of the user study presented in this paper further support 
the need for related-dedicated input devices and revealed that the 
common way of interacting with MMIs via MIDI keyboards and 
sequencer still has its place in the advent of parametrically rich 
MMIs. Particularly, users have indicated that the MIDI sequencer is 
likely suited for offline composition and controlling live performance 
on a meso musical timescale; the MIDI keyboard to be suited for 
more conventional musical outcomes such as playing melodies that 
fit within Western musical scales and tunings; and, the gestural 
controller to be suited for controlling Swivel 2 more closely to the 
traditional performer-instrument relationship — affording the  
performance of the MMI like a musical instrument, and exploring 
beyond conventional musical outcomes. 
 The user study also indicates that potential related-dedicated input 
controllers have high level of ease of use, immediacy, access to 
multiplicity of control, and precision. In addition, the actions required 
of the user possess an embodied relation with the MMI, enabling 
intuitive interaction and quicker exploration of musical ideas. This 
can be explained by the similarity between this gestural input-MMI 
system and the performer’s relationship to traditional musical 
instruments, as described by Pressing: 
“Traditional instruments have a nearly one -to-one response 
between actions of the performer and the resulting sound, a 
stimulus-response model fits well. Interaction between the person 
and the instrument takes place through the aural [visual] feedback 
loop and the performer makes decisions on that basis in real -
time.”[13] 
 While this user study is limited to a specific MMI of an instrument-
family and only three input devices, the insights gained from this 
study with regards to the characteristics of a specialized controller for 
parametrically rich MMI should serve as a guide for researchers and 
developers interested in developing MMI-specific input devices. 
Developers of MMI-specified input controllers should also conduct 
user studies to evaluate these new MMI performance systems, just as 
one would with DMIs. Future studies will include more users, 
different parametrically rich MMIs, and different new musical 
gestural controllers. 
7. REFERENCES 
[1] Birnbaum, D. et al. 2005. Towards a dimension space 
for musical devices. Proceedings of the 2005 conference 
on New interfaces for musical expression  (2005), 192–
195. 
[2] Cook, P. 2001. Principles for designing computer music 
controllers. Proceedings of the 2001 conference on New 
interfaces for musical expression (2001), 1–4. 
                                                                 
1 http://www.nintendo.com/wiiu/accessories 
214
[3] Hattwick, I. and Wanderley, M.M. 2012. A Dimension 
Space for Evaluating Collaborative Musical 
Performance Systems. NIME (2012), 21–23. 
[4] He, J. et al. 2016. Be yond the Keyboard and Sequencer: 
Strategies for Interaction between Parametrically -Dense 
Motion Sensing Devices and Robotic Musical 
Instruments. ICMA Array. Special Issue, Proceedings of 
Si15: 2nd International Symposium on Sound and 
Interactivity (2016), 79—85. 
[5] Hunt, A. and Kirk, R. 2000. Mapping strategies for 
musical performance. Trends in Gestural Control of 
Music. 21, (2000), 231–258. 
[6] Kapur, A. 2008. Digitizing North Indian Music: 
Preservation and Extension using Multimodal 
SensorSystems, Machi ne Learning and Robotics . VDM 
Verlag. 
[7] Kapur, A. et al. 2011. The Machine Orchestra: An 
Ensemble of Human Laptop Performers and Robotic 
Musical Instruments Music. Computer Music Journal . 
35, 4 (2011), 49–63. 
[8] Likert, R. 1932. A technique for the meas urement of 
attitudes. Archives of psychology. (1932). 
[9] Miranda, E.R. and Wanderley, M.M. 2006. New digital 
musical instruments: control and interaction beyond the 
keyboard. AR Editions, Inc. 
[10] Murphy, J. et al. 2016. Using Expressive Musical 
Robots: Working with An Ensemble of New 
Mechatronic Instruments. Proceedings of the 2016 
International Symposium on Electronic Art (2016). 
[11] Paine, G. 2009. Gesture and morphology in laptop music 
performance. The Oxford handbook of computer music . 
(2009), 214–232. 
[12] Paine, G. 2009. Towards unified design guidelines for 
new interfaces for musical expression. Organised Sound. 
14, 02 (2009), 142–155. 
[13] Pressing, J. 1990. Cybernetic issues in interactive 
performance systems. Computer music journal . 14, 1 
(1990), 12–25. 
[14] Shackel, B. 1990. Human factors and usability. Human-
computer interaction (1990), 27–41. 
[15] Singer, E. et al. 2003. LEMUR GuitarBot: MIDI 
Robotic String Instrument. Proceedings of the 2003 
Conference on New Interfaces for Musical Expressi on 
(Montreal, Canada, 2003). 
[16] Singer, E. et al. 2004. LEMUR’s musical robots. 
Proceedings of the 2004 conference on New interfaces 
for musical expression (2004), 181–184. 
[17] Szajner, B.H. 1982. Laser control arrangement for 
musical synthesiser - uses mirrors to reflect laser beams 
to photocells in synthesiser control circuits for control 
by beam interruption. 1982. 
[18] Weinberg, G. and Driscoll, S. 2006. Toward robotic 
musicianship. Computer Music Journal . 30, 4 (2006), 
28–45. 
 
 
 
215