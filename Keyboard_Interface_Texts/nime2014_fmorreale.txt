Musical Interface Design: 
An Experience-oriented Framework 
 
Fabio Morreale 
University of Trento 
Trento, Italy 
morreale@disi.unitn.it 
 
Antonella De Angeli 
University of Trento 
Trento, Italy 
deangeli@disi.unitn.it 
 
Sile O’Modhrain 
University of Michigan 
Ann Arbor, MI, United States 
sileo@umich.edu 
 
 
ABSTRACT 
This paper presents MINUET, a framework for musical interface 
design grounded in the experience of the player. MINUET aims to 
provide new perspectives on the design of musical interfaces, 
referred to as a  general term that comprises digital musical 
instruments and interactive installations. The ultimate purpose is to 
reduce the complexity of the design space  emphasizing the 
experience of the player. MINUET is structured as a design process 
consisting of two stages: goal and specifications. The reliability of 
MINUET is tested through a systematic comparison with the related 
work and through a case study. To this end, we present the design 
and prototyping of Hexagon, a new musical interface with learning 
purposes. 
 
Keywords 
Framework, DMIs, interactive installations, user experience 
1. INTRODUCTION 
For several years, NIME and related communities have been coping 
with the challenge of framing a design space for musical interfaces. 
Framing a design space serves the purposes of designers, providing 
them with concepts that systematize their thinking and stimulate 
thought [13]. It can inspire design, reveal potential problems [1], and 
act as a reference point for future studies on user experience [13]. 
A simple design space of musical interfaces can be described along 
a continuum that stretches from Digital Musical Instruments (DMIs) 
to interactive installations [4,31]. More articulated approaches have 
mainly focused on DMIs by differentiating, for example, among 
augmented traditional instruments, instrument-like controllers, 
instrument-inspired controllers and alternate controllers [19]. This 
categorization is device-oriented: while considering the similarity to 
traditional instruments and the technology featured in the controller, 
it fails to account for the profound variations affecting the experience 
of the player. As an exa mple, suffice it to consider the difference 
between a violinist using an augmented bow [26] and a Reactable 
player [17]. 
The other pole of the design space defines interactive installations, 
i.e. those systems “ that are only realized through a participant’ s 
actions, interpreted through computer software or electronics, and 
those actions do not require special training or talent to perform” 
[29]. Interactive installations have very different characters, scopes 
and goals and they usually emphasise the user ex perience rather 
than a set of musical activities performed on  an instrument . 
Several installations allow visitors to inspect multimodal spaces or 
artifacts and provide them with aesthetic experiences (e.g. Iamascope 
[11] and  PebbleBox [22]). Some works, in stead, specialize in 
enabling visitors “to interactively operate on music content  […] 
while listening” [7] (e.g. The Brain Opera [25], Sync’n’move [27] 
and The Music Room [ 20]). Other installations focus on thought 
stimulation, self-regulation support and behavior change by means of 
music (e.g. Sonic Cradle [28] and Piano Staircase). This concise 
review attests that (i) the design space of musical interfaces is 
complex, difficult to categorize and likely to be influenced by 
personal interpretation; (ii) the player’s experience determines the 
goals and the challenges of both DMIs and interactive installations.  
In this paper we aim at framing a conceptual understanding of 
musical interface design centered on the experience of the player. 
This framework is supposed to provide a model for reducing the 
complexity of the design space of different player experiences, 
ranging from Disklavier pianists to visitors of the Music Room [20]. 
The outcome of this work is MINUET (Musical INterfaces for User 
Experience Tracking), a design framework for the identification of 
the elements involved in musical interface design, which can help 
designers to position, shape and evaluate their systems. MINUET is 
a design process structured into two stages, following each 
other in a non -invertible sequence . The first stage analyzes the 
goals of the interface, and the second stage specifies how designers 
can actually achieve these goals.  
MINUET can serve the following purposes: (i) reducing the 
complexity of the design space of musical interfaces; (ii) specifying a 
set of success criteria; and (iii) guiding evaluation procedures. Due to 
space constraints, we are only focusing on the first point in this paper, 
leaving the analysis of the other two points to our future works. To 
achieve this goal, we grounded MINUET on related work (Section 2) 
and delivered a design process  to tackle the issue from the 
perspectives of designer goal and specifications (Section 3). The 
reliability of the framework is discussed by drawing parallels with 
existing research (Section 4) and by testing it with a case study 
(Section 5). The final part of the paper provides an insight into future 
works. 
2. RELATED WORK  
Over the last few years the investigation of the design space of 
musical interfaces has been arousing an increasing interest. Two 
meta-reviews have attempted to suggest how to catalogue these 
studies: [10] analyzes the different approaches to the definition, 
classification and modeling of DMIs , while  [18] surveys the 
methodological approaches employed in the design of DMIs. 
Most of the studies on the design space of musical interface 
restricted their investigation to DMIs. [15] tackles the challenge 
from the general perspective of the interaction modalities with 
musical instruments differentiating between  instrumental (the 
musician has control over every aspect of the instrument), ornamental 
(the system has control) and conversational (shared control).  
Similarly, [16] centers the framework on the relation between player 
and instrument, discussing issues of balance (between complexity 
and simplicity), playability, learning curve and instrument efficiency. 
[24] and [14] suggest two frameworks theorizing about the type of 
possible gestures in this context . [24] distinguishes between 
intuitiveness and perceptibility of gestures, and between  ergotic 
(gestures used to manipulate physical objects) and semiotic (gestures 
used to communicate meaningful information, such as thumb up). 
[14] analyses the strategies for mapping gestures onto synthesis 
 
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 
NIME’14, June 30 – July 03, 2014, Goldsmiths, University of London, UK. 
Copyright remains with the author(s). 
 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
467
parameters centered on cognitive modes (analytical vs. holistic). [30] 
narrows the analysis to the musical instruments that inspire long-term 
engagement and proposes seven heuristics that describe the qualities 
of these instruments : incrementality, complexity, immediacy, 
ownership, operational freedom, demonstrability and cooperation. 
[23] addresses the issue of DMI evaluation from the perspective of 
the different stakeholders  (performers, composers, audience, 
manufacturers and designers). 
Only a few studies, such as [4], broaden the scope of the design 
space of musical interfaces by considering also interactive 
installations. Birnbaum and colleagues base this design space on 
seven dimensions: required expertise, musical control, feedback 
modalities, degrees of freedom, inter-actors, distribution in space and 
role of sound [ 4]. [31] suggests taking inspiration from HCI 
techniques for the evaluation of control and usability of musical 
interfaces. In particular, this framework is based on learnability, 
explorability, feature controllability and timing controllability. Blaine 
and Fels [5] focus on collaborative musical experiences and propose 
seven primary dimensions to be borne in mind  when designing: 
physical devices, type of interaction, learning curve, pathway to 
expertise, level of physicality between players, directed interaction 
and musical range. [9] presents a model for the design and evaluation 
of musical interfaces concentrating on the modality of interaction: the 
interface can employ a simple modality  (visual or auditory), or 
multiple modalities by integrating simple modalities. 
The contribution of these works towards a better understanding of 
the design space of musical interfaces is indisputable. Most of them, 
however, have apparently failed to convince the community to adopt 
any such frameworks. A possible reason is that the quality of a 
framework should be based on how it helps designers by stimulating 
creativity and allowing them to delve into the  design process. 
Furthermore, the increasing number of interacti ve installations 
operating in the musical domain suggests that new research should be 
done in this area. 
3. MINUET 
MINUET offers a conceptual model for the understanding of the 
elements involved in musical interface design, providing a frame to 
bear on this complex design space. This frame can help designers (i) 
to position their work in a structured design space, (ii) to elaborate 
ideas and objectives when designing a new musical interface (iii) to 
guide the evaluation process. The idea is to tackle this challenge by 
clustering design elements from the point of view of the player’s 
experience. Also, rather than providing a list of design metrics and 
heuristics, MINUET integrates a temporal dimension into its 
structure, consisting of two stages which can by no means inverted. 
3.1 Method 
MINUET results from a synthesis of the related work and concepts 
developed by the authors. The final structure has been developed 
over the course of several iterations, including a workshop attended 
by a selected group of three HCI researchers , a professional 
musician, and an interactive artist. At first, the authors identified the 
related work discussed in 2.1 by means of a meta-review process. 
Related work was primarily collected in the basis of on the references 
analyzed in [10,18], while remaining papers were chosen on the basis 
of their relevance to the topic. Next, t he selected works were 
schematized in connection to the proposed dimensions (or heuristics) 
and to the related category of musical interface (DMIs and interactive 
installation). Later, we organized a one-day workshop: participants 
were encouraged to discuss affinities and connections among these 
works. The methodology adopted is affinity analysis [3]: guided by 
one of the authors, participants explored connections, similarities and 
differences among these works by using sticky notes and several A0 
cardboards. They were then asked to propose and discuss a number 
of dimensions that could correctly cluster the associated design 
guidelines from the perspective of the experience of the user. From 
this iteration, three sets of concepts  emerged: Objective of 
Interaction, Constraint and Context, that respectively answered to the 
questions: what the goal of the interface is, how this should be 
designed, where and when it should take place.  The successive 
version of the model was rearranged by the authors,  who 
redistributed the three sets of concepts into two stages of a design 
process: goal and specifications. This change was then analyzed and 
supported by some of the workshop participants. Lastly, a few minor 
modifications to the framework were made during the analysis of the 
case study, reported in this paper 
3.2 Model 
MINUET structures the designing of musical interfaces into a two-
stage process: the first stage, Goal, describes the objectives of the 
interface, while the second stage, Specifications, help designing the 
interaction in order to fulfill these objectives (Figure 1). Each stage is 
composed of some entities that address design issues on a more 
pragmatic level. To model these entities we drew up inspiration from 
the influential work by [2], which defines a conceptual model for the 
design of interactive systems. The PACT framework helps designers 
to investigate the design process by means of a user -centered 
technique based on People, Activities, Contexts and Technologies. 
The four entities detail the objectives and the constraints of the 
interface, and are specified by a series of directly applicable design 
perspectives (the elements included in the inner circles in Figure 1).  
 
 
Figure 1. MINUET framework.  
The design process starts from the analysis of the designer goal, 
which describes the purposes of interaction. This stage is articulated 
in three parts : People, identifying the end-user (who) Activities, 
specifying the kind of interactions the designer has in mind (what) 
and Contexts detailing the specifications of the environment (where 
and when). Once the interaction goals have been defined, designers 
can move on to the Specifications stage by prototyping the interface. 
This stage analyses how to design the interaction resembling the last 
entity of PACT framework,  Technologies. Unlike Benyon’s 
“Technologies”, we focus on the identification of interaction 
            ACTIVITIES
             CONTEXTS
Player
Audience
Music Style
Physical Env.
Social Env.
Motivation
Collaboration
Learning Curve
Ownership
Control
Mapping
Operational Freedom
Embodied Facilitation
Input
Feedback
 
             PEOPLE      
                TECHNOLOGIES
prototyping
evaluation
GOAL
SPECIFICATIONS
Proceedings of the International Conference on New Interfaces for Musical Expression
468
requirements rather than considering hardware and software 
implementations. Lastly, designers can evaluate the proposed 
specifications by referring back to the original goal.  
3.3 Goal 
The first stage of MINUET frames a conceptual model of the 
interface goal, in the shape of a very high-level user story [8]. The 
related concepts guide designers to reflect upon the  kind of 
experience they wish to offer. Players can use a musical interface for 
a number of reasons: to perform, to compose, to control or to learn 
music. The interface can provide players with a playful experience, 
stimulate creativity and convey a meditative experience; also, it can 
have an inf ormative meaning, and it may or may not have 
educational purposes. In order to cope with the diversity of 
interactions, this design space is simplified detecting patterns and 
suggesting insights that can assist designers’ reflections. In fact, they 
are invited to  look at their goal through the lense s of People, 
Activities and Contexts. ‘People’ looks at the designer’s objectives 
from the viewpoint of the targeted category of players and from the 
role of the audience (who). ‘Activities’ questions what the envisioned 
interaction is, by framing the type of musical interface. ‘Contexts’ 
investigates the environment and the set -up of the interface 
(where/when). The relevance of these entities varies according to the 
nature and the goals of the interface and the priority scale has to be 
given by designers themselves. 
3.3.1 People 
This entity specifies the subjects who will engage with the 
interface, namely players and audience. 
Player. A critical point when constructing specific experiences is 
to define the target user. The interface could address professional or 
amateur musicians, players that are quite familiar with music, or non-
experts. An augmented bow can be designed to enhance the playing 
experience of professional cello players, or to ease access to  the 
instrument to newcomers. 
Audience. In interactive systems the audience may not perceive 
the interactions between the performer and the system, as they would 
do with traditional instruments [12]. Designers that intend to allow 
spectators to understand such interaction specifically target this 
issue. The intelligibility or the ambiguousness of the performance 
greatly depends on the performer’s capability of showing others how 
action-reaction mechanisms work . In some interactive systems, 
spectators can also actively take part in music creation. 
3.3.2 Activities 
The objective of interaction can be considered from the lens of 
envisioned player activities. The associated concepts are: motivation, 
music style, learning curve, ownership and collaboration.  
Motivation. This concept provides insights into the motivations of 
the players, analyzing the relevance of the sonic and musical medium 
with respect to the user experience. In interactive installations, sound 
often has an accessory goal, as it is supposed to guide the exploration 
of the environment. Vice versa, the creation or the manipulation of 
sounds is the pr imary goal of traditional DMIs and interactive 
sonification systems, in which users explore particular information 
conveyed into abstract musical form. 
Learning curve. The learning curve provides information on the 
time required to gain skills with an interface and/or to understand 
how interaction works. Traditional complex interfaces requiring a 
significant investment of time should preferably allow high ceiling, 
as to ensure long-term engagement and enable players to develop 
skills, as it is the case with traditional instruments. By contrast, 
museum installations have to be experienced in a very short time, and 
therefore they must be able to reveal how they work within seconds. 
In this case the learning curve needs a low floor.  
Ownership. Interfaces that specifically address the generation of 
creative artifacts should aim at the uniqueness of musical expression. 
When the musical output is a form of self-expression, the execution, 
the identification and the recognition of unique playing styles should 
then be supported [30]. By contrast, when the interaction is not 
centered on the actual production of a musical output, replicable 
outcomes are to be expected. Ownership can also refer to the way 
players interact with the interface  if the system allows them to 
configure the interface according to their own wishes. 
Collaboration. Musical activities can be carried out alone or in a 
social context . Designing collaborative experiences means 
considering awareness of others, and factors such as synchronization 
and coordination. Sometimes, musical interfaces can only be fully 
exploited in collaborative contexts: in these cases, specific efforts 
need to be made to ensure the expected social dynamics.  
3.3.3 Contexts 
Contexts consider the environment in which the interaction takes 
place, i.e. all those elements that can help the identification of the 
interaction goals. This entity may not be very important  for 
traditional DMIs, yet it is particularly relevant for installations and 
collaborative interactive systems. The associated concepts (music 
style, physical environment and social environment), detail what 
makes an application unique in relation to similar works. 
Music style. In novel musical interfaces sound is produced by the 
computer, so the associated musical style is not influenced by the 
acoustic features of the instrument as in traditional music. This 
feature opens up new horizons for designers, who can choose the 
music style that better suits their goals, irrespective of the nature of 
the device. Identifying a musical style is crucial when it comes to 
targeting a specific population. For instance, when confronted with a 
broad target population, designers should make use of a familiar 
music language, such as tonal music [20]. On the other hand, when 
targeting a community of experts or avant -garde musicians, the 
music itself can be experimental. Addressing a specific musical style 
genre can also be the primary objective of the interface (e. g. the 
musical controllers for electronic music such as Korg Kaossilator). 
Physical environment. When reflecting on the goals of the 
interface, its own nature frames the importance of the space in which 
it is going to take place. While traditional controllers do not require a 
precise spatial configuration, it is important to consider the physical 
environment hosting the installation. Some installations are designed 
to be exhibited on the occasion of specific events while others are 
hosted by museums for long periods. 
Social environment. The social context and the proximity between 
players mirror different kinds of interpersonal relations. Intimate 
experiences need a small, secluded space, while provocative 
installations can either be open to the greatest number of passers-by 
or force individuals to interact with an object on a one-on-one basis. 
Designers might want to draw the attention of museum visitors, or to 
arouse the interest of people and of the media into a specific social 
issue. 
3.4 Specifications 
The second stage of the design process collects the goal considered 
by the previous issue  and reflects on the interaction constraints 
between the player(s) and the system. When designing for intuitive 
experiences the a mount of interaction possibilities should be 
restricted, as to guarantee an easy access to the players. By contrast, 
musical controllers should enable players to manage a multitude of 
parameters, as to have  full control on the generated music . 
Specifications must be considered according to the level of control 
and to the input and feedback modalities as well as to the presence of 
human facilitators that might result in a loss of operational freedom. 
Control. The degree to which the player controls the music varies 
along a continuum which includes the different categories  of 
interfaces. For the sake of convenience, though, we will discretize 
three categories: low, medium and high control. Low-level 
controllers provide control on each note or on specific sonic  
parameters, just like traditional instruments. They usually address an 
Proceedings of the International Conference on New Interfaces for Musical Expression
469
expert population, as they rely on musical notations that are unknown 
to non-experts. Mid-level controllers give access to higher musical 
structures at bar-level or score-level such as rhythmic pattern, melody 
direction, mode, musical processes and loops. Depending on their 
closeness to note-level, they might be more or less accessible to non–
experts. High-level controllers operate outside the musical domain, 
so they are generally open to everybody but do not allow any subtle 
interaction with musical structures. 
Mapping. The level of control influences the  complexity of 
mapping strategies. Low-level controllers require a  convergent 
mapping [14]: in ord er to produce a single pitch , a sequence of 
physical tasks needs to be performed just as playing a note on the 
guitar usually requires the synchronization of two or more fingers). 
Vice versa, in interfaces controlling high-level musical elements, a 
single mediated parameter affects many musical factors (divergent 
mapping [14]). For instance, controlling mode influences two lower-
level musical parameter, i.e. harmonic and melodic intervals. 
Sometimes interface metaphors are needed to  hide mask music 
complexity from the player’s interaction space. Interface metaphors 
indeed “allow users to readily make inferences about how to operate 
unfamiliar user interfaces by mapping existing skills and knowledge 
from some familiar source domain” [32]. In this case, a divergent 
mapping is necessary: controlling the emotional character of a piece 
will influence several musical parameters (e.g. mode, tempo, melodic 
direction and volume). 
Input. Players can interact with the musical interface through  
symbolic, para-linguistic, involuntary and subconscious modalities 
[6]. The production of sounds might require physical energy an 
ergotic gesture. Alternatively, players could interact on a semantic 
level, through visual, tactile or semiotic gestures. 
Feedback. This concept focuses on the presence and the role of 
feedback modalities (auditory, visual, tactile, or  kinesthetic [4]). 
Designers are invited to consider the effects related to different 
feedback modalities. Adding multi-sensorial feedback can augment 
the experience, but also distract a performer. In most cases, the visual 
feedback is an accessory factor but sometimes it can become the 
main focus of user experience [11].  
Operational freedom. Operational freedom defines the potential of 
players to express a creative interaction with the system and the 
flexibility of the interaction. Designers seeking to stimulate creativity, 
improvisation and adaptation should envision flexible and constraint-
free interactions, while educational or training-oriented interfaces are 
better achieved through rigid task-achievements procedures. A snare 
drum has rather limited interaction possibilities, but offers endless 
compositional options. Vice versa, The Music Room  [20] does 
permit players to creatively interact with the system, but the musical 
control depends on the choice of the piece’s emotional tone. 
Embodied facilitation. Embodied facilitation refers to the physical 
configuration of the interface, as well as to the presence of human 
facilitators. The design of the interface can impose limitations if it 
“facilitates, prohibits and hinders some actions, allowing, directing, 
and limiting behavior” [13]. In this case , the designer suggests a 
particular strategy or interaction traje ctory using constraints. 
Interaction constraints could also be forced by human and virtual 
facilitators that pilot the interaction. Facilitators can show players 
how to interact with the interface and other players and how to 
accomplish tasks. If needed, the spatial environment should support 
the envisioned interaction. When designing collaborative 
installations, the physical set -up should encourage players to 
collaborate by constraining or facilitating their interaction, while a 
round space where players have the same distance from the object of 
the interaction would be appropriate for shared-control interfaces. 
4. RELIABILITY OF THE FRAMEWORK 
In order to argument the proposed framework, in this section we 
show how previous frameworks and taxonomies are accommodated 
within it. In Table 1 the main dimensions of the related work are 
associated with the elements proposed by MINUET. 
 
Table 1. MINUET compared to related work (the first letter 
refers to the associated PACT entity). 
Related work Dimension MINUET 
Birnbaum 2005 
Required expertise P: Player 
A: Learning Curve 
Musical control T: Control  
Feedback modalities T: Feedback 
Degrees of freedom T: Operational fr. 
Inter-actors A: Collaboration 
Distribution in space C: Physical env. 
Role of sound A: Motivation 
Wallis 2013 
Incrementality 
A: Learning Curve Complexity 
Immediacy 
Ownership A: Ownership 
Operational Freedom T: Operational 
Freedom 
Demonstrability 
P: Audience 
P: Player 
Cooperation A: Collaboration 
Blaine 2000 
Physical device T: Input Player interaction 
Learning curve A: Learning curve 
Pathway to expertise A: Ownership 
Physicality between 
players 
A: Collaboration 
T: Embodied fac. 
Musical range T: Control 
Media T: Feedback  
A: Motivation 
Overholt 2009 
Gestures 
intuitiveness and 
perceptibility 
P: Player 
T: Input 
Mapping richness T: Mapping 
Range of expression T: Control 
Hunt 2000 Mapping strategies T: Mapping 
Cumhur Erkut 
2011 
Modality of 
interaction T: Input 
Johnston 2008 Modes of interaction T: Input 
Wanderley 
20002 
Learnability A: Learning curve 
Explorability T: Operational Fr. 
Feature 
controllability T: Control Timing 
controllability 
Jordà 2004 
Balance A: Learning Curve 
Instrument 
efficiency 
T: Control 
T: Operational Fr. 
Playability  A: Learning curve Learning curve 
 
5. HEXAGON, A CASE STUDY 
We are now going to examine here a case study to demonstrate the 
prove the potential of MINUET for guiding designers throughout the 
design process of musical interfaces. To this end, we introduce the 
conceptual design of Hexagon, a tangible controller designed to train 
music cognition. Consistently with the declared purposed of the 
framework, we will focus on how to frame design challenges rather 
than detailing technical implementation. 
Proceedings of the International Conference on New Interfaces for Musical Expression
470
5.1 Goal 
Hexagon is designed to give  players an interac tive hands-on 
experience, which allows them to learn how to recognize the role of 
the most relevant musical parameters in Western compositions, such 
as mode, rhythm, harmony and tempo.  
People. Hexagon targets amateurs and beginner s (Player). 
The role of spectators is not relevant (Audience). 
Activities. Hexagon is designed to train their perception of musical 
composition, i.e. to become aware of musical form and progression 
(Motivation). Following a social-constructivist approach to learning, 
cooperation among players is expected (Collaboration). Given the 
educational purposes, the initial complexity needs to be kept under 
control; a sufficient longevity, though, must be ensured in order to 
keep players engaged (Learning curve).  
Contexts. As music language we use tonal music, as it is broadly 
known and complex enough to include the most important 
parameters of Western music (Music style). The interface can be 
experienced individually or collectively , at home or at school 
(Social environment). Spatial elements are not crucial ( Physical 
environment). 
5.2 Specifications 
Once our goals  have been outlined , this stage focuses on 
prototyping possible design implementations. We want players 
to learn the influence of a number of musical parameters by 
means of physical interaction. This implies that  they will 
interact with tangible objects,  each of which is associated with 
a specific parameter. The music is generated by Robin, an  
algorithm that automatically composes an original tonal music 
[21]. Players can create music  operating on  compositional and 
performance parameters. Compositional elements are usually 
marked by the composer in the score , while performance  
behaviors are left to  performers’ interpretation during the 
execution. Hexagon simulated the same distinction : players can 
interact using one of two input modalities.  
5.2.1 Compositional Parameters.  
Six squared flat boxes  are associated with six compositional 
parameters (i.e. mode, harmony, harmonic intervals, melodic 
direction, melodic range, and melodic motion). Positioning the boxes 
on the sides of a central hexagon, players define the desired values 
for each parameter (Figure 2). These values feed the algorithmic 
composer, which generates the music accordingly. While not directly 
operating on low-level parameters, the interaction metaphor remains 
in the musical domain (divergent Mapping). An example of 
interaction is shown in Figure 2: the m usic is set in major mode, 
consonant harmony and consonant harmonic interval, and the player 
is attaching the block describing the melodic direction. 
 
Figure 2. Placing the boxes around the hexagon allows 
controlling compositional parameters .  
5.2.2 Performance behaviors  
While the music is playing, the user can subtly intervene on 
performance behaviors interacting with a multi-touch hexagon 
(Control, Figure 3 ). Here players  have direct access to sonic 
features (direct Mapping). They can influence the timing , 
volume and attack tapping  on the hexagonal pad. Furthermore, 
they can  perform a number of behaviors such as  vibrato 
(through a gentle and  fast movement of the fingers) and legato 
(through a gesture which is reminiscent of the  hammer-on 
technique performed by guitarists ). Figure 3  features the 
example of a player who is tapping tempo with his right hand 
while performing a vibrato with the left. 
 
Figure 3. Interacting with the central hexagon infl uences 
performance behaviors. 
When interacting with performance elements, players have direct 
access to the low -level control and therefore experience more 
Operational Freedom  then they do when interacting with 
compositional ones. However, this privilege is counterbalanced by 
the time required for mastering them (Learning curve). We argue that 
dividing the musical creation process into two phases will help the 
player to understand the distinction between the two sets of 
parameters (Embodied facilitation).  
6. CONCLUSIONS AND FUTURE WORK 
A general consensus suggests that a technology-oriented design well 
suits musical controllers that resemble the physicality and the 
objectives of acoustic instruments [19]. If we considered musical 
interfaces as a n umbrella term that also include s interactive 
installations [4, 31], however, this approach may prove inconsistent, 
due to the diversity and the complexity of this design space. 
Designers working on this category of interfaces, indeed, tend to 
focus on a particular user experience rather than a precise set of 
musical activities. Performing formal design and evaluation strategy 
might turn out to be very demanding.  
Through a systematic review and a reinterpretation of related 
design dimensions and heuristics, this study p roposes a new 
conceptual model that aims at tackling this issue. The fruit of this 
research, MINUET, is meant to  stimulate reflection on both an 
abstract and a practical level when designing novel musical 
interfaces. As well as  incorporating all the variet ies of musical 
interfaces (addressing this design space and leaving behind the 
somewhat artificial boundaries intrinsic to any definition), MINUET 
offers two important contributions. First, it shifts the focus of 
previous frameworks framing a design space of musical interfaces 
centered on players’ experience. Second, to the best of our 
knowledge, this is the first attempt to develop a framework with a 
precise temporal unfolding rather than a set of heuristics. 
This study laid the foundations for developing a framework for the 
design and evaluation of musical interfaces. So far, we provided a 
conceptual model that frames designer’s goals into a set of 
specifications. The next step wi ll be to investigate how these 
Proceedings of the International Conference on New Interfaces for Musical Expression
471
specifications can be validated through a series of success criteria. 
Afterwards, we will conduct a study to examine how to gather 
empirical data on the basis of success criteria in their natural setting. 
Due to the hedonic nature of the interfaces and due to the fact that 
they are often used in non -controlled live conditions, most of the 
traditional HCI techniques for evaluating users experiences cannot 
indeed be applied to this design space.  
7. ACKNOWLEDGMENTS 
We would like to thank  all the colleagues that participated to the 
workshops and in particula r Raul Masu for  providing valuable 
feedback. Furthermore, we own gratitude to Liam Bannon for 
enlightening discussions and to Ms. Costanza Vettori for editing the 
paper. 
8. REFERENCES 
[1] S. Benford and H. Schnädelbach. Expected, sensed, and 
desired: A framework for designing sensing -based 
interaction. ACM TOCHI, 12.1. 2005, 3-30. 
[2] D. Benyon, P. Turner and S. Turner. Designing interactive 
systems: People, activities, contexts, technologies. Pearson 
Education, 2005. 
[3] H. Beyer and K. Holtzblatt. Contextual design: defining 
customer-centered systems. Access Online via Elsevier, 
1997. 
[4] D. Birnbaum, R. Fiebrink, J. Malloch and M. M. 
Wanderley. Towards a Dimension Space for Musical 
Devices. In Proc. of NIME, 2006, 192–195.  
[5] T. Blaine and S. Fels. Collaborative musical experiences 
for novices. Journal of New Music Research , 32(4). 2003, 
411–428. 
[6] B. Bongers and G.C. Veer. Towards a Multimodal 
Interaction Space: categorisation and applications. 
Personal and Ubiquitous Computing, 11(8). 2007, 609–
619.  
[7] A. Camurri, C. Canepa and G. Volpe. Active listening to a 
virtual orchestra through an expressive gestural interface: 
The Orchestra Explorer. In Proc. of NIM, 2007, 56–61. 
[8] J.M. Carroll. Making use: scenario-based design of 
human-computer interactions. The MIT press, 2000.  
[9] Cumhur Erkut, A, J. and R. Disçioglu. A structured design 
and evaluation model with application to rhythmic 
interaction displays. In Proc. of NIME, 2011, 477-480. 
[10] J. Drummond. Understanding Interactive Systems. 
Organised Sound, 14(02), 124. 2009. 
[11] S. Fels and K. Mase. Iamascope: a graphical musical 
instrument. Computers & Graphics, 23(2), 1999. 277–286. 
[12] M. Gurevich and A. Cavan Fyans. Digital Musical 
Interactions: Performer–system relationships and their 
perception by spectators. Organised Sound, 2011, 16(02). 
[13] E. Hornecker and J. Buur. Getting a grip on tangible 
interaction: a framework on physical space and social 
interaction. In Proc. of CHI , 2006 437–446. 
[14] A. Hunt and R. Kirk. Mapping strategies for musical 
performance. Trends in Gestural Control of Music. 2000, 
231–258. 
[15] A. Johnston, L. Candy and E. Edmonds. Designing and 
evaluating virtual musical instruments: facilitating 
conversational user interaction. Design Studies, 2008, 
29(6). 
[16] S. Jordà. Digital Instruments and Players : Part I – 
Efficiency and Apprenticeship. Proc. of NIME, 2004, 59–
63. 
[17] S. Jordà, G. Geiger, M. Alonso and M. Kaltenbrunner. The 
ReacTable: Exploring the synergy between live music 
performance and tabletop tangible interfaces. In Proc. of 
TEI, 2007. 
[18] A. Marquez-Borbon, M. Gurevich, A.C. Fyans and P. 
Stapleton. Designing Digital Musical Interactions in 
Experimental Contexts. In Proc. of NIME, 2011, 373–376. 
[19] E.R. Miranda and M.M. Wanderley. New digital musical 
instruments: control and interaction be yond the 
keyboard (Vol. 21). AR Editions, 2006. 
[20] F. Morreale, A. De Angeli, R. Masu, P. Rota and N. 
Conci. Collaborative creativity: The Music Room. 
Personal and Ubiquitous Computing . Springer. 2013, 1-13 
[21] F. Morreale, R. Masu and A. De Angeli. Robin: an 
algorithmic composer for interactive scenarios. In  Proc. of 
SMC, 2013. 
[22] S. O’Modhrain and G. Essl. PebbleBox and CrumbleBag: 
tactile interfaces for granular synthesis. Proc of NIME, 
2004. 
[23] S. O’Modhrain. A framework for the evaluation of digital 
musical instruments. Computer Music Journal , 35(1). 
2011, 28–42. 
[24] D. Overholt. The Musical Interface Technology Design 
Space. Organised Sound, 14(02), 217. 2009. 
[25] J. A. Paradiso. The brain opera technology: New 
instruments and gestural sensors for musical interaction 
and performance. Journal of New Music Research  28.2. 
1999, 130-149. 
[26] D. Trueman and P. R. Cook. BoSSA: The Deconstructed 
Violin Reconstructed. In Proc. of ICMC. 1999, 232–239. 
[27] Varni, M. Mancini, G. Volpe and A. Camurri. 
Sync’n'Move: social interaction based on music and 
gesture. User Centric Media. 2010, 31–38. 
[28] J. Vidyarthi, B. Riecke and D. Gromala. Sonic Cradle: 
designing for an immersive experience of meditation by 
connecting respiration to music. Proc. of DI, 2012, 408–
417. 
[29] J. Votano, M. Parham and L. Ha ll. Audience participation 
and response in movement -sensing installations. Proc. of 
the ICMC, 2000, 1–6. 
[30] I. Wallis, T. Ingalls, E. Campana and C. Vuong. Amateur 
musicians, long-term engagement, and HCI. In  Music and 
human-computer interaction. Springer London. 2013, 49-
66. 
[31] M.M Wanderley and N. Orio. Evaluation of input devices 
for Musical Expression  : Borrowing Tools from HCI. 
Computer Music Journal , 26(3), 2002, 62–76. 
[32] K. Wilkie, S. Holland and P. Mulholland. Towards a 
participatory approach for interaction design based on 
conceptual metaphor theory: A case study from music 
interaction. In Music and Human-Computer 
Interaction (pp. 259-270). Springer. 2013, 259 -270.
 
Proceedings of the International Conference on New Interfaces for Musical Expression
472