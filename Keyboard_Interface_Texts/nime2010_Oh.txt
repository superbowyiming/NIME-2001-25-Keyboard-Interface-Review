Evolving The Mobile Phone Orchestra
Jieun Oh, Jorge Herrera, Nicholas J. Bryan, Luke Dahl, Ge Wang
Center for Computer Research in Music and Acoustics (CCRMA)
Stanford University
660 Lomita Drive
Stanford, California, USA
{jieun5, jorgeh, njb, lukedahl, ge}@ccrma.stanford.edu
ABSTRACT
In this paper, we describe the development of the Stan-
ford Mobile Phone Orchestra (MoPhO) since its inception
in 2007. As a newly structured ensemble of musicians with
iPhones and wearable speakers, MoPhO takes advantage
of the ubiquity and mobility of smartphones as well as
the unique interaction techniques oﬀered by such devices.
MoPhO oﬀers a new platform for research, instrument de-
sign, composition, and performance that can be juxtaposed
to that of a laptop orchestra. We trace the origins of MoPhO,
describe the motivations behind the current hardware and
software design in relation to the backdrop of current trends
in mobile music making, detail key interaction concepts
around new repertoire, and conclude with an analysis on
the development of MoPhO thus far.
Keywords
mobile phone orchestra, live performance, iPhone, mobile
music
1. INTRODUCTION
The Stanford Mobile Phone Orchestra (MoPhO) provides
a platform for research, instrument design, and sound de-
sign, as well as new paradigms for composition and perfor-
mance. The overarching objectives of MoPhO are to explore
new possibilities in research, musical performance, and ed-
ucation. Though these are in many ways similar to those
of the Stanford Laptop Orchestra (SLOrk) [18], MoPhO
uses hardware, software, and interaction techniques that are
quite unlike those that have been explored using laptops.
MoPhO has undergone signiﬁcant development since its
inception in 2007 [19]. Beyond a change in mobile device
(from the Nokia N95 phones to the Apple iPhone), the Mo-
bile Music (MoMu) Toolkit [1] has been written to facilitate
programming mobile musical instruments. With an empha-
sis on the aesthetics underlying mobile music, this paper
summarizes the context of mobile phone usage in the do-
main of music (§2), describes current cultural trends, hard-
ware, software, and interaction designs ( §3), and concludes
with an analysis of the MoPhO paradigm with ideas for
future directions (§4).
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME2010, June 15-18, 2010,Sydney, Australia
Copyright 2010, Copyright remains with the author(s).
2. ORIGINS
Mobile phones have come to greatly aﬀect the lifestyles of
people around the world. With the increasing popularity
of smartphones that oﬀer advanced capabilities and user-
friendly interfaces, mobile phones have become powerful
yet intimate devices that serve a myriad of functions far
beyond their original intended usage. Naturally, music is
one such domain in which mobile phone users have found
new cultural and social trends. This section brieﬂy surveys
developments over the past decade in the exploration of the
mobile phone as musical instrument, reviews the birth of the
mobile phone orchestra within this context, and discusses
how the aesthetics of MoPhO as an academic ensemble par-
allels the evolving societal trends of mobile phone usage in
the domain of music.
Mobile phones have been explored primarily as new inter-
faces for controlling musical parameters and as part of loca-
tive performances. For instance, Tanaka controlled stream-
ing audio using accelerometer-based custom augmented PDA
(2004) [14], and Geiger designed a touch-screen based in-
teraction paradigm with integrated synthesis on the mobile
device (2003, 2006) [6, 7]. Other examples of employing
augmented interfaces on mobile phones for music include
CaMus [13], which uses the mobile phone camera for track-
ing visual references for musical interaction.
Beyond taking advantage of sensors, other works lever-
age the phone’s mobility and ubiquity. Levin’s Dial Tones
(2001) is among the ﬁrst works to explore the concept of us-
ing mobile devices as part of the performance by having the
audience members be the primary sound sources [9], and
Tanaka and Gemeinboeck’s installation piecenet d’erive
(2006), through GPS and wireless communication, traced
and displayed position information of moving audience in
city streets [15].
Gaye, Holmquist, Behrendt, and Tanaka provide a deﬁ-
nition of mobile music and describe how it can enable novel
forms of musical experience by taking advantage of changes
in social and geographical context [5]. Wang, Essl, and
Penttinen oﬀer a more complete survey of previous works
using mobile phones [20].
In 2008 as part of theau Design Project X(which evolved
into au’s new mobile-product brand iida in 2009), Yamaha
explored transforming the mobile phone into musical instru-
ments. Based on the concept of “Musical Mobile Phones /
Instruments You Can Carry”, Yamaha exhibited their vi-
sion of future mobile phones through several prototypes.
For instance, “Band in my pocket” features ﬁve musical
interfaces including those reminiscent of a trumpet, trom-
bone, and harmonica; “Sticks in the air” splits the phone
into two sticks to hold in each hand; and “Strings for ﬁn-
gers” contains ten strings, in the body of the phone, to
pluck.[21] [22]
The founding of the Stanford Mobile Phone Orchestra in
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
82
Figure 1: The Stanford Mobile Phone Orchestra
2007, as a repertoire-based ensemble using mobile phones
as the primary instrument, was intended to further these
concepts as well as explore new possibilities. The increas-
ing computational power of mobile phones has allowed for
sound synthesis without the aid of an external computer.
In fact, mobile phones have come to be regarded more as
“small computers” with PC-like functionality than as sim-
ply phones with augmented features. For the purpose of
music making, one may even regard the phone as being
superior to computers, oﬀering light-weight yet expressive
interactive techniques made possible by its various on-board
sensors.
The aesthetics behind MoPhO design considerations em-
brace the new culture of “mobile music” enjoyed by mobile
phone users. MoPhO aims to improve the process of instru-
ment development and performance experience — not only
for students and researchers in computer music, but also for
the broader population — thereby exploring new technolog-
ical and artistic opportunities for music making. The next
section describes the present status of the ensemble against
the social backdrop of increasing mobile application usage.
3. PRESENT
The proliferation of smartphones and other mobile plat-
forms — such as Nintendo DS or the PSP — along with
their corresponding SDKs and APIs have encouraged de-
velopers to create applications for these devices. The sheer
number of mobile instruments that have been developed
recently illustrates the increasing interest in mobile music
applications. These applications cover a wide range of mu-
sical interactions and sounds, from basic sample-based and
touch-triggered instruments[2] to more advanced accelerom-
eter and multitouch based synthesizers[12].
The availability of such instruments has encouraged both
their developers and application consumers to explore their
use in a number of creative ways. For instance, the idea of a
“mobile instrument mashup”[8], where a single or multiple
performers play a variety of mobile instruments simultane-
ously in a jam session, is an interesting concept as it shows
an increasing interest in collaborative mobile music mak-
ing and means to generate musical expressions in a social
setting. Examples covering a wide spectrum of sounds and
interaction techniques can be found on the web. A short
reference list can be found in Appendix A.
In this context, mobile music ensembles have found their
place in universities and other institutions. Aside from
Stanford MoPhO, other ensembles have emerged, such as
The Michigan Mobile Phone Ensemble[10], the Helsinki Mo-
Pho[11] and the Yamaha Mobile Orchestra[23].
3.1 Hardware
Over the past year, MoPhO has seen a considerable change
in the physical hardware used for performance and instru-
ment design. Originally, MoPho developed and performed
on un-ampliﬁed Nokia N95 smartphones generously donated
by Jyri Huopaniemi of Nokia. While these ﬁrst generation
phones served well, the Apple iPhone is currently the en-
semble’s platform of choice due to its superior computing
power, numerous on-board sensors, and convenient devel-
opment environment.
To provide additional ampliﬁcation and boost bass fre-
quencies, wearable speaker designs were tested and evalu-
ated. Three initial designs were prototyped in the following
form factors: necklace, jacket, and gloves.
Each prototype was based on commodity hardware and
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
83
Figure 2: Building the Speaker Gloves
employed minimal tools. The prototype designs were quite
basic, but nonetheless allowed testing on usability and in-
teraction experience (Figs. 3, 4). While the necklace and
jacket designs served the basic purpose of ampliﬁcation,
they felt bulky and diﬃcult to control, especially the direc-
tionality of sound. On the other hand, the close proximity
and controllability of the gloves in relationship to the phone
seemed to provide a much more satisfying, immediate, and
transparent user experience.
Using the glove speakers, the natural position of hold-
ing the mobile phone with one hand and controlling the on
screen user interface elements with the other hand results
in the sound projecting both above and below with respect
to the surface of the mobile device. Performers can easily
change the directionality of the ampliﬁed sound by making
simple hand or arm gestures.
Moreover, the close distance between the speaker and
phone can be seen to more closely approximate an acoustic
instrument. As mentioned in [16, 18], strongly associating
sound sources to their respective instruments aids in the
emulation of a traditional orchestral settings and provides a
closer association between instrument and performer. This
is an essential aesthetic of MoPhO and has been apparent
through all aspects of past and current performances.
3.2 Software and OS
The change in hardware consequently led to changes in
the software environment. The previously used Nokia N95
smartphones run Symbian OS, use C++ and Python pro-
gramming languages, and require a somewhat complex ini-
Figure 3: Prototype Designs: Speaker necklace and
speaker jacket
tial development setup procedure. Such development envi-
ronment was perceived to be more formidable for new de-
velopers and constrained the resulting musical interaction
and performance.
With the change to iPhone hardware, mobile phone soft-
ware development has been greatly improved. The iPhone
SDK provides tremendous development support, largely in
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
84
Figure 4: Selected Prototype Design: Speaker
gloves
the Objective-C programming language using the XCode
development environment. The iPhone SDK provides tools
for graphically designing and testing user interfaces (Inter-
face Builder), and projects can include C or C++ code.
To streamline instrument development, an additional layer
of abstraction has been created on top of the iPhone SDK
through the MoMu Toolkit, greatly simplifying audio in-
put/output, synthesis, and on-board sensing among other
things [1].
3.3 Instruments, Interactions and
Performances
Figure 5: Stanford Mobile Phone Orchestra Con-
cert: December 3, 2009
A concert in December 2009 gave MoPhO an opportunity
to explore capabilities provided by these new hardware and
software platforms. The instruments and pieces presented
in this performance were built using the Mobile Music Toolkit
[1], and focused on achieving a tight coupling of sound and
physical interaction. In all but one piece, performers am-
pliﬁed sound using the glove speakers. InWind Chimes
described below, however, an 8-channel surround sound au-
dio system was used.
In a typical concert setting, performers are on a stage in
front of the audience. The possibility of moving around,
however, is greatly facilitated by the use of portable and
untethered devices. To take advantage of this fact, MoPhO
decided to experiment with a diﬀerent stage setup. The
concert was held in an indoor setting and the performers
were surrounded by the audience. Moreover, performers
frequently walked around in the performance space, making
various physical gestures from bouncing an imaginary ball
to swinging arms. This conﬁguration, with the perform-
ers’ initial locations shown in shaded gray, enables them to
move around the audience, thus giving each member of the
audience a diﬀerent musical experience, depending on their
position (Fig. 6).
Figure 6: Stage plan.
Colors by Jieun Oh is an instrument with an intuitive
multi-touch interface and visualization implemented using
OpenGL ES (Fig. 7). The main screen displays black and
white horizontal lines to guide the pitch layout of the screen,
much like a simpliﬁed piano keyboard. Performers trigger
polyphonic sounds by touching the screen in up to ﬁve loca-
tions; moving ﬁngertips in horizontal and vertical directions
controls the volume and pitch, respectively, of the triggered
sounds. The performers are given a visual feedback of the
precise location of touched points through colored rectangu-
lar markers. Additionally, a movement preset speciﬁc to the
piece“Colorful Gathering”under settings facilitates chang-
ing parameters between movements of the piece. The simple
and intuitive interface allows performers to trigger complex
polyphonic sounds with subtle variations in pitch and vol-
ume without needing to look at the screen. Consequently,
it encourages face-to-face interaction between performers,
much as in a classical ensemble setting. During the perfor-
mance, a performer walks up to another performer to carry
out a sonic conversation, in which various emotions—from
teasing, curiosity, to excitement and discouragement—are
conveyed.
interV by Jorge Herrera is a collaborative instrument
that uses the iPhone accelerometer as its principal expres-
sive control (Fig. 7). Basically, two axes of acceleration are
mapped to the volume of two notes simultaneously played.
The performer controls the sound using expressive gestures
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
85
Figure 7: Screenshots of Instruments: (from left to right) Colors, interV, Wind Chimes, SoundBounce.
such as gentle tilts or larger arm movements, allowing the
audience to visually map sounds to movements. Also, the
instrument is capable of receiving and displaying instruc-
tions sent by a central server during the performance. Us-
ing this instrument, MoPhO performed intraV, a piece that
takes advantage of the two main features of the instru-
ment — motion control and network message transmission.
Based on the instructions received, performers move around
the stage and walk in between the audience, while mov-
ing their hands and arms to create a continuously changing
soundscape.
Similar to this idea, Wind Chimes by Nicholas J. Bryan
leverages mobile phones as directional controllers within a
8-channel surround sound audio system (Fig. 7). To do
so, the physical metaphor of wind chimes was used to con-
nect “physical” chimes (8-channel system) to a wind force
(performer/mobile phone). For performance, one or more
players stand in the center of the playback system, orient
themselves in a speciﬁc direction, and physically blow into
the phone microphone to trigger a gradual wash of wind
chimes sounds moving across the performance space. While
the metaphor is fairly simple in concept, the familiarity and
direct interaction proved beneﬁcial and allowed audience
members to immediately associate the performers actions
to the auditory result, just as in a traditional musical en-
semble.
Finally, the piece SoundBounce by Luke Dahl is based
on the metaphor of a bouncing ball [3]. Virtual balls and
their physics are simulated, with the height of the balls con-
troling the sound synthesis. Performers are able to bounce
sounds, drop them, take aim at other performers, and throw
sounds to them, causing the sound to move spatially from
one performer to the other. The instrument is designed
to be gesturally interactive, requiring minimal interaction
with the GUI. To that aim, all instrument interactions and
state changes have audible results which contribute to the
sound-ﬁeld of the piece. The piece ends with a game in
which performers try to throw sounds and knock out other
players’ sounds. As players’ sounds are knocked, their sound
output becomes progressively more distorted until they are
ejected from the game and silenced. The winner is the last
player still making sound.
4. ANALYSIS
Since the instantiation of MoPhO in 2007, transforming mo-
bile phones into “meta-instruments” has become an achiev-
able reality. A switch to the iPhone platform, with its
powerful capabilities and well-documented SDK, has facil-
itated the process of repurposing the phone into a musi-
cal instrument. The 2008 paper on the birth of MoPhO
noted that “there is no user interface that would allow
non-programmers to easily set up their own composition
yet.” While this still remains problematic, the problem has
been partly mitigated through the newly written MoMu
Toolkit [1]. Developers can now write primarily in C/C++,
avoiding Cocoa and the iPhone SDK if they desire. Addi-
tionally, Georg Essl has authored an environment that oﬀers
abstractions for interaction and interface design on mobile
devices [4]. In this manner, many of the technical barriers
to creating a mobile phone instrument are being tackled by
numerous research groups.
Nonetheless, there are many areas for future develop-
ment, especially with regards to instrument re-use and per-
formance concepts in light of the burgeoning interest of mo-
bile music making. With the ease of software development
comes proliferation of instruments, and consequently these
“soft instruments” have become more or less disposable
items: often times, an instrument gets written for a speciﬁc
piece and gets abandoned thereafter. A public repository
on existing instruments and documentation may encourage
instrument sharing, re-use and further development.
As for exploring mobile music performance paradigms,
future work should focus on the social and geographical
elements of performance. These types of musical experi-
ences may manifest partly on-device, and partly in back-
end “cloud computing” servers, and seeks to connect users
through music-making (iPhone’s Ocarina is an early exper-
iment [17]). Future directions for the ensemble include ex-
perimenting with pieces that involve participation from the
audience as well as performers from geographically diverse
locations and potentially asynchronous models for collab-
orative performance. An architecture to facilitate social
musical interaction between performers who may be dis-
tributed in space should be developed to better understand
the phenomenon of mobile music making. Mobile phones’
ubiquity, mobility, and accessibility have begun to break
down the temporal and spatial limitations of traditional
musical performances, and we anticipate blurring of once-
distinctive roles of a composer, performer, and audience, as
one can now more easily partake in the integrated music
making experience.
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
86
5. ACKNOWLEDGMENTS
The Stanford Mobile Phone Orchestra has been generously
supported by the National Science Foundation Creative IT
grant No. IIS-0855758. We would like to thank Robert
Hamilton for helping the ensemble throughout its develop-
ment. Additionally, we would like to thank the following
people for helping to build the ensemble: Steinunn Arnar-
dottir, Michael Berger, Dave Kerr, Lepold Kristjnsson, Nick
Kruge, Chris Llorca, Uri Nieto, Adam Sheppard, Adam
Somers, and Xiang Zhang.
6. REFERENCES
[1] N. J. Bryan, J. Herrera, J. Oh, and G. Wang. Momu:
A mobile music toolkit. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Sydney, Australia, 2010.
[2] cappelnord. ipod touch step sequencer. Online video
clip. YouTube, December 2007. Accessed on 08 April
2010
http://www.youtube.com/watch?v=EPdf6c9oNIE.
[3] L. Dahl and G. Wang. Sound bounce: Physical
metaphors in designing mobile music performance. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Sydney,
Australia, 2010.
[4] G. Essl. urMus: Audio and Media Interactions and
Interfaces on Mobile Phones.
http://urmus.eecs.umich.edu/. Retrieved on Apr 7,
2010.
[5] L. Gaye, L. E. Holmquist, F. Behrendt, and
A. Tanaka. Mobile Music Technology: Report on an
Emerging Community. In Proceedings of the 6th
International Conference on New Instruments for
Musical Expression (NIME), pages 22–25, June 2006.
[6] G. Geiger. PDa: Real Time Signal Processing and
Sound Generation on Handheld Devices. In
Proceedings of the International Computer Music
Conference, Singapure, 2003.
[7] G. Geiger. Using the Touch Screen as a Controller for
Portable Computer Music Instruments. In Proceedings
of the International Conference on New Interfaces for
Musical Expression (NIME), Paris, France, 2006.
[8] HirnW. iband. Online video clip. YouTube, February
2008. Accessed on 08 April 2010
http://www.youtube.com/watch?v=Mh0VX74alwk.
[9] G. Levin. Dialtones - a telesymphony.
www.flong.com/telesymphony, Sept. 2, 2001.
Retrieved on April 1, 2007.
[10] U. O. Michigan. The michigan mobile phone
ensemble. Website, March 2010. Accessed on 08 April
2010http://mopho.eecs.umich.edu/.
[11] H. Penttinen and A. Jylh¨ a. Helsinki mobile phone
orchestra. Website, Unknown creation or update date.
Accessed on 08 April 2010http://www.acoustics.
hut.fi/projects/helsinkimopho/.
[12] rlainhart. Rudess meets bebot. Online video clip.
YouTube, February 2009. Accessed on 08 April 2010
http://www.youtube.com/watch?v=KFG7-Q0WI7Q.
[13] M. Rohs, G. Essl, and M. Roth. CaMus: Live Music
Performance using Camera Phones and Visual Grid
Tracking. InProceedings of the 6th International
Conference on New Instruments for Musical
Expression (NIME), pages 31–36, June 2006.
[14] A. Tanaka. Mobile Music Making. In NIME ’04:
Proceedings of the 2004 conference on New Interfaces
for Musical Expression, pages 154–156, June 2004.
[15] A. Tanaka and P. Gemeinboeck. net d’erive. Project
web page, 2006.
[16] D. Trueman, P. Cook, S. Smallwood, and G. Wang.
Plork: The princeton laptop orchestra, year 1. In
Proceedings of the International Computer Music
Conference, pages 443–450, New Orleans, Louisiana,
2006.
[17] G. Wang. Designing smule’s iphone ocarina. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME),
Pittsburgh, Pennsylvania, 2009.
[18] G. Wang, N. Bryan, J. Oh, and R. Hamilton.
Stanford laptop orchestra (slork). In Proceedings of
the International Computer Music Conference,
Montreal, Canada, 2009.
[19] G. Wang, G. Essl, and H. Penttinen. Do mobile
phones dream of electric orchestras? In Proceedings of
the International Computer Music Conference,
Belfast, UK, 2008.
[20] G. Wang, G. Essl, and H. Penttinen. The Mobile
Phone Orchestra. Oxford University Press, 2010.
[21] Yamaha. Kddi au: au design project: Concept design
2008. Website, 2008. Accessed on 01 February 2010
http://www.au.kddi.com/english/au_design_
project/models/2008/index.html%.
[22] Yamaha. Kddi au: au design project: The musical
instruments and mobile phones exhibit. Website,
2008. Accessed on 01 February 2010
http://www.au.kddi.com/english/au_design_
project/models/2008/gakki/inde%x.html?event=0.
[23] Yamaha. Moﬁano mobile orchestra. Website,
November 2009. Accessed on 08 April 2010 http:
//www.yamaha.co.jp/product/lsi/mofiano/mmo/.
APPENDIX
A. EXAMPLES OF MOBILE INSTRUMENT
MASHUPS
•MosKeto270. “Cool NES Synth Music Application on
iPhone.” 08 August 2009. Online video clip. YouTube.
Accessed on 08 April 2010. <http://www.youtube.
com/watch?v=-0ps1s_JvhI>
•thehighams. “iPhone Music Apps.” 30 May 2009. On-
line video clip. YouTube. Accessed on 08 April 2010.
<http://www.youtube.com/watch?v=tevO66NT1uE>
•HirnW. “iBand.” 17 February 2008. Online video clip.
YouTube. Accessed on 08 April 2010.<http://www.
youtube.com/watch?v=Mh0VX74alwk>
•CandlegravityStudio. “iPhone Instrument App Mu-
sic Mashup! SynthPond, MiniPiano, ToneBoard, Pakl-
Sound1, Kalimba, Harp.” 09 May 2009. Online video
clip. YouTube. Accessed on 08 April 2010. <http:
//www.youtube.com/watch?v=tu4PMywLgWk>
•AndreEclipseHunter. “Jordan Rudess - iPhone solo :)
(clinic).” 09 June 2009. Online video clip. YouTube.
Accessed on 08 April 2010. <http://www.youtube.
com/watch?v=JDvG1KF9FK8>
•PingMag. “Yamaha Mobile Orchestra.” 07 August
2008. Online video clip. YouTube. Accessed on 08
April 2010.<http://www.youtube.com/watch?v=Ig_-
osUuWLg>
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
87