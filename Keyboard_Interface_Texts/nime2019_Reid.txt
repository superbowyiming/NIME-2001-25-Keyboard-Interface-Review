Perspectives on Time: performance practice, mapping strategies, & composition with MIGSI  Sarah Reid  California Institute of the Arts  24700 McBean Parkway  Valencia, CA 91355 sreid@calarts.edu 
 Ryan Gaston  California Institute of the Arts   24700 McBean Parkway  Valencia, CA 91355 ryangaston@alum.calarts.edu  
 Ajay Kapur  California Institute of the Arts 24700 McBean Parkway  Valencia, CA 91355  akapur@calarts.edu    ABSTRACT This paper presents four years of development in performance and compositional practice on an electronically augmented trumpet called MIGSI. Discussion is focused on conceptual and technical approaches to data mapping, sonic interaction, and composition that are inspired by philosophical questions of time: what is now? Is time linear or multi-directional? Can we operate in multiple modes of temporal perception simultaneously? A number of mapping strategies are presented which explore these ideas through the manipulation of temporal separation between user input and sonic output. In addition to presenting technical progress, this paper will introduce a body of original repertoire composed for MIGSI, in order to illustrate how these tools and approaches have been utilized in live performance and how they may find use in other creative applications.  Author Keywords Augmented instrument, mapping, performance practice, interface design, time  CCS Concepts • Applied computing → Sound and music computing; Performing arts; • Hardware → Sensor devices and platforms;  1. INTRODUCTION Electronic musical instrument designer Don Buchla refers to electronic instruments as containing three primary components: “an input structure that we contact physically, an output structure that generates the sound, and a connection between the two [7].” Unlike traditional acoustic instruments, in which these components are most often necessarily coupled, many electronic instruments have “total independence between input and output [7].” Moreover, players of the same or similar electronic instruments may choose to map their user input to entirely different sonic results from one another. For example, three musicians each striking middle C on their own keyboard synthesizer may produce any number of sounds, depending on how the patches or presets on their instruments are configured.  Hunt, Wanderly, and Paradiso emphasize the importance of considering mapping strategies in electronic instrument design at length in [9]. The mapping strategy—the connection between input and output—can dictate not only to where each input source should be mapped, but also how the connections should be made. Since input and output are independent in electronic instruments, we can use this in-between connection as a means by which to emulate the interaction of an acoustic instrument or to create something entirely new. Regardless, the connection between input and output is a crucial factor in making an instrument playable and giving it a unique voice.  
 This paper focuses primarily on how the independence between input and output can be utilized as a vehicle for exploring musical and performative time. Since there is no requirement for user input on an electronic instrument to result in immediate sonic output, what would happen if the two were instead displaced, delayed, stretched, accumulated, or folded over onto themselves? How would this temporal manipulation alter the meaning or perception of simultaneity or “now-ness” in composition or live performance? If one gesture can produce multiple events, do they always unfold in linear sequence? Can something happening now affect the memory of something that happened earlier?  This paper presents a body of work for electronically augmented trumpet (MIGSI) that explores these notions of time and temporal manipulation. The above questions are posed as a means of providing some creative context to the compositions and strategies introduced here. Arriving at specific answers is not the primary goal—in fact, the inherent subjectivity present in them is the very thing that has fueled much of this work, and continues to propel it forward. Rather, these questions are approached through a phenomenological lens, with more emphasis placed on individual perception and experience from moment to moment. The nature of this research is exploratory and practice-led; each idea introduced here was incorporated into performance-based practice (including etudes, compositions, and improvisation sessions) for many months before reaching maturity.  Following completion of the initial MIGSI prototype in 2015, the developers dedicated four years to practicing the instrument, composing original music, and performing with it in a wide range of concert settings. The goal behind this was to develop a strong foundational performance and compositional practice from which to inform future iterations and development on the instrument. This paper will introduce a number of strategies for data mapping and sonic interaction that each approach the temporal manipulation of input and output in a different way. Additionally, a series of musical compositions will be shared as case studies to illustrate how these techniques have been incorporated into live performance.   2. BACKGROUND 2.1 Musical Time Music theorist J.D. Kramer speaks of “musical time” as a phenomenon that is distinct from the absolute time by which we live our daily lives [14]. Absolute time is like an external container, unconcerned with the events or people that exist within it. We use clocks and watches to measure this external time, and more or less agree to synchronize and organize ourselves within it on a global scale [1]. Musical time, on the other hand, is something more subjective. It is not a thing contained within time, but rather a thing that contains time. That is to say, musical time exists within a piece of music, or perhaps within those who experience it. Other theorists and psychologists refer to similar notions of internal, subjective time such as “inner clocks” [1] or “intuitive time” [20]. This kind of time is personal and experiential—constantly in flux, and not necessarily bound by the rules of absolute time. 
234
According to Kramer, when considering time in this way, “we begin to glimpse the power of music to create, alter, distort, or even destroy time itself” [14]. The work presented here is primarily concerned with manipulating this sense of musical time and altering one’s perception of now and the temporal scaling of events.  2.2 MIGSI Hardware MIGSI (Minimally Invasive Gesture Sensing Interface) is a wireless interface that attaches onto any standard B-flat or C trumpet to capture gestural data from the performer and their instrument. The interface consists of an optical sensor beneath each valve to detect continuous valve displacement information, force sensitive resistors on either side of the valve casing to detect hand tension and finger pressure of the left hand, and a 3-axis accelerometer. MIGSI builds upon the work of many others in the area of augmented instrument design, specifically EROSS [11], Hans Leuww’s Electrumpet [15, 16], Ben Neill’s Mutantrumpet [18, 19], Jonathan Impett’s Meta-Trumpet [10], and the pioneering work of Perry Cook and Dexter Morrill [4, 5]. More details about MIGSI’s design considerations and technical specifications are available in [21] (see Figure 1).   It is worth briefly mentioning that because MIGSI is an electronically augmented acoustic instrument, there is only partial independence between user input and sonic output. Any blowing or buzzing through the trumpet will produce an immediate sonic result. One could theoretically use a silent mute with a pick up (as seen in [6]) to displace the trumpet sound along with the sensor data, however the preservation of the acoustic sound and playability of the trumpet is key to the original design considerations of MIGSI. The approach taken was to instead use the electronic side of the instrument to extend the temporal possibilities of each gesture: pressing down a valve may simultaneously facilitate the playing of a specific pitch in the present and set into motion a future process or recall a previous event.  
 Figure 1. MIGSI hardware diagram 2.3 MIGSI Software Initial creative work with MIGSI involved using the device for software interaction in Max/MSP, constructing sonic resources and data mapping schemes on a piece-by-piece basis. We quickly noticed trends in our usage of the direct sensor data and created a general-purpose application in Max that provided several conditioned variants of the incoming data and formatted it for OSC output: in this way, we were able to develop a more or less standard input structure for general use (this version was presented in [21]).  It likewise became apparent that developing a general-purpose synthesis environment to accompany the data parsing application would be crucial for development of improvisational facility with the interface, as well as iteration on “successful” experiments and creation of new pieces. We settled on the idea of a modular platform with dedicated resources for synthesis and processing, as well as a host of  reconfigurable control generation and control processing “modules” which would enable any level of dependence/independence from the data received from MIGSI itself. 
                                                                    1 More information on MIGSI including musical scores, videos, and further documentation can be found at www.migsitrumpet.com 
 The current version of the MIGSI application segregates audio and control information, and further divides control information into three types:  1. External World data: MIGSI data itself as well as external audio sources and specialized OSC inputs; 2. Control data: continuous control data produced inside the application itself;  3. Pulse data: high/low logic data generated inside the application itself, typically used for initiating events, and may be transient or sustained in nature. The application is modular in that select parameters for each synthesis, sound processing, control, and pulse module offer the opportunity for influence from other modules via dedicated modulation source drop-down menus adjacent to the GUI controls for the destination parameters in question.  Each of MIGSI’s sensors is inherently divided into, at minimum, seven correlated streams of data including a scaled and smoothed continuous signal as well as numerous latching and transient pulses produced when definable ascending or descending thresholds are breached. Given the modular nature of the application, it becomes possible to further combine, process, and abstract the yielded data in various ways: sensor data may be re-shaped, mixed with data from other sensors, enabled or disabled by the activation of other sensors, delayed by arbitrary amounts of time, used to trigger events, or influence autonomous processes—it is here that the potential for obfuscating the relationship between input and output occurs, and where experimentation with the implications of temporal displacement unfolds. Following are descriptions of several experiments regarding the ramifications of temporal displacement with MIGSI.1  3. DATA PROCESSING 3.1 Information Delays Early mappings and compositions approached MIGSI much more like an acoustic instrument than an electronic one. That is to say, every gesture had an immediate result and each sensor was typically only mapped to one parameter. While this did achieve a strong sense of repeatability and predictability on the instrument, it quickly became problematic as more nuanced interactions and improvisatory freedom were desired.   MIGSI is designed to leverage pre-existing trumpet gestures for control information. For example, as each valve is depressed, data is collected from the optical sensors which can be used as continuous data or as a momentary trigger or event. The valves on a trumpet are, of course, also necessary for playing the instrument acoustically. It was found that direct one-to-one mappings with valve data can be musically limiting, especially in improvisatory settings. If every time the first valve is pressed, for example, a sound is triggered (separate from the acoustic trumpet sound), the performer may feel a need to choose between sacrificing melodic ideas for the sake of electronic results, or vice versa. On the other hand, having no perceptible connection between gesture and result can be equally problematic, giving the performer the feeling that they are playing with arbitrary sounds over which they have no connection or influence.   It was found (through the author’s experience) that temporally displacing some of the data through various information delays while maintaining certain points of immediate response could provide the performer with a sense of control and repeatability while giving them the freedom to improvise on the “acoustic side” of the trumpet. Although quite simplistic, this approach has had tremendously 
235
positive implications for the playability of MIGSI and the development of compositions. 3.1.1 Pocket Fig (2015) In this semi-improvisatory piece, descending threshold detection on each valve would detect the release of valves (from fully depressed to open) and trigger a flurry of sounds from a buffer pre-populated with trumpet samples. In order to introduce variability to the sound material, the samples were each triggered with random playback speed and window size. The piece began with immediate response on the valves, giving both performer and audience a clear sense of connection between physical input and sonic result. As the piece progressed, data from certain sensors was put through information delays of increasing lengths, gradually displacing user input and sonic output farther and farther from one another in time.   Although technically simple, this gradual temporal displacement between input and output turned out to be musically significant, particularly for the performer. The use of information delays resulted in a piece that could be structurally very similar from performance to performance, but always had a touch of unpredictability, giving the performer enough freedom to improvise and enjoy spontaneous interactions. As the piece progressed, the density of events would ebb and flow like a playful call and response, at times unclear as to whether the trumpet or the electronics was leading the sonic narrative. 
 Figure 2. Emergent simultaneities resulting from information delays  Interestingly, as the valve sensor input and subsequent output became more and more temporally displaced from one another, new simultaneous events would begin to emerge (see Figure 2). From the perspective of the performer, these emergent simultaneities would appear to come from nowhere, often injecting new energy into a passage or inspiring a sudden change in musical direction. From the audience’s perspective, the playful banter between trumpet and electronics would occasionally align in moments of synchronicity, as though echoing the opening section of the piece. While this is acoustically what was perceptible, it was functionally occurring as the result of the sonic output of accumulated past input coinciding with a present-moment acoustic event. 3.2 Integration Limits to digital resolution often result in stepped data that does not translate well in contexts when smooth contours make more musical sense. Regardless the employed method, data smoothing results in lagged response: typically involving an averaging of values received over a predetermined number of polling cycles, smoothed values inevitably have to play “catch up” with the current raw sensor value. This is in essence a type of lowpass 
                                                                    2 This is similar to smoothing functions in many analog synthesizers as well, referred to as integrators, slope generators, slew rate limiters, lag processors, etc.  Given our experience with and influence from Serge 
filtering (specifically, Moving Average Filtering), and is employed on all of MIGSI’s sensor inputs.2  When extended to moderate averaging windows, this slewing method breaches into the territory of another familiar technique: envelope detection. “Envelope detection” as such typically employs faster rising slew and slower falling slew. Since “envelope detection” per se is typically used to extract an audio signal’s dynamic profile, this approach is favorable: it allows for reasonable analogous level detection/conversion (from the slow falling slew) while not sacrificing the potential for transient detection (thanks to the fast rising slew) [22].  Adjusting one’s perspective and breaching even further into this technique by extending the rising slew time opens up yet another realm of potential: detection of an activity’s relative persistence over a specified duration. It is in this case that our use of the concept of integration comes into focus: a long-term window in which past states are averaged with more recent states in favor of generating an integrated, singular, and steady signal representative of how the magnitude of events of the past and the present relate to one another—and by extension, how the magnitude of a present event may relate to its inevitably commingled counterparts in the future. 3.2.1 MIGSI & The Bots (2017) This approach was first explored in “MIGSI and the Bots,” a piece focused around integrating MIGSI with a series of mechatronic percussion instruments created by A. Kapur [12] and the CalArts Machine Orchestra [13]. A semi-improvisational work, this piece focused on extracting data from multiple sensors and assessing their temporal persistence in order to set subsequent chains of events into action.  This was achieved by first summing all of the valves’ outputs, and then processing the summed signal through an integrator with a long rising and falling slew time. With an adequately long rising integration window, the net result is that momentary valve presses are more or less “insignificant,” resulting in little change in the output signal level, whereas more persistently depressed valves gradually increase the output signal level. A long falling integration window holds the output value relatively steady until a sufficient amount of “insignificant” time has passed. The less the magnitude of subsequent events, the more significant the decrease in output value. In practice, the end result is that passages with sparse valve activity result in low average output values; passages with a lot of valve activity result in higher average output value, which increases proportionally with a higher concentration of repeated valve combinations or very persistent fast passages; and passages with constantly held valves (featuring long notes or melodic passages that involve one or more valves common between several pitches) generally produce faster accumulation to high output values. In the case of “MIGSI and the Bots,” this functions as a crude method of detecting the performer’s relative level of melodic activity.  By further processing the resulting integrated signal through a multi-window comparator, several isolated zones of intensity were mapped to initiate different musical processes for the mechatronic instruments: “medium-high” intensities resulted in the activation of more aggressive and persistent patterns among some instruments, with “very high” intensities resulting in the activation of several more instruments. Very high intensities also re-routed the integrated signal to speed control for these final instruments, making it such that in times of very high valve persistence, the average valve activity would influence the tempo of the mechatronic instruments, resulting in momentary flurries Tcherepnine’s modular instruments, including general-purpose slew processors in our modular software environment felt natural. 
236
and discontinuities when valves were quickly lifted and re-depressed. This provided an interesting contrast in musical materials: roughly speaking, higher valve persistence generally relates to slower passages and longer tones from the trumpet, but in this case produced more chaotic results from the tandem mechatronic instruments.  From the performer’s perspective, this mapping created a sense of trying to coax the mechatronic instruments to life—knowing in time that the user input through MIGSI would cause them to speak but not knowing precisely what amount of persistence would be required. This added a certain degree of intentionality and meaning to each gesture performed, which numerous audience members reported to add a sense of “wonder” to the performance. As momentum in the piece grew, so too did the response from the mechatronic instruments, as though they were now awake and listening.   This piece presented and proved two interesting applications of integration as “persistence detection”: use of persistence to directly impact perceived musical results, and use of persistence to alter the sensor mapping itself. 3.3 Pulse Processing Thomas Clifton states that time itself does not move, but events do. It is the relationship we form with the events we live that gives us a sense of passing or flow of time [3]. The passing of time in a typical day is framed by the events one experiences: waking up, brushing your teeth, drinking coffee, heading to the office, and so on. The act of “drinking coffee” may be experienced as a singular event, but it itself is made up of many micro-events or nows. According to Richard Muller, our experience of time passing is in fact the experience of flowing through a series of ever-changing nows [17]. J.D. Kramer likens the passing of life events (drinking coffee) to musical events, expressing that we form the same kind of relationship with musical events to help us understand the flow or passing of time [14]. The accumulation of nows is what gives us the ability to remember, predict, and form relationships with these larger temporal events.   The MIGSI application is set up to receive and detect pulses from all sensors upon breach of assigned thresholds, or from within the application itself as high/low data. Each pulse or trigger can be considered a signifier of a now event. Pulse processing enables us to monitor and experience the unfolding of musical time based on the occurrence of individual discrete events or the accumulation and persistence of events over a window of absolute time (as discussed in 3.2). Events that appear to be singular, such as drinking coffee or pressing a valve on the trumpet, in fact contain numerous micro-events that may go unnoticed. By recognizing and highlighting these individual nows we are in effect expanding or dilating our perception of time and the impact that each action may have. 3.3.1 Consider (2017) In this piece, MIGSI interfaces with a granular/waveguide synthesis program that receives gestural input captured in real time from the trumpet and its performer. The source material is a 3-second sample of the performer’s voice, speaking an imperative statement: “Consider [X],” where [X] is an issue that the performer regards with passion. The program has 10 states, each with a different level of intensity and density. The states are divided into 2 groups: 5 that have higher density and activity, and 5 that have lower density and activity. The high activity states all operate at a relentless, rigid, aggressive tempo. The low activity states are unmetered.   Every 40 seconds the program enters a 14.5-second long “listening period,” in which valve activity is monitored. Depending on the number of pulses captured from valve activity within this window, either a high or low activity state will be chosen at the end of the listening period, to continue for the following 40 seconds. The effect of state changes is an internal re-mapping of certain sensor data to new parameters. 
 The use of pulse processing is interesting because it may or may not be an audible process to the performer or audience members. In the case of “Consider,” the capturing of pulses was left inaudible to both parties—only the resulting change in state and mappings was heard. The performer understood that based on overall valve activity a change in sensor mappings may occur, but never knew precisely to which state the program would navigate. As such, there was once again a sense of intentionality behind each gesture and a sense of awareness that each action in the present moment could have an impact on future musical events. In this way, pulse processing can be considered as a collection of nows or micro events, in which the individual moments themselves are not necessarily uniquely identifiable but the aggregate impact or influence of them is.  4. FEEDBACK & SYNTHESIS Iteration in the development of the MIGSI application has led us to create a number of synthesis facilities to complement the possibilities offered by the MIGSI interface. For the most part, our goal for these synthesis systems is that they strike a balance between controllability and the capacity for self-sustaining, albeit unpredictable behaviors. The reasons for seeking this balance are numerous: we wanted to utilize the synthesis systems themselves to create nuanced results that could be influenced by MIGSI without requiring constant occupation of the performer’s physical bandwidth, and we were concerned with exploring means of creating interesting sounds through exploring our already-developing ideas about temporal displacement as a means of generating musically interesting control structures.  Our investigations since have hinged on creating synthesis structures which utilize variable time-delayed feedback in such a way that the resultant pitch, timbre, rhythm, etc. themselves become emergent properties of the system itself rather than directly controllable parameters. Rather than producing sounds with directly accessible pitch or timbre controls, for instance, the sound at any given time instead unfolds partly as a result of its current “settings,” but also as an emergent side-effect of all past fluctuations in the signal. Because feedback and delay lines are deeply embedded into all of these processes, it becomes difficult to predict exactly how and how profoundly interaction will influence them, embedding an element of playful risk and keen awareness of the relationship between a sound’s present state, its past, and its potential futures.  We will briefly present three of MIGSI’s primary synthesis resources. Note, however, that each of these resources represent a significant investment of time and experimentation, and a full explanation of their inner workings and implementation is beyond the scope of this discussion. 4.1 Temporal Reflection Interval Processor The Temporal Reflection Interval Processor (TRIP) provides a chaotic synthesis facility based on recursive processes: a variable feedback network whose aim is to generate cascading sonic changes with minimal synthesis resources and minimal user input. TRIP comprises a sine wave generator, variable delay line, multiplier, and crossfader, as well as three internal variable modulation indices, including two opportunities for variable-intensity signal feedback (see Figure 3).  With all of the aforementioned modulation controls set to have no influence, the signal path is straightforward: the sine generator seeds the delay line; the instantaneous sine value and delay line value are multiplied; the direct output from the sine generator and the multiplier are crossfaded (with the crossfader itself as output node). The sine frequency, delay time, and crossfader balance are all arbitrarily definable.  The three internal modulation routings provide continuously variable indices, which we have named Forward Influence, Reverse Influence, and Vertical Influence, respectively. The Forward Influence path allows modulation of the delay interval from the sine generator 
237
itself, while the Reverse Influence path enables frequency modulation of the sine generator from the delay line’s direct output. Vertical Influence provides another FM path to the sine generator: in this case, the crossfader’s final output is used as a modulator, such that both changes to the delay and manual changes to the crossfade amount impact the preceding generative structures.  By permitting internal modulation from “post-delay” resources to the “pre-delay” generator, one may create sonic structures whose properties continuously change in a cascading fashion. Moreover, introducing nonlinear temporal displacement within this structure decouples each individual control from its obvious functional purpose, turning each parameter into a multimodal control that impacts pitch, rhythm, timbre, and overall sonic stability in their own respective fashions. 
 Figure 3. Block diagram of TRIP 4.2 Temporal Conduit Oscillator  The Temporal Conduit Oscillator (TCO) is a more complex structure which takes advantage of feedback in its own peculiar way. On its surface, the TCO draws direct influence from the Buchla 259 Programmable Complex Waveform Generator, as well as Rob Hordijk’s Blippoo Box, an instrument itself centered around unpredictable chaotic structures resulting from the interaction of multiple internal feedback paths, including the use of a multi-tapped shift register to influence the signals that act as its own clock and data inputs (a Rungler, in Hordijk’s own terms) [8].  Like the 259, the TCO is a dual oscillator, configured such that one oscillator acts primarily as a modulation source, and the other as an audio generator with complex wave shaping capabilities. Where the TCO primarily differs is in its capacity for introduction of a variable delay line between the wave shaper’s output and its own input. Both the delay line’s output and the primary oscillator’s output are fed into a crossfader prior to the wave shaper: this crossfader, labeled “Now–Then,” offers control for the proportional balance between the oscillator signal and feedback in the wave shaping path.  The wave shaper itself offers a small collection of controls for biasing, folding, and windowing the signal passing through it. In combination with the delay line itself, which provides variable delay interval and variable-frequency resonant lowpass filter, this shaping scheme is prone to generating abrasive oscillations, sputters, and peculiar rhythmic patterns even with static settings. Every parameter may internally be controlled via the modulation oscillator itself, a Rungler not unlike Hordijk's own, and of course from MIGSI itself.  Care was taken in scaling all of the TCO’s internal parameters to ensure that any combination of settings would generate compelling results: once adequate scaling was achieved, it became possible to generate consistently interesting results simply by randomizing all of the TCO’s settings. Randomization as a single-source control method for the TCO has proven quite useful over time. Much like the TRIP, the TCO’s own internal parameters become significantly abstracted from their base functions with adequate feedback and delay applied. The TCO also similarly exhibits chaotic behaviors that emerge not only from instantaneous parameter settings, but also from the relationship of its own past states to its present. As with TRIP, this 
makes it possible for minimal input to generate complex results, leaving the performer’s bandwidth relatively uninhibited. 4.3 Granular Processor  The MIGSI application also features a granular processor which utilizes a variable-size buffer to store and recall past audio from an external input (usually from a trumpet, naturally), the TCO, and the TRIP. Typical granular synthesis parameters are present, including grain duration, pitch, window shape, density, regularity, etc., as well as per-grain randomization of all parameters. As with the TCO, the granular processor’s various parameters have been scaled in such a way that virtually all combinations of settings produce interesting results, making it such that it survives global randomization quite gracefully.  The processor’s typical mode of operation is continuous recording: once the buffer size has been defined, the processor records constantly, overwriting its buffer's contents in a scrolling fashion. In addition to its typical expected functionality, the granular processor provides an option for relatively high-gain feedback and wave shaping between its own output and recording input. Because this enables recording of "live" material and the granulated versions of "past" material simultaneously, it becomes conceivable that material from the past can sporadically re-materialize long after its initial execution. Furthermore, the use of wave shaping in the feedback path makes it possible to create not only various distortions of past material, but also unexpected accumulations of sound that seem to bear little resemblance to the sounds which seeded their generation.  4.3.1 Static Respirator (2017) “Static Respirator” is a piece that features both the TCO and the MIGSI granular processor. Both the external trumpet sound and TCO are sent into the granular processor, which is continuously recording and processing new input. The granular processor is configured to receive input from approximately 50% external sources (the trumpet and TCO) and 50% internal feedback (its own output). This creates distinctive tails of granularized sound that continue past and at times interrupt the trumpet and TCO voices.  The combination of the chaotic TCO and the feedback-heavy granular processor results in a musical texture that is very noisy and dense, with bursts of intense sound followed by momentary silence.   The force sensitive resistors (FSRs) on either side of the trumpet’s valve casing are assigned thresholds to control the randomization, silencing, and latching of the TCO voice. In the middle section of the piece, extreme pressure on the front FSR is used to breach the uppermost threshold, triggering a stored TCO preset (a high-pitched tone). Valve activity in this piece is used to control how much of the granular processor output is heard at any given moment. Perhaps the most notable takeaway from the process of composing and performing “Static Respirator” was the mapping of large changes in sound or density to descending thresholds. That is to say, all significant changes—be it sudden silence, chaotic randomization, or sudden stasis and calm—would be triggered by a release in tension on a given sensor. It was found that triggering these events upon release rather than application of pressure resulted in a much more natural and embodied experience for the performer and prevented accidental triggering. 5. CONCLUSIONS & FUTURE WORK The pieces discussed in this paper were all critical stepping stones in the development of MIGSI as an augmented instrument. In addition to performing the work presented here, the primary player of MIGSI (S. Reid) has also performed extensively with the instrument over the last 4 years as an 
238
improviser and ensemble musician. This body of work has presented an opportunity not only to explore new technical and creative ideas, but also—and perhaps more importantly—to form the foundation of an augmented instrument performance practice.   Despite the fact that one of MIGSI’s original design goals was to leverage pre-existing technique on the trumpet, this instrument still requires significant practice and dedication to master. The compositions and data mapping strategies presented here were developed gradually and in tandem with the player’s developing technique and facility on the instrument. Over time, as new techniques were mastered, we were able to work with more sophisticated and multi-parametric mappings (i.e. thresholds on individual sensors as in “Static Respirator” and state-specific mappings as in “Consider”). In effect, this process of development is not terribly different from that of practicing a traditional acoustic instrument and taking on more nuanced and virtuosic repertoire as skills progress. For many, this becomes a lifelong endeavor.    All of the compositions presented in this paper are united insofar that they explore the concept of temporally displacing user input from sonic output. In the first year of development with MIGSI, we struggled to find a balance between repeatability and spontaneity/range of expression on the instrument. Early sensor mappings were often immediate, direct, and one-to-one. This made it challenging for the performer to form a fluid, embodied relationship with the instrument. Instead, the experience felt more like playing trumpet while simultaneously attempting to navigate external controls. The various data mapping strategies presented throughout this paper (information delays, integration, feedback, etc.) were successful solutions to this problem because they helped to strike a balance between user control and creative freedom. Something as simple as splitting data from a sensor to produce results both in the present moment and at a future time was enough to shift the performer’s focus away from the execution of technical tasks and more toward listening, anticipating, and reacting in the musical moment. A similar notion of heightened listening and engagement when performing with interactive systems is discussed in Casal’s study on human and algorithmic improvisation [2].   In addition to the development of technical facility on MIGSI, this work has challenged performers of this instrument to reconsider their perception of performative and musical time. Since MIGSI is both acoustic and electronic, performing with it requires an adjustment of awareness and musical intention to include both present and possible future events. The act of playing MIGSI is just as much about present moment sound and gestures as it is about seeding, influencing, anticipating, or surrendering to potential future outcomes. In some respects, this means the performer must learn to operate on multiple modes of time simultaneously: physical action taken upon the instrument may simultaneously result in immediate acoustic response, immediate or delayed electronic response, the onset or change of audible or inaudible computational processes, the storage of present sound or data for future use, and so on.  Future work endeavors to expand upon the work presented here by continuing to grow the body of musical repertoire available for MIGSI, as well as continuing to experiment with time-related topics and how they may be integrated into live performance. Composers outside of the immediate MIGSI development team will be invited to contribute new works and participate in collaborations in order to broaden the scope of dialogue and practice surrounding this instrument. Finally, we intend to expand upon the possibilities of data mapping by incorporating machine learning into future iterations of MIGSI. 
6. ACKNOWLEDGMENTS Thank you to California Institute of the Arts for their continued support of this work, and to P. Cook, D. Rosenboom, and V. Golia for ongoing support and collaboration. Thank you to the reviewers for their detailed comments and feedback. 7. REFERENCES [1] B. R. Barry. Musical Time: The Sense of Order. Pendragon Press, 1990. [2] D. P. Casal. Time after Time: Short-circuiting the Emotional Distance between Algorithm and Human Improvisors. In Proc. of ICMC, 2008. [3] T. Clifton. Music as Heard: A Study in Applied Phenomenology. Yale University Press, 1983, 55. [4] P. Cook and D. Morrill. Hardware, Software, and Compositional Tools for a Real-Time Improvised Solo Trumpet Work. Center for Computer Research in Music and Acoustics, Stanford University, 1989. [5] P. Cook, D. Morrill, and J. O. Smith. A MIDI Control and Performance System for Brass Instruments. In Proc. of ICMC, 1993. [6] T. Craig and B. Factor. Trumpet MIDI Controller. Available online at https://people.ece.cornell.edu/land/courses/ece4760/ FinalProjects/s2008/twc22_bef23/twc22_bef23/index.html. Accessed Jan. 26, 2019. [7] J. K. Diliberto. An Interview with Donald Buchla. Polyphony., 8, 5 (Nov. 1983) 14–35.  [8] R. Hordijk. The Blippoo Box: A Chaotic Electronic Music Instrument, Bent by Design. Leonardo Music Journal, 19, (2009), 35-43. [9] A. Hunt, M. M. Wanderley, and M. Paradiso. The Importance of Parameter Mapping in Electronic Instrument Design. Journal of New Music Research., 32, 4 (Dec. 2003), 429–440. [10] J. Impett. A Meta-Trumpet(er). In Proc. of ICMC, 1994. [11] L. Jenkins, W. Page, S. Trail, G. Tzanetakis, and P. Driessen. An Easily Removable, wireless Optical Sensing System (EROSS) for the trumpet. In Proc. of NIME, 2013, 352–357. [12] A. Kapur. A History of robotic Musical Instruments. In Proc. of ICMC, 2005. [13] A. Kapur et al. The Machine Orchestra: An Ensemble of Human Laptop Performers and Robotic Musical Instruments. Computer Music Journal., 35, 4 (Dec. 2011), 49–63. [14] J. D. Kramer. The Time of Music: New Meanings, New Temporalities, New Listening Strategies. New York : London: Schirmer Books, 1988. [15] H. Leeuw. The electrumpet, a hybrid electro- acoustic instrument. In Proc. of NIME, 2009, 7–12. [16] H. Leeuw. The electrumpet, additions and revisions. In Proc. of NIME, (Ann Arbor, Michigan, 2012). [17] R. A. Muller. Now: The Physics of Time. W. W. Norton & Company, New York, 2016. [18] B. Neill. The Mutantrumpet. In Proc. of NIME, 2017, 489–490. [19] B. Neill and B. Jones. Posthorn. In Proc. of the 28th International Conference Extended Abstracts on Human Factors in Computing Systems (ACM), 2010, 3107–3112. [20] J. Piaget. The child’s conception of time. Routledge & K. Paul, 1969. [21] S. Reid, R. Gaston, C. Honigman, and A. Kapur. Minimally Invasive Gesture Sensing Interface (MIGSI) for Trumpet. In Proc. of NIME, 2016, 419–424. [22] A. Strange. Electronic Music: Systems, Techniques, and Controls. 2nd edition. Dubuque, Iowa: William C Brown Pub, 1983, 53–54.
239