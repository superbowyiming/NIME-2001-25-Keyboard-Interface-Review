The Space Between Us: A Live Performance with Musical 
Score Generated via Affective Correlates Measured in 
EEG of One Performer and an Audience Member  
 
 
Joel Eaton 
Interdisciplinary Centre for Computer 
Music Research (ICCMR) 
Plymouth University, UK  
joel.eaton@postgrad.plymouth.ac.uk
 
Weiwei Jin 
Music, Technology and Innovation 
Research Centre (MTIRC)    
De Montfort University, Leicester, UK  
weiwei.jin@dmu.ac.uk
 
Eduardo Miranda 
Interdisciplinary Centre for Computer 
Music Research (ICCMR) 
Plymouth University, UK  
eduardo.miranda@plymouth.ac.uk 
 
 
     
ABSTRACT 
The Space Between Us is a performance piece for vocals, piano and 
live electronics using a Brain -Computer Music Interface system 
currently in development. The brainwaves of one performer and one 
audience member are measured throughout the performance and the 
system generates a real-time score mapped to emotional features 
associated with the brain signals. The system not only aims to portray 
emotional states through music but also to direct and induce 
emotional states through the real -time generation of the score, 
highlighting the potential of direct neural-emotional manipulation by 
live performance. Two accepted emotional descriptors, valence and 
arousal, are measured via electroencephalogram (EEG) recordings 
and the two-dimensional correlates of averaged windows are then 
mapped to musical phrases. These pre-composed phrases contain 
associated emotional content based on the KTH Performance Rules 
System (Director Musices). The piece is in three movements, the first 
two are led by the emotions of each subject respectively, whilst the 
third movement interpolates the combined response of the performer 
and audience member. The system not only aims to reflect the 
individuals’ emotional states but also attempts to induce a shared 
emotional experience by drawing the two responses together. This 
work highlights the potential available in effecting neural-emotional 
manipulation within live performance and demonstrates a new 
approach to real-time, affectively-driven composition. 
 
Keywords 
NIME, BCI, brain-computer music interfacing, live per formance, 
real-time composition, affective states, emotions and music. 
1. INTRODUCTION 
The Space Between Us is an 18-minute long live performance for a 
singer and pianist , and with electronic sounds generated by a 
computer. During the performance both the singer and an audience 
member who is positioned in the front of the auditorium wear a brain 
cap. The caps are connected to the system and are  fitted with 
electrodes that detect emotional readings in their brain signals. At 
regular intervals during the performance the system measures the 
emotions of the performer and audience member and uses these 
readings to select the next musical phrase. An electronic score is 
presented to the singer and pianist via computer screens and the 
system also controls a processed piano feedback system with 
microphones and speakers positioned inside the piano, as well as  
computer generated electronic sounds. The piece aims to explore and 
influence the emotional connection between both parties; performer 
and audience member. 
 Emotion has long been considered an intangible condicio sine qua 
non for human interactions with music, be it whilst listening, 
performing, or composing. Mood induction in a multimodal 
environment is not a straightforward task , with a wide range of 
influential factors to take into account. The affective influence of 
music when combined with visual stimuli (for example, as signifiers 
and cues for emotion in film), suggest that utilising multimodality 
can increase affective responses in the listener [3]. Furthermore, there 
is some evidence that the range of affective responses in existing 
EEG/BCI (Brain-Computer Interface) systems can be increased by 
utilising multimodal stimuli [17]. The Space Between Us uses the 
medium of live performance with emphasis on em otional 
communication and induction through music and text alongside stage 
production dramaturgy. 
 Using brainwave control outside of laboratory environments in 
performance settings has become more feasible in recent years , 
helped by low-cost EEG 1 headsets that can interface with consumer-
grade computers. Brainwaves have been used for a range of musical 
applications including therapeutic uses [1 9], collaborative 
composition [11] and real-time performance [5].   
 In 1962 Joe Kamaya reported that it was possible to train subjects 
to perform voluntary control over alpha waves (8-13Hz) with 100% 
accuracy [13]. The most common method of control was relaxing 
and activating mental processes (which can be considered basic  
meditative emotional descriptors within EEG patterns ). The 
subsequent piece Music for Solo Performer (1965) by Alvin Lucier 
pioneered this approach of alpha control [18].. 
 In 2013 Giraldo and Ramirez conducted experiments using an 
EEG based system for mapping valence and arousal (the emotional 
indicators within EEG) to synthesise parameters providing emotional 
feedback through music [9]. In the same year Kirke and Miranda 
report on a system that composes generative music in response to 
subjects’ emotional levels that were recorded whilst listening  to 
emotionally charged music [14].  The system for The Space Between 
Us uses affective response to arrange a composition in real-time, but 
aims to add an additional dimension; employing an adaptive 
feedback loop for affective induction to manipulate mus ical 
parameters. In turn, this creates a new compositional arrangement 
with every performance. By incorporating both the audience 
member’s and the performer’s emotional perspective the system also 
                                                                    
1 The electroencephalogram (EEG) is the measurement of 
electrical activity in the brain by electrodes placed on the 
scalp. 
 
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 
NIME’14, June 30 – July 03, 2014, Goldsmiths, University of London, UK. 
Copyright remains with the author(s). 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
593
provides a platform for observing and analysing embodied emotional 
relationships between the two 
 
2. PASSIVE BRAINWAVE CONTROL 
BCI control for music commonly falls under one of two categories, 
those with active or passive control. Active systems are designed for 
explicit user control of brainwaves mapped to musical parameters 
[10, 21]. Passive systems utilise implicit control of brainwaves where 
control is not governed by user decision-making [8]. Natural shifts of 
emotion are not generally achieved through explicit choice and the 
system is designed to feedback the changes in affective states to the 
participants through music. It is hoped this  may help reflect the 
uncontrollable emotional interactions people encounter when 
engaged in active listening, re -enforcing these through live 
performance (Figure 1). However, as the system listens to the 
emotions of the users it adapts to the responses based on certain 
mapping rules. For example, we hypothesise that it is possible to 
produce music for enhancing a specific mood or to maintain a state, 
such as ‘relaxed’. This could be extended to provide therapeutic 
benefits or to help design more complex emotional ‘experiences’ 
within performances and other real-time applications such a s 
composing or collaborating. 
 
 
Figure 1. Conceptual diagram of the performance   
3. Measuring emotion in EEG 
Research is still in in the early stages of interpreting emotional 
language within EEG, and the information that can be detected is at 
best generalised descriptors of complex phenomena. Russell’s model 
of affect [23] provides a way of parameterising emotional responses 
to musical stimuli in two dimensions: valence (positivity) and arousal 
(energy or activation). Other models of music and emotion have been 
used in the past but the 2-dimensional model was implemented in 
this work as it has been well documented in respect to music and to 
neurophysical measurement by means of EEG [9, 24]. There are a 
number of known methods reported to measure levels of valence 
(scale of attraction) and arousal (alertness) within EEG. Aftanas and 
Golocheikine have measured levels of theta and alpha bands across 
the scalp to successfully determine synchronicity in the brain. This 
symmetry across the hemispheres of the brain, observed during 
meditation, is associated with positive emotions and can be used to 
provide a scale for valence [1]. In 2001 Schmitt and Trainor proposed 
a means of categorising emotional r esponses to music in EEG 
through measuring levels of valence and arousal in the alpha band via 
electrodes places on the frontal lobe. Here the level of arousal 
correlates to the spectral power of the band [23]. Their experiments 
indicated that during active listening (attentive focusing or feeling the 
music) music with known emotional qualities can induce predictive 
EEG patterns. In 2010 Lin et al. [16] monitored levels across three 
bands, delta (1 - 3Hz), beta (4 – 7Hz), alpha and gamma (31 – 50Hz) 
to discern relative levels of emotion recorded in response to music 
listening between self-reported emotions from subjects.  
 To measure brainwaves unobtrusively in a live performance 
environment a minimal number of electrodes is appropriate. The 
system presented here adopts Ramirez and Vamvakuosis’ approach 
of measuring valence and arousal across the alpha and beta bands at 
the AF3, AF4, A3 and A4 electrode positions [22]. EEG is sampled 
at a frequency of 256Hz and pre-processed using a notch filter to 
reduce mai ns power interference. To handle interference from 
blinking, muscle activity or movement related artifacts we adopt 
Tenke and Kaysers method of segmenting incoming EEG into 
epochs of 1 second (50% overlap; Hanning window) and rejecting 
those that are clipped above a threshold of +100 µv [25]. The raw 
EEG passes through butterworth bandpass filters to isolate the 
spectral power of each frequency band. Mean values of spectral 
power are normalised across the last 20 seconds of one and a half 
minute windows to gauge subjects’ response to the previously 
selected window of music. This method is designed to account for 
the previously documented effect of diminishing arousal over time as 
subjects familiarise themselves with the stimulus set and the test 
environment [4]. This would also be present in any additional bio-
sensors we may add to the system in the future to help improve 
measuring affective responses [7]. We are then able to extract values 
for the four quadrants of the 2-Dimensional arousal/valence model, 
broadly speaking; joy (positive valence and high arousal), anger 
(negative valence and high arousal), sadness (negative valence and 
low arousal) and relaxation (positive valence and low arousal) 
through the equations shown below. 
 
Arousal  = Valence =  
 
 
The resulting emotional trajectory is plotted across 2 
dimensions on Russell’s  [23] circumplex model of affect . The 
model is further bisected to create 12 regions of emotional 
intensity with associated values ( e-values) used for mapping to 
the musical engine of the system (Figure 2).  
 
 
Figure 2. Russell's 2-d model of valence and arousal [19 ] 
combined with e-values (a1, j1 and so on) of emotional 
feature and intensity. 
3.1 System design 
In the past developing musical systems with brainwave control has 
been a costly endeavour. In the current climate of more affordable 
EEG headsets we are looking to design systems using more 
accessible platforms. Unfortunately with cheaper components comes 
a trade-off in signal quality; a major concern when measuring and 
interpreting brain wave signals in real-time, especially in noisy and 
unpredictable environments away from the laboratory. With this in 
mind The Space Between Us provides an interoperable platform for 
format agnostic BCI hardware. In a bid to incorporate open source 
platforms calibrated EEG Signals from other proprietary or open 
source hardware can be connected to the performance software, as 
long as the pre-processed data can be sent via the OSC protocol. The 
system has been designed and tested with g.tec Sahara electrodes and 
MOBIlab+ amplifiers.  
 There are two elements to the system for The Space Between Us 
(Figure 3). The first element (made up of grey boxes) handles the 
incoming raw EEG signal before passing the pre-processed signal to 
the second element (made up of the white boxes), a musical engine 
that extracts the emotional features used as a control signal to 
generate the musical score and live electronic control. The music is 
comprised of phrases of pre-composed notation for voice and piano, 
Proceedings of the International Conference on New Interfaces for Musical Expression
594
and adaptable live electronics including real-time control over a live 
piano feedback processing unit.  
 The EEG of one performer (the singer) and one audience member 
is sent wirelessly from the amplifiers to nearby laptop computers. A 
third laptop receives the pre -processed signal over a network 
connection. Once the emotional features are extracted a mapping 
algorithm selects the next musical  phrase. This is sent to the 
electronic music processing algorithm and presented as a score on 
computer monitors to both the pianist and vocalist who are 
synchronised via a visual metronome. 
 
 
Figure 3. System overview. 
4. EEG CLASSIFICATION 
In order to obtain accurate measurements of valence and arousal 
classification of the incoming EEG is required. Popular methods used 
to determine a meaningful hyperplane to distinguish values of 
valence and arousal include SVM [17 ] and CSP  [15], but both 
require an involved course of training unsuitable for live performance 
environments. Therefore the musical phrases need to be built such 
that they elicit the desired emotional responses, using classification, 
in both the performer and the audience member for the system to 
function as desired. Matching music to one individual’s affective 
response is challenging – however, initial tests have indicated that 
profiles can be matched to achieve an approximately level playing 
field across a subset of listeners. This highlights a particular issue 
with a system of this kind,  (creating music that can elicit similar 
affective responses across individuals) which should be the subject 
for further investigation. 
 As levels o f valence and arousal differ for each individual our 
system adopts a simplified approach to determine basic calibration 
prior to performance. Previous studies have demonstrated that it is 
possible to measure affective responses to music in EEG [16, 24], 
and initial system tests have shown that the system is capable of 
reflecting states in subjects’ response to previously heard music. 
There are a number of factors that make eliciting desired affective 
states on–the-fly extremely difficult to achieve. Initia l tests hav e 
indicated that individuals can elicit unpredictable responses to music, 
especially across a high -resolution model. This is likely to be 
dependent on a range of factors including mood, taste, age, as well as 
social and cultural interpretations. Where standardised libraries such 
as the Affective Digitized Sounds database (IADS) of audio stimuli 
for emotion induction [2] exist to form a baseline against a range of 
subjects, it is clear that the calibration of a system should be relative 
to the stimuli itself to make sure that stimuli can elicit the desired 
states during performance.  
 We determine valence as a relative ratio of asymmetry in the 
frontal region for wh ich no training is necessary , but which we 
calibrate against the pre-composed musical phrases. [6]. 
5. MAPPING EMOTIONS TO MUSIC 
The piece is performed in three movements, the first two are led by 
the emotions of each subject in turn, and the third movement is based 
on the correlated response of both subjects.  
 The calculated 2-D coordinate values are converted to e-values 
based upon their location within the 2 -D space. The e-values 
represent a scale of emotional feature and intensity. The performance 
is split into twelve 90 second windows and the calculated e-value of 
each window is used to determine the outcome of the musical 
mapping for the next window. The e-value is derived from the last 20 
seconds of the window in order to capture the effect of the previous 
musical phrase. An added offset is used to present the beginning of 
the next phrase to the musicians before the current has ended, akin to 
reading a traditional musical score. Each window has an associated 
12 element array of pre-composed musical scores for each part, and 
these are categorised by corresponding e-values. In total there are 48 
possible musical variations for each  element (voice, piano, 
electronics) over each  movement (12 e-values x 4 windows) . 
Movement 1 records the initial emotional state of the performer and 
attempts to direct their emotion towards the opposing e-value in the 
2-d plane. The score for window 1 is selected at random and the 
resulting 2-d coordinate (x = v, y = a) recorded at the end of the 
window is saved as the initial state. A target coordinate is determined 
from: 
 
which reflects the opposite emotion within the plane. Multiplying the 
target co-ordinate by 0.33, 0.63 and 1 respectively sets a trajectory 
across the next three windows. Associated e-values are selected that 
span the projected path and select the corresponding musical phrase 
from the array. Movement 2 then follows a similar procedure for the 
audience member . The mapping for movement 3  uses the 
coordinates of each subject from the fourth window of movement 2 
as initial values, p (performer) and a (audience member). The 
difference between each individuals’ emotional sate, calculated as: 
                        
becomes the target value for movement 3’s third window, again with 
multiplication factors of 0.33, 0.66 for the preceding two windows, 
plotting a trajectory. The final window in movement 3 selects a target 
value of positive valence, at the closest e-value to the difference 
between p and a to induce a positive emotional ending to the 
performance experience. 
 
6. MUSICAL PARAMETERS 
The Director Musices system [7] is a well established model for 
deriving emotional features in music. Changes in mode, tempo, 
dynamics and articulation are known qualitative factors for 
influencing the emotional state. For example, it is widely accepted 
that a change from a major key to a minor key is well correlated to 
a change in perceived mood  [18]. Additional factors, scales of 
lyrical content and performative dramaturgy, have been constructed. 
The lyrics are adapted from the (public domain) prose of the 
romantic poet Percy Bysshe Shelley, a writer well known for his 
emotional language. For the text to be communicated clearly to the 
audience, it is projected at the rear of the stage.  
 Secondly, we have designed, through pre-production rehearsals, a 
key representing the 12 e-values which are presented on screen to 
each performer. Varying the intensity of the score’s background 
colours provides a symbolic prompt for each performer to add their 
own interpretation of feeling to the performance. Facial expressions, 
physical gesture and subtle performance deliveries are semi -
improvised by each performer in response, which help provide a 
more humanised input to the otherwise automated system. 
 
7. DISCUSSION 
As an artistic application of real-time composition using affective 
states this system demonstrates a work in progress proof of concept. 
The system and associated performance touch upon a wide range of 
Proceedings of the International Conference on New Interfaces for Musical Expression
595
areas that are open for deeper investigation, crucially the 
psychological effects of EEG mood induction and emotional 
perception in multimodal environments. 
 The system aims to highlight the potentials of mapping affective 
states to associated music with a simple method for plotting 
trajectories to move an individual from one state to another, as well 
as drawing the affective states of towards each other across  a 2 -
dimensional model.  
 This is, to the best of the authors’ knowledge, the first time such a 
system has been proposed ; specifically for emotion -based 
collaborative real-time compo sition in a performance setting, 
although sonifying emotional characteristics of EEG and measuring 
brain responses to music are areas investigated by a number of 
researchers over the last ten or so years [12, 22]. 
 From a technical perspective we acknowledge that greater 
resolution in converting emotional features into musical mappings 
could be higher and more expressive.  An automated generative 
musical system may be more suitable for this, or a more complex 
hybrid of this with our approach, but our wish was to integrate 
known emotion based features with human composition. We 
acknowledge the need for more user testing in order to understand 
the process of shifting a user’s state as this process is currently under 
researched. 
 Currently EEG is extremely problematic to measure during 
physical activities. In order to bypass the inherent artefacts created by 
the act of singing and performing EEG is recorded for 20 seconds at 
the end of each 1 minute 30 second window whilst the singer is 
instructed to remain still and focus on their emotions at that time in 
response to the previous window’s music. Until this issue is resolved 
our approach appears to be a simple and effective solution. 
 The importance of examining the outcomes of performance as 
experimental data should not be overlooked, both in terms of EEG 
readings and user experience. We hope to use the information 
recorded from upcoming performances to feed into future work. 
 Work towards manipulating emotions through  a responsive 
musical system, as well as the synchronisation of the emotional levels 
of more than one person engaged in acts of music making, is an 
exciting prospect. This concept has further potential in a wider range 
of interactive media applications where designers, composers and 
artists who w ish to explore emotional links to develop shared 
emotional experiences in response to art have a working foundation 
upon which to build. 
8. REFERENCES 
[1] L.I. Aftanas and S.A. Golocheikine, High-resolution EEG 
investigation of meditation.pdf. Neuroscience Letters, 2001. 
310(1): p. 4. 
[2] M. Bradley and P. Lang, The International Affective Digitized 
Sounds (2nd Edition; IADS-2): Affective ratings of sounds and 
instruction manual. Technical report B-3, 2007, University of 
Florida, Gainesville, Fl. 
[3] A. J. Cohen. Music as a source of emotion in film. In: Juslin PN, 
Sloboda JA (eds) Music and emotion: Theory and research. 
Series in affective science. Oxford University Press, New York, 
NY, US, 2001. p. 249-272 
[4] F. Dillman Carpentier and R.F. Potter. Effects of music on 
physiological arousal: Explorations into tempo and genre. 
Media Psychology, 2007 10 (3):339-363 
[5] J. Eaton and E. Miranda. BCMI Systems for Musical 
Performance. in 10th International Symposium on Computer 
Music Multidisciplinary Research (CMMR): Sound, Music and 
Motion. 2013. Marseille, France. 
[6] H. S. Fairclough. Physiological computing: Interfacing with the 
human nervous system. in Sensing emotions, 1-20. 2011 
[7] A. Friberg, R. Breson, and J. Sundberg, Overview of the KTH 
rule system for musical performance. Advances in Cognitive 
Psychology, 2006. 2(2-3): p. 145 – 161 
[8] L. George. and A. Le ́ cuyer. An overview of research on 
“passive” brain-computer interfaces for implicit human-
computer interaction. in International Conference on Applied 
Bionics and Biomechanics ICABB 2010, 2010. Venice Italy. 
[9] S. Giraldo. and R. Ramirez. Brain-Activity-Driven Real-Time 
Music Emotive Control. in 3rd International Conference on 
Music & Emotion (ICME3). 2013. Jyväskylä, Finland: Luck, G 
& Brabant, O. 
[10] M. Grierson. Composing With Brainwaves: Minimal Trial 
P300 Recognition as an Indication of Subjective Preference for 
the Control of a Musical Instrument. in ICMC. 2008. Belfast, 
Ireland. 
[11] S. Le Groux, Situated, Perceptual, Emotive and Cognitive 
Music Systems - A Psychologically Grounded Approach to 
Interactive Music Composition, in Departament de Tecnologies 
de la Informació i les Comunicacions 2011, Universitat 
Pompeu Fabra. 
[12] P. N. Juslin and D. Västfjäll. Emotional responses to music: the 
need to consider underlying mechanisms in The behavioural 
and brain sciences, 31(5), 2008. 
[13] J. Kamiya, Conditioned Discrimination of the EEG Alpha 
Rhythm in Humans, in Western Psychological Association1962: 
San Franscisco, USA. 
[14] A. Kirke and E. Miranda. Combining EEG Frontal Asymmetry 
Studies with Affective Algorithmic Composition and Expressive 
Performance Models. in International Computer Music 
Conference (ICMC 2011). 2013. Huddersfield, UK. 
[15] M. Li and B. Lu. Valence Emotion Classification Based on 
Gamma-band EEG. in 31st Annual International Conference of 
the IEEE EMBS. 2009. Minneapolis, Minnesota, USA. 
[16] Y. Lin, et al., EEG-Based Emotion Recognition in Music 
Listening. IEEE Transactions on Biomedical Engineering 2010. 
57(7): p. 9. 
[17] S. D. Lipscomb. Cross-modal integration: Synchronization of 
auditory and visual components in simple and complex media. 
The Journal of the Acoustical Society of America, 1999. 105(2) 
[18] S. R. Livingstone, R. Muhlberger, A. R. Brown, A. Loch. 
Controlling Musical Emotionality: An Affective Computational 
Architecture for Influencing Musical Emotions. in Digital 
Creativity 18, 2007. 
[19] A. Lucier. Statement on: Music for Solo Performer (1971), in 
Biofeedback and the Arts: Results of Early Experiments, D. 
Rosemboom, Editor. 1976, Aesthetic Research Center of 
Canada: Vancouver. 
[20] E. Miranda, W. Magee, J. Wilson, J. Eaton, R. Palaniappan, 
Brain-Computer Music Interfacing (BCMI): From Basic 
Research to the Real World of Special Needs. Music and 
Medicine, 2011. 3(3): p. 134-140. 
[21] E. Miranda, K. Sharman, K. Kilborn and A. Duncan. On 
Harnessing the Electroencephalogram for the Musical 
Braincap. Computer Music Journal, 2003. 27(2): p. 80 - 102. 
[22] R. Ramirez. and Z. Vamvakousis, Detecting Emotion from 
EEG Signals Using the Emotive Epoc Device, in Brain 
Informatics Lecture Notes in Computer Science. 2012, 
Springer. p. 175-184. 
[23] J. A. Russell. A Circumplex Model of Affect. Journal of 
Personality and Social Psychology, 1980. 39(6). 
[24] L.A. Schmidt. and L.J. Trainor, Frontal brain electrical activity 
(EEG) distinguishes valence and intensity of musical emotions. 
Cognition and Emotion, 2001. 15(4): p. 13. 
[25] C. E. Tenke. and J. Kayser, Reference-free quantification of 
EEG spectra: combining current source density (CSD) and 
frequency principal components analysis (fPCA). Clin 
Neurophysiol, 2005. 116(12): p. 2826-46. 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
596