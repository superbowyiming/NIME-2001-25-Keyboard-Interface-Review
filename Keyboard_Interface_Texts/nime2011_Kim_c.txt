Polyh ymnia: An automatic piano performance system with
statistical modeling of polyphonic expression and musical
symbol interpretation
Tae Hun Kim, Satoru Fukayama, Takuya Nishimoto and Shigeki Sagayama
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
{kim, fukayama, nishi, sagayama}@hil.t.u-tokyo.ac.jp
ABSTRACT
We developed an automatic piano performance system called
Polyhymnia that is able to generate expressive polyphonic
piano performances with music scores so that it can be used
as a computer-based tool for an expressive performance.
The system automatically renders expressive piano music
by means of automatic musical symbol interpretation and
statistical models of structure-expression relations regard-
ing polyphonic features of piano performance. Experimen-
tal results indicate that the generated performances of vari-
ous piano pieces with diverse trained models had polyphonic
expression and sounded expressively. In addition, the mod-
els trained with diÔ¨Äerent performance styles reÔ¨Çected the
styles observed in the training performances, and they were
well distinguishable by human listeners. Polyhymnia won
the Ô¨Årst prize in the autonomous section of the Performance
Rendering Contest for Computer Systems (Rencon) 2010.
Keywords
performance rendering, polyphonic expression, statistical
modeling, conditional random Ô¨Åelds
1. INTRODUCTION
We developed an automatic piano performance system called
Polyhymnia. To our knowledge, it is the Ô¨Årst system that
is able to learn and predict polyphonic expression in pi-
ano music with diverse performance styles. Human prefer
an expressive performance rather than a Ô¨Çat performance
obtained by direct converting into MIDI format, and there-
fore computer-based tools for an expressive music perfor-
mance would be useful for computer-aided music creations
and performances. Unfortunately, automatic rendering of
an expressive performance with a music score is a very dif-
Ô¨Åcult problem since expressive performance is one of the
most complicated human tasks, and its mechanism is still
not clear.
There exist many instruments for performing music. Since
each instrument has diÔ¨Äerent mechanical design, developing
an universal method for automatic renditions of any musical
instruments is extremely diÔ¨Écult. We are focusing on piano
renditions since piano has abundant solo pieces so that it
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
NIME‚Äô11,30 May‚Äì1 June 2011, Oslo, Norway.
Copyright remains with the author(s).
promises a useful application for computer-aided music cre-
ations and performances. Fortunately, musical expression
in piano music can be represented with only 3 expression
parameters: instantaneous tempo, loudness (velocity) and
performed duration. Such a simple parametric representa-
tion allows us to develop a simple model of piano perfor-
mance that can be well encoded in MIDI format.
Polyhymnia fully automates an expressive piano perfor-
mance. Musical symbols provide a basic guideline for an
expressive performance and they can be interpreted in sev-
eral ways. Therefore we propose Ô¨Çexible parametric models
for their automatic interpretation. Polyphonic features of
expressive piano performance is very important since pi-
ano music is usually polyphonic. We discuss them in this
paper and call musical expression with such featurespoly-
phonic expression . We proposed a statistical modeling of
polyphonic piano renditions and showed that generated per-
formances with polyphonic expression sounded better than
performances without it [4]. We brieÔ¨Çy describe the idea
behind the proposed modeling and show how to implement
it with Conditional Random Fields (CRFs).
An automatic piano performance system should be able
to deal with various unknown piano pieces. Experimen-
tal results on performances generated by Polyhymnia with
various compositions indicate that they had polyphonic ex-
pression and sounded expressively. A piano piece can be
performed with diverse performance styles. One of the ben-
eÔ¨Åts of the proposed modeling is that diverse models can be
easily obtained by training with various performance styles.
Experimental results on diverse performances generated by
Polyhymnia indicate that each trained model reÔ¨Çected the
style observed in the training performance set.
Polyhymnia participated in the Performance Rendering
Contest for Computer Systems (Rencon) 2010, and won the
Ô¨Årst prize in the autonomous section of the contest.
2. RELATED WORK
Several systems for automatic piano renditions are proposed
[5]. Director Musices and the Rubato system utilize sets
of performance rules extracted by music experts. Kagu-
rame series and COPER are based on several searching al-
gorithms from human performances. ESP, YQX and Us-
api try to statistically model musical expression in piano
music, whose parameters are learned from training perfor-
mances. Most of those systems discuss renditions of mono-
phonic melodies, and polyphonic renditions have not been
well discussed due to computational complexity and neces-
sity of a huge amount of data. In addition, automatic inter-
pretation of musical symbols were not well discussed since
they input a score in MIDI-like format, and it is based on
very simple rules.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
96
&YQSFTTJWF
QFSGPSNBODF	.*%*
&YQSFTTJWF
QFSGPSNBODF	.1
)BSNPOJDFYQSFTTJPO
FTUJNBUPS
.FMPEJDFYQSFTTJPO
FTUJNBUPS
.VTJDBMTZNCPM
JOUFSQSFUPS
1JBOPTDPSF #BTJDUFNQP	PQU
 4DBMJOHSBUJP	PQU
JOQVUTCZVTFS
PVUQVUTCZTZTUFN
1PMZIZNOJB
	.VTJD9.-
%FWJBUJPOHFOFSBUPS
Figure
1: Polyhymnia architecture.
Some commercial notation softwares are also able to pro-
vide an expressive performance of a given piece. Although
it is unclear how they generate musical expression, their
methods are probably based on interpretation of musical
symbols with simple rules.
3. SYSTEM OVERVIEW
To obtain an expressive piano performance with Polyhym-
nia, users requires only to input an piano score in Mu-
sicXML1 format without any other conÔ¨Ågurations. Unlike
MIDI format, MusicXML is able to encode almost all kinds
of musical symbols digitally. Encoded musical symbols are
automatically interpreted with parametric models that are
Ô¨Çexible to generate various interpretations of each occur-
rence of a symbol. Polyphonic expression is learned and
generated with Conditional Random Fields for polyphonic
piano renditions. The depth of generated musical expres-
sion can be controlled by scaling ratios. The system pro-
vides expressive performances in MP3 and MIDI formats
(Figure 1).
4. MUSICAL SYMBOL INTERPRETATION
4.1 Expression marks
Dynamic marks such asp, mf and so on, should be mapped
to concrete MIDI velocity values. To Ô¨Ånd such mapping, 15
performances of V. Ashkenazy in CrestMuse PEDB [2] were
analyzed. Table 1 shows the analytical result indicating that
interpretation of each occurrence of a mark is distributed,
and its interpretations in upper and lower staÔ¨Äs are diÔ¨Äerent
over all dynamic marks, for example, marks in lower staÔ¨Ä
are performed softer than those in upper staÔ¨Ä. In order to
interpret dynamic marks automatically, given marks should
be mapped to concrete values with various maps. As a sim-
ple solution, Polyhymnia simply maps given marks to the
estimated mean values for upper and lower staÔ¨Äs, respec-
tively. However, this can be improved by proper selection
of a map for each occurrence of a mark.
crescendo, diminuendo and ritardando should be inter-
preted with gradual changes of loudness and tempo. It
is well known that human perceives them by exponential
changes of sound energy and tempo in BPM. With analysis
of human performances, we found that human performers
performs such changes in various forms, and interpretri-
tardando with gradual decreasing tempo and loudness. To
model such interpretations, we propose an parametric math-
ematical model for loudness and tempo changes. Letdt be
loudness in MIDI-velocity 2 or instantaneous tempo in log-
1http
://www.recordare.com/musicxml
2MIDI-velocity can be regarded as a logarithmic scale for
Table 1: Human interpretation of dynamic marks.
Note that all averages and standard deviations are
in MIDI velocity.ppp and mp were not occurred in
the data.
Upper staÔ¨Ä
ppp pp p mp mf f Ô¨Ä Ô¨Äf
Occur. - 157 208
7 - 67 1490 418 19
Avg. - 50 52 - 58 67 76 98
St. dev. - 14 15 - 8 15 16 2
Lower
staÔ¨Ä
ppp pp p mp mf f Ô¨Ä Ô¨Äf
Occur. - 150 316
9 - 53 1538 353 12
Avg. - 37 37 - 47 57 73 101
St. dev. - 13 11 - 9 20 19 11
BPM at
time t. Then, its gradual changes over t can be
modeled as
dt = d0(  t + 1); (1)
where d0 is start value,  is the parameter for expression
depths and  is the parameter for shapes. If  is 1.0, energy
and tempo in BPM are changing exponentially. With dif-
ferent setting of and , each occurrence of a mark can be
interpreted in various forms. As a simple solution, Polyhym-
nia interpret all occurrences of a mark with Ô¨Åxed parameter
values. However, this can be improved by automatic deter-
mination of parameter values for each occurrence of a mark.
4.2 Ornaments
Mordent, turn, trill and grace notes are performed with ad-
ditional notes. Since such additional notes decorate their
parent notes, we can assume that their loudness is deter-
mined based on their parent note‚Äôs loudness. However, hu-
man is not able to perform a note sequence with a constant
velocity. Assuming that such motor error is following Gaus-
sian distribution, loudness of the ith additional note di can
be modeled as
di = d0 + N (0; 2); (2)
where d0 is the loudness of the parent note. 2 controls
Ô¨Çuctuation ranges of loudness.
Arpeggio indicates that onset time of each arpeggiated
note should be delayed one after another. Since human is
not able to perform such notes with a constant delay, we
can assume that it contains Gaussian noise. Then, onset
time of ith arpeggiated note di can be modeled as
di = d0 + i  ‚àÜ + N (0; 2); (3)
where ‚àÜ is a delay time, and d0 is the onset time of the
lowest arpeggiated note.
5. STATISTICAL MODELING OF
POLYPHONIC PIANO RENDITIONS
5.1 Polyphonic expression in piano music
Although musical symbols provides a basic guideline for an
expressive performance, musical expression in piano music is
much more complicated, for example, instantaneous tempo,
loudness and performed duration are Ô¨Çuctuating over time,
even if there are no musical symbols for them. In addition,
an expressive piano performance has polyphonic expression
whose features include:
 Each voice expression has Ô¨Çuctuations of loudness and
performed duration over time, and it is not always
same to the other voices.
soun
d energy.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
97
.FMPEJDDPNQPTJUJPOBMTUSVDUVSF
.VTJDBMFYQSFTTJPOPGBNFMPEJDOPUF
.FMPEJDEFQFOEFODZ
)BSNPOJDDPNQPTJUJPOBMTUSVDUVSF
.VTJDBMFYQSFTTJPOPGBIBSNPOJDOPUF
)BSNPOJDEFQFOEFODZ
a)
b)
Figure
2: Complex dependency networks of poly-
phonic expression in piano music (a). Simplied
dependency networks by introducing melodic and
harmonic dependencies (b).
 Expression of each note in a chord is not always same
to the other notes in the chords. Playing a chord with
diÔ¨Äerent combinations of note expression results dif-
ferent sounds of the chord.
In order to learn and predict polyphonic expression in pi-
ano music, we proposed a statistical modeling of polyphonic
piano renditions and showed its eÔ¨Éciency for improving a
machine-rendered piano performance [4]. In following sub-
sections, we brieÔ¨Çy describe the idea behind the modeling
and show how to implement it with Conditional Random
Fields.
5.2 Probabilistic formulation
Prediction of an expressive performance D given a piano
score S can be formulated probabilistically such as
ÀÜD = arg max
D
P (DjS ; Œò); (4)
where Œò is the parameters of the distribution. To model
P (DjS; Œò), we assume that a note expression is dependent
on its compositional structure represented with score fea-
tures and on the other note expressions. Figure 2a shows a
dependency network of polyphonic piano music. In case of
polyphonic expression, such dependency is very complex,
and therefore it is hard to model it with computational
tractability, and a huge amount of training data is necessary
for learning model parameters Œò. Therefore, an approxima-
tion to polyphonic expression is necessary for a tractable
modeling.
To simplify dependency in polyphonic renditions, we pro-
posed an approximation with melodic and harmonic depen-
dencies. Figure 2b shows an example of simpliÔ¨Åed depen-
dency network with the proposed approximation. We be-
lieve that such approximation promises a perceptually best
performance. This is because human perceives
 diÔ¨Äerent voice expressions sounding simultaneously,
 diÔ¨Äerent sounds of a given harmony,
 expressions of outer voices easier than that of inner
voices [3].
Hence, P (DjS; Œò) can be approximated such as
P (DjS) =P (Dmu
jS mu
)  P (Dml
jS ml
)
Hu
Y
hu=1
P (Dhu
jS hu
) 
Hl
Y
hl=1
P (Dhl
jS hl
); (5)
where P (Dmu
jS mu
) and P (Dml
jS ml
) are distributions of
melodic expression in the uppermost and lowermost voices,
respectively, and P (Dhu
jS hu
) and P (Dhl
jS hl
) are distri-
butions of harmonic expressions in upper and lower staÔ¨Äs,
respectively.
Since such approximation allows Markov assumption on
both of melodic and harmonic dependencies, they can be
modeled with statistical models with hidden state transi-
tions, such as Dynamic Bayesian Networks, Hidden Markov
Models and Conditional Random Fields. Considering that
our goal is to estimate a note expression sequence given a
sequence of score feature vectors representing a piano score,
we believe that CRF is one of the best frameworks for mod-
eling those dependencies.
5.3 Modeling with Conditional Random Fields
We assume that a melodic compositional structure is rep-
resented with score features, such as pitch, duration, note
interval and so on, and a harmonic compositional structures
is represented with score features, such as pitch, duration
and so on. Also, we assume that melodic expression is rep-
resented with instantaneous tempo, loudness and performed
duration, and harmonic expression is represented with onset
time diÔ¨Äerences, loudness and performed duration3.
Although score features and expression parameters of melodic
and harmonic dependencies are diÔ¨Äerent to each other, they
can be modeled with CRFs with the same model structure.
Let dn and D be the nth melodic or harmonic expression
and its sequence, respectively. Let S be a sequence of score
feature vectors representing melodic or harmonic composi-
tional structures, and sk be the kth score feature. Assuming
that dn is only dependent on dn 1 (Markov assumption), we
can deÔ¨Åne the jth feature function Fj such as
Fj(D; S) =
NX
n=1
(fdn 1; dn; skgj; n); (6)
where () returns 1, if the jth triple from all possible triples
of fdn 1; dn; skg is occurred at position n, and 0, otherwise.
Introducing a weight variable j for each Fj and accord-
ing to the Maximum Entropy Principle, P (DjS; Œò) can be
deÔ¨Åned such as
P (DjS; Œò) = 1
Z(S; Œò) exp
X
j
jFj(D; S)
; (7)
where
Z(S; Œò) =
X
D0
exp
X
j
jF (D0; S): (8)
Model parameters Œò can be learned from training per-
formances with Maximum Likelihood Estimation by an it-
erative algorithm, such as Stochastic Gradient Descent [1].
Once Œò is estimated, we can predict an expressive perfor-
mance with equation (4), and this can be eÔ¨Éciently com-
puted with Forward-backward algorithm and Dynamic Pro-
gramming technique [6].
6. EXPERIMENTAL EVALUATION
6.1 Generation quality
An automatic music performance system should be able to
render unknown pieces in various compositional styles. In
order to evaluate Polyhymnia in this aspect, piano pieces
in various compositional styles were rendered by the sys-
tem, and evaluated by 19 human listeners 4. We rendered
3Detail
s of the melodic and harmonic score features and the
expression parameters can be found in [4]
42 professional musicians, 13 hobby musicians and 2 non-
musicians participated in the listening experiments.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
98
Table 2: Test pieces used in the experiment.
ID Composer Piece Tempo
CF Chopi
n Mazu
rka no. 5, op. 7-1 fast
CS Chopi
n Son
ata no. 2-3, op. 35 slow
MF Moza
rt Son
atina no.5-3, KV. 439 fast
MS Moza
rt Marc
he Funebre, KV. 453a slow
RT S. Jopl
in The En
tertainer (ragtime) middl
e
GR Grieg 7 Lyric Piec
., 7. Rem., op. 71 slow
0
20
40
60
80
100
120
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1
Ashkenazy-model, upper
Gould-model, upper
Ashkenazy-model, lower
Gould-model, lower
.*%*7FMPDJUZ
#FBUQPT
.FBTVSF 13 14 15 16 17
.
.
.
.2 3 ..
4
.
.
5
&
?
bbbbb
bbbbb
13
...
25 4 ...
5 4 ...
5 4
...
1
...
1
...
2
f
*
..
..
4
R
...
3
.
..
..
.
n
2
&
*
'$IPQJO4POBUB/P0QVT.FBTVSF
Figure
3: Generated performances by Polyhymnia:
F. Chopin, Sonata no. 2-3, op. 35.
6 unknown pieces with Polyhymnia as shown in Table 2.
Note that the test pieces included not only F. Chopin and
W. A. Mozart‚Äôs pieces, but also E. Grieg and S. Joplin‚Äôs
pieces whose compositional styles are quite diÔ¨Äerent to the
training pieces.
In order to render expressive performances with diÔ¨Äer-
ent performance styles, we prepared two diÔ¨Äerent models
such asAshkenazy-modeland Gould-modeltrained with
15 performances of V. Ashkenazy 5 and 7 performances of
G. Gould 6, respectively (CrestMuse PEDB). We prepared
5 performances for each piece such as performance with-
out expression (D), by musical symbol interpretation only
(E), generated withAshkenazy-model(S1), generated with
Gould-model(S2) and by a human performer (H). All of
those sound samples were blind to the listeners, and their
human-likeness and musicality were evaluated using 6-level-
scales.
Figure 3 shows an example of generated performances
by Polyhymnia. The results indicate that the had poly-
phonic expression, and their Ô¨Çuctuations were diÔ¨Äerent to
each other. Figure 4a shows the average scores of the 6
test pieces. Analysis of Variance on those average diÔ¨Äer-
ences with p < 0:05 indicate that performances generated
by Polyhymnia sounded better than performances without
expression. Score diÔ¨Äerence between S1 and H was not
signiÔ¨Åcant. This means that performances generated with
Ashkenazy-modelsounded expressively like human perfor-
mances do. Score diÔ¨Äerences between S2 and H were not
signiÔ¨Åcant in some particular pieces. This means that some
performances generated withGould-modelsounded expres-
sively like human performances do.
5Prelude
no. 1, 4, 7, 15, 20, Etude op. 10-3, 10-4, 25-11,
Waltz op. 18, 34-2, 64-2, 69-1, 69-2, Nocturne no. 2, 10.
6Piano Sonata KV279-1, 279-2, 279-3, 331-1, 545-1, 545-2,
545-3.
2.77 
3.68 
4.18 
3.71 
4.43 
3.04 
3.83 3.85 3.73 3.99 
0.00 
1.00 
2.00 
3.00 
4.00 
5.00 
6.00 
D E S1 S2 H
Human-likeness Musicality
37%
89%
68% 74%
95%
79%
63%
11%
32% 26%
5%
21%
CF CS MF MS RT GR
Percept as S1 Percept as S2
b) Performances predicted with Ashke-model (S1)a) Average scores of the 6 pieces
Figure
4: Average scores of the 6 pieces (a). Style
classication result of the 6 S1 (b) …Ω
6.2 Subjective style identiÔ¨Åcation
In order to know if each trained model reÔ¨Çected the style
observed in the training data, we conducted an another lis-
tening experiment for subjective style identiÔ¨Åcation. 3 pi-
ano pieces, which were not included in the test pieces, were
generated with both trained models (total 6 performances)
and the participants listened to them to remember the style
each model reÔ¨Çected. After that, the participants listened
to the 12 S1 and S2 blind in a random order.
Figure 4b shows the style identiÔ¨Åcation result of the 6 S1.
The result shows that 5 out of 6 pieces were well identi-
Ô¨Åed by the listeners, and the average identiÔ¨Åcation rate was
73.6%. The identiÔ¨Åcation result of the 6 S2 was similar, and
the average identiÔ¨Åcation rate was 73.6%. Those results in-
dicate that each trained model reÔ¨Çected the style observed
in training data, and those styles were perceptually distin-
guishable by human listeners.
7. CONCLUSION
We introduced an automatic piano performance system called
Polyhymnia that is able to learn and predict polyphonic ex-
pression, and interpret musical symbols automatically. Ex-
perimental evaluations on generated performances indicate
that diverse performances of various compositions gener-
ated by the system had polyphonic expression and sounded
expressively, and their performance styles were perceptually
well distinguishable by human listeners.
We believe that modeling hierarchical structures of a given
piece would improve a machine-rendered piano performance.
By introducing additional model parameters controlled by
users through an interface, Polyhymnia can be extended to
an interactive music performance system.
8. REFERENCES
[1] L. Bottou. Stochastic gradient learning in neural
networks. In Proc. Neuro-Nimes, Nimes, France, 1991.
EC2.
[2] M. Hashida and et al. A new database describing
deviation information of performance expressions. In
Proc. ISMIR, pp. 489‚Äì494, 2008.
[3] D. Huron and et al. The avoidance of inner-voice
entries: perceptual evidence and musical practice.
Music Perception, 7(1):43‚Äì48, 1989.
[4] T. H. Kim and et al. Performance rendering for
polyphonic piano music with a combination of
probabilistic models for melody and harmony. In
Proc. SMC, pp. 23‚Äì30, 2010.
[5] A. Kirke and et al. A survey of computer systems for
expressive music performance. ACM Comput. Surv.,
42(1), 2009.
[6] J. LaÔ¨Äerty and et al. Conditional random Ô¨Åelds:
probabilistic models for segmenting and labeling
sequence data. InProc. ICML, pp. 282‚Äì289, 2001.
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
99