Kinéphone: Exploring the Musical Potential of an
Actuated Pin-Based Shape Display
Xiao Xiao, Donald Derek Haddad, Thomas Sanchez, Akito van Troyer, Rébecca
Kleinberger, Penny Webb, Joe Paradiso, Tod Machover, Hiroshi Ishii
MIT Media Lab
75 Amherst Street
Cambridge, MA, 02114, USA
[x_x, ddh, thomassl, akito, rebklein, pewebb, joep, tod, ishii]@media.mit.edu
ABSTRACT
This paper explores how an actuated pin-based shape dis-
play may serve as a platform on which to build musical
instruments and controllers. We designed and prototyped
three new instruments that use the shape display not only
as an input device, but also as a source of acoustic sound.
These cover a range of interaction paradigms to generate
ambient textures, polyrhythms, and melodies. This paper
ﬁrst presents existing work from which we drew interactions
and metaphors for our designs. We then introduce each of
our instruments and the back-end software we used to pro-
totype them. Finally, we oﬀer reﬂections on some central
themes of NIME, including the relationship between musi-
cian and machine.
Author Keywords
Shape Display, Radical Atoms, Shape Changing Interfaces,
Sequencer, Gesture, Bricolage
ACM Classiﬁcation
H.5.5 [Information Interfaces and Presentation] Sound and
Music Computing, H.5.2 [Information Interfaces and Pre-
sentation] User Interfaces—Haptic I/O, I.2.9 Robotics
1. INTRODUCTION
In recent years we have seen a growing trend in dynamic,
physical actuation of matter in diverse domains, from ar-
chitecture to biology [36, 9]. Looking into the future, re-
searchers have envisioned a world where physical atoms may
be just as dynamic and malleable as bits [11]. To design for
this future, HCI researchers have used currently available
”enabling” technologies to build novel interactions and ap-
plications, following Alan Kay’s idea that ”the best way
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’16,July 11-15, 2016, Grifﬁth University, Brisbane, Australia.
.
to predict the future is to invent it” [17]. One popular
enabling technology is the pin-based, actuated shape dis-
play. Originally designed to render shape content for haptic
feedback [13], the shape display has become a platform on
which to imagine future interactions in applications includ-
ing computer-aided design, data visualization, and telepres-
ence [7, 19].
Our work explores how the pin-based shape display may
become a generalized platform for creating custom acous-
tic musical instruments. Additionally, we also demonstrate
how the pins may serve as input interface (musical con-
troller) and sound producing object. Though the shape dis-
play was not designed expressly for music, this research fol-
lows a long history where innovative technologies are adapted
for musical purposes. This practice not only opens creative
avenues for music-making, but also helps to push forward
the technologies themselves. Moreover, probing the musical
properties of the shape display oﬀers novel perspectives on
major themes of NIME, such as the relationship between
the physical and digital, the control and output, the per-
former and instrument-maker, as well as the musician and
machine.
As a ﬁrst step of exploring the musical potential of the
shape display, we designed and prototyped three instru-
ments on TRANSFORM, a state-of-the-art shape display
[10]. This paper begins with a background that describes
TRANSFORM and presents examples of existing instru-
ments and interfaces which inspired our designs. We then
describe each of our new instruments as well as the software
system that drives them. We conclude with a set of reﬂec-
tions on key themes of NIME, closing with a vision for the
future of the shape display as a musical platform.
2. BACKGROUND
TRANSFORM comprises three separate shape displays of
16x24 pins. Each pin measures approximately 1’x1’ and
extends 100mm from the surface. Based on the same hard-
ware as inForm, TRANSFORM features custom Arduino
boards running a PID controller to control the position of
polystyrene pins through motorized slide potentiometers [7,
10]. Actuation speed is 0.0655 m/s, with up to 1.08 Newtons
of force. TRANSFORM can detect user input from each
259
pin based on changes in position and includes an overhead
Kinect to detect users’ gesture and movement. A program
written in C++/OpenFrameworks acts as the main software
interface for TRANSFORM, which updates pin positions at
30fps. For more information, see [7, 10].
TRANSFORM was originally built as an interactive art
installation and featured 3 modes: a wave generator respon-
sive to visitors’ movements, an abstract animated narrative,
and a kinectic sculpture where pins guide the movement of
passive red balls. The pleasing variety of natural sounds
of the machine itself and the interplay between the ma-
chine and passive objects became our ﬁrst inspiration to use
TRANSFORM as a platform to build acoustic instruments.
For insights on shaping our new instruments, we look to
existing work on mechatronics, tabletop tangible interfaces,
and gestural control applied to music.
2.1 Mechatronic Music
Works done by Zimoun, Pe Lang and Zareei et al., demon-
strate the potential in using mechatronic noises themselves
as the source of musical sounds [25, 44]. Many of Zimoun
and Pe Lang’s work incorporate a large number of DC mo-
tors to create sound-emitting mechanisms with and without
other objects. Mutor is a mechatronic sound art that uses
the sonic artifacts of DC motors. The continuous humming
sounds from DC motors is aesthetically modulated to cre-
ate a drone chorus. We may apply a similar principle to
repurpose the sounds of TRANSFORM’s motorized slide
potentiometers.
Instruments using mechanisms to actuate passive sound-
producing objects have existed since the dawn of the ma-
chine age in the 18th century [8]. Sometimes, as in the case
of the harpsicord and the pianoforte, these instruments re-
quire human actuation of the mechanism. Other times, as
with the music box and player piano, these instruments me-
chanically imitate how humans play music, such as plucking,
bowing, hammering, and blowing [3].
More recently, works within the NIME community have
used robotic actuation to empower humans to create acous-
tic music never possible before [34]. A popular approach
uses robotic actuation to create percussion instruments with
greater speed and accuracy than a human player [15]. These
instruments may be controlled digitally, as in the case of
the Machine Orchestra, an ensemble of human laptop per-
formers and robotic musical instruments [16]. The ﬁeld of
robotic musicianship embodies another approach where the
robot acts as an intelligent agent capable of higher level
musical exchange with a human player [41].
2.2 Tabletop Tangible Interface for Music
The notion of tangible interfaces has been applied to the
control of digital music, to oﬀer physical aﬀordances and
constraints not present in purely digital controllers [12].
The core mechanics of this interaction model is the map-
ping between the tangible controls and the resulting digital
sounds. One lineage of works [[31, 14, 20]] is based on the
tabletop metaphor, where the conﬁguration of physical to-
kens dictates the synthesis of digital sounds and rhythmic
patterns .
A core idea of Tangible Interfaces is to leverage the rich
relationships people already have with everyday objects in
interactions with the computer [12]. This idea has been ap-
plied to music in projects such as Drumtop, which invites
the user to discover the acoustic properties of everyday ob-
jects [38].
Another family of peudo-tabletop interfaces, such as the
Tenori-on and the Monome, features a grid of back-lit LED
buttons, which allow user input and act as visual feedback
for the digitally synthesized sounds [27, 1]. The form factor
of the grid make these devices ideal for layered, rhythmic
compositions, a model to apply for music on the pin-grid of
the shape display.
2.3 Gesture Control of Music
Research on gesture is complex, with varying deﬁnition across
disciplines [24]. To contexualize related works, we follow
Wanderley’s deﬁnition of gesture—the characteristic actions
of music instrumentalists during performance [40]. To fur-
ther specify our scope, we focus on free-handed gestures—
gestures that do not have physical contact with an object—
and their control of musical parameters. We are interested
in both discrete event and continuous control of gestures,
both of which are powerful expressive tools [42].
The analysis of free-hand gestures is an on-going active
research and a signiﬁcant amount of eﬀort has been made
both in music and in HCI using a variety of input technolo-
gies. Two common approaches include capacitive sensing
and electric ﬁeld sensing, demonstrated respectively by Max
Matthew’s Radio Baton [23] and the Sensor Chair used in
the Brain Opera [30]. Another technique uses wearable sys-
tems, including handheld devices [39] as well as bio signals
[35].
As the TRANSFORM system includes a Kinect camera,
we look more to related work on using computer vision
systems to detect and process gestures for musical perfor-
mance. EyesWeb is a camera-based system for the real-time
analysis of body movement and gesture [4]. Similar ap-
proaches to EyeWeb may be seen in several camera-based
musical systems [43, 33, 28]. In addition, machine learning
techniques in conjuction with computer vision have become
a popular approach to analyze and classify gestures for mu-
sic performances [26].
3. SHAPE DISPLAY INSTRUMENTS
Drawing from the works described in the previous section,
we designed and prototyped three new musical instruments
on the shape display. Each instrument uses one 16x24 mod-
ule of TRANSFORM and can be played alone, with the oth-
ers, or with any other musical instrument. All three feature
tangible and gestural controls and output entirely acoustic
sounds.
Our goal in creating these instruments is to demonstrate
the versatility of the shape display as a general music-making
platform. Thus, these instruments are designed to cover a
variety of input and output paradigms to suggest a larger
space of possible designs. Some elements of our designs
have been dictated by by the existing hardware constraints
of TRANSFORM. These constraints are mentioned where
relevant along with suggestions of improvement to facilitate
music-making on future versions of shape displays.
3.1 Gestural Wave
The ﬁrst instrument uses free-hand gestures to control am-
bient textural noises generated by the the acoustic sounds
of TRANSFORM’s actuation. We implemented three types
of waves: a sinusoid wave, a Perlin noise wave [32] and a
vertical cross wave. All three were inspired by patterns
from TRANSFORM’s original applications and were se-
lected based on the distinct sounds they produced. The
sinusoid wave outputs a smooth, ”undulating” sound. Due
to more surface contact between adjacent pins, the cross
wave produces a louder ”rustling” noise. The Perlin wave
features the most ”jumps” in the pins and is much noisier
and ”chaotic” sounding than the other two.
For more variation in sound, all three waves were re-coded
to expose parameters targeted for modulating sound (ﬁgure
260
Figure 1: Sinusoid (left) and cross wave (right)
5). Based on extensive experimentation, we identiﬁed four
parameters of each wave and describe how they change the
acoustic properties of the sound output:
• Amplitude: Controls the height of the pins which
corresponds to the overall volume.
• Ordinary frequency: Adjusts the repetition of the
acoustic waveform shape. More repetition increases
friction between adjacent pins.
• Phase: Determines the speed of the pins, which also
controls volume.
• Center: Positions of the center of waves, which changes
the directional focus of the sound.
For real-time performance, we detect the position and
shape of a user’s hands with the overhead Kinect. The
depth image from the Kinect is used to process a threshold
distance image which then is passed to OpenCV for blob
detection (see ﬁgure 2).
By default, the vertical position of one hand controls the
amplitude of the selected wave, which corresponds to the
overall volume and heights of the pins. This gives users the
most immediately noticeable change in sound in response to
their movement. With a second hand, the user may mod-
ulate the frequency of the physical wave, which changes its
texture. The opening and closing of the hand may be used
to switch between the selected wave and a random pattern
of pins, which adds an instantaneous accent to the sound.
With this, it is possible to create staccato rhythms to punc-
tuate the more ambient waves. Currently, a GUI is used
to switch between the three diﬀerent waveforms. A logical
future extension would be to use gesture (e.g. holding out
diﬀerent numbers of ﬁngers) for mode-switching.
Figure 2: Threshold image with area of detection
(left) and blob detection (right)
3.2 Step Sequencer
Our second instrument uses the shape display to sequence
and play layered rhythms, inspired by interfaces like the
Tenori-on [27]. It features up to 8 simultaneous tracks,
each mapped to every other column on one TRANSFORM
module. Within each column, the pins are divided into 2
regions. Four pins in of the top portion act as actuators,
and each is augmented with a ”shaker” cap made from clear
polyester ﬁlm. Selected for both visual appeal and its acous-
tic properties, the ﬁlm is cut and folded to form a box of
1’x1’x2”, with a 1/2” extension at the bottom to ﬁt over a
pin. The cap is secured with a small piece of double-sided
tape. To diﬀerentiate between tracks, the caps of each col-
umn are ﬁlled with diﬀerent materials (e.g. beads, bells,
wood scraps, buttons, nails).
The actuators of each row take turns making sounds based
on the sequence given by the 16 pins directly below, which
represent a repeating pattern of 16 steps. These pins may be
set to an up or down state to program the pattern. Pushing
on a pin in the down position sets it to up while pulling on
an up pin returns it to down. The very last pin at the bot-
tom of each column acts as a button that toggles whether
that sequence plays or pauses.
On the far right edge of the display is a column of 16 pins
with a ”cursor” shown by a slightly raised pin that indicates
the current position in the sequence of 16 steps. Based on
the position of the ”cursor”, the top pins for each column
move given a step set to up and rest when the step is set
to down. The very last pin on the cursor column controls
pause for the entire sequencer.
The four actuators take turns making sounds to compen-
sate for a limitation in the shape display hardware. Even
though the pins have a refresh rate of 30fps, we found that
successive movements of large distances (> 0.5 of the max-
imum position change) occur at a much slower rate due to
friction. Additionally, our prototype treats the a shaker
pin’s up motion and down motion as equivalent sounds
even though down is much louder than up. This decision
is due to another limitation in the system. To only use the
downwards movement for sound production, we must reset
the pin after each movement. Because pins contain sound-
producing objects, we are limited to a slow, gradual reset to
prevent extraneous noise. However, slowly resetting all the
shaker pins interferes with our touch detection. These ex-
periences reveal limitation with the shape display hardware
that previous applications had not encountered.
Figure 3: Objects for the sequencer (left) and key-
board (right)
3.3 Modular Keyboard
Our third instrument uses TRANSFORM’s pins to strike
sound-producing objects, inspired by Drumtop [38] and by
the piano. Since textures and rhythms have been explored
by our two other instruments, we focused on objects that
emit pitched tones for playing melodies though striking ob-
jects may also produce sound eﬀects and rhythms.
Our prototype plays tones of two diﬀerent timbres, taken
from a disassembled wooden xylophone and a set of metallic
chimes. Pins in the top portion of one TRANSFORM mod-
ule are raised to hold the objects in place. The xylophone
bars are attached with foam feet on each end and placed
directly in their holders. For the chimes, caps ﬁtted with
foam are placed on the hold pins to help with resonance.
Currently, our prototype supports 7 slots for the bars and
chimes. Under each slot is a pin with a cap that contains
a wooden ball which acts as a hammer. The order of bars
261
and chimes could be customized at will to correspond to
diﬀerent intervals and scales.
The bottom row of pins acts as a ”keyboard” interface,
with raised pins in the same column as the hammers which
act as ”keys”. Pressing on each key activates its correspond-
ing hammer to strike. Holding down a key triggers multiple
successive strikes. Hammers may also be played through a
computer keyboard, where the computer keys trigger both
the striking of the hammer and the depression of its coupled
shape display key. Sequences of melodies may also be pro-
grammed on the computer to play and loop on our modular
keyboard.
Due to the existing implementation of touch detection on
TRANSFORM, there is an approximately 200ms latency for
touch events to register. The delay arises from the touch
detection algorithm which tries to prevent false positives
since touch is currently detected from reading the positions
of pins from their backdriven motors. This same latency
is present for the Step Sequencer, but it does not pose a
major problem since sequence setting and actuation are not
not directly coupled.
Though 200ms is a signiﬁcant delay considering studies
done in network music [5], we found that a player may com-
pensate for it if they imagine hammer strikes to be mapped
to key up rather than key down. Players may also use
the computer keyboard for latency-free playing. Latency
in touch detection is an important issue to address in fu-
ture iterations of shape display hardware and software. Fu-
ture implementations will also delve more into the passive
haptic feedback from the pins to design interfaces for more
expressive control.
4. SOFTW ARE IMPLEMENTATION
Prior interactive applications for TRANSFORM [10] have
all been implemented in OpenFrameworks, where heights
are represented by a 2D pixel map shown in a runtime GUI.
To enable faster development, we built a software architec-
ture that allows external applications to control the shape
display. A Node.js application acts as a middleware server
between external applications and OpenFrameworks. Us-
ing OSC over UDP, the Node server passes height messages
from external applications to TRANSFORM and input mes-
sages (touch and Kinect) from TRANSFORM to external
applications.
Within OpenFrameworks, all three modules of TRANS-
FORM are indexed together like one large shape display.
The Node server allows external applications to control one
module of TRANSFORM at a time. Our main external
development environment is xForm, a JavaScript client ap-
plication served by Node over http that runs on localhost.
xForm oﬀers a 3D preview of TRANSFORM written with
3js and includes live scripting using the Ace editor. This al-
lows a developer to try out shapes and movements virtually
before sending to TRANSFORM. The xForm UI includes
a toggle to connect the virtual model to the physical ma-
chine. When on, it sends heights and receives input. Both
the sequencer and the keyboard are written using this en-
vironment.
Our architecture also allows developers to code for the
shape display in any language of their choice, as long as
they pass OSC messages in the proper format. The Gestural
Wave instrument was written in Processing. We were also
able to interface with TRANSFORM using Cinder while
prototyping our instruments.
5. DISCUSSION
Figure 4: (top) Software architecture, (bottom)
xForm simulator for the TRANSFORM
Figure 5: Processing GUI to control parameters of
the sinusoid (left) and Perlin wave (right)
We ﬁrst summarize the overall space of musical possibilities
of instruments on the shape display as suggested by our
three prototypes. We then oﬀer reﬂections on key topics in
NIME relating to the machine and the musician.
5.1 Musical Possibilities
5.1.1 Parameters of Music
Our three instruments give the player control of all four
basic parameters of music: pitch, loudness, timbre, and du-
ration [18]. The Gestural Wave controls loudness, timbre,
and duration of sound; the Step Sequencer explores timbre;
and the Modular Keyboard covers pitch and timbre. While
the Sequencer and the Keyboard do not control the dura-
tion of individual tones, they do allow control of timing—in
other words, the duration of silence.
5.1.2 Control Paradigms
Our prototypes demonstrate 3 diﬀerent control paradigms
based on metaphors from existing instruments and inter-
faces, but they are by no means the only way to control each
instrument. For example, the ”shaker” pins of the Step Se-
quencer could also be played the same way as the keyboard
and sequenced based on the playing. In this input model,
we may introduce the equivalent of a looper pedal, where
pin movement based on user input is repeated and layered.
Free-hand gestures and movement could also be used to
control patterns of scales and arpeggios on the keyboard.
Additionally, all three instruments could be played via live
coding in their respective software environments.
5.1.3 Interface to the Digital
Though this paper focused on acoustic sound production,
the shape display could also serve as an interface for digital
262
music. In that scenario, all the interaction paradigms that
we discussed would still apply. The same movement of the
physical pins to generate sound would then serve as visual
and haptic feedback on the state of the digital music.
When used as a digital controller, the sounds of the pins
should be minimized in order not to interfere with the dig-
ital sounds. Ampliﬁcation of the digital sounds could also
hide the noise of the physical sounds. Additionally, the
shape display could be used in the context of remote mu-
sical performances. For instance, the gestures of remote
performers could be rendered on the shape display, as envi-
sions by [19]
5.2 Machine and Musician
5.2.1 Player, Controller & Sound Producing Object
Within the NIME community, one common way of describ-
ing instruments is through the paradigm of the player, the
controller (or interface), and the sound-producing object [6].
In traditional acoustic instruments, such as the violin, the
”interface” and sound-producing object are intimately con-
nected. Thus, there is no latency, and the player receives
subtle feedback through both sound and haptics [22]
In electronic and digital instruments, the controller and
sound-producing object (synthesizer) are connected by map-
pings created by the designer. While these instruments of-
fer more ﬂexibility in both interaction and sound synthe-
sis, the lack of tight coupling between controller and syn-
thesizer poses problems. Perry Cook points out 3 major
ﬂaws of the paradigm: (1) the lack of haptic feedback from
the controller to the player, (2) the introduction of distor-
tions/delays between the controller and the sound-producer,
and (3) the lack of any sense that sound comes from the in-
strument [6].
In our instruments, the tangibility and actuation of the
shape display serve as haptic feedback, taking care of (1).
Moreover, all of our sounds are acoustically produced by
the physical instrument, taking care of (3). Noticeable la-
tency only arises for one of our instruments, but it is due
to the implementation of the platform and could conceiv-
ably be removed in the future. Our prototype instruments
represent a hybrid of physical and digital, where a digi-
tal layer connects the two physical sides of controller and
sound-producer. While physicality imposes constraints on
the potential space of controller and sound design, it oﬀers
advantages of purely physical instruments with the ﬂexibil-
ity to design digital mappings [22]
5.2.2 Beginner & Expert
Another key question of NIME is how to support a low-
ﬂoor-high-ceiling usage on new musical instruments [29]. A
core feature of shape displays is their capacity for dynamic
aﬀordances and constraints, which may help beginners to
make sense of a new interface [7, 21]. For players with more
experience, musical interfaces on the shape display could be
designed to mimic existing instruments, as our prototypes
have demonstrated. This allows allowing players to adapt
their existing technique and musical understanding to new
instruments.
Additionally, the shape display’s ﬂexibility and ease of
programmability make it an ideal platform for music ped-
agogy. Part of learning to play music is the reconciliation
of musical understanding with embodied actions on the in-
strument [2]. The shape display allows users an easy way
to physically encode their own evolving musical understand-
ing in the controller’s form and function. It also encourages
bricolage in both instrument design and music-making, pro-
moting playful learning [37].
6. FUTURE WORK
Based on the explorations of this project, we now look far
into the future to imagine how people may interact with
music in a world where shape displays have become an es-
sential part of everyday computing. Just as the computer
has become a standard way of interfacing with digital mu-
sic, shape displays may also become a standard platform for
a new genre of hybrid physical/digital musical instruments.
Musicians around the world will be able to quickly share
their designs and prototypes of new instruments, which may
be downloaded and simulated on any standard shape dis-
play. A culture akin to today’s open source movement may
arise for new musical instruments on this platform. To pop-
ularize their designs, instrument builders may share tutori-
als and encourage other musicians to download, try out, and
ultimately ”fork” their designs, much like code on Github.
Similar to how digital instruments coexist happily with
traditional instruments today, the shape display will not
take the place of existing instruments. Nor will it prevent
designers from building custom digital instruments and con-
trollers. Rather, it will provide an additional means of mu-
sical expression for musicians of across genres, roles, and
levels.
7. CONCLUSIONS
We began this research to assess the versatility of the shape
display as a platform for music making, focusing our eﬀorts
on the physical nature of both control and sound produc-
tion. A state-of-the-art pin-based shape display was used
as an enabling technology. We ﬁrst studied its properties
and looked to several types of existing instruments and
controller for inspiration. We then prototyped three de-
signs that demonstrate a variety of controller paradigms
and methods of sound production. These cover a range of
musical parameters and suggest a wider space of possible
instruments on the shape display. Finally, we discuss the
themes of musician and machine, ending with a vision of
the shape display as a general platform for future music-
making.
On a meta-level, this paper has followed the approach
of Vision-Based Research advocated by Prof. Hiroshi Ishii
[11]. In this approach, existing technologies become vehi-
cles for prototyping an envisioned future, allowing designers
to look beyond current technical constraints to invent radi-
cally new interactions and applications. While constructing
functional instrument for today will always be important,
we encourage the NIME community to try out this approach
to re-invent musical instruments for the future.
8. ACKNOWLEDGMENTS
We are grateful to the members of the original inForm and
TRANSFORM teams for their pioneering work and to the
Tangible Media Group for their support.
9. REFERENCES
[1] R. Arar and A. Kapur. A history of sequencers:
Interfaces for organizing pattern-based music. 2013.
[2] J. S. Bamberger. Action knowledge and symbolic
knowledge: The computer as mediator . Oxford
University Press, 2013.
[3] Q. D. Bowers. Encyclopedia of automatic musical
instruments. Vestal, NY: Vestal Press, 1972, 1973
printing., 1972.
[4] A. Camurri et al. Eyesweb: Toward gesture and aﬀect
recognition in interactive dance and music systems.
Computer Music Journal , 24(1):57–69, 2000.
263
[5] C. Chafe and M. Gurevich. Network time delay and
ensemble accuracy: Eﬀects of latency, asymmetry. In
Audio Engineering Society Convention 117 . Audio
Engineering Society, 2004.
[6] P. R. Cook. Remutualizing the instrument: Co-design
of synthesis algorithms and controllers. In Proc.
SMAC ’03, 2003.
[7] S. Follmer et al. inform: Dynamic physical
aﬀordances and constraints through shape and object
actuation. In Proc. UIST ’13, UIST ’13, pages
417–426, New York, NY, USA, 2013. ACM.
[8] C. B. Fowler. The museum of music: A history of
mechanical instruments. Music Educators Journal ,
pages 45–49, 1967.
[9] A. S. Gladman et al. Biomimetic 4d printing. Nature
Materials.
[10] H. Ishii et al. Transform: Embodiment of ”radical
atoms” at milano design week. In Proc. CHI ’15, CHI
EA ’15, pages 687–694, New York, NY, USA, 2015.
ACM.
[11] H. Ishii, D. Lakatos, L. Bonanni, and J.-B. Labrune.
Radical atoms: Beyond tangible bits, toward
transformable materials. interactions, 19(1):38–51,
Jan. 2012.
[12] H. Ishii and B. Ullmer. Tangible bits: Towards
seamless interfaces between people, bits and atoms. In
Proc. CHI ’97, CHI ’97, pages 234–241, New York,
NY, USA, 1997. ACM.
[13] H. Iwata et al. Project feelex: Adding haptic surface
to graphics. In Proc. SIGGRAPH ’01, SIGGRAPH
’01, pages 469–476, New York, NY, USA, 2001. ACM.
[14] M. Kaltenbranner et al. The reactable*: A
collaborative musical instrument. In Enabling
Technologies: Infrastructure for Collaborative
Enterprises, 2006. WETICE’06. 15th IEEE
International Workshops on , pages 406–411. IEEE,
2006.
[15] A. Kapur. A history of robotic musical instruments.
In Proc. ICMA ’05, pages 21–28. Citeseer, 2005.
[16] A. Kapur et al. The machine orchestra: An ensemble
of human laptop performers and robotic musical
instruments. Computer Music Journal , 35(4):49–63,
2011.
[17] A. Kay. Learning vs. teaching with educational
technologies.
[18] T. Kvifte and A. R. Jensenius. Towards a coherent
terminology and model of instrument description and
design. In Proc. NIME ’06, NIME ’06, pages 220–225,
Paris, France, France, 2006. IRCAM Centre
Pompidou.
[19] D. Leithinger et al. Physical telepresence: Shape
capture and display for embodied, computer-mediated
remote collaboration. In Proc. UIST ’14, UIST ’14,
pages 461–470, New York, NY, USA, 2014. ACM.
[20] G. Levin. The table is the score: An
augmented-reality interface for real-time, tangible,
spectrographic performance. In Proc. ICMC ’06, 2006.
[21] T. Magnusson. Designing constraints: Composing and
performing with digital musical systems. Computer
Music Journal, 34:62–73, 2010.
[22] T. Magnusson and E. H. Mendieta. The acoustic, the
digital and the body: A survey on musical
instruments. In Proc. NIME ’07, 2007.
[23] M. V. Mathews. The radio baton and conductor
program, or: Pitch, the most important and least
expressive part of music. Computer Music Journal ,
15(4):37–46, 1991.
[24] E. R. Miranda and M. M. Wanderley. New digital
musical instruments: control and interaction beyond
the keyboard, volume 21. AR Editions, Inc., 2006.
[25] J. Murphy et al. Musical robotics in a loudspeaker
world: Developments in alternative approaches to
localization and spatialization. Leonardo Music
Journal, 22:41–48, 2012.
[26] E. J. Nattinger. The body parametric: abstraction of
vocal and physical expression in performance . PhD
thesis, Massachusetts Institute of Technology, 2014.
[27] Y. Nishibori and T. Iwai. Tenori-on. In Proc. NIME
’06, pages 172–175. IRCAM-Centre Pompidou, 2006.
[28] G. Odowichuk, S. Trail, P. Driessen, W. Nie, and
W. Page. Sensor fusion: Towards a fully expressive 3d
music control interface. In Communications,
Computers and Signal Processing (PacRim), 2011
IEEE Paciﬁc Rim Conference on , pages 836–841.
IEEE, 2011.
[29] S. Oore. Learning advanced skills on new instruments.
In Proc. NIME ’06, pages 60–64. IRCAM Centre
Pompidou, 2005.
[30] J. A. Paradiso. The brain opera technology: New
instruments and gestural sensors for musical
interaction and performance. Journal of New Music
Research, 28(2):130–149, 1999.
[31] J. Patten et al. Audiopad: a tag-based interface for
musical performance. In Proc. NIME ’02, pages 1–6.
Media Lab Europe, 2002.
[32] K. Perlin. An image synthesizer. In Proc. SIGGRAPH
’85, SIGGRAPH ’85, pages 287–296, New York, NY,
USA, 1985. ACM.
[33] S. Sent ¨urk, S. W. Lee, A. Sastry, A. Daruwalla, and
G. Weinberg. Crossole: A gestural interface for
composition, improvisation and performance using
kinect. In NIME, 2012.
[34] E. Singer et al. Lemur’s musical robots. In Proc.
NIME ’04, pages 181–184. Shizuoka University of Art
and Culture, 2004.
[35] A. Tanaka. Musical performance practice on
sensor-based instruments. Trends in Gestural Control
of Music, 13(389-405):284, 2000.
[36] S. Tibbits. Design to self-assembly. Architectural
Design, 82:68–73, 2012.
[37] S. Turkle and S. Papert. Epistemological pluralism
and the revaluation of the concrete. 11, 1992.
[38] A. Van Troyer. Drumtop: Playing with everyday
objects.
[39] M. Waisvisz. The hands: A set of remote
midi-controllers. Ann Arbor, MI: Michigan
Publishing, University of Michigan Library, 1985.
[40] M. M. Wanderley. Gestural control of music. In
International Workshop Human Supervision and
Control in Engineering and Music , pages 632–644,
2001.
[41] G. Weinberg and S. Driscoll. Toward robotic
musicianship. Computer Music Journal , 30(4):28–45,
2006.
[42] D. Wessel and M. Wright. Problems and prospects for
intimate musical control of computers. Computer
Music Journal, 26(3):11–22, 2002.
[43] M.-J. Yoo, J.-W. Beak, and I.-K. Lee. Creating
musical expression using kinect. In NIME, pages
324–325, 2011.
[44] M. H. Zareei et al. Mutor: Drone chorus of metrically
muted motors. 2014.
264