Repurposing Video Game Software for Musical 
Expression: A Perceptual Approach 
 
  
Adriana Sa  
EAVI group, Computing Department 
Goldsmiths, University of London 
  adrianasa@gold.ac.uk 
 
 
 
 
 
 
 
 
ABSTRACT 
The text exposes a perceptual approach to instrument design 
and composition, and introduces an instrument that combines 
acoustic sound, digital sound, and digital image. We explore 
disparities between human perception and digital analysis as 
creative material. Because the instrument repurposes software 
intended to create video games, we establish a distinction 
between the notion of “flow” in music and gaming, questioning 
how it may substantiate in interaction design. Furthermore, we 
describe how the projected image creates a reactive stage scene 
without deviating attention from the music.  
 
Keywords 
Perception, flow, musical expression, audio-visual mapping 
1. INTRODUCTION 
Our audio-visual instrument includes a custom zither (an 
acoustic multi-string instrument with a fretboard) and 3D 
software called AG#2, which operates based on amplitude and 
pitch detection from the zither input. The way in which AG#2 
adapts software originally meant to create video games 
emphasizes a difference between musical instruments an d 
musical games.  
Several artists and researchers explored links between 
video games and music. J. Klima [15], M. Grierson [6], R. 
Hamilton [7] or F. Berthaut et al. [1] made explicit references 
to gaming while exploring participatory interaction, video game 
player-paradigms and allusive iconography.  
This is not the our case. The audience does not interact with 
the instrument, there are no allusive icons or player-paradigms, 
and the performer does not face the screen. The interaction with 
the instrument is effortull, and the image creates a reactive 
stage scene without distracting the audience from the music. 
 The instrument and its configuration are developed from a 
perceptual approach described in previous publications [ 18, 19, 
20]. This approach resorts to cognition/ attention research and 
controlled experimentation, defining three design principles for 
the creation of audio-visual instruments: 
- Sound: to threshold the performer’s control over the 
instrument and the instrument’s unpredictability, so as to 
convey sonic complexity and expression.  
- Image: to create overall visual continuity , by applying 
gestaltist principles to visual dynamics and dispensing with 
sudden visual changes, which would automatically attract 
attention and subordinate audition to vision.  
- Audio-visual relationship: to create a fungible mapping, 
which fosters a sense of causation, but invites the audience to 
desist trying to understand the instrument, and focus on the 
experience itself. Such mapping includes synchronized 
components and seemingly non-related components; 
complexity and inconsistency confound the actual cause and 
effect relationships.  
2. MUSICAL FLOW AND EXPRESSION 
There is a lack of discussion about flow in musicology. 
Repurposing video game software to convey musical 
expression raises a primary question about interaction: is that 
which conveys a sense of flow in music equal to that which 
conveys a sense of flow in gaming?  M. Csikszentmihalyi 
sought for a general theory about flow, which became  
influential in gaming [10, 21]. According to him, flow entails: 
concentration, merging of action and awareness, loss of self -
consciousness, transformation of time, challenge, clear goals, 
direct feedback, and sense of control [4]. It seems consensual 
that musical flow requires t h e interaction to enable 
concentration, merging of action and awareness , and 
transformation of time. The other features require clarification.  
Loss of self-consciousness may not be applicable when a 
musician is on stage, highly aware of the audience. The s ense 
of self may rather expand as one permeates the wide membrane 
of sensory complexity [18, 20]. As Z. Karkowski said, music 
can "heighten consciousness" and "increase intensity of the 
mind" because “art communicates before it is understood” [9]. 
Challenge exists in both gaming and music, but possibly at 
different levels. Games are meant to rapidly engage the player, 
and difficulties are adjusted accordingly. Designers seek 
methodologies for video games to adapt to different types of 
players, while keeping all of them engaged [8]. There is a  
salient, related discourse in musical interface design [11, 22]. 
Alternatively, one can defend that being skilled with a n 
instrument requires great investment in playing [3]: it takes 
time to incorporate new conceptual understandings and bodily 
memories, be the instrument acoustic or digital. 
A game can be very complex, but the interface itself is 
meant to be effortless. Designers apply the study of the 
mechanisms through which we naturally perceive the world,  
because overlooking the interface brings a sense of immediate 
interaction [5], enabling the player to focus on the goal of the 
game. J. Ryan explains why NIME controllers should rather be 
effortful: "effort is closely related to expression... It is the 
element of energy and desire, of attraction and repulsion in the 
movement of music… Effort maps complex territories onto the 
simple grid of pitch and harmony.” [17:7].  
Ryan’s “grid” seems related with J. Pressing’s hierarchical 
complexity, which refers to the existence of a structure across 
many levels [16]. And his notion of “effort” seems related with 
Pressing’s dynamic or adaptive complexity, which refers to a  
rich range of behaviors over time, or an adaptation to 
unpredictable conditions, or a monitoring of results in relation 
to a reference source, or an anticipation of changes in oneself or 
the environment [16].  
In gaming, clear goals enable the player to act strategically 
 
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use i s  g r a n t e d  w i t h o u t  f e e  p r o v i d e d  t h a t  c o p i e s  a r e  n o t  m a d e  o r  
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, to republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 
NIME’14, June 30 – July 03, 2014, Goldsmiths, University of London, UK. 
Copyright remains with the author(s). 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
331
in order to win. Music is not about goals. It creates an arena of 
expectations, which can be simultaneously teased and fulfilled; 
it is about that experience. The closest to a goal – if forming 
gestalts is a psychophysical goal - is perhaps what B. Snyder 
relates to the gestalt of closure [23]. Closure manifests when 
we mentally complete a shape despite any existing gaps [25]. In 
music, this ‘completion’ depends on psychophysical processes 
and internalised musical traditions. Partial closure occurs when 
some aspects of the music fulfill expectations, and others do 
not. That creates expectations of eventual closure. More 
completely closed musical phrases appearing afterwards will 
also close previous, less closed phrases.  
In video games, sense of control requires the interface to 
respond in predictable ways. Conversely, an effortful music 
interface causes unexpected events to emerge, creating 
compelling performative tension. As performers, we f e e l  that 
this tension makes us acknowledge and respond to sensory 
details we would otherwise not be consciously aware of. It 
generates a minimal, yet graspable hesitation, which the 
audience senses as suspense. Musical expression substantiates 
as the performer addresses the unexpected resourcefully.  
  Given this notion of musical expression, the performer needs 
direct feedback f r o m  t h e  i n s t r u m e n t .  This is an interesting, 
problematic aspect when it comes to digital instruments. Regardless 
of whether any sensors can capture the resilient nuances of 
physical gesture, software is necessarily symbolic, and physical 
action will always be mediated through c ode. As T. Magnusson 
points out, in digital instruments the physical force can be 
mapped from force-sensitive input devices to parameters in the 
sound engine, but that mapping is always arbitrary; in contrast, 
with acoustic instruments the physical body and the sound 
engine are one and the same [12:172]. 
  Speaking of music from the Serialists and John Cage, Ryan 
observed that the immediate process of choice was restrained or 
conditioned so as to deny the habitual or the hackneyed ; in 
contrast, “with computer music… the distance comes for free 
but a distance which can only be viewed as problematical . The 
emphasis may in fact be shifting back towards a quest for 
immediacy in music.” [17:3] 
 There are necessary disparities between an immediate and  a 
mediated output. The combination conveys a musical language 
formed of surreptitious chromaticisms and timings, where 
expression comes from avoiding easy developments. Whereas 
mediated interaction brings certain unpredictability, immediate 
interaction enables the music to shift in good time and 
direction, so that the unexpected becomes gloriously “right”. 
3. HOW THE AUDIO-VISUAL CONVEYS 
THE AUDIENCE’S SONIC EXPERIENCE  
Enabling the audience to experience complex sonic 
constructions in combination with a moving image is 
challenging: usually the audio-visual relationship is skewed in 
favor of the visual. We endeavored to clarify how it can create 
a reactive stage scene without distracting attention from the 
music [18, 19].  
We created a taxonomy relating  automatic and deliberate 
attention with different types of continuities and discontinuities 
[18]. This taxonomy shows that to keep the music in the 
foreground one must dispense with disruptive visual changes, 
i.e. radical discontinuities. Those would automatically attract 
attention, and subordinate audition to vision in multisensory 
integration. There can be a wealth of discontinuities at a level 
of detail, but one should apply gestaltist principles to visual 
dynamics so as to enable perceptual simplification.  
Visual dynamics can exhibit what we have called 
progressive continuities and ambivalent discontinuities . With 
progressive continuities, successive events display a similar 
interval of motion. This fulfills the expectation that once 
something begins to move in a certain direction, it will continue 
to move in that direction (gestalt of good continuation [25]). 
Ambivalent discontinuities  are simultaneously continuous and 
discontinuous. At low perceptual resolution, one senses 
continuity. Deliberate attention makes one optimize perceptual 
resolution, causing discontinuities to become more intense. The 
foreseeable logic shifts, yet not disruptively. 
    Clearly perceivable cause- effect relationships would also be 
problematic for the music, because we are driven to form 
conclusive concepts at the expense of overlooking or skewing 
any conflicting information [18]. We conducted a study to 
demonstrate what we call a fungible audio-visual mapping [19]. 
The fungible mapping produces a sense of causation, yet it 
confounds the base cause and effect relationships . It combines 
synchronized and seemingly non- related components; 
complexity and inconsistency throttle the fit between the sonic 
and visual events. With a fungible mapping, perception keeps 
acknowledging conflicting information, embracing 
convergences and divergences as inconclusive concepts.  
4. INSTRUMENT AND CONFIGURATION 
According to A. Tanaka, NIME are open-ended systems, which 
depend on content, context and configuration [24] . He also 
points out that an acoustical instrument can be thought of as 
data to be “sensed” and analyzed ; the application of mapping 
techniques distinguishes the use of microphone as interactive 
sensor from simple effects. 
 Our input device is an autonomous, self-contained acoustical 
instrument: the digital sound and the digital 3D image are 
modulated upon the audio input from a zither (Figure 1). 
Technically, the AG#2 software would operate upon any audio 
input detection. But its design specifications, mappings, 
parameterisations and structural sections are made for a specific 
zither with aged strings, a personal tuning system, and personal 
zither playing techniques.  
 
 
  
Videos: http://adrianasa.planetaclix.pt/research/AG2.htm 
 
The 3D engine is written by John Klima, using an iOs/Android 
system for video games called Marmalade [13] and an audio 
Figure 1. Zither, digital interface, projected image, and 
technical diagram of the instrument. 
 
 
 
Proceedings of the International Conference on New Interfaces for Musical Expression
332
library called Maximilian [14] . The specifications, audio, 3D 
world and parameterisations are by Adriana Sa.  
 AG#2 has a configuration file, which substantiates a 
threshold between instrument and composition. The instrument 
alone would not guarantee compliance with our three design 
principles: sonic complexity, visual continuity, and fungible 
audio-visual relationships. Thus we will describe the instrument 
and its configuration as a whole. 
4.1 Thresholding Control And 
Unpredictability for Sonic Expression 
The instrument has two interfaces for digital audio: the 
analysis from the zither input and a few switches assigned to 
audio sample banks (Figure 1). Whereas the switches provide 
control in defining musical sections, digital analysis creates 
certain unpredictability.  
The sonic construction welcomes disparities between human 
perception and digital analysis. Whilst software operates based 
on mathematical calculations, humans sample and process the 
information based on attention, cognitive principles, and cross-
sensorial context [18]. The disparities are particularly tangible 
with pitch analysis. For example, a sound may vary in pitch 
during attack, sustain and release, and nevertheless we group 
and hierarchize those pitch variations as we segregate the sound 
from the soundscape [2, 23]. In contrast, the software slices the 
spectrum according to a buffer size, which may lead overtones 
or resonance frequencies to be extracted as fundamentals. Also, 
an overtone can be intense due to the musical structure, without 
the pitch being fundamental according to mathematical 
formulas.  
As the zither playing dwells with digital mappings and 
constraints, the music shifts in- between tonal centers . Some 
zither strings are deliberately aged, and others are from bass or 
Portuguese guitar. Most are tuned beyond tones and halftones: 
around Bb, B, D, E, Eb, F and F#, but not exactly. The detected 
pitch is mapped to the closest tone or half tone, which is 
mapped to a prerecorded sound. The sound plays back twice, 
with a pitch down value applied to the second playback. This 
value is equal to the difference between the detected pitch and 
the closest tone or halftone.  
There are three audio sample banks assigned to switches, 
each containing twelve prerecorded sounds. We have been 
using specific zither playing techniques with each sample bank: 
Table 1. Combinations of sample banks and zither techniques 
Sample banks Zither techniques 
1.  bass guitar, ocean waves, water 
drops, thunder, wind 
dribbling and bow 
2.  dobro, bass guitar, zither 
(bottleneck and pick) 
hands, bottlenecks 
and pick 
3.  piano notes and digital timbres bow and bottleneck 
Certain technical aspects convey performative control . The 
zither is audible whether the software detects it or not. Each 
sample bank is activated or muted through a separate switch. 
Another switch turns the digital audio globally on or off. 
Other technical aspects convey performative tension. The 
detection threshold is high, and one does not fully predict 
where this threshold is while fluidly playing the zither. Given 
the personal tuning, the piezo amplification, and the 
particularities of pitch analysis, a single string can also activate 
several audio samples, each mapped to different tones/ 
halftones. When the input sound has no particular tonality the 
software response is particularly unpredictable. And, the 
performer knows the source-samples stored in digital banks, but 
they become new sounds as pitch down is applied. 
4.2 Image and Audio-Visual Relationship  
The performer stands in front of the screen, dressing white, 
facing the audience.  The projected image is a shifting camera 
view over a digital 3D world, which morphs upon audio input 
detection. The visual shifts upon detection create smooth 
progressive continuities. The engine implements a GLES 2.0 
real-time shader for shadow and lighting and effects; the gestalt 
of invariance respects to how we recognize an object 
independently of rotation, translation, scale, elastic 
deformations, different lighting, and different component 
features [25]. 
Figure 2 and Table 2 describe the 3D world and its dynamics. 
 
Figure 2. 3D world structure (cut). 
Table 2. 3D world structure and visual dynamics 
1 Background: static image rendered full screen, the backdrop. 
2 Sky dome:  an enclosing, rotational image that rotates 
consistently, independently from input sound ; its  motion 
creates progressive continuit ies. Sky dome transparencies 
allow portions of the background image to become visible, 
creating ambivalent discontinuities. 
3 Environment terrain : a large -scale 64x64 vertex mesh , 
which can be thou ght of as a distant "landscape". It is not 
affected by sound. Yet the image applied to the mesh enables 
ambivalent discontinuities , as its t ransparencies allo w 
portions of the sky dome and the background to become 
visible. 
4 Foreground terrain : a 64x64 vertex mesh  with an image 
applied, which can be thought of as the ground the 3D 
camera is standing upon. Audio input produces undulations , 
modulated by sine wav es mapped to pitch . It also displaces 
individual vertices, creating elevation s. Such visual 
dynamics convey the gestalts of invariance and similarity 
(similar elements are grouped as part of the same form [25]). 
Image t ransparencies allow portions of the e nvironment 
terrain, the sky dome and the background to become visible , 
multiplying ambivalent discontinuities. 
5 Water surface : a  s m a l l-scale 64x64 vertex mesh , which 
moves consistently,  creating progressive continuit ies. The 
global t ransparency of the im age is mapped to sound ; its 
changes convey the gestalt of invariance.  Because the 
camera stands upon the foreground terrain, t he water surface 
layers upon the sky dome and the background , which creates 
additional ambivalent discontinuities.  
The audio- visual mapping is fungible. Certain visual 
changes are synchronized with sound, other changes exhibit 
variable delay upon detection, and there are also visual changes 
that do not depend on sound (Table 3).  
Camera direction shifts and terrain elevations are 
synchronized with audio detection. A digital sound is emitted, a 
terrain elevation emerges, and the camera view shifts toward a 
new target direction. Water transparency changes mapped to 
that same detection occur in synchrony with the second, 
pitched-down audio sample. 
The foreground terrain undulates according to variable 
Proceedings of the International Conference on New Interfaces for Musical Expression
333
frequencies; frequency changes occur with variable delay times 
upon detection, but there are also points of synchrony. The sky 
dome and the water surface move consistently, at different 
pace, but independently from audio detection. 
Table 3. Fungible audio-visual mapping in AG#2 
Visual parameters  Active audio parameters 
Synchronized with detection:  
Camera view (target direction) Closest tone or halftone 
Terrain elevation  
(target height)  
Amplitude + difference 
between detected pitch and 
nearest tone/ halftone 
Distance between terrain 
elevation and 3D camera Closest tone or halftone 
Synchronized with playback 
but not with detection:  
Water transparency 
Difference between 
detected pitch and nearest 
tone/ halftone 
Delayed upon detection  
Wave frequency applied to 
foreground terrain 
Delay time = difference 
between detected pitch and 
nearest tone/ halftone 
Frequency = nearest 
tone/halftone 
Dynamic visual events independent from input detection 
Sky dome rotation 
Shader applied to water surface (water reflexions) 
Shader applied to foreground terrain (surface effects) 
 
Perception binds sonic and visual shapes that change or 
move simultaneously (gestalt of common fate [25]), as well as 
those that change adequately proximal in time (gestalt of 
proximity [25]/ sequential integration [2]). Points of synchrony 
bring a sense of causation. As complexity confounds the base 
cause and effect relationships, the sense of causation extends to 
embrace all motions of sound and image.  
5. FURTHER RESEARCH 
We w i l l  soon focus upon how the 3D image can affect the 
digital sound. Sound spatialization will depend upon the 
position of terrain elevations/ sound sources in the 3D world, 
relative to the 3D camera/ listener. The performer does not have 
direct control over the sound processing derived from the 
image, and that may become an issue. In developing the 
instrument and its configurations, some aspects are clear a 
priori, and others must be defined through practice. 
6. ACKNOWLEDGMENTS 
This work is supported by FCT - F o u n d a t i o n  f o r  S c i e n c e  a n d  
Technology. T h a n k s t o J o h n  K l i m a  f o r  h i s  d e d i c a t e d  c o l l a b o r a t i o n  
and his code, which are undispensable in this research. Thanx to Atau 
Tanaka and Mick Grierson, f o r  t h e i r  k n o w l e d g a b l e  advises and 
support. 
7. REFERENCES 
[1] F. Berthaut, H. Katayose, H. Wakama, N. Totani and Y. 
Sato. First Person Shooters as Collaborative Multiprocess 
Instruments. In NIME Proceedings, Oslo (2011), 44–47.  
[2] A. Bregman. Auditory Scene Analysis: The Perceptual 
Organization of Sound, Cambridge, MIT Press, 1990. 
[3] J. Cannon and S. Favilla. The Investment of Play: 
Expression and Affordances in Digital Musical Instrument 
Design. In ICMC proceedings, Ljubljana (2012), 459-466. 
[4] M. Csikszentmihalyi. Creativity: Flow and the Psychology 
of Discovery and Invention. HarperCollins, 1996. 
[5] P. Dourish. Where the Action Is: The Foundations of 
Embodied Interaction. Mit Press, 2004. 
[6] M. Grierson. Noisescape: A 3d Audiovisual Multi-User 
Composition Environment. In The World as Virtual 
Environment, Dresden, Trans-Media-Akademie Hellerau, 
2007, 160-168.  
[7] R. Hamilton. Maps and Legends: Designing Fps-based 
Interfaces for Multi-user Composition, Improvisation and 
Immersive Performance. In CMMR 2007 Proceedings, 
Copenhagen (2008), 478-486. 
[8] R. Hunicke and V. Chapman. AI for Dynamic Difficult 
Adjustment in Games. In Proceedings of the Challenges in 
Game AI Workshop, 19th National Conference on 
Artificial Intelligence, 2004. 
[9] Z. Karkowski. The Method Is Science, the Aim Is 
Religion. 1992. http://www.desk.nl/~northam/oro/zk2.htm 
(accessed December 17th, 2013) 
[10] R. Koster. A Theory of Fun for Game Design. Paraglyph 
Press, 2004 
[11] T. Machover. Beyond Guitar Hero - Towards a New 
Musical Ecology. In RSA Journal, London, 2009. 
[12] T. Magnusson. Of Epistemic Tools: musical instruments 
as cognitive extensions. In Organised Sound 14, no. 2, 
2009, 168-176 
[13] Marmalade software, official website. 
https://www.madewithmarmalade.com 
[14] Maximilian software, official website. 
https://github.com/micknoise/Maximilian. 
[15] C. Paul. Digital Art. Thames & Hudson, 2008. 
[16] J. Pressing. Cognitive Complexity and the Structure of 
Musical Patterns. In Generative Processes in music, ed. by 
J. Sloboda, 129-178 (New York: Oxford University Press, 
1987). 
[17] J. Ryan. Some Remarks on Musical Instrument Design at 
STEIM. In Contemporary music review 6, 1 (1991), 3-17. 
[18] A. Sa. How an Audio-Visual Instrument Can Foster the 
Sonic Experience. Live Visuals, ed. L. Aceti, S. Gibson, S. 
Arisona, O. Sahin, Leonardo Almanac 19, No. 3, 2013, 
284-305. 
[19] A. Sa. A Study About Confounding Causation in Audio-
Visual Mapping. In XCoAX Proceedings, Porto (2014)  
[20] A. Sa. Exploring Disparities Between Acoustic and Digital 
outputs. In Leonardo Transactions, ed. A. McLean (2014) 
[21] P. Sweetser and P. Wyeth. GameFlow: a model for 
evaluating player enjoyment in games. In CIE - 
Theoretical and Practical Computer Applications in 
Entertainment 3, no. 3 (2005), 3. 
[22] F. Pachet. Enhancing Individual Creativity with 
Interactive Musical Reflective Systems. In Musical 
Creativity: Current Research in Theory and Practice, ed. 
I. Deliège and G. Wiggins, Psychology Press (2004) 
[23] B. Snyder. Music and Memory: An Introduction, 
Cambridge, MIT Press, 2001. 
[24] A. Tanaka. Sensor-Based Musical Instruments and 
Interactive Music. In The Oxford Handbook of Computer 
Music, ed. Dean, R., Oxford University Press, 2009, 233-
257. 
[25] M. Wertheimer. Laws of Organization in Perceptual 
Forms. In A Source Book of Gestalt Psychology, ed. W. 
Ellis, London, Routledge & Kegan Paul, 1938, 71-88.
 
Proceedings of the International Conference on New Interfaces for Musical Expression
334