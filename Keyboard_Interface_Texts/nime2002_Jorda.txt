Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02), Dublin, Ireland, May 24-26, 2002  
 NIME02-01 
Afasia: the Ultimate Homeric One-man-multimedia-band
Sergi Jordà 
Music Technology Group, Audiovisual Institute 
Pompeu Fabra University 
Passeig de la Circumval·lació 8, 08003 Barcelona, Spain 
sergi.jorda@iua.upf.es 
 
Abstract 
In this paper we present Afasia, an interactive multime-
dia performance based in Homer’s Odyssey [2]. Afasia is 
a one-man digital theater play in which a lone performer 
fitted with a sensor-suit conducts, like Homer, the whole 
show by himself, controlling 2D animations, DVD video 
and conducting the music mechanically performed by a 
robot quartet. After contextualizing the piece, all of its 
technical elements, starting with the hardware input and 
output components, are described. A special emphasis is 
given to the interactivity strategies and the subsequent 
software design. Since its first version premiered in Bar-
celona in 1998, Afasia has been performed in many 
European and American countries and has received sev-
eral international awards. 
Keywords 
Multimedia interaction, musical robots, real-time musi-
cal systems. 
INTRODUCTION: CONTEXT AND 
ANTECEDENTS 
Aphasia: the loss of the ability to speak and to under-
stand speech due to cerebral disturbance. 
Afasia is an interactive multimedia revision of Homer’s 
epic poem, The Odyssey . Homer, probably a poet per-
former, had only his voice for reciting or chanting a 
story. Afasia takes out speech proposing a wordless ver-
sion, which by no means converts this free version of the 
Odyssey into a minimalist piece. Quite the reverse, as we 
will see. 
Afasia is the third interactive project produced by the 
visual artist and performer Marcel.lí Antúnez with the 
collaboration of the mechanical sculptor Roland Olbeter 
and myself, in charge of the interactivity, music and 
software design. And it can be seen as a logical devel-
opment of the two previous pieces developed by the 
same team, both from a technical and from a conceptual 
point of view.  
In JoAn l’home de carn (1992), we build a gesticulating 
human “being” (or robot) completely covered with natu-
ral pig skin. A real flesh nude body, which moved the 
head and the arms, in accordance with the words or the 
sound of the spectators’ voices. Like many XIXth and 
early XX th century automatons, JoAn was sitting on a 
chair inside a glass cabinet, and was exhibited in public 
places (specially food markets), where he would greet 
and establish a “restricted dialog” with those who would 
talk to him [1]. Although being myself a musician, this 
installation was soundless; sound was not one of  the 
system outputs but it was its only input, the cause of 
JoAn’s limited interactivity.  
In Epizoo (1994) Antúnez takes JoAn’s place, and his 
body, submitted to a complex computer-controlled 
pneumatic system, is exposed to the wills of the audience 
through a videogame-like interface [1,3 and 5]. This 
controversial performance piece, exhibited in art galler-
ies, media art festivals, theaters and raves, in Europe and 
America for about three years, incorporates interactive 
music and animations. Its game-like GUI allows mem-
bers of the audience to manipulate parts of the body of 
the performer (nose, buttocks, chests, mouth and ears), 
but makes them also responsible for constructing in real 
time the whole show music. It is however worth men-
tionable that transforming the audience into a kind of 
instrument of martyrdom, too often vanishes any musical 
aspiration from this active audience! 
AFASIA: THE ELEMENTS OF THE SHOW 
In Afasia, Antúnez, tired of being manipulated, becomes 
the big puppeteer, the one who pulls the strings. Fitted 
with a sensor-suit (the exoskeleton), he conducts, like 
Homer, the whole show by himself. Afasia can be con-
sidered a theatre play but no other actors are on stage; 
only a bunch of mechanical and digital elements con-
trolled by a computer that executes Antúnez’s instruc-
tions.  
The show that lasts for about an hour is usually per-
formed in theatre rooms and has been devised to be seen 
from the front. The front stage is occupied by a robot 
quartet constituted by an electric guitar, a one-string 
violin, a drum and a three-bagpipe “horn section”. These 
robots play mechanical music, while interactive 2-D an-
imations or movies are projected onto a big screen that 
fills all the backstage. The performer determines in real 
time all these sonic and visual elements. 
Other elements that can be heard (but not seen on stage) 
include a sampler, an audio CD player, three effect racks 
and a mixing console. Like the robots, all of them are 
MIDI controlled from the central computer. The same 
computer is also responsible for generating the interac-
tive animations, controlling a DVD player and remotely 
switching the video projector’s SVGA or video inputs. 
Figure 1 shows a diagram of the whole system. 
 
 
 NIME02-02 
Figure 1. Afasia hardware and communications diagram 
The Input: Exoskeleton and Sensors 
I have insisted in other articles [4] that when conceiving 
and building new music instruments or interactive sys-
tems, a parallel design between the controller and the 
generator components is very convenient. However, 
Afasia takes essentially the opposite approach. Because 
of the diversity of tasks it has to solve, the input system 
was conceived as a robust, basic and polyvalent control-
ler. The design process took therefore more into account 
the performer who would be using (i.e. Antúnez) and the 
general conditions of the show, than the specific func-
tionalities of the related output system. 
One of the show premises was that all the interaction 
protocols should be easily seen and understood by the 
audience. The input system has been designed to be visi-
bly obvious (the show can be quite dark at moments), 
and according to the personality of the performer, who 
likes to move in brusque and coarse movements. It in-
cludes four continuous controllers, a set of keyboard-like 
switches and four mercury tilt sensors for detecting ori-
entation and fast movements.  
Afasia’s sensor system is built into an exoskeleton worn 
by the performer, designed and constructed by Roland 
Olbeter, and somehow aesthetically reminiscent of the 
gladiators’ “fashion” found in peplum movies. Many 
sophisticated bodysuits and control exoskeletons, like 
the BodySynth by Van Raalte and Severinghaus [6] have 
been designed for musical purposes or in the Virtual 
Reality field. Compared to most of them, Afasia’s sys-
tem employs quite simple technologies. It consists of a 
breast cuirass made of fiber glass and metal, a backpack 
that centralizes all the electric connections, a metal belt, 
sensor gloves and arms and legs metal prosthesis, all 
made to measure to the performer’s body.  
The four continuous controllers are situated at each of 
the performers extremities’ articulations (elbows and 
knees). Instead of bend sensors, robust and traditional 
turn potentiometer are used, firmly tied to the performer 
by means of mechanical legs and arms, as it can be seen 
in figure 2.  
 
Figure 2. Antúnez with the exoskeleton, interacting 
with the violin robot (photography by Darius Koehli) 
For a one-hour long show with many different parts and 
settings, a sort of a keyboard (i.e any device that can 
offer unambiguous multiple discrete choices) is a trusty 
element. In Afasia this “keyboard” is build into the 
breast cuirass, by means of five metal hemispheres of 
Computer
 Computer
Effects racks
MIDI controlled
Audio Mixer
/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0
/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0
/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0
/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0/i0
MIDI
MIDI
OUT
Internal PC
CD Audio
Internal PC
Sampler Audio
To parallel D/A cards
Rack mounted
D/A parallel
cards and relays
Parallel
communication
Switches to
electro-valves and
step motors
Serial
RS-232
selects
projector
input
Screen fills the whole backstage
Air Compressor
Compressed air to
electro-valves
Violin Guitar Bagpipes
S-Video
from
internal
DVD
SVGA with
2D
animations
486 with
custom card
for decoding
wireless
transmission
Pentium II
with CD
Audio,
internal
sampler,
DVD and
MIDI OUT,
running
Afasia
Software
Projector
Drum
Performer with
exoskeleton
sends wireless
data to 486
4 Turn
potenciometers
continous
controllers
4 Mercury
tilt sensors
“Keys”
Video
Input
switcher
PedalsAux
 
 NIME02-03 
about seven centimeters of diameter each. These electri-
cally isolated metal parts function as keys or buttons, 
closing electric circuits when touched with the special 
finger showed on figure 3.  
 
 
Figure 3. Detail of one “contact finger” with its mer-
cury sensor 
By applying different resistors to each of these hemi-
spheric metal parts, this very simple technology allows 
to use only one analog input channel for all the breast 
buttons. Furthermore, it could also allow distinguishing 
between all of the performer’s fingers, if using a differ-
ent analog channel for each of the fingers. This former 
solution was however finally discarded as it proved to be 
quite confusing for the performer. Consequently, only 
the forefinger of each hand is active. In addition, five 
contact zones following the same principle are also built 
into the performer’s belt, giving the performer a total of 
ten easily available “keys”. 
The four remaining sensors are cheap mercury tilt sen-
sors. They are situated on each forefinger (as shown in 
figure 3), and also on the bottom of each leg prosthesis. 
Mercury switches are often employed as a cheap alterna-
tive to accelerometers for tilt or orientation purposes. 
Here, they are simply used as fast gesture detectors or as 
average activity indicators. 
The Backpack: Wired and Wireless Transmission 
All the electrical connections for the aforementioned 
potentiometers and switches are plugged into a small 
backpack tight to the cuirass, and sent from there to the 
computer analog inputs. For that purpose, two different 
analog input systems where developed. The first version 
used a wired connection between the backpack and the 
computer, with a small analog to digital box plugged into 
one of the computer’s serial port. The performer was 
therefore linked to the main computer by a 10-meter 
umbilical cord. 
A wireless version has been developed later, using wire-
less microphone technology and an old 486 computer 
fitted with a custom card that works as a digital decoder. 
This less powerful computer is serially connected to the 
main computer. After several corrections, this wireless 
version has proved to be quite reliable, although the at-
tainable input sampling frequency is only 15 Hz, which 
is slower than the one available with the wired version.  
The Output: The Robot Quartet 
A big part of Afasia’s music is mechanically played on 
stage by a robot quartet, integrated by an electric guitar, 
a one-string violin, a drum and a three-bagpipe “horn 
section”, all designed and build by Roland Olbeter.  
Robots’ Physical and Logical Communications and 
Control 
The communication between the computer and the ro-
bots is assured by means of several digital to analog 
cards daisy-chained to the computer parallel port. These 
cards control altogether almost a hundred relays and 
several step motors. All these mechanisms (cards, relays, 
etc.) are mounted into four racks, one for each of the 
robots. These racks are chained-connected to the main 
computer parallel port, while each robot is plugged to its 
respective rack using 19-pin S ocapex industrial connec-
tors1, conforming a reliable and easy to mount system. 
The whole robots setup is MIDI controlled. Robots are 
able to play from strictly sequenced sections to free parts 
where the performer uses the exoskeleton-robots combi-
nation as a real musical instrument. Sequenced parts can 
be directly triggered by the performer or indirectly acti-
vated by the system, as in the case of musical fragments 
linked to given film sequences. 
To simplify the bridge between the musical MIDI mes-
sages addressed to the robots by the interactive software, 
and the D/A cards switches, each of the robots has its 
own virtual MIDI driver installed in the main computer. 
Robots’ Construction Guidelines 
Afasia’s robots are not anthropomorphic; they look like 
instruments, not like players.  
The drum, the guitar and the bagpipe robots use exclu-
sively pneumatic mechanisms. A common air compres-
sor feeds them all. The air that reaches the robots is gov-
erned by electro-valves, each of them connected to a 
separated relay. The number of air-channels or relays 
employed by each robot varies from the two channel 
used in the drum to the 72 channels of the guitar. The 
violin, on the other side, uses only step motors.  
The drum and the bagpipe robot are acoustic instru-
ments, while the guitar and the violin robots are electric; 
their monophonic audio line outputs are connected to the 
mixing deck.  
The following sections describe in more details some of 
the specificities of each of these robots. 
The Drum Robot 
This is the simplest of them all. It consists of a tom drum 
with two independent sticks; only two electro-valves and 
two relay switches are therefore used. However, unlike 
                                                                 
1 <http://www.amphenol-socapex.com> 
 
 NIME02-04 
the other robots, this drum can also slowly move on 
stage; the drum is always lied down and its cylindrical 
body can spin under the computer control.  
The Electric Guitar Robot 
This is the biggest and heaviest robot. As seen in figure 
4, this robot does not have a “right hand”. Instead, it 
plays tapping the frets with its “72-fingers left hand” in 
the manner of guitarists Stanley Jordan or Adrian Belew.  
Figure 4. The electric guitar robot. The upper plastic 
tubes are connected to the electro-valves that tap 
each of the guitar frets. The lower “legs” are used for 
twisting the guitar neck (photography by Darius 
Koehli). 
The instrument has six strings. Five are regular electric 
guitar strings, while the lower one is a bass guitar low 
string. Each string has 12 frets and each fret is accessible 
by an independent and small hammer-finger controlled 
by an electro-valve. The virtual MIDI driver of this ro-
bot addresses six MIDI channels, one for each of the 
strings. 
The guitar big neck is also able to twist with two degrees 
of freedom under the computer control, allowing for 
simple but effective rock’n roll choreographies! The 
output of the instrument goes into two pedals (a fuzz and 
a wah-wah) that are not MIDI-controlled but can be 
played by the performer. 
The Three-bagpipe Robot 
This robot, shown in figure 5, is made of three bagpipe 
tubes. The principle is similar to the one employed in the 
guitar. Each hole can be closed by a dedicated finger, 
controlled by an electro-valve. Three additional electro-
valves and three pressure stabilizers are used for turning 
the blow on and off. Its virtual MIDI driver addresses 
three monophonic MIDI channels (one for each bag-
pipe). The mapping between MIDI notes and relays is 
obviously not so straightforward as in the guitar, as fin-
gering techniques have to be considered.  
 
Figure 5. The three-bagpipe robot (photography by 
Darius Koehli). 
Each bagpipe looks like a 2-meter tall standard lamp. 
Like “real big-band saxophonist”, bagpipes are able to 
pivot front and back about 45O. Unlike real big-band 
saxophonists, Afasia’s bagpipes are never in tune! 
The Violin Robot 
The violin robot has only one (electric guitar) string and 
one glissando finger. This string is fretted by an Ebow2 
while the finger, controlled by a step motor, can slide up 
and down. As shown in figure 6, the violin neck is a 
transparent plastic cylinder with a lamp inside. This cyl-
inder is sustained in its gravity center and can delicately 
oscillate around it with two degrees of freedom.  
The purity of its sound and the grace of its movements 
make the violin the perfect counterpoint of the clumsy 
and brutish electric guitar robot fellow. 
Complementary Music and Sound Control 
The music mechanically played by the electric robots 
(i.e. guitar and violin), is mixed together with the music 
coming from an internal sampler card and with occa-
sional prerecorded audio fragments triggered from an 
internal CD audio player, into a MIDI-controlled Ya-
maha audio mixer. This output is eventually processed 
through three effects processors racks, which are also 
MIDI controlled by the main computer. 
                                                                 
2 < http://www.ebow.com> 

 
 NIME02-05 
Figure 6. The violin robot (photography by Darius 
Koehli). 
INTERACTIVITY STRATEGIES AND SOFTWARE 
DESIGN 
Interactive language is often closer to being a metaphor 
for a geographical or spatial area than for a time se-
quence. Using Ulysses’ voyage as a metaphor, Afasia is 
structured on different expressive and space-like levels, 
proposing a series of interrelations between sound, vi-
sion and performance. The division of the show in vari-
ous islands or independent interactive environments is 
also a technical consequence of the complexity of the 
elements to be controlled as opposed to the limited se-
mantics of the sensors employed.  
Afasia’s Odyssey is consequently a sea with islands, 
sections or states, each with its own interactive elements 
and behavior, each using the interfaces, the robots and 
the visuals in different ways, with varied input-output 
mappings and varied restricted degrees of freedom.  
Although the navigation main menu that permits to jump 
from one island-state to another, and the control of the 
interactive visuals, both bring many innovative solutions, 
the rest of the paper will only cover the interactive music 
aspects. 
Afasia Interactive MIDI Files 
In Afasia, all musical interactivity is treated by means of 
format 1 Standard MIDI Files, expanded with the inclu-
sion of proprietary text meta-events and reproduced 
through a custom interactive sequencer.  
Text events are often used by sequencers to name parts 
of the tracks or to display synchronized lyrics. In the 
Afasia software they are used as commands that tell the 
custom sequencer player how to deal with, and process, 
the recorded MIDI events, according to the performer’s 
gestures. This approach enables to write, record and 
program interactive Afasia music using any standard 
sequencer software, although the interactivity is only 
available when the file is played in the custom Afasia 
sequencer player (when Afasia Standard MIDI files are 
reproduced on a conventional sequencer no interactivity 
is attained, as sequencers simply skip text meta-events). 
Here is a brief description of the Afasia MIDI files struc-
ture: 
• Each part (or island) of the show is associated with 
one Standard MIDI file.  
• Each MIDI file is made of any number of blocks 
(an Afasia concept not present in standard MIDI 
files). 
• A block  is a group of sequencer’s tracks that be-
haves like a permutable section of a score, only one 
block being active at any time. 
• Each block is made of a special control track and 
any number of conventional MIDI tracks. 
• Control tracks  only contain text meta-events that 
indicate how to interact with the other block’s 
tracks, alter the block structure or jump to other 
blocks. 
• Conventional tracks can also contain text meta-
events that indicate how to interact or modify the 
MIDI data inside the track. 
• As conventional sequencer tracks, each track (ex-
cept for control tracks) is directed to a specific port 
or device, and to a specific channel within that de-
vice. 
• Six MIDI ports are used in Afasia (a custom port 
for each of the four robots, the standard internal 
MIDI port for controlling the soundcard sampler, 
and the standard external MIDI port for controlling 
the audio mixer and the three effects processors). 
• Each device can have several MIDI channels (e.g. 
the electric guitar robot has one channel for each of 
its strings, while the bagpipe section MIDI port 
uses three channels, one for each bagpipe). 
The text meta-events embedded in the sequencer tracks, 
specify interactive commands and do always have the 
following structure: 
COMMAND_NAME=param1,param2,...,paramN$ 
The number of parameters ( N) and their meanings varies 
with each command type. The first parameter usually 
corresponds to an input sensor number while the second 

 
 NIME02-06 
indicates which track of the block the command applies 
to. Following parameters define how these incoming 
sensor values have to be mapped.  
For example, the command TRANSPOSE=4,3,12$ indi-
cates that the third track of the block will be transposed 
according to the incoming values of sensor #4, in a 
maximum range of +-12 semitones. Many commands 
(TRANSPOSE is one of them) have an end version com-
mand (e.g. ENDTRANSPOSE) that disables the effect.  
More than forty commands are implemented in Afasia. 
They allow to dynamically switch between blocks, 
mute/unmute tracks, transpose the tracks, loop any se-
quence and modify their lengths, modify the current play 
position, generate or modify any MIDI controller (either 
setting directly the new value, its increment or the range 
of random variation), quantize tracks, delay tracks, con-
trol tempo, compress/expand temporal events, trigger 
audio CD tracks or define chords or scales to which new 
generated notes will be matched.  
Several additional macro-commands called solo modes  
can also be activated, each one defining a peculiar way 
of playing (i.e. entering new MIDI note messages into 
the system). While the previous commands use simple 
one-to-one mappings, solo modes  apply more sophisti-
cated gesture detection mechanisms, that allow for ex-
ample to play the robot-guitar in an almost “conven-
tional” manner, controlling pitch with the performer’s 
left elbow angle, while triggering notes with different 
right-arm fast movements that enable the performer to 
play chords, arpeggios or monophonic lines. Twelve 
solo modes (three for each robot) have been defined. 
Animations and DVD Quick Overview 
Afasia is not only an interactive music system. As it has 
already been told, the software also allows the performer 
to control 2D animations or access a DVD video. All 
this is attained by means of complementary text files that 
constitute the interactive script of the play. The principle 
of these multimedia script files is similar (although sim-
pler) to the one developed for the interactive MIDI.  
Each island or part of the show is a section of the text. It 
defines the input channels, the output elements employed 
(sprites or image files, DVD tracks, etc.) and the map-
pings applied between the inputs and the outputs.  
In a given part, a continuous controller could, for in-
stance, determine the position of a sprite in the screen or 
the frame number of an animated sequence, while a dis-
crete controller could select a chapter or a section of the 
DVD. Several image-processing techniques (e.g. zoom, 
color modifications, etc.) are also available. Besides, as 
only one projector is used, the system automatically con-
trols the active input of the beamer, switching as needed 
between the video (DVD) and the SVGA inputs by 
means of RS-232 messages. 
CONCLUSIONS 
The first version of Afasia was premiered in 1998 in the 
Teatre Nacional de Catalunya of Barcelona. Since then it 
has been performed in several European and American 
countries and has received several international awards. 
The system has proved to be so reliable and easy to set-
up that when touring, only one technician accompanies 
the performer.  
The system has also proved to be a flexible framework 
for interactive music composition and performance. Its 
varied interactive possibilities turn it into a powerful and 
flexible interactive system, which permits anything from 
free audiovisual improvisation to completely pre-
recorded sequence playing. The Afasia software can be 
considered in that sense, as a multimedia instrument, for 
which the Afasia interactive play is its first (and for the 
moment only) composition. Besides all of its multimedia 
capabilities, the approach taken by Afasia’s interactive 
MIDI kernel seems also especially suitable for interac-
tive soundtracks in videogames or other multimedia en-
vironments and should be further explored. 
REFERENCES 
[1] Gianetti, C., Marcel.lí Antúnez Roca: Perform-
ances, Objects and Drawings, MECAD, Barcelona 
(1998). 
[2] Homer (Fitzerald, R. translator),  The Oddissey, An-
chor Books, Garden City, NJ (1989). 
[3] Jordà, S., “EPIZOO: Cruelty and Gratuitousness in 
Real Virtuality.” Proceedings of 5CYBERCONF, 
fifth International Conference on Cyberspace 
(1996), at 
<http://www.telefonica.es/fat/ejorda.html>. 
[4] Jordà, S. “Improvising with Computers: A personal 
Survey (1989-2001).” Proceedings of the 2001 In-
ternational Computer Music Conference. San Fran-
cisco: International Computer Music Association. 
[5] Macri, T., Il corpo postorganico, Sconfinamento 
della performance, Costa & Nolan, Genova (1996). 
[6] Severinghaus, E. and C.V. Raalte. “The BodySynth” 
<http://www.synthzone.com/bsynth.html>. 
 
 