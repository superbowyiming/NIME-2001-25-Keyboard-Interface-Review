Reimagining the Computer Keyboard as a Musical Interface    Si Waite Faculty of Arts and Creative Technologies   Staffordshire University  Stafford, ST18 0AD  s.j.waite@staffs.ac.uk  
    
ABSTRACT This paper discusses the use of typed text as a real-time input for interactive performance systems. A brief review of the literature discusses text-based generative systems, links between typing and playing percussion instruments and the use of typing gestures in contemporary performance practice. The paper then documents the author’s audio-visual system that is driven by the typing of text/lyrics in real-time. It is argued that the system promotes the sensation of liveness through clear, perceptible links between the performer’s gestures, the system’s audio outputs and the its visual outputs. The system also provides a novel approach to the use of generative techniques in the composition and live performance of songs. Future developments would include the use of dynamic text effects linked to sound generation and greater interaction between human performer and the visuals.   Author Keywords Text, typing, computer keyboard, live performance, Max, system   ACM Classification H.5.2 [Information Interfaces and Presentation] User Interfaces– Input devices and strategies, I.7 [Document and Text Processing] Miscellaneous, H.5.5 [Information Interfaces and Presentation] Sound and Music Computing.  1. INTRODUCTION 1.1 Text as Musical Input The link between text and musical notation is well established. In recent years, real-time computer music systems have facilitated the exploration of this link in live performance. For example, Guido’s eleventh-century method for composing chant melodies can be implemented as a real-time system for live text input [1]; speech recognition software has been used to convert vocal performances into text streams that control sound generators [2] and live text input can be used to generate and manipulate scores for instrumental performers during a concert [3]. 1.2 Typing as Musical Gesture There is a clear similarity between the act of typing on a keyboard and that of playing a percussion instrument such as a piano [4]. A recent study has demonstrated that proficient piano-players are able to generate text at comparable speeds to touch-typists [5]. This gestural relationship has been exploited in compositions such as 
Leroy Anderson’s “The Typewriter” [6] and Steve Reich and Beryl Korot’s “The Cave” [7], which also featured the live projection of the text as it was rhythmically typed by the performers.   It has been argued that many computer users display a degree of virtuosity on a computer keyboard that is comparable to virtuosity on a musical instrument. Digital instrument designers have exploited this to create computer keyboard-based instruments that do not require extensive practice [8]. Furthermore, each key does not need to be tied to a particular pitch, meaning that similar gestures can be easily transformed to yield very different sonic results [9]. 2. SYSTEM DESCRIPTION 2.1 Aims of the System The author’s system aims to incorporate several elements described in the previous section. The text of the piece is treated as the score, which is performed through typing. The live stream of text controls and influences melody, rhythm, timbre and visuals. This stream is projected as it is typed, letter by letter, to reinforce the perception of liveness (a strong connection between a performer’s physical gesture and resultant sound [10]) for both audience and performer. A variety of gestural extraction techniques (detailed below) process this live stream to control audio and visual outputs. 2.2 Compositional Goals Just as Robert Ashley’s “Automatic Writing” explored speech gestures that are produced involuntarily and therefore lack conscious design [11], “Kafka-esque” explores how the rhythmic and melodic aspects of typing can be captured to create musical output that is not totally consciously designed by the performer. It is anticipated that audiences will sense that the music has a rhythmic and melodic quality, but that these qualities remain tantalizingly elusive. This kind of approach to songwriting and performance is indicative of the author’s wider creative goals [12]. 2.3 System Overview The system is realized in Max. An overview of its architecture is given in Figure 1. The inputs (under direct control by the performer) are a computer keyboard and a USB control surface to manipulate the volumes and stereo positions of the various sound-producing elements. 2.4 Score Following and Visuals Several keywords in the text are identified which serve as triggers for visual outputs, such as fading between video sources and initiating effects. Combined with the real-time display of the text, this creates a score/narrative for both the performer and the audience to follow. A feedback loop in which the overall video output affects the timbre of the synthesizers further strengthens the audio-visual relationships. 2.5 The Singing Computer Stored samples of sung vowel sounds as well as synthesized vowel sounds are triggered by the live text input. For example, typing “you” or “room” would initiate playback of a sung “oo” sound.  
 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA. Copyright remains with the author(s).  
168
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
 The pitches of vocal sounds are controlled by a real-time version of Guido’s system, a basic generative system that assigns incoming vowels a pitch value [1] and by a cyclical, pre-determined melody in which each press of the space bar instigates the next note in the sequence. 
 Figure 1: An overview of the system  2.6 Rhythmic Typing Gestures Two methods for capturing rhythmic gestures are used that involve the use of a double “listener and player” mechanism to enable simultaneous listening and playback [13]. One of the engines is under the performer’s direct control; the other simply plays back the real-time input following a short delay. Using keyboard shortcuts, the performer is able to initiate, change or stop rhythmic playback during the course of the performance. 2.7 Role of the Human Performer The performer should sit side-on to the audience and the projection in order to promote the idea that they are not the central aspect of the performance and are, like the audience, following the visual “score”. To promote the notion of liveness, the performer’s typing actions (particularly to generate rhythms and to control the cyclical melody with the space bar) and use of the control surface should be exaggerated.  3. CONCLUSIONS Although this system does not introduce new techniques, the combination of existing techniques into a novel system affords the performer low latency response; the simultaneous creation of layered melodies and rhythms; the display of text as it is typed and a high degree of control and expression. Together with the emphasis on gestural performance and the system’s ability to respond to these gestures (not to mention the immediate display of typing errors!), audiences should perceive a high degree of liveness.  The system is also successful in providing a novel approach to the performance of songs, by taking the focus away from the performer and their vocal/instrumental prowess and and placing it instead on the lyrics. The combination of typing rhythms, electronic and natural timbres, cyclical and generative melodies and glitchy video create an aesthetic that sits well with both contemporary popular and experimental styles.  The video recordings add additional channels of communication without giving the performer too much to do. Moreover, because they are an integral part of the system (the video output controls timbres of synths), they should contribute to the perception of liveness rather than distract from it.  
 While there is a lot of time-consuming pre-programming in terms of selecting videos, sound samples, keywords and vowel combinations, the system is still highly adaptable and configurable for different pieces to be performed.   Further possibilities for exploration include the creation of dynamic text effects that map to audio generators, further reinforcing the link between the audio and visual elements. This could be developed further through using additional sensors to enable the performer to interact with the projected text in a more physical sense [14].  4. ACKNOWLEDGEMENTS Thank you to John Richards, Bret Battey, Joshua Banks Mailman, Ben Ramsay, Kerrie Williamson and Jude Waite for their invaluable feedback and support during the creation of the system and piece.  5. REFERENCES [1] Rowe, R. Interactive Music Systems: Machine Listening and Composing. MIT Press, Cambridge MA, 1993. [2] Rouas, J., Mansencal, B. and Larralde, J. Tale following: real-time speech recognition applied to live performance. In Proceedings of the Sound and Music Computing Conference 2013, (Stockholm, Sweden, 2013). 389-394. [3] Freeman, J. Bringing Instrumental Musicians into Interactive Music Systems through Notation. Leonardo Music Journal 21. 15-16. [4] Hirt, K. When Machines Play Chopin: Musical Spirit and Automation in Nineteenth-Century German Literature. Walter de Gruyter, Berlin, 2010. [5] Feit, A. and Oulasvirta, A. PianoText: Transferring Musical Expertise to Text Entry. In Proceedings of CHI Conference on Human Factors in Computing Systems (CHI ’13), (Paris, France, April 27-May 02, 2013). ACM Press, New York, NY, 2013, 3043-3046. [6] Anderson, L. The Typewriter: Leroy Anderson Favorites (CD). RCA Victor, 1995. [7] Reich, S. and Korot, B. The Cave (CD). Nonesuch, 2005. [8] Fallgatter, J. Foundation of Aqwertyan™ Music. Aqwertyan Music Systems. 2013. Retrieved 7th January 2015. http://www.aqwertian.com/ [9] Kirn, P. QWERTY Keyboard Instrument: Samchillian Tip Tip Tip Cheeepeeeee. Create Digital Music. 2004. Retrieved 7th January 2015. http://createdigitalmusic.com/2004/11/qwerty-keyboard-instrument-samchillian-tip-tip-tip-cheeepeeeee/  [10] Bown, O., Bell, R. and Parkinson, A. Examining the Perception of Liveness and Activity in Laptop Music: Listeners' Inference about What the Performer is Doing from the Audio Alone. In Proceedings of the International Conference on New Interfaces for Musical Expression (NIME 2014), (London, UK, June 30-July 04, 2014). 13-18. [11] Steingo, G. Robert Ashley and the Tourettic Voice. Review of Disability Studies: An International Journal 4:1. 30-32. [12] Waite, S. Sensation and Control: Indeterminate Approaches in Popular Music. Leonardo Music Journal 24. 78-79. [13] Winkler, T. Composing Interactive Music: Techniques and Ideas using Max. MIT Press, Cambridge MA, 1998. [14] Fajardo, L. “.txt” by Fernando Nabais, Fernando Galrito, and Stephan Jürgens. I Love E-Poetry: Short-form scholarship on born-digital poetry and poetics. 2013. Retrieved 7th January, 2015. http://iloveepoetry.com/?p=6787   6. APPENDICES The software for this system is available by request to the author at the above email address. A video demonstrating the system can be found at http://vimeo.com/83867750 169
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 