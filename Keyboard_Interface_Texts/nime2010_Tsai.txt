AN INTERACTIVE RESPONSIVE SKIN FOR MUSIC 
PERFORMERS, AIDA 
 
Chih-Chieh Tsai 
National Yunlin University of 
Science and Technology 
123, Sec. 3, University Rd., Douliu 
City, Yunlin County 64002, Taiwan 
886-5-5342601 ext. 6290 
thedogtsai@gmail.com 
 
Cha-Lin Liu 
National Yunlin University of 
Science and Technology 
123, Sec. 3, University Rd., Douliu 
City, Yunlin County 64002, Taiwan 
886-5-5342601 ext. 6517 
chalin@yuntech.edu.tw 
 
Teng-Wen Chang 
National Yunlin University of 
Science and Technology 
123, Sec. 3, University Rd., Douliu 
City, Yunlin County 64002, Taiwan 
886-5-5342601 ext. 6510 
tengwen@yuntech.edu.tw 
 
ABSTRACT 
With the decreasing audience of classical music performance, 
this research aims to develop a performance- enhancement 
system, called AIDA, to help classical performers better 
communicating with their audiences. With three procedures 
Input-Processing-Output, AIDA system can sense and analyze 
the body information of performers and further reflect  it onto 
the responsive skin. Thus abstract and intangible emotional 
expressions of performers are transformed into tangible and 
concrete visual elements, which clearly facilitating the 
audiences’ threshold for music appreciation. 
Keywords 
Interactive Performance, Ambient Environment, Responsive  
Skin, Music performance. 
1. INTRODUCTION 
The audience of classical music is decreasing rapidly especially 
in contrast with pop music. Conventional classical music 
performance thoroughly depends on audio transmission, such as 
timbres, tempi, dynamic, ambient sound, melodies, and chords a 
performance may evolve. It is the  above-mentioned audio 
factors that create different emoti on in listeners and all of them 
decide listeners’ perception for music. Usually, a successful live 
performance is reflected by listeners ’ reaction, i.e. , whether 
listeners are satisfied with the effect performers present onstage. 
Although music is conceive d of universal language, many 
people still have difficulty understand it and further appreciate 
it due to its intangibility .  Thus, this research aims to develop a 
performance-enhancement system, helping classical performers 
better illustrate their concerts in the live show. 
Unlike traditional classical performance, which purely relies on 
audio effect to communicate with the audiences, this 
performance-enhancement system adds some visual elements 
into the performance and makes it a lively and enthusiastic 
visual-audio live show. This visualization -aided enhancement  
system (called AIDA)  not only deliver messages between 
performers and listeners better but also bring more enjoyment 
for the listeners when attending a classical concert. 
The concept of the AIDA system is to change the wall of the 
concert hall from fixed one to an elastic and flexible skin. Any 
stimuli caused or produced by performance will be recorded 
and reflected on the skin. In other words, the IDEA system 
visualizes the audio information, transforms t he audio effect 
from intangible to tangible, from invisible to visible, so that 
audiences who lack of music training may catch each  subtle 
alteration made by performers even if they overhear the music. 
Interaction is the key point for AIDA system. Usually,  p eople 
perceive the space (ambient environment) surrounds our bodies 
as a separation of the personal space from outside to inside. 
Through “the involvement of the body”, behaviors are 
conducted, and participated in so called “field” or “existence” 
of interaction. Following this concept, AIDA will provide an 
ambient environment that embeds the power of performance 
into the space itself. Thus, audiences not only listen and 
visualize the performance, but also perceive the meaning of 
music within the atmosphere.  
1.1 Motivation: Interaction on stage 
How can performers interact with the skin AIDA supported? 
Here we believe that performers’ physical changes will reflect 
onto their behaviors, such as facial expression, skin temperature, 
heart beats, postures, body movements, eyeballs ’ sizes, etc. 
Since these onstage behavior changes can be measured  and 
analyzed, we can  utilize this body information to interact  with 
the skin. 
Bernhardt and Robinson implements an  i nteractive control of 
music using emotional body expressions [1]. Once the 
performer moves the body, the  movement will be measured, 
recorded, and analyzed. This analysis information may contain 
the action ’s direction, weight, its position, and possible route . 
The analysis information then is divided into two sections: one 
is assigned to a machine which is responsible for finding the 
pattern of the body motion and starting  the self-taught process. 
Its purpose is  for better recogni zing the behavior or imitating  
and representing it when it appears  for the next time. Another 
direction is that the body information will be decoded to 
emotion code, so that the corresponding default music library 
may produce appropriate music excerpt which  bounces back to 
performers. This result  is a musical performance cyclical 
pattern. Detailed is shown in figure 1.  
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME2010, June 15-18, 2010, Sydney, Australia 
Copyright remains with the author(s). 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
399
 
Figure 1. Design of the emotional music mixing interface [1] 
 
Another case from music therapy is SiMS [2].  Built for detect 
the relationship between brain activity, physiology, emotion and 
musical features , SiMS has three parts: 1)  physiological input 
(sensor organ) to detect electroencephalogram (EEG) and heart-
rate (HR) signal  which may trigger human’s behavior, 2) 
processing model, the processing and decision-making (cen tral 
nervous system) part, and 3) motor output (muscle system and 
glands). With the translation of some emotional properties from 
the brain activity and HR signal into the musical domain , the 
system tries  to influence the physiological response  with 
corresponding music.  
1.2 Our Approach 
With these two cases above, we adapt the process from Figure 1 
and add the concept of SiMS to form the basic sensing 
framework of AIDA. 
Three steps are used in this research. Firstly, we analyze the 
informative output from the performer, including body motions, 
facial expression, and biological information. Secondly, 
following the input-process-output paradigm , the responsive 
skin is implemented as a computational reificati on of concept . 
Thirdly, a test case is conducted for understanding  the details of 
interaction behaviors.  
2. FINDING INTERACTIVE BEHAVIORS 
Following the case described above, we adapt the methodology 
of motion analysis for finding the interactive behaviors  on stage. 
Starting with posture and behavior analysis, we narrow down 
the behaviors that are then verified by interviewing with 
performers. 
2.1 Posture and Behavior Analysis 
From the system point of views, the system can only gain the 
input as the postures or motion by the performers other than the 
music performed. The purpose of post ure analysis is to find out 
the posture profile and identify the postures. The process of 
posture analysis must be clear and reflecting the concurrent 
emotion of performers. In addition, such analysis has to be in 
real-time. This might be a problem without invoking advance 
technology such as machine learning or artificial neural 
network. Addition to the posture analysis is bio -sensors. Bio-
sensor can be used for explicating  the inner emotion. With aid 
of bio-sensors and posture analysis, the behaviors  of performers 
can be defined and adapted as the interaction trigger. 
The analysis will be conducted in a sequence of lab experiments 
that will generate the data for the verification. 
2.2 Interview with Performers 
For reifying the  analysis outcome from analysis conducted, a  
professional pianist is brought  in for the in-depth interview for 
validating (1) the approach of using interaction design to 
support performers and (2) the analyzed  data and corresponded 
emotion.  
Before verifying the data, the interview conducted focus on the 
substantiation on using interaction design to support performers 
on stage. One thing mentioned by the pianist is that t he 
introverted emotion of performers has to be performed 
explicitly through the gesture and acoustic cues  for the purpose 
of persuading listeners to have the same mood as performers. In 
other words, the introverted emotions of performers have to be 
transformed into external emotions.  
By going through the mapping of posture analysis with bio-
sensors and the mapping to the emotions, the behaviors are 
approved are the temperature and heartbeats. The gestures are 
also part of valid sources for revealing the inner emotion of 
performers.  
2.3 Test Case 
A video -clip of Bridge’s Fantasy is chosen for the purpose of 
analysis. The subtle differences of performers’ (pianist in this 
clip) behaviors and postures, such as facial expressions, body 
motions, humidity, etc., are notated and mapped to appropriate 
visual expression. The outcome is then reified by the interview 
described above. 
3. RESPONSIVE SKIN 
3.1 The Concept 
The concept of responsive skin comes from architecture 
domains. In architecture, a skin provides a sheltered place that 
defines the relationship between internal and external spaces. 
Further with concurrent researches in parametric designs and 
responsive technologies, architectural skin has been designed 
with light-weight structure and provides a set of responsive 
behaviors according to the habitants under the skin. 
3.2 Responsive Technology 
The key issue for responsive technology is the environment will 
be changed according to sensed behaviors conducted by the 
users. Consequently, the environment will be changed 
automatically based on user behavior, this concept also 
describes the information-intensive [3]. 
The responsive technology can be divided into sensing and 
actuators. T he sensing technology transform s the space into a 
sensible space. Such spaces can then covey physical 
information such as chemical, biomass and so on into the 
possibility of re-usable electronic devices and output signals. 
Sensing technology blurs the relationship between the physical 
and virtual information, and led us into an irregular, but the rich 
concept of innovative interface. By developing of sensing 
technology, space interface should be with (1) perceiving 
internal and external environmental conditions; (2) detecting 
user activities; (3) trigging a response from the skin. 
Further with actuator, the skin can then response with moti on or 
transformation based on input from sensing  technology. 
Combined with interactive mechanism of digital technology 
(sensing → computing → actuator→ response), we can then 
explore a new type of space and its significance. 
3.3 Case Studies 
For finding the s olution suitable for our purpose, two cases are 
studied: Son-O House and Hylozoic Soil. 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
400
3.3.1 Son-O House 
Son-O House [4] is a mixing chamber of instruments, musical 
notes, a combination, human exposure in it can hear the 
wonderful music, and can also participat e in the creation of 
music. Breaking the shackles of traditional architecture and 
tools, this project creates new digital-based hardware and 
software to the architectural facade design and its  computer 
design potentials. In the input and output of system, this project 
creates a reaction mechanism with responsive perceived 
interaction. 
 
Figure 2. Son-O House. 
 
3.3.2 Hylozoic Soil 
Hylozoic Soil [5]  is an i nteractive installation  and power art 
work which is similar  with artificial forest done by Philip 
Beesley. It uses s hape memory alloy  as drive, with f eather-like 
veins and the capacitance sensor, f eather-like veins will squirm 
their interactions produce similar respiratory , and building the 
mutual influence environment between  artificial and natural 
systems when roaming the forest . Feather-like veins produce 
respiratory fretting behavior is from the distributed sensor 
network controlling the dozens of micro-sensors; however, 
other dozens of panel controls the  overall interactive 
environment. The whole work presents interaction between 
virtual and physical environment. 
 
Figure 3. Hylozoic Soil. 
 
4. AIDA SYSTEM 
4.1 System Design Concept 
The Concept of AIDA system is reflecting the emotional 
expression of performer without affect ing the performers on 
stage. AIDA emphasizes on harmonious combination among 
the audience, performers and music atmosphere of all three  
(figure 4). 
 
Figure 4. AIDA System diagram. 
 
4.2 The Operations 
The operations of AIDA are described in three parts: inp ut-
processing-output as below. 
4.2.1 Input 
The system detects the physiological responses and external 
behaviors when performing, with contextual data. For example, 
when performing an agitated piece, anxious facial expressions 
as well as tensed body movements may appear automatically. In 
the contrast, performer’s body will swing along with the rhythm 
of the piece unconsciously. In addition, the bio-sensors are used 
as collecting the real-time information of performers while they 
perform on stage. 
4.2.2 Processing 
According to collected information from input, interactive re -
analysis and computing mechanism is applied, and then 
response the results back to the responsive skin. This research 
focuses on the visualization of performer’s emotion and 
behavior. On the other words, the system transforms the internal 
emotion of performer to external environment. 
4.2.3 Output 
The results before computing are showing in responsive skin. 
Responsive skin not only frames the space, but joins the factors 
of crossing time and songs, and then makes a perception of 
space, visual space, auditory space, perceptual space. 
4.3 Technologies 
The current AIDA is built on Arduino and bio-sensors as the 
input/process parts. The dynamic transformation of responsive 
skin is done by  a set of s ervo motors and metal structures. F or 
testing the overall outcome, this research  designs a simple 
machinery structure to simulate the presentation in the real 
concert hall. 
5. IMPLEMTATION 
An experimental implementation is conducted for testifying the 
system concept. According to the interactive behavior analysis, 
heart beats and skin temperature of performers are chosen as the 
input. The corresponded changes of responsive skin are 
following the case studies. 
5.1 The design of the responsive skin 
The responsive skin is surrounding the performer and the major 
display components are facing the audiences (as shown in 
figure 5 ). The bio-sensors are carried by the performer and 
using wireless to transmit the information to the responsive skin. 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
401
 
Figure 5. Angle of depression of the concert hall. 
 
The responsive skin is surrounding the performer and the major 
responses of the skin are located in the middle of stage and 
facing the audience as shown in figure 5. 
5.2 The Mapping Mechanism 
5.2.1 The heartbeats 
The mapping mechanism between heartbeats is shown in figure 
6. The highest will release the umbrella . When the heartbeats 
change to lower, the surface starts to shrink gradually. 
 
Figure 6. The structures of heartbeat a wave crest and 
trough 
5.2.2 The body temperatures 
Body temperatures of performers are captured and mapped to 
the responsive skin shown in figure 7. 
 
Figure 7. Audience view and the left (right) is high (low) 
body temperature of performer. 
5.3 The reflection of the responsive skin in 
colors and form 
With the aid of sensing technology, emotions of different forms 
are captured and transformed into the responsive skin in the 
form of light, fabric, and mechanical conversions.  The mapping 
is shown in figure 7. The implementation is conducted as an 
experimental concert. 
6. DISCUSSION AND CONCLUSION 
With the support of AIDA system, abstract and intangible 
emotional expressions of performers are transformed into 
tangible and concrete visual elements, which clearly facilitating 
the audiences’ threshold for music appreciation. 
The comparison between the traditional concert hall and the one 
with AIDA system is created (table 1). 
 
Table 1. Comparison between traditional music hall and 
AIDA 
 Traditional Music Hall AIDA Music Hall 
Performer 
Besides music, 
performers only from the 
physical movements and 
expressions to convey 
musical emotion. 
Through the original 
musical elements, 
physical movement, and 
with the environment of 
the assistance, to 
enhance performance a 
sense of 
accomplishment; to 
share the music, 
atmosphere with the 
audience. 
Audience 
Besides music, the 
audiences only feel the 
music emotion by body 
movements from the 
performers. 
Existing music feast, 
together with the 
performers’ body 
movements and facial 
expressions, and 
integrating of 
environmental change; 
audience enjoy more 
visual stimulation. 
The 
Milestone of 
Performing 
Arts 
Development 
Classical music is only 
with a formal dress and 
dressing up in general, it 
is difficult with other 
modern emerging field 
of performing arts 
combined. 
In addition to the field of 
music, it merges the 
building, stage effects 
and so on arts, and 
integrates a variety of 
performing arts at the 
same time. 
 
AIDA provides a way to connect  the emotional expression 
between performer and audiences, and the affect is clearly and 
direct. However, due to the representation of mapping and 
responsive skin, the usability and affection of AIDA remains a 
further study. 
7. REFERENCES 
[1] Bernhardt, D. and P. Robinson. Interactive Control of 
Music Using Emotional Body Expressions. In Conference 
on Human Factors in Computing Systems. 2008. Florence, 
Italy: ACM New York, NY, USA. 
[2] Sylvain Le Groux; Paul F. M. J. Verschure. Situated 
Interactive Music System: Connecting Mind and Body 
Through Musical Interaction. In Proceedings of the 
International Computer Music Conference. 2009. 
[3] Weiser, M.. The Computer of the 21st Century. Scientific 
American. 1991. 265(3): p. 66-75. 
[4] Spuybroek, L.. NOX: Machining Architecture. Thames & 
Hudson. 2004. 
[5] Philip Beesley. Hylozoic Soil: Geotextile Installations. 
Riverside Architectural Press. 2007.
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
402