 Mechatronic Performance in Computer Music Compositions  Nathan Villicaña-Shaw  California Institute of the Arts  24700 McBean Pkwy  Valencia, California  nathanv@cca.edu 
 Ajay Kapur California Institute of the Arts 24700 McBean Pkwy Valencia, California akapur@calarts.edu 
 Spencer Salazar  California Institute of the Arts 24700 McBean Pkwy  Valencia, California  ssalazar@calarts.edu  ABSTRACT This paper introduces seven mechatronic compositions performed over three years at the California Institute of the Arts (CalArts). Each composition is discussed in regard to how it addresses the performative elements of mechatronic music concerts. The compositions are grouped into four classifications according to the types of interactions between human and robotic performers they afford: Non-Interactive, Mechatronic Instruments Played by Humans, Mechatronic Instruments Playing with Humans, and Social Interaction as Performance. The orchestration of each composition is described along with an overview of the piece’s compositional philosophy. Observations on how specific extra-musical compositional techniques can be incorporated into future mechatronic performances by human-robot performance ensembles are addressed.   Author Keywords Mechatronic performance, mechatronic compositions, social interaction as musical performance, sonic art  CCS Concepts • Applied computing~Performing arts   • Applied computing~Sound and music computing   • Hardware~Sensors and actuators 1. INTRODUCTION The use of mechatronic instruments (MIs) in live music performance is becoming increasingly common both inside and outside of the NIME community [1], [2]. The focus on designing new MIs for use in performance settings has generated a plethora of insightful research on the topic of constructing increasingly advanced hardware and software systems for MIs [3]–[6]. However, there has been considerably less research focused on the performative and compositional considerations of mechatronic instruments [7]. The compositions presented in this paper explore a variety of performance techniques which MIs can employ to foster a more stimulating concert experience.  Due to the nature of their design, musical robots are not afforded the same emotive qualities as their human counterparts. MIs are typically designed for their musical functionality and perhaps, to a lesser extent, for their aesthetic appeal. As a result, musical performance with mechatronic instrumentalists tends to feature less visual excitement than similar music played by living performers which are capable of jumping, moving across the stage area, emoting, and numerous other forms of extra-musical performance.   Seven compositions, conducted over three years at CalArts, which each exercise diversified techniques for facilitating multi-sensory mechatronic performance are introduced in sections 3-7. Section 3, Non-Interactive, covers compositions which do not require human performers and exhibit no interactivity.  
 The next section, Mechatronic Instruments Played by Humans, in contrast, discusses a performance where the mechatronic instruments are directly controlled by human performers in real time. Section 5, Humans Leading Robots in Musical Performance, presents a composition where the mechatronic performers are led by the actions of humans but are able to exhibit limited agency over the specifics of their actions. Lastly, section 6, Social Interaction as Performance, uses social interactions between humans and robots as a compositional drive. Each of the performances introduced in this paper seek to explore, in contrastive ways, the roles mechatronic entities can fulfill in sonic performance art. 2. RELATED WORK Godfried Willem Raes’ research with the Logos Foundation addresses the relationships between robot and human performers along with the sonic and theatrical aspects of mechatronic performance [8], [9]. In 1989, with the development of Autosax, Raes championed the Logos Foundation into a new era of human-robot performance. The foundation began building MIs and started writing compositions which take advantage of the extra-human capabilities of MIs as well as the liberties afforded onto human instrumentalists when they are able to interact with their instrument in unconventional manners.  Raes’ most recent research has focused on the adoption of gesture recognition with natural user interfaces (NUIs) to interface with MIs [10], [11].  Gil Weinberg and Scott Driscoll observed, in their 2006 paper “Toward Robotic Musicianship”, that most research conducted in the field of musical robotics is concerned with the sound producing mechanisms and/or the capabilities of the instruments instead of the perceptual aspects of musicianship: namely listening, analysis, improvisation, and group interactions. They defined robotic musical instruments as mechanical constructions that are played by live musicians or triggered by prerecorded sequences. This was distinguished from anthropomorphic musical robots which they defined as hominoid robots which attempt to imitate the action of human musicians. Weinberg and Driscoll developed an anthropomorphic musical robot, Haile,  which listens to the other musicians on stage and reacts to their actions intelligently. Two compositions written for Haile, Pow and Jam’aa, employed a compositional tactic of call-and-response. Both of these compositions involve Haile participating in an improvisational drum circle along with up to three other human performers. Haile imitates the rhythms played by human co-performers before showcasing her own unique mechatronic capabilities [12].  Cynthia Breazeal’s Personal Robots Group, at the MIT Media Lab, has been conducting research on topics including teamwork and expressive interactions between humans and robots from the late 90’s onward [13].  The goal of the research conducted by the Personal Robots Group, and most other research groups active in fields relating to human-robot interactions, is to create sociable robots which imitate humans [14]. To this end, Breazeal and her group has worked on developing many advanced social robots including Kismet, Cog, and Leonardo over the last three decades - pioneering the field of social robotics [15]. 
NIME Proceedings Template for LaTeX
Ben Trovato⇤
Institute for Clarity in
Documentation
1932 Wallamaloo Lane
Wallamaloo, New Zealand
trovato@corporation.com
G.K.M. Tobin†
Institute for Clarity in
Documentation
P .O. Box 1212
Dublin, Ohio 43017-6221
webmaster@marysville-
ohio.com
Lars Thørväld‡
The Thørväld Group
1 Thørväld Circle
Hekla, Iceland
larst@afﬁliation.org
Lawrence P . LeipunerBrookhaven Laboratories
Brookhaven National Lab
P .O. Box 5000
lleipuner@researchlabs.org
Sean FogartyNASA Ames Research Center
Moffett Field
California 94035
fogartys@amesres.org
Anon NymousRedacted
8600 Datapoint Drive
San Antonio, Texas 78229
cpalmer@prl.com
ABSTRACT
This paper provides a sample of a LATEX document for the
NIME conference series. It conforms, somewhat loosely, to
the formatting guidelines for ACM SIG Proceedings. It is an
alternatestyle which produces atighter-lookingpaper and
was designed in response to concerns expressed, by authors,
over page-budgets. It complements the documentAuthor’s
(Alternate) Guide to Preparing ACM SIG Proceedings Us-
ing LATEX2✏ and BibTEX. This source ﬁle has been written
with the intention of being compiled under LATEX2✏ and
BibTeX.
To make best use of this sample document, run it through
LATEX and BibTeX, and compare this source code with your
compiled PDF ﬁle. A compiled PDF version is available to
help you with the ‘look and feel.’The paper submit-
ted to the NIME conference must be stored in an
A4-sized PDF ﬁle, so North Americans should take
care not to inadvertently generate letterpaper-sized
PDF ﬁles.This paper template should prevent that from
happening if thepdflatexprogram is used to generate the
PDF ﬁle.
The abstract should preferably be between 100 and 200
words.
Author Keywords
NIME, proceedings, LATEX, template
CCS Concepts
•Applied computing! Sound and music comput-
ing; Performing arts;•Information systems! Music
retrieval;
⇤Dr. Trovato insisted his name be ﬁrst.†The secretary disavows any knowledge of this author’s ac-
tions.‡This author is the one who did all the really hard work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
Please read the comments in the nime-template.tex
ﬁle to see how to create the CCS Concept Classiﬁ-
cations!
1. INTRODUCTION
Theproceedingsare the records of a conference. ACM seeks
to give these conference by-products a uniform, high-quality
appearance. To do this, ACM has some rigid requirements
for the format of the proceedings documents: there is a
speciﬁed format (balanced double columns), a speciﬁed set
of fonts (Arial or Helvetica and Times Roman) in certain
speciﬁed sizes (for instance, 9 point for body copy).
The good news is, with only a handful of manual set-
tings,1 the LATEX document class ﬁle handles all of this for
you.
The remainder of this document is concerned with show-
ing, in the context of an “actual” document, the LATEXc o m -
mands speciﬁcally available for denoting the structure of a
proceedings paper, rather than with giving rigorous descrip-
tions or explanations of such commands.
2. THE BODYOF THE PAPER
Typically, the body of a paper is organized into a hierar-
chical structure, with numbered or unnumbered headings
for sections, subsections, sub-subsections, and even smaller
sections. The command\sectionthat precedes this para-
graph is part of such a hierarchy.2 LATEX handles the num-
bering and placement of these headings for you, when you
use the appropriate heading commands around the titles of
the headings. If you want a sub-subsection or smaller part
to be unnumbered in your output, simply append an aster-
isk to the command name. Examples of both numbered and
unnumbered headings will appear throughout the balance
of this sample document.
Because the entire article is contained in thedocument
environment, you can indicate the start of a new paragraph
with a blank line in your input ﬁle; that is why this sentence
forms a separate paragraph.
1Two of these, the \numberofauthorsand \alignau-
thor commands, you have already used; another,\bal-
ancecolumns, will be used in your very last run of LATEX
to ensure balanced column heights on the last page.2This is the second footnote. It starts a series of three
footnotes that add nothing informational, but just give an
idea of how footnotes work and look. It is a wordy one, just
so you see how a longish one plays out.
413
 Michael Gurevich, from the University of Michigan, has created semi-autonomous musical robots which allow for shared control between human and software actors. The topics which Michael addresses with his instrument Stringtrees, include semiotics, intelligent musical systems, and cybernetics - themes present in some of the performances in this paper [16]. 2.1 Machine Lab at CalArts The compositions Beatles, Hello Humans, Robots Improvisational Jam, and Robot Whispers each were composed for, and performed with, the robots residing in the Machine Lab at CalArts.  The Machine Lab is home to the mechatronic instruments that make up the Machine Orchestra and functions as the hub of the Music Technology department as well as the Digital Arts Minor at CalArts. Information about the design and capabilities of these bots can be found in [17]–[19]. 3. NON-INTERACTIVE This paper defines non-interactive compositions as compositions which involve no human performers, contain no interactions between the human composer and the robotic musical instruments, and involve no communication between individual mechatronic instruments during the performance. For these works, it is common that the composer is not on stage during the performance and/or does not introduce the work. This can have a decentralizing effect on the focus of the audience’s attention. Without a human to focus the attention on, performances of this kind can be compared to the music performances of the Orchestrions, Player Pianos, or Disklaviers of the 19th and 20th centuries [20], [21]. These mechanical marvels, along with the 21st century mechatronic ensembles which are the topic of this paper, take on a distinct quality when performing without the presence of a puppeteer, accompanying human performer, or conductor.   Although there are notable exception by composers such as Alvin Lucier and Karlheinz Stockhausen [22], these works can additionally be compared to the acousmatic tradition of the 20th century where the presentation typically focuses on the sonic qualities of the composition instead of the performative aspect. Likewise, these performances share qualities with musical traditions which obscure the musical performers such as Opera, Ballet, and Shadow Puppet theater, but often do not contain as many theatric elements. 3.1 Beatles 
 Figure 1: Beatles is composed for MalletOTon (top) and Lydia. Beatles is an eight-minute etude written for the Machine Orchestra bots MalletOTon[18] and Lydia [19] and is performed, in a concert setting, with no on-stage human presence or sound re-enforcement. Beatles is coded in ChucK [23] and was first performed in the Machine Lab at CalArts in 2014.  The piece quietly begins with many low velocity events being randomly sent to Lydia’s solenoid beaters. Each individual actuation 
is barely noticeable and only produces minute mechanical flutters - failing to approach the minimal force required to strike the piano strings with the solenoid plungers. The triggering rate, and overall velocity, of these message slowly increase over a minute. After this gradual build, the messages begin to slow down and fade out as MalletOTon, positioned on the opposite side of the performance space, begins to exhibit the same behavior. Over the next minute MalletOTon goes through the same cycle Lydia just experienced. The two instruments trade off several times, progressively getting louder and faster with each transition. In the next section, both Lydia and MalletOTon activate and begin receiving trigger messages from the server. Together the two instruments receive messages at an increased rate at random intervals. Occasionally, one of the instruments will receive a message for the same solenoid in rapid succession causing the actuator to strike the instrument; resulting in a tone. For several moments the performance maintains this condition until, gradually, the messages slow. During the last two minutes of the composition the messages eventually stop as the piece draws to a close. 3.1.1 Sub-Velocities   One of the compositional priorities of Beatles is to showcase the delicate, subtle side of the MIs it was written for. The composition refrains from loud actualizations of the instruments’ motors and solenoids. Instead of composing with sonic events which involve the robots’ actuators coming into contact with their corresponding sound producing mechanisms, Beatles is written without using velocities which provide enough force to strike the instrument on their own. Analogous to fretboard noise on a guitar, or the sounds of a flautist inhaling, the mechanical sounds produced by these “sub-velocities” can comfortably be categorized as extended-techniques and serve as the primary sonic content of the performance. 3.1.2 Randomness The composition relies on random number generators to determine both which actuator is activated and at what specific velocity the device is triggered at. While no single event on an actuator provides enough power to cause the mechanism to strike its instrument, the random nature of how the actuators are chosen sometimes results in a single component being selected in rapid succession. When this happens the combined energy created by the multiple actuations provides enough mechanical force to softly strike the instrument.   These gently articulated strikes occur more commonly on solenoids which have a tendency to over-actuate. This limits those strikes to a small handful of notes which serve as the melodic content for the composition. Before each performance of Beatles the upper and lower bounds of the random number generators are calibrated to the mechanical state of the robots. The goal of the calibration process is to adjust the parameters to produce one combined strike once every fifteen to twenty seconds (at the start of the composition). As the composition progresses the frequency of the sonic events increases as well as the mean output of the random number generators, resulting in an increased number of instrument strikes. 3.1.3 Robot Placement Another technique used by Beatles to create an interesting performance is careful placement of the MIs within the performance space. For the concert, Lydia and MalletOTon are moved to opposite sides of the Machine Lab. Additionally, the bots are placed at the same elevation as the audience. Both of these choices are made with an intent of dissolving the distinction between a stage area and an audience area – a common expectation of music concert attendees.  The quiet nature of the composition, in combination with the distant placement of the bots, required listeners who wanted to hear the entire composition to move around the space during the performance in order to follow the sounds – which they are encouraged to do. While not an interactive performance in terms of the musical or visual content generated, encouraging the audience to move around the space to discover sounds created a dynamic experience quite different from sitting down and listening for eight minutes. 3.2 Hello Humans Hello Humans is a four-minute-long mechatronic music composition written for 10 of the robots in the CalArts Machine Lab. The composition is through-composed with an ABC song structure and results in the same performance with each realization.  
414
 Following in the footsteps of Conlon Nancarrow [24], The compositional goal of Hello Humans is to create an arrangement which exploits the extra-human capabilities of its mechatronic performers. The composition is conceptually imagined to be written by the MIs as a friendly, yet competitive, salutation to the human audience consisting primarily of academically trained musicians. As a result, Hello Humans showcases intricate rhythmic and metric modulation where accurate execution is paramount. These demonstrations are an attempt to impress the human audience primarily consisting of academically trained musicians.   
 Figure 2: Bot positions in the Machine Lab for Hello Humans. 3.2.1 Robot Spacialization To create a visually and sonically engaging concert experience, Hello Humans relies on careful consideration of the spacial location of its robotic performers. This composition does not rely on a traditional concert hall configuration, with a large stage for the performers on one side of the building and the audience all facing the stage on the other side of the room, the stage configuration is decentralized. Six of the mechatronic instruments are suspended from the room’s rigging grid on one half of the room - with the bottoms of the bots resting 7-8 feet above the ground. The remaining robots are placed on the floor, dispersed throughout the rest of the room as seen in Figure 2. During concerts, chairs are placed along the walls while large rugs cover the floor providing locations for the audience to sit - although standing and moving around the space is highly encouraged.   When composing Hello Humans careful attention was taken when considering the location of each instrument within the venue. This included techniques such as allowing instruments on opposite sides of the room to trade melodic and rhythmic motifs, engage in call-and-response passages, and otherwise musically interact with one another. There spatially separated sonic events foster a livelier, more dynamic performance than if all of the bots were placed in a row and the same composition was played.  3.2.2 Self-Illumination Throughout the composition combinations of the 20 Clapper robots are triggered at very low velocity levels (MIDI velocities below 8). These sub-velocities differ from the types of messages sent in the composition Beatles (covered in section 2) as these events are silent as they do not produce enough power to cause any mechanical actualization. While these messages don’t produce any sound, they do trigger the blue LEDs positioned on each of the Clappers. By activating the Clappers individually or in groups in this manner it is possible to visually focus audience attention to the location of the bot(s). This effect is used in Hello Humans to create visual movement and to introduce specific bots at key moments of the composition.  3.3 Computer Music Computer Music is an eight hour long musical performance written for the Computer Music Ensemble consisting of eight floppy disk drives (FDDs), four CD-ROM drives, four stepper motors, and four hard disk drives (HDDs)(see Figure 3). The drives were hacked to allow for each component to be controlled by MIDI messages. The FDDs and CD-ROMs produce pitched notes while the HDDs sonically function as filtered noise generators. For the purpose of the 
installation/performance the ensemble is controlled using Ableton Live running on a MacBook Pro.   Computer Music is a piece of mechatronic phase music inspired by the works of American minimalist composers such as Phillip Glass and Steve Reich.  Each of the twelve pitch producing instruments play the same short two-bar motif at the start of the composition. However, each instrument pauses for a slightly different duration before repeating the theme. The variations in the lengths of the cadences are minuscule, fractions of a millisecond, but throughout the eight-hour composition the drives gradually fall out of sync before realigning at the performance finale.  
  Figure 3: Computer Music as installed at the 2016 Digital Arts Expo.  Adhering to a theme common in the works presented in this document, this composition attempts to create music which is idiosyncratic to robotic performers. It is a piece which would be extremely difficult for humans to accurately realize due to the orchestration and aspects of the composition itself. This includes the instrumentation, length of the piece and the precision required to execute the subtle rhythmic phasing which the performance is centered around.   As the composition is approximately eight hours long, it is presented as an installation instead of a traditional concert performance, allowing guests to come and go as they please. This passive presentations style strives to be undemanding on the audience allowing them to experience the slow-scale phase piece without listening for the full eight hours.  3.3.1 Computer Music Ensemble Table 1. Capabilities of the Computer Music Ensemble.  Number Amplification Primary Use Secondary Use Musical Content CD-ROMs 4 None Pitched stepper motors Disk treys About 2 octaves HDDs 4 None Filtered noise Percussive trey drop Noise FDDs 8 Yes Pitched stepper motors None Between 2-3 octaves  The Computer Music Ensemble was designed and built by the first author in the Spring of 2016 and consists of four CD-ROMs, four HDDs, and eight FDDs (see table 1). The FDD’s are amplified through studio monitors using guitar pickups and a powered mixer. The stepper motors raise and lower the HDD covers, which are constantly spinning, to provide a source of noise for the piece. If the HDD’s lid is closed, the drive is silent. When the lid is open the disk spinning inside can he heard as white(ish) noise. When the lid is partially open the noise is filtered. The CD-ROM drives utilize both their reading head motors, as well as their treys to participate in the music. 3.3.2 Theatric Movement The HDD components of the Computer Music Ensemble are primarily used attract gallery visitors and serve as a source of visual stimulation. The original intent of the HDDs was to provide filtered noise which could be used in tandem with the notes produced by the FDDs and CD-ROMs. After some prototyping it became apparent how visually captivating the HDDs are and they were re-integrated into the composition as visual ques instead of sonic events. 
415
 In Computer Music, each of the HDDs mirror the unique timing of its paired CD-ROM. A stepper motor opens and closes the cover to the HDD at the start of every eight repetitions of the melodic motif in its CD-ROM counterpart. The opening and closing of the drives allows visitors to identify parts of the musical structure in the underlying composition without having to listen - serving as a convenient visualization of the musical phasing occurring in the performance.  While not as visually dramatic as the HDDs, the CD-ROMs and FDDs are also presented in a way to create as much visual interest as possible. All of the drives are angled toward the viewer allowing for a natural view into the inner workings which are normally hidden from view. The enclosures are removed to allow users to see the reading heads which move with each produced note. Lastly, the CD-ROMs eject their treys with every nine repetitions of the motif providing visual drama as well as another indicator of the phase nature of the composition. 4. MECHATRONIC INSTRUMENTS PLAYED BY HUMANS  Compositions which we classify as mechatronic instruments played by humans feature mechatronic instruments which are controlled with varying granularity either triggering individual notes or entire musical phrases or sections. In these performances, the mechatronics are always directly controlled by human performers and exhibit no independence over their actions. These performances can be compared, to a certain extent, to a typical string quartet in which each human performer plays a non-mechatronic instrument. However, this analogy quickly breaks down. In this scenario, each instrument is played by a single performer and it is usually clear who is responsible for each sonic event. Performances which follow a performance pattern of mechatronic instruments played by humans, in contrast, observe some of the same performance issues faced by other electronic musicians in terms of establishing a cause and effect relationship between the human performer and the mechatronic instrument [25], [26]. 4.1 Robots Improvisational Jam The Robots Improvisational Jam is a twelve-minute, human-mechatronic, improvisational musical performance featuring eight student musicians, a variety of mechatronic instruments, digital synthesis engines, audio feedback, and traditional string instruments. The Robots Improvisational Jam was the headline act for the Composing for Robots final showcase concert and was performed in the CalArts Machine Lab in the fall of 2015.   During the performance, mechatronic instruments in the Machine Lab are controlled using MIDI interfaces, laptops, microphones, and a variety of software systems and programming languages including ChucK, Ableton Live, Reaktor, and Python. The Robots Improvisational Jam, contrary to what the name suggests, is a composition in which human musicians improvise using (mostly) mechatronic instruments. Throughout the performance, the rhythmic, harmonic, and melodic content is dictated by the actions of human performers while the robotics exhibit no agency over their actions. The goal of the composition is both to showcase the robots residing in the Machine Lab as well as the improvisational abilities of the human performers. With so many performers, and no score or conductor, the piece requires each participant to exercise considerable sonic restraint. Each instrumentalist is responsible for adding to the soundscape’s texture while maintaining an open structure which is conductive to soloing.  4.1.1 Transparency in Action Partly in an attempt to reveal to the audience what exactly the eight performers were doing, we decided to reconfigure the room to expose each performers actions. MalletOTon was placed in the center of the performance space with all of the human musicians surrounding the instrument. MalletOTon was chosen as a center piece because it was the only robot which had a solo performance that night, it’s large size, and aesthetic appeal.  With this configuration, the musicians faced the center of the room with their laptops, controllers, and instruments. The performers which played analog non-mechatronic instruments are positioned a little further back from the center of the room, giving them enough space to perform. The Audience radiates out from the 
performers and fill the room. They are able to look at the screens of the performers’ laptops as well as see the performers respond to the musical environment. This configuration makes it easier for the audience to directly watch the actions of the performers - hopefully alleviating some questions regarding sonic cause and effect [27].  To further clarify the roles of each performer, instrumentalists take at least one instrumental solo over the course of the composition. The solos created movement and excitement over the ambient sound-scape created by the ensemble. In the experience of the Authors, the inclusion of instrumental solos made it easier for the audience to determine what performer is responsible for which sonic event – a common problem with large electronic ensembles. We believe that these methods for creating transparency in the actions of the ensemble’s performers helped garner trust between the musicians and the audience - ultimately leading to an enjoyable experience for both parties.  5. MECHATRONIC INSTRUMENTALISTS PLAYING WITH HUMANS We define the category of mechatronic instrumentalists playing with humans as compositions in which reactive mechatronic systems sense input from human performers to determine, or influence, their respective course of action.  In this category, the mechatronic instruments exhibit varying degrees of agency over what notes they play and when they play them but their actions are not completely independent as their overall behavior is dictated by humans in real-time.   These performances tend to involve the use of hardware and software systems which are more complicated than those used by simple mechatronic instruments. The added complexity of these systems originate from the need to grant the mechatronic instrumentalist the ability to sense its environment, sonically or otherwise, and intelligently react to its readings. In these works, humans do not make all the decisions related to the performance, such as what notes to play, how to play them, and at what rhythmic intervals. Instead, these responsibilities have to be computed by either the mechatronic instrumentalist itself or some other electronic entity. With these compositions, the composer has to account for the behavior, and intelligence, of the mechatronic performer(s). Instead of simply creating a score for musicians to realize, the composer must additionally address the specialized (limited) nature of musical AIs. 5.1  Robot Whispers Robot Whispers was performed in the CalArts Machine Lab in the fall of 2015 for an audience of a few dozen over approximately ten minutes. Composed and performed by the first author in collaboration with fellow MFA students Eric Heep and Danny Clarke, Robot Whispers features three human performers who influence the behavior of over half a dozen of the Machine Lab’s mechatronic instruments. The composers of Robot Whispers wanted to create a system which allowed humans to sonically collaborate with mechatronic instruments in a manner which explores the compositional affordances of textual language and the psychological difference between listening to music and listening to the spoken word.  5.1.1  Unconventional Mappings Instead of directly dictating notes for each of the robots to play we decided to create a system for the robots to listen to, and attempt to sonically mimic, the frequency content of our voices. During Robot Whispers two human performers read text into a microphone. The text is computationally generated by a custom Python [28] script and serves as a textual score for the performance. The score contains information about the words that should be spoken, the tone in which they should be uttered, and the desired speaking speed.  The frequency content of the performers voices is extracted in real-time using ChucK and is mapped to actuators on instruments in the Machine Lab which produce musical events with a similar frequency spectrum. The audio picked up by the microphones is processed with reverb, delay and other effects before being amplified and sent through quad sound into the performance space. Only the wet signal is amplified and after the extensive effects processing the original audio content is unintelligible. The sonic result is an abstract call-and-response where the robot ensemble attempts to sonically recreate the humans reading of the text. The third human performer is in charge of controlling the 
416
sensitivity of each group of instruments with a MIDI controller, serving a role consisting of one part conductor and one part orchestrator. The controller adjusts the threshold for actuator groupings dictating how often, and with how much force, the mechatronic instruments will respond to the text. When the amount of energy contained in a frequency bands exceeds the threshold set by the MIDI controller, the robots will activate and begin to emulate the sounds uttered into the microphones: attempting to echo our language.  6. SOCIAL INTERACTION AS PERFORMANCE The performances in this section explore social situations, inter-personal relations, and communication as compositional drives. Instead of strictly interacting with the MIs on musical terms (rhythm, pitch, etc.), the performances outlined in this section embrace social (or antisocial) interactions between people and robots to supplement traditional techniques. This includes ritualistic feeding/eating, social anxiety, and xenophobic mechatronic communities. The performances discussed in this section fall into two sub-categories: installation and concert. AntiSocial and No Humans Allowed are installations in which MIs are placed in public locations for people to interact with. In contrast, Hedonism Bot features similar interactions but in a concert setting with a different “personality” than the installations. 6.1 The Mechatronic Personalities Each of the works in this section, involve the creation of a mechatronic personality. These personalities function as basic AIs and adopt a group of interactive possibilities, and tendencies, to which they react to their environment and other beings (robotic or human). This creates a heightened opportunity to insert narrative into the performance.  Seizing this opportunity, each of the three performances covered in this section both contain an introductory story to the performance and a narrative which drives the musical score. AntiSocial is concerned with the robot Craig, a social outcast, who is using exposure therapy to alleviate his social awkwardness. In Hedonism Bot, the mechatronic entity of the same name falls ill and asked his subjects (the human performers) to feed him sonic medicine produced by Auraglyph [29]. No Humans Allowed exhibits a xenophobic community which is fearful of the human population - attempting to defend its society over the course of the week-long installation. 6.2 AntiSocial AntiSocial is a sonic social performance which involves the mechatronic personality Craig intermingling with strangers at an informal social gathering. AntiSocial was showcased in the fall of 2016 at the MTIID Masters Show where Craig “hung-out” for about three hours with concert goers in several locations before and after a staged concert performance.   This piece is not strictly a musical performance, nor is it purely an interactive art installation, instead it is a performance of social interaction where the “personality” traits of Craig, his expressive agency, and the input from audience members results in an audio/visual performance. AntiSocial explores imbuing human personality traits, in this case shyness and social anxiety, onto a mechatronic entity and is the beginning of the work concerned with mechatronic personalities which is continued in sections 6.2 and 6.3. 6.2.1 Meet Craig Craig suffers from severe social anxiety and is poor at socializing with both human and mechanical entities.  Craig exhibits three patterns of behavior in AntiSocial: preparing for interaction, paralysis, and conversing. These states are triggered by the amount of people surrounding the robot in addition to the relative distance of each person from the robot. If Craig does not detect any strangers with his array of proximity sensors, he enters into the “social preparation” state. He attempts to build confidence for the impending social interactions which he expresses via slow, quiet, rhythmic pulsing of his lights and solenoids. As time goes on and his confidence builds the pulses gradually become faster and louder.  When Craig detects people somewhat close to him (4-8 feet), his confidence fades and he is suddenly unable to speak. Craig retreats 
into himself and is gripped by social paralysis - no longer activating any of his lights or solenoids.   If someone gets close enough that Craig is unable to ignore them (within 4 feet) he does his best to interact. Unfortunately, these encounters never go well as he fumbles for the correct words, loudness, and pose to express his thoughts. In this state Craig ineptly attempts to carry on a conversation by arhythmically triggering his solenoids (talking over others), flashing his bright LEDs (awkward body language), and never directly responding to what anyone else has to say. 6.3 No Humans Allowed 
 Figure 4: No Humans Allowed as seen in the WaveCave gallery at CalArts. No Humans Allowed (NHA) is an interactive mechatronic sonic installation where human participants are repelled by xenophobic mechatronic entities. NHA was installed in the WaveCave gallery in the Spring of 2017 for a week of intermittent performance. Just like AntiSocial, NHA is not strictly a musical performance nor is it an installation. Instead, it is a performance of social interaction, fear, and isolationism. It invites us to question not only human-robot social interaction but conjointly human-human interaction on both personal and societal levels. 6.3.1 Xenophobic Community  Parallel to the interactions seen in AntiSocial, each of the personalities in NHA have a total of three states (or moods). When no outsiders are detected in the gallery, the personalities are in their productive, happy state. They rhythmically activate their solenoids simulating a fully functioning productive state of mind. If a personality detects an outside presence in the gallery, such as a gallery viewer, its productivity steeply declines as it enters into the surveillance state: preoccupied with the outsider and unable to accomplish its daily ‘work’. If one of the personalities notices an outsider it alerts the other bots in the community which likewise enter into a state of surveillance. The third state, or mood, is activated when an outsider ventures too close to the robot. The personality turns hostile as it flashes its lights, which are primarily dormant in the other states, and slams its solenoids in a disturbingly loud, fast, and unpleasant assault. The hostile personality alerts the other personalities in the gallery to the threat causing them to enter into either a state of surveillance or hostility. 6.4  Hedonism Bot HedonismBot, composed in collaboration with Kyle McCarthy, Jake Turpin and Ivy Liu, is a social ritual that was performed in a concert setting with both mechatronic and human talents in the fall of 2016 at CalArts.   HedonsimBot begins the performance in a deactivated state and only through being fed can the bot activate fully, and lead the ritual to an end. Each of the three performers use the iPad app Auraglyph to create soundscapes from simple waveforms and effects. Once a performer is happy with the state of their synthesis engine they approach HedonismBot, kneel down in front of it, and present the iPad to the bot. The feeding temporally activates HedonsimBot’s solenoids as it briefly attempts to communicate before shutting down again. The performer stands up and returns to their position giving room for one of the others to attempt to revive HedonsimBot. For the first few feedings, HedonismBot is unable to maintain enough energy to activate for longer than a few seconds but as the performance 
417
progresses HedonismBot gets stronger and is able to play louder and longer with each meal. Eventually, HedonismBot is fed enough to come to life and the social dynamics of the performance shift. Instead of the human performers leading the pace and narrative of the ritual by feeding the robot HedonismBot takes the musical lead. The robot cycles through its actuators and produces a consistent rhythm while the human performers fade their instruments out. During the last minute of the performance, Hedonism bot guides the audience through an outro without the accompaniment of any other performers.  
 Figure 5: Ivy Liu feeding HedonismBot during a performance of Hedonism Bot. 7. CONCLUSION This paper focused on the performative aspects of mechatronic music composition. The seven compositions in this paper demonstrated varied techniques for human/mechatronic ensembles to encourage dynamic, engaging performances. Some works focused on social interaction between rudimentary AIs and humans as a source of compositional drive while other projects did not exhibit an interactive element at all. While varied in their interactivity, aesthetic, and orchestration, each of these compositions explored the performative affordances enabled when composing performance for mechatronic instruments and instrumentalists in their own way.  8. REFERENCES [1] Jim Murphy, Ajay Kapur, and Dale Carnegie, “Musical Robotics in a Loudspeaker World: Developments in Alternative Approaches to Localization and Spatialization,” Leonardo Music J., vol. 22, pp. 41–48, 2012. [2] Tarek M. Sobh and Bei Wang, “Experimental Robot Musicians,” J. Intell. Robot. Syst., pp. 197–212, 2003. [3] A. Eigenfeldt and A. Kapur, “An Agent-based System for Robotic Musical Performance.,” in NIME, 2008, pp. 144–149. [4] J. Murphy, “Expressive musical robots: building, evaluating, and interfacing with an ensemble of mechatronic instruments,” Ph.D. dissertation, Dept. Comp. Science, Victoria University of Wellington, Wellington, New Zealand, 2014. [5] J. Long, J. W. Murphy, A. Kapur, and D. Carnegie, “A methodology for evaluating robotic striking mechanisms for musical contexts,” in International Conference on New Interfaces for Musical Expression, 2015. [6] T. Kaneda, S. Fujisawa, T. Yoshida, Y. Yoshitani, T. Nishi, and K. Hiroguchi, “Subject of making music performance robots and their ensemble,” 1999, vol. 2, p. 12B4/1-12B4/6. [7] J. Murphy, D. A. Carnegie, and A. Kapur, “Using Expressive Musical Robots: Working with An Ensemble of New Mechatronic Instruments,” in Proceedings of the 2016 International Symposium on Electronic Art, 2016. [8] L. Maes, G.-W. Raes, and T. Rogers, “The Man and Machine Robot Orchestra at Logos,” Comput. Music J., vol. 35, no. 4, pp. 28–48, 2011. [9] G.-W. Raes, “Expression control in automated musical instruments,” in a paper presented at the, 2015. [10] G.-W. Raes, Gesture controlled virtual musical instruments. Ghent, 1999. [11] G.-W. Raes, Namuda Studies. Ghent, 2012. [12] G. Weinberg and S. Driscoll, “Toward Robotic Musicianship,” Comput. Music J., vol. 30, no. 4, pp. 28–45, 2006. 
[13] C. L. Breazeal, Designing sociable robots. Cambridge, Mass: MIT Press, 2002. [14] C. Breazeal and B. Scassellati, “Robots that imitate humans,” Trends Cogn. Sci., vol. 6, no. 11, pp. 481–487, Nov. 2002. [15] C. Breazeal, “Social Robots: From Research to Commercialization,” 2017, pp. 1–1. [16] M. Gurevich, “Distributed Control in a Mechatronic Musical Instrument.,” in NIME, 2014, pp. 487–490. [17] A. Kapur et al., “The Machine Orchestra: An Ensemble of Human Laptop Performers and Robotic Musical Instruments,” Comput. Music J., vol. 35, no. 4, pp. 49–63, Dec. 2011. [18] A. Kapur, J. Murphy, M. Darling, E. Heep, B. Lott, and N. Morris, “MalletOTon and the Modulets: Modular and Extensible Musical Robots,” New Interfaces Music. Expr., pp. 69–72, 2016. [19] Nathan Villicaña-Shaw, Spencer Salazar, and Ajay Kapur, “The Machine Lab: A Modern Classroom to Teach Mechatronic Music,” Int. Comput. Music Conf., 2017. [20] A. Kapur, “A History of Robotic Musical Instruments.,” presented at the International Computer Music Conference, 2005, vol. 31. [21] Victor J. Meyer, “The Orchestion Collection of Franklin Corya,” Ball State University, Muncie, Indiana, 1997. [22] C. Burns, “Realizing Lucier and Stockhausen: Case Studies in the Performance Practice of Electroacoustic Music,” J. New Music Res., vol. 31, no. 1, pp. 59–68, Mar. 2002. [23] G. Wang, P. R. Cook, and S. Salazar, “Chuck: A Strongly Timed Computer Music Language,” Comput. Music J., 2016. [24] K. Gann, The music of Conlon Nancarrow. Cambridge: Cambridge University Press, 2006. [25] S. Smallwood, D. Trueman, P. R. Cook, and G. Wang, “Composing for laptop orchestra,” Comput. Music J., vol. 32, no. 1, pp. 9–25, 2008. [26] C. Stuart, “The Object of Performance: Aural Performativity in Contemporary Laptop Music,” Contemp. Music Rev., vol. 22, no. 4, pp. 59–65, Dec. 2003. [27] M. Ciciliani, “Towards an Aesthetic of Electronic-Music Performance Practice,” in ICMC, 2014. [28] J. Python, “Python (programming language),” Python Program. Lang. 1 CPython 13 Python Softw. Found. 15, p. 1, 2009. [29] S. Salazar and G. Wang, “Handwriting Input for Computer-based Music Composition and Design,” New Interfaces Music. Expr., 2014. 
418