DeviceCycle: rapid and reusable prototyping of gestural
interfaces, applied to audio browsing by similarity
Christian Frisson, Benoˆıt Macq
Universit´e catholique de Louvain, TELE Lab
Place du Levant, 2
B-1348 Louvain-la-Neuve
forename.surname@uclouvain.be
St´ephane Dupont, Xavier Siebert,
Damien Tardieu, Thierry Dutoit
Universit´e de Mons, TCTS and MathRo Labs
Boulevard Dolez, 31
B-7000 Mons
forename.surname@umons.ac.be
ABSTRACT
This paper presents the development of rapid and reusable
gestural interface prototypes for navigation by similarity in
an audio database and for sound manipulation, using the
AudioCycle application. For this purpose, we propose and
follow guidelines for rapid prototyping that we apply using
the PureData visual programming environment. We have
mainly developed three prototypes of manual control: one
combining a 3D mouse and a jog wheel, a second featur-
ing a force-feedback 3D mouse, and a third taking advan-
tage of the multitouch trackpad. We discuss beneﬁts and
shortcomings we experienced while prototyping using this
approach.
Keywords
Human-computer interaction, gestural interfaces, rapid pro-
totyping, browsing by similarity, audio database
1. INTRODUCTION
Human-computer interaction gathers multiple, interdisci-
plinary ﬁelds of expertise: interaction design, software de-
velopment, cognition, ergonomics, and so on... There is
a trend among researchers in engineering sciences to put
forward new technologies before ﬁnding applications for it,
while designers would rather ﬁrst understand user require-
ments before tailoring interfaces dedicated to their needs.
In practise, it is convenient to use both methods side by
side.
1.1 Our Approach
We started conducting research from an engineering point
of view: trying to improve methods and algorithms for, in
our case, hypermedia navigation or browsing by similarity
in multimedia databases. This is an emerging ﬁeld cover-
ing several application domains and use cases. Browsing
in audio database by similarity is not yet a common prac-
tise, it aims at solving current limitations in sound search,
retrieval and discovery; used in domains such as sound de-
sign, soundtrack composition, DJ/VJ’ing, electroacoustic
music composition, personal audio library listening, among
others. The knowledge and experience regarding interactive
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME2010, Sydney, Australia
Copyright 2010, Copyright remains with the author(s).
systems dedicated to these tasks vary amongst practition-
ers and disciplines. Addressing their various needs thus
requires a disciplined methodology.
1.1.1 General user-centered method, parallelized us-
ability tasks
We extended our initial approach by borrowing a method-
ology from the ﬁeld of usability. The recent book by Bernsen
and Dybkjaer [3] addresses this topic to a great extent. The
knowledge acquired by gathering literature regarding spe-
ciﬁc use cases is highly increased by undertaking contextual
inquiries, which consist in interviewing experts, possibly
with questionnaires, so as to understand their habits and
needs regarding their practises. Brainstorming with these
experts to produce paper mockups of the desired user inter-
faces is cost-eﬀective, especially when it prevents the unnec-
essary development of would-be inadequate prototypes. We
have started investigating these axes. After collecting this
information, several cycles of prototypes of user interfaces
need to be designed, built, tested with users, and reﬁned.
The time-consuming task of software and hardware devel-
opment should be run in parallel with these usability tasks
so as to ensure on-time delivery, and to ease the assembly
of prototypes.
1.1.2 Focus on rapid prototyping
In this paper, we will focus on one single task pertain-
ing to this approach: the rapid prototyping of software and
hardware interfaces, following the design phase, but before
running mandatory usability tests for validation of the pro-
totypes.
Frameworks or toolboxes for rapid prototyping of multi-
modal applications should meet the following requirements:
• rapid: quicker to develop than ﬁnalized products;
• modular: proposing a component-based software and
hardware architecture;
• realtime: seamless human-computer interaction with
no perceivable latency;
• reusable: modules or components developed should ﬁt
well into new projects with little adaptation;
• understandable: visual representations of the under-
lying pipelines or mappings should be obvious;
• ﬂexible: using libraries released under open licenses;
• generic: should work and ﬁt well with most platforms
and peripherals;
• sustainable: leaning towards the choice of environmental-
friendly hardware.
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
473
Several frameworks, toolboxes and workbenches for rapid
prototyping of multimodal interfaces have been proposed
in the past, among which the following are still available
and maintained: Amico1, HephaisTK2, OpenInterface3[10],
Squidy Lib4. Some of these are surveyed in [6]. However,
we opted for dataﬂow environments, initially pushed for-
ward by the signal processing communities, because simple
interfacing is oﬀered (audio and video input/output, gestu-
ral controllers and sensors interfacing, network communi-
cation) and because we had previously acquired knowledge
and training in using these.
For the prototyping of user interfaces, we chose to use
PureData5, as it is an opensource (free), cross-platform,
modular environment for visual programming. One conve-
nient feature of PureData for interaction designers is the
ability to edit mappings at runtime, on-the-ﬂy, without re-
compiling. Jamoma6 similarly simpliﬁes the prototyping of
dataﬂow applications, but inside the commercial Max/MSP
environment, and less dedicated to multimodal interaction.
Lawson’s paper [10] compares the OpenInterface multimodal
workbench with the PureData dataﬂow environment.
1.2 Our context of application: browsing au-
dio loops by similarity and manipulation
1.2.1 AudioCycle, our test application
The AudioCycle application had already been presented
[13, 7]: it oﬀers an interactive visualization of a database
of audio loops, with distances estimated from extracted
signal-based features. A two-dimensional visualization has
already been proposed, as illustrated in ﬁgure1. This ap-
plication has since then been extended to the MediaCycle
framework, supporting other types of media such as im-
ages and videos. The framework relies on cross-platform
opensource libraries, for instance for the AudioCycle ap-
plication: OpenAL for the audio feedback, OSG for the
OpenGL scene-based view, and Qt for the GUI.
Figure 1: Screenshot of our test prototype for audio
browsing by similarity: AudioCycle.
1.2.2 Other applications featuring dedicated gestu-
ral interfaces
1http://amico.sourceforge.net
2http://sourceforge.net/projects/hephaistk/
3http://www.openinterface.org
4http://www.squidy-lib.de
5http://www.puredata.info
6http://www.jamoma.org
A consistent survey of emerging applications for brows-
ing audio databases by similarity, notably Musicream, Mu-
sicRainbow, SmartMusicKiosk and SoundSpotter, is avail-
able in [4]. The type of media in the audio database varies
amongst these applications: samples, loops, and music li-
braries. As we are focusing on gestural control, we note that
among these, MusicRainbow allows one to dial music genres
for browsing a song database intuitively using a rotary con-
troller. Alternatively, applications such as SongExplorer [9]
have been proposed for interacting with sound content, for
instance a music collection, using multitouch tables and/or
“tangible” objects. In SoundTorch [8], the user can browse
through a song library using a Wii remote controller.
Other inspiring dedicated controllers have been associ-
ated to audio manipulation applications, extending beyond
browsing by similarity. Villar et al designed a low-cost sys-
tem for DJ’s using repurposed hard-drives [14]. Beamish et
al [1] proposed force-feedback versions of the DJ elementary
controllers: turntables and crossfaders. Chu [5] designed a
force-feedback rotative knob for sound editing purposes.
1.2.3 Restriction to manual controllers
Expert users that would use a similarity-based approach
to browse a collection of sounds for professional purposes re-
quire small movements considering their day-long sessions.
We thus chose to address only manual controllers that can
sit on top of desks or tables, requiring less bodily movements
or aerial gestures. Relatedly, Dan Saﬀer opposes “touch”
to “free-form” modalities in his taxonomy of gestural input
modalities [11].
2. METHOD AND ACHIEVEMENTS
2.1 OpenSoundControl support
We have added OpenSoundControl (OSC) support to the
MediaCycle framework with a dedicated namespace, so as
to allow the external control of MediaCycle applications by
more modalities, particularly gestural input.
We have identiﬁed two modes of interaction with Audio-
Cycle: navigation (such as moving, hovering, zooming the
database view) and manipulation (notably the playback of
sound, with optional pitch-independent time stretching or
beat-matching). For prospective collaborative uses of the
application, using multiple control agents, we have chosen
to specify numbers identifying the user and possible agents
tied to the modes of interaction. For example, a single user
using one browser to move the 2D view space may send:
/audiocycle/1/browser/1/move<x><y>.
2.2 Discovering off-the-shelf devices
Before creating new dedicated gestural interfaces, we wan-
ted to explore the limitations of oﬀ-the-shelf devices. We
have investigated three categories of devices:
1. USB Human Interface Devices (HID), particularly Con-
tour Design Shuttle jog wheels and 3dconnexion Space
Navigator 3D mice, using the[hidio] object by Steiner
et al [12], oﬀering several improvements over [hid]
(notably hotplugging devices);
2. force-feedback devices, notably the Novint Falcon 3DOF
force-feedback mouse, operated with the[np
nifalcon]
object using a reverse-engineering driver library 7, as-
sorted with the HSP set of abstractions by Berdahl et
al [2] facilitating the starting up with basic physical
eﬀects;
7http://sourceforge.net/projects/libnifalcon
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
474
3. multitouch trackpads, using the [fingerpinger] ob-
ject that we ported from its initial Max/MSP imple-
mentation8 to PureData, allowing to access informa-
tion of blobs detected from ﬁngers hovering the Apple
Macbook Multitouch Trackpad, only available on Ap-
ple OSX currently.
For instant reusability of the controllers and associated
objects, we created a set of abstractions following the same
scheme for each (for instance a toggle connected at the left
input of each enables to turn on or oﬀ the probing of the
device), we ensured that the layout of the data visualization
widgets on the patch remains coarsely equivalent regarding
how related controls are laid out on the devices, as illus-
trated in ﬁgure 2.
Figure 2: Abstraction for rapid interfacing of a Con-
tour Design Shuttle Pro2 jog wheel.
For most USB HID devices, we had to uninstall the oﬃcial
drivers that circumvent implementation issues by convert-
ing raw events into mouse/keyboard events.
2.3 Rapid prototyping: producing mappings
We opted for three test prototypes:
1. bimanual control featuring a 3D mouse for the navi-
gation in the audio database and a jog wheel for the
manipulation of audio loops, is illustrated in ﬁgure 3;
2. a force-feedback version of the navigation, replacing
the 3D mouse by a 3DOF force-feedback mouse;
3. a keyboard and multitouch trackpad combination
Figure 4 illustrates that mappings can be easily edited.
Basic control widgets, such as toggles and sliders, are pro-
vided with the default PureData installation, and allow one
to visualize values and test mappings oﬄine. Objects and
abstractions from the PureData extended release, such as
8http://www.anyma.ch/2009/research/
multitouch-external-for-maxmsp/
Figure 3: Bimanual audio browsing (left hand, 3D
mouse) and manipulation (right hand, jog wheel) in
action.
[scale] from the maxlib library, increase the speed of de-
velopment and help reduce the visual clutter in the patch.
For less straightforward event management, the [expr] ob-
ject helps to deﬁne conditional sequences. OSC objects help
to easily send ﬁltered and conditioned events to the Au-
dioCycle application, and to receive information for force-
feedback.
3. DISCUSSION
3.1 Prototypes diversity
The ﬁrst prototype reduces the level of movements down
to wrists-only. As each device is assigned to only one in-
teraction mode (navigation vs manipulation), the devices
can be placed on a desk or table at convenient locations
regarding hands so as to prevent long-term injuries.
Regarding the second prototype, even if usability settings
are provided with the device, a suitable position is hard to
ﬁnd due to the size of the device, especially if coupled with
another device. One strength of this prototype is the added
value of force-feedback; we believe that it might increase
the speed of ﬁnding a given target sound. For instance, as
represented in ﬁgure 4, the friction activated when passing
through loops might decrease the speed of reaching a given
target, but increase the accuracy of positioning. A gravita-
tional system analogy of attracting the force-feeback pointer
towards barycenters of clusters can also be considered. Such
systems can also beneﬁt to users with disabilities, such as
blind people.
For multi-purposed applications, the mouse and keyboard
combination should remain usable as it is still the standard
setup. This was the ﬁrst supported user interface of the
AudioCycle application. The third prototype augments this
combination with a multitouch trackpad. This prototype
can alternatively be used for prototyping small form factor
multitouch applications.
3.2 Visual versus script-based prototyping
Some simple event mapping transformation were less straight-
forward than expected, for instance changing rotation from
absolute to relative. This could have been more easily ac-
complished with script-based programming, which is more
eﬃcient for conditional and procedural statements, data
recording at given time stamps (storing in a variable), and
large datasets using dedicated types. OpenInterface ad-
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
475
Figure 4: Rapid mapping example using a Novint
Falcon force-feedback 3D mouse: samples are hov-
ered with friction using the X and Y axes, zoom by
the Z axis forced to its initial position.
dresses this issue by allowing the execution of Matlab scripts.
Dataﬂow patches remain eﬃcient for giving an overall sum-
mary of the executed pipeline.
3.3 Initial requirements assessment
Most of our initial requirements for rapid prototyping are
met, with the following exceptions:
• visual programming prevents easy reusability between
the prototype and the ﬁnal product, as visual code
needs to be converted into textual code;
• the GPL licence of PureData prevents from using it
in commercial projects.
4. CONCLUSIONS AND PERSPECTIVES
We have set up a working environment for rapid and
reusable prototyping of gestural interfaces. The next step
consists in deﬁning an usability protocol so as to determine
which gestural interfaces provide the best results in a given
application scenario with speciﬁc users and application do-
mains. Since we have tested oﬀ-the-shelf components, we
will try to repurpose some of these devices as in [14], for
instance for the creation of a force-feedback jog wheel.
5. ACKNOWLEDGMENTS
All authors are supported by numediart9, a long-term
research program centered on Digital Media Arts, funded
by R´ egion Wallonne, Belgium (grant N◦716631).
We wish to express our gratitude to our collaborators
that contributed to the development of AudioCycle: Nicolas
d’Alessandro, Thomas Dubuisson, Rapha¨ el Sebbe, J´ erˆ ome
Urbain.
9http://www.numediart.org
6. REFERENCES
[1] T. Beamish, K. van den Doel, K. MacLean, and
S. Fels. D’groove: A haptic turntable for digital audio
control. InProceedings of the 2003 International
Conference on Auditory Display (ICAD), 2003.
[2] E. Berdahl, G. Niemeyer, and J. O. Smith III. HSP:
A simple and eﬀective open-source platform for
implementing haptic musical instruments. In
Proceedings of NIME, 2009.
[3] N. O. Bernsen and L. Dybkjaer. Multimodal Usability.
Human-Computer Interaction Series. Springer, 2009.
[4] M. A. Casey, R. Veltkamp, M. Goto, M. Leman,
C. Rhodes, and M. Slaney. Content-based music
information retrieval: Current directions and future
challenges. InProceedings of the IEEE, volume 96, 4
2008.
[5] L. L. Chu. Using haptics for digital audio navigation.
In Proceedings of the International Computer Music
Conference (ICMC), 2002.
[6] B. Dumas, D. Lalanne, and S. Oviatt. Human
Machine Interaction: Research Results of the MMI
Program, chapter Multimodal Interfaces: A Survey of
Principles, Models and Frameworks, pages 3–26.
Springer-Verlag, Berlin, Heidelberg, 2009.
[7] S. Dupont, T. Dubuisson, J. Urbain, C. Frisson,
R. Sebbe, and N. d’Alessandro. Audiocycle: Browsing
musical loop libraries. In7th International Workshop
on Content-Based Multimedia Indexing (CBMI),
pages 73–80, Chania, Crete, June 3-5 2009.
[8] S. Heise, M. Hlatky, and J. Loviscach. Soundtorch:
Quick browsing in large audio collections. In 125th
Audio Engineering Society Convention, number 7544,
2008.
[9] C. F. Juli` a and S. Jord` a. Songexplorer: A tabletop
application for exploring large collections of songs. In
10th International Society for Music Information
Retrieval Conference (ISMIR 2009), 2009.
[10] J.-Y. L. Lawson, A.-A. Al-Akkad, J. Vanderdonckt,
and B. Macq. An open source workbench for
prototyping multimodal interactions based on
oﬀ-the-shelf heterogeneous components. In
Proceedings of the 1st ACM SIGCHI symposium on
Engineering interactive computing systems (EICS’09),
2009.
[11] D. Saﬀer. Designing Gestural Interfaces. O’Reilly
Media, Inc., 2009.
[12] H.-C. Steiner, D. Merrill, and O. Matthes. A uniﬁed
toolkit for accessing human interface devices in pure
data and max/msp. InProceedings of the 2007
Conference on New Interfaces for Musical Expression
(NIME07), 2007.
[13] J. Urbain, T. Dubuisson, S. Dupont, C. Frisson,
R. Sebbe, and N. d’Alessandro. Audiocycle: A
similarity-based visualization of musical libraries. In
IEEE International Conference on Multimedia and
Expo (ICME), pages 1847–1848, Cancun, Mexico,
June 28 - July 3 2009.
[14] N. Villar, M. Jervis, A. Lang, and H. Gellersen. The
colordex dj system: A novel interface for music
mixing. In Proc. New Interfaces for Musical
Expression (NIME’07), 2007.
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
476