Disembodied and Collaborative Musical Interaction in the
Multimodal Brain Orchestra
Sylvain Le Groux
Laboratory for Synthetic
Perceptive Emotive and
Cognitive Systems uSPECSv
Universitat Pompeu Fabra
Barcelonaz Spain
sylvain.legroux@upf.edu
Jonatas Manzolli
Interdisciplinary Nucleus of
Sound Studies uNICSv
UNICAMP
Campinasz Brazil
jonatas@nics.unicamp.br
Paul F .M.J Verschure
∗
Institucio Catalana de
Recerca i Estudis Avancats
uICREAv and SPECS
Barcelonaz Spain
paul.verschure@upf.edu
ABSTRACT
sost new digital musical interfaces have evolved upon the
intuitive idea that there is a causality between sonic output
and physical actionsQ teverthelessO the advent of brainP
computer interfaces KhioL now allows us to directly access
subjective mental states and express these in the physical
world without bodily actionsQ on the context of an interacP
tive and collaborative live performanceO we propose to exP
ploit novel brainPcomputer technologies to achieve unmediP
ated brain control over music generation and expressionQ
¨e introduce a general framework for the generationO synP
chronization and modulation of musical material from brain
signal and describe its use in the realization of XmotionO a
multimodal performance for a “brain quartet”Q
Keywords
hrainPcomputer onterfaceO hiosignalsO onteractive susic yysP
temO iollaborative susical verformance
3y INTRODUCTION
zhe sultimodal hrain urchestra KshuL demonstrates
interactiveO aﬀectPbased and selfPgenerated musical content
based on novel hio technologyQ ot is an exploration of
the musical creative potential of a collection of unmediated
brains directly interfaced to the worldO bypassing their bodP
iesQ
une of the very ﬁrst piece to use brainwave for generating
music was “susic for solo performer” composed by glvin
rucier in T“YX [U[]Q ne used brainwaves as a generative
source for the whole pieceQ on this pieceO the electroenP
cephalogram KkkmL signal from the performer was ampliP
ﬁed and relayed to a set of loudspeakers coupled with perP
cussion instrumentsQ yome years laterO the composer javid
xosenboom started to use biofeedback devices Kespecially
kkmL to allow performers to create sounds and music usP
ing their own brainwaves [UX]Q sore recent research has atP
tempted to create complex musical interaction between parP
∗cQfQ yection Y ”gdditional guthors” for the list of all adP
ditional authors
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME2010, Sydney, Australia
Copyright 2010, Copyright remains with the author(s).
ticular brainwaves and corresponding sound events where
the listener kkm control a music generator imitating the
style of a previously listened sample [US]Q jata soniﬁcation
in general and kkm soniﬁcation in particular has been the
subject of various studies [TV] showing the ability of the huP
man auditory system to deal with and understand highly
complex sonic representation of dataQ
glthough there has been an renewed interest in brainP
based music over the recent yearsO most projects are only
based on direct mappings from the kkm spectral content
to sound generatorsQ zhey do not rely on explicit volitional
controlQ zhe sultimodal hrain urchestra KshuL takes
a diﬀerent approach by integrating advanced hio Khrain
iomputer onterfaceL technology that allows the performer
complete volitional control over the command signals that
are generatedQ shu preserves the level of control of the
instrumentalist by relying on classiﬁcation of speciﬁc stimP
ulus triggered events in the kkmQ gnother unique aspect of
the shu is that it allows for a multimodal and collaboraP
tive performance involving four brain orchestra membersO a
musical conductor and realPtime visualizationQ
4y SYSTEM ARCHITECTURE
4y3 Overview( A clientwserver Architecture
for Multimodal Interaction
zhe interactive music system of the sultimodal hrain
urchestra is based on a clientPserver modular architectureO
where interPmodule communication follows the upen yound
iontrol KuyiL protocol [VS]Q zhe shu consists of three
main components Kligure TL namely the orchestra membersO
the multimodal interactive systemO and the conductorQ TL
zhe four members of the “brain quartet” are wired up to
two diﬀerent types of brainPcomputer interfaces] the vVSS
and the yy—kv KyteadyPytate —isual kvoked votentialsLKcf
section UQULQ UL zhe computerPbased interactive multimedia
system processes inputs from the conductor and the hios
to generate music and visualization in realPtimeQ zhis is
the core of the system where most of the interaction design
choices are madeQ zhe interactive multimedia component
can itself be decomposed into three subsystems] the kkm
signal processing moduleO the yisy Kyituated onteractive
susic yystemL music server [TZ] and the realPtime visualP
izerQ linallyO the conductor uses a ¨iiPhaton Kcf section
UQXL to modulate the tempo of the interactive music genP
erationO trigger diﬀerent sections of the pieceO and cue the
orchestra members Kligure TLQ
4y4 Brain Computer Interface
zhe musicians of the orchestra are all connected to brainP
computer interfaces that allow them to control sound events
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
309
EEG Signal Processing
Visualization 
Wii Baton
OSC
MIDI
OSC
SSVEPP 300
SiMS Music Server
virtual strings
Conductor
sampler
Figure pz The Multimodal Brain Orchestra is a
modular interactive system based on a clientlserver
architecture using the OSC communication protol
colm See text for further informationm
and music expressiveness during the performanceQ zhese
hios provide a new communication channel between a brain
and a computerQ zhese interfaces are based on the princiP
ple that mental activity can lead to observable changes of
electrophysiological signals in the brainQ zhese signals can
be measuredO processedO and later transformed into useful
high level messages or commands [U“O “O TU]Q
zhe shu is based on two diﬀerent nonPinvasive hio conP
cepts which control the generation and modulation of music
and soundscapesO namely the vVSS and yy—kvQ ¨e worked
with mQtec medical engineering mmbn productsO providing
hio hardware devices KgQ–yhampL and corresponding realP
time processing software for sgzrghRyimulink1Q zhe
control commands generated by the classiﬁcation of the
kkm using theO so calledO vVSS and yy—kv protocols were
sent to the music server and visualization module via a
simulink yPfunction implementing using the uyi protocol
for satlab2Q
2.2.1 The P300 Speller
zhe vVSS is an event related potential KkxvL that can
be measured with eight electrodes at a latency of approxiP
mately VSSms after an infrequent stimuli occursQ ¨e used
the vVSS speller paradigm introduced by [[]Q on our caseO
two orchestra members were using a Y by Y symbol matrix
containing alphaPnumeric characters Kligure VL in which a
rowO column or single cell was randomly ﬂashed onQ zhe
orchestra member has to focus on the cell containing the
symbol to be communicated and to mentally count every
time the cell ﬂashes Kthis is to distinguish between common
and rare stimuliLQ zhis elicits an attention dependent posP
1http]RRwwwQmathworksQcom
2http]RRandyQschmederQnetRsoftwareR
itive deﬂection of the kkm about USS msec after stimulus
onsetO the vVSSO that can be associated to the speciﬁc symP
bol by the system Kligure UL [TU]Q ¨e used this interface
to trigger discrete sound events in realPtimeQ hecause it
is diﬃcult to control the exact time of occurrence of vVSS
signalsO our music server yisy Kcf section UQWL took care of
beatPsynchronizing the diﬀerent vVSS events with the rest
of the compositionQ
g vVSS interface is normally trained with XPWS characters
which corresponds to a training time of about XPWX minutesQ
g group study with TSS people showed that after a training
with X characters onlyO ZU H of the users could spell a X
character word without any mistake [TU]Q zhis motivated
the decision to limit the number of symbols used during the
performance Kyection VQWLQ
Figure rz Psooz a rare event triggers an ERP soo
ms after the onset of the event indicated by the
green arrowm
Figure sz A v by v symbol matrix is presented to the
Psoo orchestra member who can potentially trigger
sv speciﬁc discrete sound eventsm
2.2.2 SSVEP
gnother type of interface was provided by steadyPstate
visually evoked potentials Kyy—kvL triggered by ﬂickering
lightQ zhis method relies on the fact that when the retina is
excited by a ﬂickering light with a frequency ¿ VQX nzO the
brain generates activity at the same frequency [TO U]Q zhe
interface is composed of four diﬀerent light sources ﬂickerP
ing at diﬀerent frequencies and provides additional “step
controllers” Kligure WLQ
zhe yy—kv hio interface is trained for about X minutes
during which the user has to look several times at every
ﬂickering rkjQ zhenO a user speciﬁc classiﬁer is calculated
that allows onPline controlQ on contrast to the vVSSO the
yy—kv hio gives a continuous control signal that switches
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
310
from one state to another within about UPV secondsQ zhe
yy—kv hio solves also the zeroPclass problemQ of the user is
not looking at one of the rkjs then no decision is made[UW]Q
yy—kv was used to control changes in articulation and
dynamics of the music generated by yisyQ
Figure tz Two members of the orchestra connected
to their SSVEPlbased BCI interfaces [rt]
4y5 Visual Feedback
¨e designed a module that gave realPtime visualization of
the hio outputQ sore preciselyO the diﬀerent possible conP
trol messages detected by gQtec analysis software from the
brain signal were sent to the visualizer via uyi and illusP
trated with simple color coded iconsQ lrom the two memP
bers of the orchestra using the vVSS hio interface we can
receive VY distinct control messagesQ kach of these VY symP
bols was represented using a combination of six geometrical
shapes and six diﬀerent colorsQ zhe two members of the
orchestra using the yy—kv hio interfaces were able to trigP
ger four possible events corresponding to the four diﬀerent
states Kor in other wordsO four brain activity frequenciesLO
but continuouslyQ kach line in the display corresponded to
a member of the orchestra] the ﬁrst two using vVSS and the
last two yyk—vQ ¨hen a vVSS member triggered an eventO
the associated geometrical shape appeared in the left side
and moved from left to right according to timeQ lor the
yy—kv eventsO the current state was shown in green and
the past changes could be seen as they moved from left to
rightQ
zhe realPtime visualization played the role of a real time
scoreQ ot provided feedback to the audience and was funP
damental for the conductor to know when the requested
events were actually triggeredQ zhe conductor could inP
dicate to the orchestra member when to trigger an event
KvVSS or yy—kvL but the conﬁrmation of its triggering was
indicated by the real time visual score as well as by its muP
sical consequencesQ
4y6 The Situated Interactive Music System rSiMSs
unce the signal is extracted from brain activity and transP
formed into highPlevel commands by gQtec software suiteO a
speciﬁc uyi message is sent to the yisy music server [TZ]
and to the visualization moduleQ yy—kv and vVSS interP
faces provide us with a set of discrete commands we want to
transform into musical parameters driving the yisy serverQ
yisy is an interactive music system inspired by xoboserO
a midiPbased composition system that has previously been
applied to the soniﬁcation of robots and people’s trajecP
tories [ZO T[]Q yisy is entirely based on a networked arP
chitectureQ ot implements various algorithmic composition
tools KeQg] generation of tonalO brownian and serial series of
pitches and rhythmsL and a set of synthesis techniques valP
Figure uz The realltime visualizer allows for reall
time visualization of Psoo system output ethe two
upper rows show combinations of shapes and colorsf
and SSVEP system output ethe two lower rowsf
idated by psychoacoustical tests [TZO TX]Q onspired by preP
vious works on musical performance modeling [TS]O yisy
allows to modulate the expressiveness of music generation
by varying parameters such as phrasingO articulation and
performance noise[TZ]Q
yisy is implemented as a set of saxRsyv abstractions
and iNN externals [VT]Q ¨e have tested yisy within difP
ferent sensing environments such as biosignals KheartPrateO
electroencephalogramL [TYO TZO TX]O or virtual and mixedP
reality sensors KcameraO gazersO lasersO pressure sensitive
ﬂoorsO QQQL [V]Q gfter constantly reﬁning its design and funcP
tionalities to adapt to those diﬀerent contexts of useO we
opted for an architecture consisting of a hierarchy of perP
ceptually and musically meaningful agents interacting and
communicating via the uyi protocol [VS] Kligure YLQ lor
this project we focused on interfacing hio to yisyQ
yisy follows a biomimetic architecture that is multiPlevel
and loosly distinguishes sensing KeQg electrodes attached to
the scalp using a capL from processing Kmusical mappings
and processesL and actions Kchanges of musical parametersLQ
ot has to be emphasized though that we do not believe that
these stages are discrete modulesQ xatherO they will share
biPdirectional interactions both internal to the architecture
as through the environment itselfQ on this respect it is a
further advance from the traditional separation of sensingO
processing and response paradigm[UY] which was at the core
of traditional go modelsQ
4y7 Wiiwmote Conductor Baton
¨e provided the orchestra conductor with additional conP
trol over the musical output using the ¨iiPmote KtintendoL
as a batonQ jiﬀerent sections of the quartet could be trigP
gered by pressing a speciﬁc buttonO and the gestures of the
conductor were recorded and analyzedQ g processing modP
ule in yisy Kligure ZL ﬁltered the accelerometers and the
timePvarying accelerations were interpreted in terms of beat
pulse and mapped to small tempo modulation in the yisy
playerQ
5y XMOTION( A BRAINwBASED MUSICAL
PERFORMANCE
5y3 Emotionv Cognition and Musical Compow
sition
une of the original motivations of the shu project was
to explore the potential creativity of hios as they allow
to access subjective mental states and express these in the
physical world without bodily actionsQ zhe name Xsotion
designate those states that can be generated and experiP
enced by the unmediated brain when it is both immersed
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
311
Rhythm Generator
Pitch Classes 
Generator
Register Generator
Dynamics Generator
Articulation Generator
Monophonic Voice
Rhythm Generator
Chord Generator
Register Generator
Dynamics Generator
Articulation 
Generator
Polyphonic Voice
Tempo
Panning
Channel
Instrument
Modulation
Bend
Midi Synthesizer
Tristimulus
Envelope
Damping
Inharmonicity
Noisiness
Even/Odd
Reverb
Perceptual Synthesizer
Spatialization
Figure vz SiMS music server is built as a hierarchy
of musical agents and can be integrated into various
sensate environmentsm See text for further informal
tionm
Figure wz The wiilbaton module analyzes sD accell
eration data trom the wiilmote so the conductor can
use it to modulate the tempo and to trigger speciﬁc
sections of the piecem
and in charge of the multimodal experience in which it ﬁnds
itselfQ
zhe Xsotion performance is based on the assumption
that mental states can be organized along the threePdimensional
space of valenceO arousal and representational content [UT]Q
–sually emotion is described as decoupled from cognition
in a low dimensional space such as the circumplex model of
xussell [UZ]Q zhis is a very eﬀective description of emotional
states in terms of their valence and arousalQ noweverO these
emotional dimensions are not independant of other dimenP
sions such as the representational capacity of consciousness
which allows us to evaluate and alter the emotional dimenP
sions [TW]Q zhe musical piece composed for Xsotion proP
poses to combine both models into a framework where the
emotional dimensions of arousal and valence are expressed
by the musicO while the conductor evaluates its representaP
tional dimensionQ
hasing our ideas on previous emotion research studies
[TTO TX]O we decided to control the modulation of music from
xussell’s biPdimensional model of emotions [UZ]Q zhe higher
the values of the dynamicsO the higher the expressed arousal
and similarlyO the longer the articulationO the higher the vaP
lenceQ on additionO a database of sound samples was created
where each sample was classiﬁed according to the grousal
and —alence taxonomy Kzable TLQ
Figure xz Russel’s circumplex model of aﬀect reprel
sents emotions on a rD map of Valence and Arousal
[rw]m
5y4 Musical Material
zhe musical composition by ponatas sanzolli consisted of
three layersO namely the virtual string quartetO a ﬁxed elecP
troacoustic tape and live triggering of sound eventsQ zhe
four voices of a traditional string quartet setup up were
precomposed oﬄine and stored as sojo events to be moduP
lated Karticulation and accentuationL by the shu members
connected to the yy—kv interfacesQ zhe sound rendering
was done using state of the art orchestral string sampling
technology Kusing the rondon yymphony urchestra library
with qontakt sampler3LQ zhe second layer consisted of a
ﬁxed audio tape soundtrack synchronized with the string
quartet material with rive4 audio time stretching algoP
rithmsQ gdditionalyO we used discrete sound events trigP
gered by the vVSS brain orchestra membersQ zhe orchestra
members were coordinated by the musical conductor standP
ing in front of themQ
5y5 String Quartet
zhe basic composition strategy was to associate diﬀerP
ent melodic and rhythmic patterns of musical textures to
variations in dynamics and articulation producing textural
changes in the compositionQ zhe inspiration for this music
architecture was the so called netPstructure technique creP
ated by rigeti using patternPmeccanico material [W]Q zhe
second aspect of the composition was to produce transposiP
tion of beats producing an eﬀect of phasePshifting[X]Q zhese
two aspects produced a twoPdimension gradual transformaP
tion in the string quartet texturesQ on one direction the
melodic proﬁle was gradually transformed by the articulaP
tion changesQ un the otherO the shift of accentuation and
gradual tempo changes produced phasePshiftsQ on the ﬁrst
movement a chromatic pattern is repeated and legato inP
creased the superposition of notesQ zhe second and fourth
3http]RRwwwQnativePinstrumentsQcomR
4http]RRwwwQabletonQcom
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
312
movements worked with a constant chord modulation chain
and the third with a canonical structureQ
une member of the orchestra used the yy—kv to moduP
late the articulation of the string quartet Kfour levels from
legato to staccato corresponding to the four light sources
frequenciesL while the other member modulated the accenP
tuation Kfrom piano to forteL of the quartetQ
5y6 Soundscape
zhe soundscape was made of a ﬁxed tape piece compoP
sition and discrete sound events triggered according to afP
fective contentQ zhe sound events are driven by the conP
ductor’s cues and relate to the visual realmQ zhe tape
was created using four primitive sound qualitiesQ zhe idea
was to associate mental states with changes of sound maP
terialQ “vVSS performers” produced discrete events related
to four letters] g Ksharp strongLO h Kshort percussiveLO i
Kwater ﬂowL and j Kharmonic spectrumLQ un the conducP
tor’s cueO the performers concentrated on a speciﬁc column
and row and triggered the desired soundQ zwo members of
the orchestra were using vVSS hundred and concentrated
on W symbols eachQ kach symbol triggered a sound sample
from the “emotional database” corresponding to the aﬀecP
tive taxonomy associated with the symbol Kfor each symbol
or sound quality we had a set of W possible sound samplesLQ
yound wuality ytate grousal —alence
yharp ytrong g nigh tegative
yhort vercussive h nigh tegative
¨ater llow i row vositive
narmonic ypectrum j row vositive
Table pz An aﬀective taxonomy was used to classify
the sound database
Figure yz The MBO performance setup at FET Eul
ropean conference in Prague in July rooym
6y CONCLUSIONS
¨e presented a disembodied interactive system designed
for the generation and modulation of musical material from
brain signalO and described XsotionO an interactive “brain
quartet” piece based on novel brain computer interface techP
nologiesQ zhe shu shows how novel hio technologies can
be used in a multimodal collaborative context where the
performers have volitional control over their mental state
and the music generation processQ ionsidering that the
response time delays of the yy—kv and vVSS interfaces
are well above audio rateO we do not claim that these inP
terfaces provide the level of subtlety and intimate control
more traditional instruments can aﬀordQ teverthelessO it is
a promising ﬁrst step towards the exploration of the creP
ative potential of collaborative brainPbased interaction for
audioPvisual content generationQ ot is part of a larger eﬀort
to include physiological feedback in the interactive generaP
tion of musicQ ¨e can envision many applications of this
brainPbased systems beyond the area of performance includP
ing music therapy Kthis system fosters musical collaboration
and would allow disable people to play music togetherLO neuP
rofeedback [TYO YO T“] and motor rehabilitation KeQgQ the use
of musical feedback for neurofeedback training might be a
good alternative to visual feedback for people with visual
impairmentL[UUO UV]Q ¨e are further exploring both these
artistic and practical applications of the shuQ
7y ACKNOWLEDGMENT
¨e would like to thank visual artist hehdad xezazadehO
the brain orchestra members kncarni sarcosO gndre ruviP
zottoO grmin juﬀO knrique sartinezO gn nong and gQtec
staﬀ for their patience and dedicationQ zhe shu is supP
ported by lundacio ra sarato de z—V and the kuropean
iommission oiz lvZ projects xetaihipO yynthetic lorP
agerO xehabilitation maming yystemO and vresencciaQ
.y ADDITIONAL AUTHORS
sarti yanchez*O gndre ruvizotto*O gnna sura*O glekP
sander —aljamae*O ihristoph muger+O xobert vrueckl+O –lysses
hernardet*Q
• *yvkiyO –niversitat vompeu labraO harcelonaO ypain
• +gQtec muger zechnologies ukmO nerbersteinstrasse
YSO [SUSmrazO gustria
:y REFERENCES
[T] hQ ZQ gllisonO jQ pQ sclarlandO mQ ychalkO yQ jQ
ZhengO sQ sQ packsonO and pQ xQ ¨olpawQ zowards
an independent brainPcomputer interface using steady
state visual evoked potentialsQClin NeurophysiolO
TT“KUL]V““–WS[O leb USS[Q
[U] hQ ZQ gllison and pQ gQ vinedaQ krps evoked by
diﬀerent matrix sizes] implications for a brain
computer interface KbciL systemQIEEE Trans Neural
Syst Rehabil EngO TTKUL]TTS–VO pun USSVQ
[V] –Q hernardetO yQ hQ i hadiaO gQ juﬀO sQ onderbitzinO
yQ rQ mrouxO pQ sanzolliO ZQ sathewsO gQ suraO
gQ —aljamaeO and vQ lQ sQ pQ —erschureQThe
eXperience Induction Machine: A New Paradigm for
Mixed Reality Interaction Design and Psychological
ExperimentationQ ypringerO USS“Q [TZ dec USS“] on
vressQ
[W] pQ ilendinningQ zhe patternPmeccanico compositions
of myorgy rigetiQPerspectives of New MusicO
VTKTL]T“U–UVWO T““VQ
[X] xQ iohnQ zranspositional iombination of heatPilass
yets in yteve xeich’s vhasePyhifting susicQ
Perspectives of New MusicO VSKUL]TWY–TZZO T““UQ
[Y] zQ kgner and pQ mruzelierQ kcological validity of
neurofeedback] modulation of slow wave kkm
enhances musical performanceQNeuroreportO
TWK“L]TUUTO USSVQ
[Z] qQ kngO gQ hablerO –Q hernardetO sQ hlanchardO
sQ iostaO zQ jelbruckO xQ pQ jouglasO qQ neppO
jQ qleinO pQ sanzolliO sQ sintzO lQ xothO
–Q xutishauserO qQ ¨assermannO gQ sQ ¨hatleyO
gQ ¨ittmannO xQ ¨yssO and vQ lQ sQ pQ —erschureQ
gda P intelligent space] an artiﬁcial creature for the
swissexpoQSUQRobotics and Automationu 2yy3w
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
313
Proceedingsw ICRA ’y3w IEEE International
Conference onO V]WTXW–WTX“ volQVO USSVQ
[[] rQ larwell and kQ jonchinQ zalking oﬀthe top of your
head] toward a mental prosthesis utilizing
eventPrelated brain potentialsQElectroencephalography
and clinical NeurophysiologyO ZSKYL]XTS–XUVO T“[[Q
[“] kQ gQ leltonO pQ gQ ¨ilsonO pQ iQ ¨illiamsO and vQ iQ
marellQ klectrocorticographically controlled
brainPcomputer interfaces using motor and sensory
imagery in patients with temporary subdural
electrode implantsQ report of four casesQJ NeurosurgO
TSYKVL]W“X–XSSO sar USSZQ
[TS] gQ lribergO xQ hresinO and pQ yundbergQ uverview of
the kth rule system for musical performanceQ
Advances in Cognitive Psychologyu Special Issue on
Music PerformanceO UKUPVL]TWX–TYTO USSYQ
[TT] gQ mabrielsson and kQ rindstr¨ omQMusic and Emotion
v Theory and ResearchO chapter zhe onﬂuence of
susical ytructure on kmotional kxpressionQ yeries in
gﬀective ycienceQ uxford –niversity vressO tew YorkO
USSTQ
[TU] iQ mugerO yQ jabanO kQ yellersO iQ nolznerO
mQ qrauszO xQ iarabalonaO lQ mramaticaO and
mQ kdlingerQ now many people are able to control a
vVSSPbased brainPcomputer interface KhioLe
Neuroscience lettersO WYUKTL]“W–“[O USS“Q
[TV] zQ ninterberger and mQ haierQ varametric orchestral
soniﬁcation of eeg in real timeQIEEE MultiMediaO
TUKUL]ZS–Z“O USSXQ
[TW] yQ raureysQ zhe neural correlate of KunL awareness]
lessons from the vegetative stateQTrends in cognitive
sciencesO “KTUL]XXY–XX“O USSXQ
[TX] yQ re mrouxO gQ —aljamaeO pQ sanzolliO and vQ lQ
sQ pQ —erschureQ omplicit physiological interaction for
the generation of aﬀective musicQ onProceedings of the
International Computer Music ConferenceO helfastO
–qO gugust USS[Q wueens –niversity helfastQ
[TY] yQ re mroux and vQ lQ sQ pQ —erschureQ teuromuse]
zraining your brain through musical interactionQ on
Proceedings of the International Conference on
Auditory DisplayO iopenhagenO jenmarkO say T[PUU
USS“Q
[TZ] yQ re mroux and vQ lQ sQ pQ —erschureQ yituated
interactive music system] ionnecting mind and body
through musical interactionQ onProceedings of the
International Computer Music ConferenceO sontrealO
ianadaO gugust USS“Q sc mill –niversityQ
[T[] pQ sanzolli and vQ lQ sQ pQ —erschureQ xoboser] g
realPworld composition systemQComputwMusic JwO
U“KVL]XX–ZWO USSXQ
[T“] mQ sindlin and mQ xozelleQ hrain music therapy]
home neurofeedback for insomniaO depressionO and
anxietyQ onInternational Society for Neuronal
Regulation 14vth Annual conferenceu Atlantau GeorgiaO
pages TU–TVO USSYQ
[US] kQ xQ sirandaO qQ yharmanO qQ qilbornO and
gQ juncanQ un harnessing the electroencephalogram
for the musical braincapQComputw Music JwO
UZKUL][S–TSUO USSVQ
[UT] gQ suraO pQ sanzolliO hQ xezazadehO yQ rQ mrouxO
sQ yanchezO gQ —aljameO gQ ruvizottoO iQ mugerO
–Q hernardetO and vQ lQ —erschureQ zhe multimodal
brain orchestra] art through technologyQ zechnical
reportO yvkiyO USS“Q
[UU] lQ tijboerO gQ lurdeaO oQ munstO pQ sellingerO
jQ sclarlandO tQ hirbaumerO and gQ q
”ublerQ gn auditory brainPcomputer interface KhioLQ
Journal of neuroscience methodsO TYZKTL]WV–XSO USS[Q
[UV] sQ vhamO zQ ninterbergerO tQ teumannO gQ qublerO
tQ nofmayerO gQ mretherO hQ ¨ilhelmO pQ —atineO and
tQ hirbaumerQ gn auditory brainPcomputer interface
based on the selfPregulation of slow cortical
potentialsQ Neurorehabilitation and Neural RepairO
T“KVL]USYO USSXQ
[UW] xQ vrueckl and iQ mugerQ g hrainPiomputer
onterface hased on yteady ytate —isual kvoked
votentials for iontrolling a xobotQBiovInspired
Systems: Computational and Ambient IntelligenceO
pages Y“S–Y“ZQ
[UX] jQ xosenboomQ hiofeedback and the arts] xesults of
early experimentsQ onComputer Music JournalO
volume TVO pages [Y–[[O T“[“Q
[UY] xQ xoweQInteractive music systems: machine
listening and composingQ soz vressO iambridgeO sgO
–ygO T““UQ
[UZ] pQ gQ xussellQ g circumplex model of aﬀectQJournal of
Personality and Social PsychologyO V“]VWX–VXYO T“[SQ
[U[] ¨ikipediaQ glvin lucier — wikipediaO the free
encyclopediaO USS[Q [unlinea accessed
U[PpanuaryPUSS“]Q
[U“] pQ xQ ¨olpawQ hrainPcomputer interfaces as new brain
output pathwaysQJ PhysiolO XZ“Kvt VL]YTV–“O sar
USSZQ
[VS] sQ ¨rightQ upen sound control] an enabling
technology for musical networkingQOrgw SoundO
TSKVL]T“V–USSO USSXQ
[VT] jQ ZicarelliQ now o learned to love a program that
does nothingQ Computer Music JournalO KUYL]WW–XTO
USSUQ
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
314