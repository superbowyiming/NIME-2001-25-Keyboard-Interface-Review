Toward Direct Brain-Computer Musical Interfaces 
Eduardo Miranda 
University of Plymouth 
Computer Music Research 
Drake Circus, Plymouth, U.K. 
+44 (0)1752 232579 
eduardo.miranda@plymouth.ac.uk 
 Andrew Brouse 
University of Plymouth 
Computer Music Research 
Drake Circus, Plymouth, U.K. 
+44 (0)1752 232579 
andrew.brouse@plymouth.ac.uk 
 
 
ABSTRACT 
 
Musicians and composers have been using brainwaves as 
generative sources in music for at least 40 years and the 
possibility of a brain-computer interface for direct communication 
and control was first seriously investigated in the early 1970s.  
Work has been done by many artists and technologists in the 
intervening years to attempt to control music systems with 
brainwaves and - indeed - many other biological signals. Despite 
the richness of EEG, fMRI and other data which can be read from 
the human brain, there has up to now been only limited success in 
translating the complex encephalographic data into satisfactory 
musical results. We are currently pursuing research which we 
believe will lead to the possibility of direct brain-computer 
interfaces for rich and expressive musical control. This report will 
outline the directions of our current research and results.   
Keywords 
 
Brain-Computer Interface, BCI, Electroencephalogram, EEG, 
brainwaves, music and the brain, interactive music systems. 
1. INTRODUCTION 
 
Human brainwaves were first measured in 1924 by Hans Berger 
[3]. Today, the EEG has become one of the most useful tools in 
the diagnosis of epilepsy and other neurological disorders. The 
fact that a machine can read signals from the brain has sparked the 
imaginations of scientists, artists and other enthusiasts, and the 
EEG has made its way into a myriad of other applications. In the 
early 1970s, Jacques Vidal did the first tentative work towards a 
system interfacing the human EEG to a computer. The results of 
this work were published in 1973 in a paper entitled Toward 
Direct Brain-Computer Communication [27]. This domain of 
research is known as Brain-Computer Interfacing (BCI) and there 
are an increasing number of researchers worldwide working in the 
field. Vidal’s work was followed by many other attempts at BCI 
which had varying degrees of success. For example, in 1990, 
Jonathan Wolpaw and colleagues developed a system to allow 
primitive control of a computer cursor by subjects with severe 
motor deficits.  Subjects were trained to use their EEG to move 
the cursor in simple ways [28]. For recent reports on BCI research 
please refer to the special issue of IEEE Transactions on 
Biomedical Engineering published in June 2004 (Vol. 51, #6).   
We are focused on the development of BCI systems for musical 
applications and thus pay special attention to the development of 
appropriate techniques geared to real-time music systems. One 
might call such systems Brain-Computer Musical Interfaces 
(BCMI). 
As early as 1934, a paper in the journal Brain had reported a 
method to listen to the EEG [1] but it is now generally accepted 
that it was composer Alvin Lucier, who composed the first 
musical piece using EEG in 1965: Music for Solo Performer [13]. 
Pioneers such as Richard Teitelbaum [25], David Rosenboom [23, 
24] and a few others produced compositions and a number of 
interesting systems and pieces. In 1975 David Rosenboom edited 
a remarkable book on the topic [22] and more recently one of the 
current authors, Andrew Brouse, published a comprehensive 
review on using brainwaves to produce music [6]. 
Our research builds on the work developed by these pioneers in a 
number of ways. Firstly, we are employing and developing more 
sophisticated analysis techniques to harness the EEG signal. In 
addition, we are developing new psychophysical experiments in 
order to gain a better understanding of the EEG components 
associated with musical cognition and methods to train subjects to 
generate such EEG components. Finally, we are developing 
generative techniques especially designed for musical 
composition and performance with a BCMI. This paper focuses 
on the first and the latter. More information on our 
psychophysical experiments can be found in [15, 16]. 
Before we proceed, note that the BCI research community 
understands that a BCI system is a system that allows for the 
control of a machine by explicitly thinking the task(s) in question; 
e.g., control a robotic arm by thinking explicitly about moving an 
arm. This is an extremely difficult problem to solve. The system 
presented in this paper does not address this type of explicit 
control which would be even more difficult in the case of music. 
We are not interested in a system that plays a melody by thinking 
the melody itself. Rather, we are furnishing our systems with 
Artificial Intelligence in order to allow them make their own 
interpretation of the meanings of the EEG patterns. Such machine-
interpretations may not always be accurate or realistic, but this is 
exactly the type of human-machine interaction that we are 
addressing in our work. 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
Nime’05, May 26-28, , 2005, Vancouver, BC, Canada. 
Copyright remains with the author(s). 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
216
2. THE ELECTROENCEPHALOGRAM 
 
Neural activity generates electric fields that can be recorded with 
electrodes attached on the scalp. The electroencephalogram, or 
EEG, properly refers to the visual plotting of this signal, but 
currently has come to connote the method of measurement or just 
the electric fields themselves. These electric fields are extremely 
faint, with amplitudes on the order of only a few microvolts. In 
order to be displayed and/or processed, these signals must be 
greatly amplified [17, 19]. 
An EEG derivation is measured as the voltage difference between 
two electrodes on the surface of the scalp, with a separate ground 
to reduce common-mode interference. The EEG expresses the 
overall activity of millions of neurons in the brain in terms of 
charge movement, but the electrodes can detect this only in the 
most superficial regions of the cerebral cortex. The EEG is a 
difficult signal to handle because it is filtered by the meninges, the 
skull and the scalp before it reaches the electrodes. Furthermore, 
the signals arriving at the electrodes are sums of signals arising 
from many possible sources, including artifacts like the heartbeat, 
muscle contractions and eye blinks. Although experts can 
diagnose brain malfunctioning from raw EEG plots, this signal 
needs to be further scrutinized with signal processing and analysis 
techniques in order to be of any use for our research. 
There are a number of useful approaches to quantitative EEG 
analysis, such as power spectrum, spectral centroid, Hjorth, event-
related potential (ERP) and correlation, to cite but five. A brief 
non-mathematical introduction to EEG power spectrum and 
Hjorth analyses is given below due to their relevance to the 
systems introduced in this paper. A discussion on other analysis 
techniques and how they have been used in neuroscience of music 
research can be found in the given references [4, 10, 11, 18, 26].  
2.1 Power Spectrum Analysis 
 
Power spectrum analysis is derived from techniques of Fourier 
analysis, such as the Discrete Fourier Transform (DFT). In short, 
DFT analysis breaks the EEG signal into different frequency 
bands and reveals the distribution of power between them. This is 
useful because the distribution of power in the spectrum of the 
EEG can reflect certain states of mind. For example, a spectrum 
with salient low-frequency components can be associated with a 
state of drowsiness, whereas a spectrum with salient high-
frequency components could be associated with a state of 
alertness. There are five recognised frequency bands of EEG 
activity, also referred to as EEG rhythms, each of which is 
associated with specific mental states: delta, theta, alpha, low beta 
and high beta rhythms. There is, however, some controversy as to 
the exact frequency boundaries of these bands and about the 
mental states with which they are associated.  
Also related to power spectrum analysis is the spectral centroid 
analysis which calculates the spectral “centre of gravity” of the 
signal; that is, the midpoint of its energy distribution. It is a 
measure of the average frequency weighted by amplitude. This is 
usually averaged over time. 
 
 
2.2 Hjorth Analysis 
 
Hjorth introduced an interesting method for clinical EEG analysis 
[9], which measures three attributes of the signal: its activity, 
mobility and complexity. Essentially, it is a time-based amplitude 
analysis. This method is interesting because it represents each 
time step (or window) using only these three attributes and this is 
done without conventional frequency domain description. The 
signal is measured for successive epochs (or windows) of one to 
several seconds. Two of the attributes are obtained from the first 
and second time derivatives of the amplitude fluctuations in the 
signal. The first derivative is the rate of change of the signal’s 
amplitude. At peaks and troughs the first derivative is zero. At 
other points it will be positive or negative depending on whether 
the amplitude is increasing or decreasing with time. The steeper 
the slope of the wave, the greater will be the amplitude of the first 
derivative. The second derivative is determined by taking the first 
derivative of the first derivative of the signal. Peaks and troughs in 
the first derivative, which correspond to points of greatest slope in 
the original signal, result in zero amplitude in the second 
derivative, and so forth.   
Activity is the variance of the amplitude fluctuations in the epoch. 
Mobility is calculated by taking the square root of the variance of 
the first derivative divided by the variance of the primary signal. 
Complexity is the ratio of the mobility of the first derivative of the 
signal to the mobility of the signal itself. A sine wave has a 
complexity equal to 1. Figure 1 shows an example of Hjorth 
analysis. A raw EEG signal is plotted at the top (C:1) and its 
respective Hjorth analysis is plotted below: activity (C:2), 
mobility (C:3) and complexity (C:4).  (Figure 1.) 
There is no clear agreement as to what these measurements mean 
in terms of mental states. It is common sense to assume that the 
longer a subject remains focused on a specific mental task, the 
more stable is the signal, and therefore the lower is the variance of 
the amplitude fluctuation. However, this point questions the 
possible affects of fatigue, habituation and boredom, which we 
have not yet accounted for in our research. 
 
 
Figure 1. An Example of a Hjorth analysis. 
 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
217
3. THE BCMI-PIANO SYSTEM 
 
The BCMI-Piano falls into the category of BCI computer-oriented 
systems. These systems rely on the capacity of the users to learn 
to control specific aspects of their EEG, affording them the ability 
to exert some control over events in their environments. Examples 
have been shown where subjects learn how to steer their EEG to 
select letters for writing words on the computer screen [5]. 
However, the motivation for the BCMI-Piano departed from a 
slightly different angle from other BCI systems. We aimed for a 
system that would make music by guessing the meaning of the 
EEG of the subject rather than a system for explicit control of 
music by the subject. Learning to steer the system by means of 
biofeedback would be possible, but we have not systematically 
investigated this possibility yet. We acknowledge that the notion 
of “guessing the meaning of the EEG” here is simplistic, but it is 
nevertheless plausible: it is based on the assumption that 
physiological information can be associated with specific mental 
activities or states [21, 2].  
The system is programmed to look for information in the EEG 
signal and match the findings with assigned generative musical 
processes corresponding to different musical styles. For example, 
if the system detects prominent alpha rhythms in the EEG, then it 
might activate certain assigned processes that generate musical 
passages in a given style.  
The BCI-Piano is composed of four main modules: braincap, 
analysis, music engine and performance. The EEG is sensed with 
7 pairs of gold EEG electrodes on the scalp, roughly forming a 
circle around the head. In this case we are not looking for signals 
emanating from specific cortical sites but to sense the EEG over 
the whole surface of the cortex. The electrodes are connected to a 
biosignal amplifier and a real-time acquisition system. The 
analysis module is programmed in Matlab and Simulink [14] to 
perform power spectrum and Hjorth analyses in real-time. The 
analysis module generates two streams of control parameters. One 
stream contains information about the most prominent frequency 
band in the signal and is used by the music engine to generate the 
music. In the current version, the system activates rules for two 
different styles of music, depending on whether the EEG indicates 
salient low-frequency or high-frequency components. The other 
stream contains information about the complexity of the signal 
and is used by the music engine to control the tempo of the music  
The core of the music engine module is a set of generative music 
rules, each of which produce a musical bar, or measure. The 
system works as follows: every time it has to produce a bar, it 
checks the power spectrum of the EEG at that moment and 
activates rules associated with the most prominent EEG rhythm in 
the signal. The system is initialised with a reference tempo (e.g., 
120 beats per minute), which is constantly modulated by the 
signal complexity analysis (i.e., Hjorth analysis). The music 
engine sends out MIDI information for performance. We have 
implemented a demonstration using a Yamaha Disklavier piano. 
In order to have greater control over this system, we are 
developing methods to train subjects to achieve specific EEG 
patterns in order to play the BCMI-Piano system. We have initial 
evidence that this can be made possible using a technique 
commonly known as biofeedback. Biofeedback is when biological 
signals are converted to some auditory or visual display which in 
turn affects the performer’s physiological state. As a result the 
subject can learn to modify their physiological state and thus 
subsequently gaining greater control of the biological signals 
which it produces. Biofeedback technology has been used to treat 
and control a number of conditions including migraine headaches, 
epilepsy and attention deficit disorder. In addition it has been 
explored as a means of artistic exploration by performers, 
composers and artists [22, 23, 24].  
The aim of our current research in this area is to create a 
methodology to train subjects to use BCMI technology through 
the use of biofeedback. If the results of this research are valid, this 
could lead to more independent expressive control during music 
composition and performance. 
 
4. CONCLUSION 
 
Our research work in this area owes an historical debt to the 
pioneering works of people such as David Rosenboom, Richard 
Teitelbaum and Alvin Lucier, but extends those tentative 
experiments with new possibilities for much finer granularity of 
control over real-time musical processes.  
The work presented in this paper is the result of multidisciplinary 
research, ranging from neuroscience and medical engineering to 
music technology and composition. We acknowledge that our 
research so far has only suggested some tentative directions in a 
potentially vast field. There remain many cumbersome issues to 
be resolved before we can realise our ultimate goal: an affordable, 
flexible and practically feasible Brain-Computer Musical 
Interface. 
Key issues still need to be addressed, notably, the task of 
interpreting the meaning of the EEG. Although powerful 
mathematical tools for analysing the EEG already exist, we still 
lack a good understanding of their analytical semantics in relation 
to musical cognition, however, continual advances in the field of 
cognitive neuroscience [20] are increasingly shining light upon 
these obscurities. Once these issues are better understood we will 
be able to program our devices to recognise patterns of cognitive 
activity in the brainwaves and thus trigger appropriate musical 
algorithms associated with such patterns. Some preliminary work 
in this regard has been reported to date [15, 16]. 
Another aspect which needs to be addressed is the non-ergonomic 
nature of the sensing technology for measuring the EEG which 
can be awkward and uncomfortable to wear. New electrode 
technology coupled with ‘smart fabrics’ could lead to increased 
comfort and portability of the equipment. 
We also see many possibilities for innovations in the hardware 
design of EEG capture devices. Given current electronic 
technology it should possible to build wearable EEG amplifiers 
with built-in measurement, signal processing and wireless data 
transmission.  
Significant reduction in the cost of a functional BCMI is also 
possible by increasing using commodity hardware and software 
such as personal computers, USB A-D interfaces, WiFi and/or 
BlueTooth wireless devices and miniaturised biosignal amplifiers. 
We are actively pursuing collaborations with other developers to 
advance these goals. 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
218
5. REFERENCES 
 
[1] Adrian, E.D., and Matthews, B.H.C. (1934), "The Berger 
Rhythm: Potential Changes from the Occipital Lobes in 
Man." Brain, 57(4): 355-85. 
[2] Anderson, C. and Sijercic, Z. (1996), “Classification of EEG 
signals from four subjects during five mental tasks”, Solving 
Engineering Problems with Neural Networks: Proceedings of 
the Conference on Engineering Applications in Neural 
Networks (EANN’96), pp. 507-414. 
[3] Berger, H. (1969), “On the Electroencephalogram of Man”, 
The Fourteen Original Reports on the Human 
Electroencephalogram, Electroencephalography and Clinical 
Neurophysiology,  Supplement No. 28. Amsterdam: Elsevier. 
[4] Besson, M. and Faita, F. (1995), “An event-related potential 
(ERO) study of musical expectancy: Comparison of 
musicians with non-musicians”, Journal of Experimental 
Psychology: Human Perception and Performance 21:1278-
1296. 
[5] Birbaumer, N., Ghanayin, N., Hinterberger, T., Iversen, I., 
Kotchoubey, B., Kubler, A., Perelmeouter, J., Taub, E. and 
Flor, H. (1999), “A spelling device for the paralysed”, Nature 
398:297-298. 
[6] Brouse, A. 2004, “Petit guide de la musique des ondes 
cérébrales”, Horizon 0, No. 15. http://www.horizonzero.ca, 
Accessed 31 January 2005. 
[7] Cope, D. 2001. Virtual Music. Cambridge, MA: The MIT 
Press. 
[8] Cope, D. 1996. Experiments in Musical Intelligence. 
Madison, WI: A-R Editions Inc. 
[9] Hjorth, B. (1970), “EEG analysis based on time series 
properties”, Electroencephalography  and Clinical 
Neurophysiology, 29:306-310. 
[10] Janata, P. and Petsche, H. (1993), “Spectral analysis of the 
EEG as a tool for evaluating expectancy violations of 
musical contexts”, Musical Perception, 10(3):281-304. 
[11] Koelsch, S., Schroger, E., Gunter, T.C. (2002), “Music 
matters: Preattentive musicality of the human brain”, 
Psychophysiology 39(2002): 38-48. 
[12] Kohonen, T., Laine P., Tiits, K and Torkkola, K. (1991), “A 
Nonheuristic Automatic Composing Method”, P. Todd and 
D. G. Loy (Eds.), Music and Connectionism. Cambridge 
(MA): The MIT Press. 
[13] Lucier, A. (1976), “Statement On: Music for Solo 
Performer”, D. Rosenboom (Ed.), Biofeedback and the Arts, 
Results of Early Experiments. Vancouver: Aesthetic 
Research Centre of Canada Publications. 
[14] Mathworks (2004), The MathWorks, 
http://www.mathworks.com, Accessed 18 June 2004. 
[15] Miranda, E.R., Sharman, K., Kilborn, K. and Duncan, A. 
(2003), “On Harnessing the Electroencephalogram for the 
Musical Braincap”, Computer Music Journal, 27(2):80-102. 
[16] Miranda, E. R., Roberts, S. and Stokes, M. (2004). "On 
Generating EEG for Controlling Musical Systems", 
Biomedizinische Technik, 49(1):75-76. 
[17] Misulis, K.E. (1997), Essentials of Clinical 
Neurophysiology. Boston, Massachusetts: Butterworth-
Heinemann. 
[18] Näätänen, R. (1990), “The role of attention in auditory 
information processing as revealed by event-related 
potentials and other brain measures of cognitive function”, 
Behavioural and Brain Sciences 13:201-288. 
[19] Niedermeyer, E. and Lopes da Silva, F. H. (Eds.) (1987) 
Electroencephalography (2nd edition). Munich: Urban and 
Schwartzenberg. 
[20] Peretz, I. and Zatorre, R.J. (2003), The Cognitive 
Neuroscience of Music. Oxford: Oxford University Press. 
[21] Petsche, H. and Etlinger, S.C. (1998), EEG and Thinking. 
Vienna: Austrian Academy of Sciences. 
[22] Rosenboom, D. (Ed.) (1975), Biofeedback and the Arts, 
Results of Early Experiments. Vancouver: Aesthetic 
Research Centre of Canada Publications. 
[23] Rosenboom, D. (1990), Extended Musical Interface with the 
Human Nervous System, Leonardo Monograph Series No. 1. 
Berkeley, California: International Society for the Arts, 
Science and Technology. 
[24] Rosenboom, D. (1990), “The Performing Brain”. Computer 
Music Journal 14(1):48-65. 
[25] Teitelbaum, R. (1976), “In Tune: Some Early Experiments in 
Biofeedback Music (1966-1974)”, D. Rosenboom (Ed.), 
Biofeedback and the Arts, Results of Early Experiments. 
Vancouver: Aesthetic Research Centre of Canada 
Publications. 
[26] Tervaniemi, M. (1999), “Pre-Attentive Processing of Musical 
Information in the Human Brain”. Journal of New Music 
Research 28(3):237-245. 
[27] Vidal, J.J. (1973), "Toward Direct Brain-Computer 
Communication", L. J. Mullins (Ed.), Annual Review of 
Biophysics and Bioengineering, pp. 157— 80. 
[28] Wolpaw, J., McFarland, D., Neat, G., Forneris. C. (1991), 
"An Eeg-Based Brain-Computer Interface for Cursor 
Control", Electroencephalography and Clinical 
Neurophysiology, 78(3):252-9. 
 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
219