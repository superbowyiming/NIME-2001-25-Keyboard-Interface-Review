Real-time control and creative convolution 
Exchanging techniques between distinct genres  
 
 Trond Engum 
Music technology 
NTNU, Department of music        
7049 –N Trondheim 
(+47 73590092) 
trond.engum@ntnu.no 
 
 
 
ABSTRACT 
This paper covers and also describes an ongoing research 
project focusing on new artistic possibilities by exchanging 
music technological methods and techniques between two 
distinct musical genres. 
 Through my background as a guitarist and composer in an 
experimental metal band I  have experienced a vast 
development in music technology during the last 20 years. This 
development has made a great impact in changing the 
procedures for composing and producing music within my 
genre without necessarily changing the strategies of how the 
technology is used. The transition from analogue to digital 
sound technology not only opened up new ways of 
manipulating and manoeuvring sound, it also opened up 
challenges in how to integrate and control the digital sound 
technology as a seamless part of my musical genre. By using 
techniques and methods known from electro-acoustic/computer 
music, and adapting them for use within my tradition, this 
research aims to find new strategies for composing and 
producing music within my genre. 
 
Keywords 
Artistic research, strategies for composition and production, 
convolution, environmental sounds, real time control 
1. INTRODUCTION 
The relationship between electro-acoustic and rock/metal music 
(as a part of the popular music umbrella) has a complex history 
relating to musical directions, intentions, the use of synthesis 
and manipulation. Nevertheless it can be said that both genres 
have embraced and integrated the technological tools made 
available at their present time. Even thou gh there are several 
arguments pointing towards a blending of the use of technology 
between the genres, there are still many transfer possibilities 
and potential for exchange. 
 In my field the utilization of digital sound technology to a 
large degree still follows the same mindset that has been 
developed through the history of analogue sound technology.  
It is therefore a large resource of unrevealed potential in 
contemporary technology for use within my genre. 
In this research I address the following question: 
How is it possible to transfer methods and techniques from one 
tradition to another without loosing the idiomatic characteristics 
of a genre, and how can you use this knowledge to add new 
aesthetics?  
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME’11, 30 May–1 June 2011, Oslo, Norway. 
Copyright remains with the author(s). 
 
 
2. METHODS AND TECHNIQUES 
Throughout the project the course of events has consisted of 
three main stages: 
1. Studying and interpreting a selection of methods and 
techniques within electro-acoustic music and how to receive 
experience based knowledge of its possibilities and limitations. 
2.  Translating and adapting these methods and techniques for 
practical use within my genres aesthetics. 
3.  Proposing different ways of controlling these as an 
extended part of the instrumentation within my genre. 
 The focus areas throughout these different stages have been: 
2.1 The studio as a compositional tool 
2.2 Musical integration of environmental sounds 
2.3 Creative use of convolution  
2.4 Real – time control 
 
2.1 The studio as a compositional tool 
In maintaining the content of this progress, it was an obvious 
consequence to start with the sound studio as a framework and 
basis for several reasons. The sound studio has been a mutual 
point of focus and also a necessity for developing the aesthetics 
of both electro-acoustic and popular music. At the same time 
this meeting point divides these genres when it comes to 
working procedures. While electro-acoustic music has an 
acousmatic tradition being composed in a studio environment, 
the tradition within rock music is that recording normally takes 
place at the end of a composition process. In other words, the 
composing and rehearsal takes place in a dialog process 
between the different performers in real time. The use of the 
sound studio early in this process therefore leads to a challenge 
in how to maintain this dialog principle. In the electro 
acoustical tradition the division between the composer a nd 
producer has in some degree been absent. Within popular music 
this situation has been the opposite. In this case the producer 
becomes an important part of the ‘so-called’ music industry, 
and is given credibility as a part of the creative process. As 
early as 1978 Brian Eno talked about the obliteration of the 
composer/producer role within popular music when developing 
his ambient music.[5] He suggested that the sound studio as a 
compositional tool was one of the clearest characteristics in 
new music, and that this would become the main focus for 
compositional attention in the future.[3] Even though the DAW 
to a large degree has replaced the traditional recording studio, 
and the compositional procedures within popular music shifts 
against a use of the DAW earlier in the process, several of the 
mentioned conventions are still present. So how is it possible to 
reveal more of the potential in contemporary technology for use 
within my genre? 
 
 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
519
2.2 Musical integration of environmental 
sounds 
The use of environmental sounds as building blocks in 
compositional works has been a significant progression within 
the electro acoustic tradition. Ever since musique concrete in 
the 50`s and up until today this direction has been developing, 
and is still a basis for different musical directions and 
expressions.  This aesthetical approach is relatively unexplored 
within my genre. Working with environmental sound 
challenges the sonorous attributes within my genres 
conventional expression, but it also raises the question of how 
to control and integrate pre-recorded material as a part of a real 
time performance. In my research the selection of sounds has 
mainly been focused on industrial noise.[11]   
Sound example 1:  
This is a preview of a composition build up of drums, vocals, 
angle grinders, trains, boats and chains. The recordings of the 
angle grinders and trains are edited, tuned and organized as 
tonal instruments, the boats and chains as percussive 
instruments.  
2.3 Creative use of convolution 
Convolution tools have been available for composers since the 
early nineties [10]. In popular music they are most commonly 
used in reverberation units where they are based on recorded 
impulse responses from different rooms. These impulses are 
then stored in order to be convolved with a desired input. In 
addition to this approach there are no limits as to which sounds 
that can be convolved with each other, and the exploitation of 
these possibilities is where the research of this projects aims. 
Other examples of approaches to this technique is Roberto 
Aimi`s percussion instrument [1], or “the sound of touch” [4]. 
A more creative use of this technique can be found in some of 
Barry Truax`s works[13] within art music, or The Soundbyte`s 
“City of Glass” [12] within a popular music genre. As far as I 
am aware there has been very limited documented artistic 
research on convolving different sound sources with each other, 
and because of that most descriptions of use are focused more 
on technical than aesthetic aspects.[10] By using a wide variety 
of different environmental sounds as impulse responses this 
project has explored which possibilities and limitations 
convolution between digital sound files imply both at a micro 
level, but also in a broader musical context. This work has 
resulted in three different approaches. 
2.3.1 Convolution in postproduction 
The first approach to this work started with empirical 
experiments with a wide variety of pre recorded environmental 
sounds consisting of different attributes. The central aim 
through this experience was to be able to predict how different 
inputs and impulses would interact with each other, in order to 
control these parameters against a wanted output.  
 
 
 
Figure 1. First setup 
 
Sound example 2:  
This is an example of a tonal approach, convoluting an electric 
guitar with the sound of a train . 
Sound example 3:  
This is an example of a rhythmical approach, convoluting 
handclaps with a recording of a chain 
Sound example 4:  
This is an example of both rhythmical and tonal approach put in 
a musical context  
2.3.2 Real-time convolution 
The second approach was finding ways to interact with this 
technique in real time by opening up a two-way communication 
between a musician and the output. In order to realize this two-
way communication it was crucial that the musician was 
separated from the acoustic sound of the instrument in order to 
interact with the processed signal. This was maintained by 
feeding back the processed signal through headphones. By 
changing the impulse responses, and tailoring them to suit the 
present instrument, it was possible to affect the performance 
without the musician feeling unfamiliar with the mechanical 
presence or playing techniques of his own instrument. During 
these experiments both dry input signal and processed signal 
were recorded in order to analyze what caused the sonic 
changes, but also what made the musician make different 
artistic choices when interacting through this two-way 
communication. 
 
 
Figure 2. Second setup 
 
 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
520
Video example 1:  
A video example of real time use of this set up (guitar and 
environmental sounds) together with real time convolution 
between drums and environmental sounds propose an artistic 
use of the points mentioned above.  
 
2.3.3 The impulse sampler 
The third approach came as a result of the experiences gained 
from the first two setups. The idea was to be able to record an 
impulse response and interact with it in a real-time situation. 
This setup gave the opportunity to sample impulses from my 
own instrument, other musicians or sound sources, and directly 
convolute them with another chosen sound-source in real-time.  
The use of this setup gave several advantages. Firstly the 
implementation of this function made the whole process of 
trying different sounds against each other much faster and 
effective. Secondly the artistic value of being able to control 
samples of fellow musicians with my own instrument in real-
time, opened up some exciting possibilities and results. The 
program was implemented in Csound, and runs in Ableton Live 
as a Max For Live device. [2]  
 
 
Figure 3. Impulse sampler 
Sound example 5:  
This is an example using the Impulse sampler to convolute a 
guitar with itself while playing. 
Sound example 6:  
This is an example using the Impulse sampler to convolute a 
guitar with an anglegrinder. 
2.4 Real-time control 
A challenge throughout the project has been finding ways to 
control these techniques in real time, and being able to use this 
in a musical dialog together with other musicians as an 
extended part of the conventional instrumentation. Since 
working in the studio in recent years to a large degree has 
changed from manoeuvring large mixing consoles to 
controlling everything through the DAW with a mouse and a 
keyboard, it felt natural to follow up on this workflow also in a 
real time situation. Even though there are several custom made 
interfaces for these operations on the market, few of them are 
made for integration on an existing instrument. As a guitarist 
both hands and feet are occupied at the same time concentrating 
on the guitar and foot pedals, disabling the player to handle a 
different standalone interface at the same time. The first step 
was to place a numerical keyboard directly on the guitar in 
order to control the DAW without interfering with the 
conventional playing. This solution opened up two different 
directions. 
2.4.1 Controlling the DAW from the guitar 
In a conventional guitar set up the closest solution for 
controlling a DAW lies in the use of a midi floorboard. Many 
of these floorboards already contain most of the functions 
needed for controlling both static and dynamic parameters in a 
software environment through its different stomp and 
expression pedals. At the same time this approach leads to a 
practical challenge in operating both the DAW and external 
hardware guitar processors at the same time from the same 
interface. The first approach was to attach a keypad directly 
onto the guitar in order to take care of the non-guitar operations 
in the DAW, and at the same time separate the control of the 
guitar processors and the DAW by using two midi floorboards.  
 
 
Figure 4. First setup 
  
This figure can be seen as a miniature set up of a conventional 
studio event, and as a first attempt at bringing the traditional 
studio environment into the real time domain. Through this 
solution the traditional roles of the producer and musician are 
moulded together, but the system setup still consists of two 
parallel lines of control. This led to a search for a new solution 
where these roles were more seamlessly integrated with each 
other, and at the same time more individually flexible and 
comprehensive. 
 
2.4.2 Augmentation of the guitar based on 
extended techniques 
The functionality and practical use of contemporary digital 
guitar controllers are mainly based on a heritage stemming 
from electrical reproduction conventions, (different stomp 
boxes and expression pedals), resulting in a large amount of 
different digital floorboard and multi effects solutions. There 
are other approaches for digital augmentations of the electric 
guitar like the multimodal guitar[8] [9] or the Manson guitar [7]. 
Besides these there has been a limited documented research on 
digital augmentation solutions attached and controlled directly 
on the Electric guitar. At the same time the possibilities and 
functionality of tailor-made guitar software are poor compared 
to tools you find in most DAW programs, and it would 
therefore be natural to start with the DAW as a processing 
engine controlled from the guitar.  From a musicians point of 
view it would be natural to integrate interfaces directly into the 
instrument, enabling real time control over the digital 
functionality without interfering with the playing of the 
instrument. The approach in this project has been to put 
together well-known and intuitive interfaces and to attach them 
directly to the instrument in order to control the digital software 
in real time. The direct integration of a keypad and a track pad 
enables a player to send both static and dynamical control 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
521
messages to different software and hardware in real time 
without removing the physical focus from the instrument or 
interfering with the idiomatic characteristics of the guitar. 
These interfaces are also very intuitive because of their use in 
other application on daily basis, and also quite inexpensive 
compared to custom-made solutions.  
 
 
Figure 5. Placement of the interfaces 
 
The physical placement of the two different pads was decided 
upon based on two well established extended, guitar 
techniques. The keypad was positioned in a typical guitar 
channel selector area, based on an on/off technique known as 
kill-switch.[6] The track-pad was placed in the volume/tone 
control area on the guitar based on an extended technique 
called volume swell. [14] The volume swell technique enables 
the player to use the volume knob dynamically without 
removing the right hand position from the instrument. This was 
the basis for the second guitar setup. 
 
 
Figure 6. Second setup 
 
This set up gave several advantages. First of all it gave the 
possibility of removing some of the components from the first 
setup without compromising the DAW control or preset 
changing in the Guitar-FX hardware. This was done by running 
all incoming control messages from the different interfaces 
directly through the DAW for mapping and further distribution. 
Secondly, the track-pad opened up an easier and more intuitive 
way of controlling XY parameters compared to using two 
expression pedals at the same time. The physical placement of 
the track-pad also contributed to the possibility of using the XY 
parameters without removing the right hand position as in 
contradiction to the Manson guitar system. [7]  
2.4.3 Interface output 
Both the keypad and track-pad outputs are translated to midi 
signals through two different Max For Live devices [15]. The 
keypad can be used to perform static operations like on/off and 
momentary messages. The track-pad can be used to perform 
dynamic operations like volume, morphing between different 
effects, surround sound operations or other applications 
demanding XY control.  
Video example 2:  
Demonstration of the guitar setup, using the track- pad as an XY 
controller in a granular synthesis plug in. 
Sound example 7:  
Demonstration of the guitar setup in a real-time improvisation 
with other musicians playing convoluted piano and percussion. 
3. SUMMARY 
This is still an ongoing research project, where all mentioned 
themes and work are constantly under a refinement process. 
The next step is to proceed with the research through an even 
more practical approach. This will be done by recording and 
doing concerts with different musicians within a real time 
context for experiencing points for further technical and 
aesthetical improvements.   
4. REFERENCES 
[1] Aimi R. M.(2007) ”Hybrid Percussion : Extending 
Physical Instruments Using Sampled Acoustics” PhD 
thesis, Massachusetts Institute of Technology. 
[2] Brandtsegg, Øyvind (2011): ”The Impulse sampler was 
implemented in Csound and Max For Live by Øyvind 
Brandtsegg” oyvind.brandtsegg@ntnu.no   
[3] Cox, Christoph and Warner, Daniel (2004): “Audio 
Culture: readings in Modern Music”, The continuum 
International Publishing group Inc.  
[4] D. Merrill, H. Raffle, R. Aimi.  (2008) “The Sound of 
Touch: Physical Manipulation of Digital Sound ”. In the 
Proceedings the SIGCHI conference on Human factors in 
computing systems (CHI'08). Florence, Italy. 
[5] Eno, Brian (1978):“Interview with Brian Eno”, viewed 10. 
April 2011, 
http://music.hyperreal.org/artists/brian_eno/interviews 
[6] Killswitch, viewed 10. April 2011,  
http://www.instructables.com/id/Guitar-Killswitch-Strat.-
design/ 
[7] Manson guitar, viewed 10. April  2011,                   
http://www.mansonguitars.co.uk/  
[8] Multimodal guitar, viewed 10. April  2011,  
http://www.numediart.org/projects/07-1-multimodal-
guitar/  
[9] O. Lahdeoja (2008). An approach to instrument 
augmentation : the electric guitar. In Proc. of the  
2008 Conf. on New Interfaces for Musical Expression  
(NIME08).  
[10] Roads, Curtis (1996): “the computer music tutorial”, The 
MIT Press. 
[11] Russolo, Luigi (1913): “The art of noises” 
[12] The Soundbyte/Irgens (2007), City of Glass, Voices Music 
Publishing 
[13] Truax, Barry, viewed 10. April  2011,  
http://www.sfu.ca/~truax/conv.html 
[14] Volume swell, viewed 10. April  2011,  
http://en.wikipedia.org/wiki/Volume_swell 
[15] Wærstad, Bernt Isak (2010): “The trackpad translator was 
implemented in Max For Live by Bernt Isak Wærstad” 
http://partikkelaudio.com/extras/mfl/  
5. Appendix 
All sound and video examples can be found at: 
http://thesoundbyte.com/nime
 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
522