MMODM: Massively Multiplayer Online Drum Machine
Basheer Tome
Tangible Media Group
MIT Media Lab
75 Amherst St.
Cambridge, MA 02142
basheer@media.mit.edu
Donald Derek Haddad
Responsive Environments
MIT Media Lab
75 Amherst St.
Cambridge, MA 02142
ddh@mit.edu
Tod Machover
Opera of the Future
MIT Media Lab
75 Amherst St.
Cambridge, MA 02142
tod@media.mit.edu
Joseph A. Paradiso
Responsive Environments
MIT Media Lab
75 Amherst St.
Cambridge, MA 02142
joep@media.mit.edu
ABSTRACT
Twitter has provided a social platform for everyone to en-
ter the previously exclusive world of the internet, enriching
this online social tapestry with cultural diversity and en-
abling revolutions. We believe this same tool can be used
to also change the world of music creation. Thus we present
MMODM, an online drum machine based on the Twitter
streaming API, using tweets from around the world to cre-
ate and perform musical sequences together in real time.
Users anywhere can express 16-beat note sequences across
26 di↵erent instruments using plain text tweets on their fa-
vorite device, in real-time. Meanwhile, users on the site it-
self can use the graphical interface to locally DJ the rhythm,
ﬁlters, and sequence blending.
By harnessing this duo of website and Twitter network,
MMODM enables a whole new scale of synchronous musi-
cal collaboration between users locally, remotely, across a
wide variety of computing devices, and across a variety of
cultures.
Author Keywords
Collaborative, Synchronous, Twitter, Web, Music Interface
ACM Classiﬁcation
H.5.3 [Information Interfaces and Presentation] Group and
Organization Interfaces — Web-based interaction, H.5.3 [In-
formation Interfaces and Presentation] Group and Organi-
zation Interfaces — Collaborative computing, H.5.3 [Infor-
mation Interfaces and Presentation] Group and Organiza-
tion Interfaces — Synchronous interaction
1. INTRODUCTION
Music and, increasingly, social networks are both incredibly
powerful categories of tools for connecting people together
and creating positive ﬂow-like experiences. We strongly be-
lieve that a more intimate overlap and connection between
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’15,May 31-June 3, 2015, Louisiana State Univ., Baton Rouge, LA.
Copyright remains with the author(s).
Figure 1: MMODM site during a live performance
the two can be instrumental in better connecting even more
and diverse types of people.
1.1 The Rise of Twitter
Twitter usage has quickly skyrocketed since its invention
back in 2006 by Jack Dorsey. Despite any negative uses of
Twitter, it has also helped make a positive impact on society
and has provided a variety of uses for members across the
globe. During the earthquakes that took place in California,
it proved to be one of the main sources of the most up to
date activity of this monstrous natural disaster. There’s
no better way to share information than to have a large
network of friends all connected in one system, who can
reply to your messages and even retweet them to others at
their convenience.
1.2 Related Work
The realm of technology-supported music collaboration is
quite large but work in the spaces of live coding and web-
based audience participation served as the most critical ref-
erences in the creation of this project. The sc140 Super-
Collider album [1], which curated a series of pieces written
in tweet-length SuperCollider code snippets, shows at the
extreme end the expressive ability of just 140 characters.
Using that same programming language in a longer form,
Magnusson [2] is able to live code complex, beautiful musi-
cal pieces in real time. Furthermore, the LOLC collabora-
tive textual performance environment [3] shows the dynamic
performative musical collaboration that expert music tech-
285
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
nologists can have together with powerful tools like these.
We realize that the simpliﬁed language syntax of MMODM
could never surpass the expressiveness and complexity of
these programming languages but we seek to make it ro-
bust enough that the loss in complexity is worth the gains
in accessibility to new musicians.
There have been other web-based collaborative music apps
done both before and after Twitter’s inception. One of
the ﬁrst was Eric Metois’ collaborative sequencer, Spinning
Disks, that he created for the MIT Media Lab’s Brain Opera
[4]. It allowed users to enable samples on rotating disks, vi-
sually, in an interactive client-side web applet using their
mouse. Another example is Jorge Herrera’s Horgie, a web-
based Networked Music Performance tool that synthesizes
multiple remote users’ input on a centralized server through
the Flash plugin [5]. On the mobile side, Nathan Weitzner’s
massMobile [6] enables audience members using mobile de-
vices to draw the score collaboratively, in real time, for a
live jazz ensemble. These examples are powerful tools, but,
because they rely on a singular input & output method for
each remote contributor controlling a synchronized musical
space [7], they solve the distance limitations of the physical
world but still exhibit some of the limitations in the scale
and speed of participation. Twitter’s ubiquity, latency, and
diverse input device compatibility truly facilitate the ability
to push collaboration to the next level. It’s ability to ﬂexi-
bly work as a communication medium between communities
of 10 to 10 million people is truly unique.
That said, works utilizing Twitter itself for music cre-
ation are also plentiful. A great example is The Listening
Machine by Daniel Jones and Peter Gregson [8]. It is an
automated system that generates a continuous piece of mu-
sic based on the activity of 500 Twitter users around the
United Kingdom. Their conversations, thoughts and feel-
ings are translated into musical patterns in real time, which
you can tune in to at any point through any web-connected
device. However, while it and others served as solid in-
spiration for us during the creation of MMODM, they do
not allow for real-time, intentional collaboration, and in-
stead generate music incidentally from the interactions on
the site. We propose that by giving users more direct, in-
tentional, and active control through Twitter, we open up
the possibilities for the type of music that can be created.
2. INTERACTION DESIGN
Because our multi-part system has asymmetric participa-
tion roles, we faced many issues and nuances in delegating
tasks between site users and twitter users, in order to create
a balanced dynamic between creator, editor, and audience.
We outline below the user-facing features that ﬂowered into
our core set enabling sequence creation, closer collaboration
& sharing, playback controls, ﬁlters & e↵ects, and sequence
modiﬁcation.
Figure 2: Flowing color bar at the top of the page
corresponding to new notes from the stream
The limitations of various browser rendering engines im-
posed visual and graphical constraints on what we were able
to do visually. However, our small design details like the
ﬂowing color bar and pulsing colored squares representing
the notes help to create the feeling of a more dynamic, col-
laborative experience using simple shapes and a simple color
palette.
2.1 Sequence Creation
At the heart of MMODM is sequence creation. Twitter
users anywhere can express 16-beat note sequences across
26 di↵erent instruments using plain text tweets on their fa-
vorite device, in real-time. To deﬁne a sequence, use square
brackets [ ], a string of 16 letters A through Z correspond-
ing to various pre-selected instruments, and add the hashtag
#mmodm. There are 16 beats total in a loop and a “-” is
used to deﬁne a rest. For example:
ri↵ on this [a–a–a–abc–cc] with me on #mmodm
2.2 Sharing & Collaboration
Users can share the current state of their local session on
the site through the share button in the top left corner.
Clicking it locks the notes in place and generates a link (ex:
mmodm.co/sm/ezwg8u5o). Sharing that link allows other
users to both listen to the sequence as well as tweak it and
build on top of it.
Figure 3: Interface popup used to create a hashtag-
exclusive room to jam
When there are a lot of users using the #mmodm hashtag,
it can be tougher to have a more intimate jam session with
a few friends locally or remotely. By clicking on the rooms
button (Figure 3), users can create a separate musical space
requiring both the #mmodm hashtag as well as a hashtag
of their choosing such as #spectre.
2.3 Playback & Tempo
Standard playback support of pause, play, and stop are in-
cluded in order to aid in timing in performances and with
other instruments.
Figure 4: Buttons used to control pause, play, loop-
ing, and tempo
Some interesting, looping e↵ects can be achieved using
the restart button (Figure 4) that restarts playback at the
beginnign of the loop without interruption. Lastly, the
tempo is also modiﬁable on a local level and can be com-
bined with the e↵ects (such as beat skipping) to create more
interesting and complex sounds.
2.4 Filters & Effects
While many types of ﬁlters and e↵ects were considered, a
brief survey of what musicians actually used and what ef-
fects were most commonly found in other types of drum
machines led us to incorporate a small but important sub-
set of ﬁlters and e↵ects [9]. We added high & low pass
ﬁlters, stuttering, and beat skipping (Figure 5).
286
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
Figure 6: Flow diagram describing in detail the technical system architecture design
Figure 5: Buttons used to select between various
ratios of stutter timing and beat skipping e↵ects
The stuttering operates on a two-note bu↵er loop, with
options to loop on-tempo, 2x, 4x, and 8x. The “gater” e↵ect
skips playing either every-other beat or every-two beats.
Furthermore, users can more expertly employ these ef-
fects by using the keyboard shortcuts (0-9) to speedily en-
able and layer.
2.5 Sequence Modiﬁcation
Nearly as important to the sequence creation functions is
modiﬁcation through the column locking and sequence clear-
ing features. (Figure 7)
Figure 7: Clicking on a column locks those beats in
place with a line, preventing them from fading out
By clicking various columns with their mouse, users can
prevent changes to that note column and preserve instrument-
speciﬁc sequences that are working well. This prevents the
natural fading of notes over time, stops the clear button
from removing those notes, and ignores newly tweeted notes
of that speciﬁc instrument. It’s through well-considered use
of this feature that enables users to locally shape the chaos
of collaboration into beautiful arrangements.
3. ARCHITECTURE
Unlike many other common web applications, MMODM re-
lies almost as equally on the back-end server architecture
as it does on the client-side javascript code running in the
user’s browser to create the full musical experience. This
is necessary in order to avoid extraneous streaming connec-
tion requests to Twitter while also running the core timing
loops locally to ensure speed, accuracy, and latency while
jamming.
.
3.1 Server Architecture
The back-end architecture, as shown in Figure 6, relies on
cutting-edge technologies to serve thousands of incoming
requests. At its foundation lies Varnish-Cache, a web ap-
plication accelerator, running on port 80 to reverse-proxy
incoming requests to the Node.js server cluster. The Node
server cluster runs the master and slave processes. The
master cluster’s main job is to keep slave processes up and
running. It also, watches over the root hashtag (#mmodm)
via Twitter’s stream API, i.e., the master process will cap-
ture, process and store any tweet having this hashtag. The
incoming tweets are checked by the master process using
regular expressions to ensure that they ﬁt MMODM’s text-
to-beat constraints outlined in the interface design section.
If passed, these tweets are stored in a MongoDB database
running on port 27017 before being broadcasted to con-
nected peers via Websockets.
The slave processes spawn the core application layer run-
ning Express.js, a lightweight and modular Node.js web
framework, and Socket.io enabling real-time bidirectional
event-based communication. The Express.js http server
runs on port 8080 for production and defaults to port 3000
for development or testing. Finally, on port 6379 Redis
server, a key-value memory storage, uniﬁes the memory pool
used across the master and slaves processes enabling a hor-
izontally scalable engine. The Express.js application layer
follows a classical MVC design pattern [10] where Jade, an
287
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
HTML pre-processor and template engine, is used to or-
ganize views. Mongoose is then used to provide a simple
object document model (ODM) for MongoDB [11]. Errors
are handled by both master and slave processes: errors com-
ing from Twitter’s Stream API are handled and logged by
the master process whereas app-routing errors are handled
by slave processes.PM2 ,a production process manager for
Node.js applications, is used to keep our web-app up and
running by reloading it without downtime. It also facilitates
common system admin tasks like logging and scaling.
The full server-side software stack includes:
• Varnish-Cache 4.0
• MongoDB 2.6.5
• Redis 2.8.19
• Node.js 0.10.36
• Cluster 0.10.36
• Express.js 3.3.6
• Socket.io 1.2.1
• Jade 1.9.1
• Mongoose 3.8.22
• PM2 0.10.12
3.2 Client-side Architecture
The client-side architecture is only compatible with mod-
ern browsers that support both Websockets and the web-
audio API. The main Javascript ﬁle connects to our back-
end server to update the browser’s data object model locally
as new Tweets ﬂow across the drum machine as well as to
pull the past few tweets from the cache. When incoming
messages update the DOM, sound is generated by trigger-
ing each audio ﬁle that corresponds to the letter in that
sequence as the interval-timed script loops through the el-
ements. New notes decay after 30 seconds if not locked or
newly tweeted as described in the interaction design section.
While sequences are distributed globally, timing and ef-
fects are local to a session in order to enable more controlled
performance. Low and high pass ﬁlters are implemented on
a per-note basis using web-audio’s core ﬁlters functions.
Lastly, by using the live document itself as the primary
local data structure, we open the opportunity for simple-to-
implement user-based hacks and tweaks to the sound and
functionality of the site through CSS stylesheets applied
by browser extensions such as Greasemonkey or Google
Chrome’s drawer Console. For example: a holiday theme
with a bright red background and bell notes.
4. CONCLUSION
Through a live and launched online application of MMODM
at http://mmodm.co/, we have created a new tool for syn-
chronous musical collaboration between users locally and
remotely over Twitter.
By harnessing this powerful network, we’re able to take
advantage of its ﬂexibility and simplicity. Our hope is that
MMODM is able to ﬂexibly scale from 5 users to 5 mil-
lion both musically and technically, across a wide variety of
computing devices, and across a variety of cultures in real-
time. We see it being used in a myriad of scenarios from
passerby interacting with an installation via cellphones, to
a medium-scale performance in an amphitheatre using both
remote and local audience input as a layer in their compo-
sition, to large groups of asynchronous strangers weaving a
complex mesh of beats together purely online. Using feed-
back from the launch, multiple live performances, and some
viral attention, we’re already planning the next iteration of
the software that will alleviate some of the usability and
accessibility issues as well as make the performative aspects
even more engaging.
4.1 Evaluation
Since the initial launch, we’ve performed multiple early stages
of case studies. Our initial test case, a small group of stu-
dents in a classroom setting, revealed some inconsistencies
with the interface and, more chronically, issues with our
ﬁrst set of instrument samples.
Further exposure and feedback through a post on Hacker
News helped us to iterate and create a much more pleasant
and engaging sonic experience by reworking the mixing, the
samples themselves, and the way notes are birthed and aged.
Finally, through a live on-stage performance to an audience
of around 100 people, we identiﬁed new features that we
intend to implement in the near future.
4.2 Future Work
We have already begun security improvements by signiﬁ-
cantly modifying our Twitter oAuth feature to be much less
aggressive over permissions. We also plan to address some
usability and predictability concerns by allowing users to
preview what a note sequence might sound like before send-
ing the tweet. Lastly, we plan to implement much more
robust ﬁltering features through a re-write of the sound
engine. This will enable us to add e↵ects to the overall
sequence rather than the current note-by-note basis.
5. REFERENCES
[1] Dan Stowell. Supercollider sc140.
http://supercollider.sourceforge.net/sc140/,
October 2009. (Visited on 04/02/2015).
[2] Thor Magnusson. ixi lang: a supercollider parasite for
live coding.Proceedings of International Computer
Music Conference, pages 503–506, 2011.
[3] Jason Freeman, Sang Won Lee, Shannon Yao, and
Aaron Albin. Lolc for laptop music ensemble. In
Proceedings of the 8th ACM Conference on Creativity
and Cognition, C&#38;C ’11, pages 433–434, New
York, NY, USA, 2011. ACM.
[4] Joseph A. Paradiso. Electronic music: New ways to
play.IEEE Spectr., 34(12):18–30, December 1997.
[5] Jorge Herrera. The horgie: Collaborative online
synthesizer, 2009.
[6] Nathan Weitzner, Jason Freeman, Yan-Ling Chen,
and Stephen Garrett. massmobile: towards a ﬂexible
framework for large-scale participatory collaborations
in live performances.Organised Sound,1 8 : 3 0 – 4 2 ,4
2013.
[7] ´Alvaro Barbosa. Displaced soundscapes: A survey of
network systems for music and sonic art creation.
Leonardo Music Journal,1 3 : 5 3 – 5 9 ,2 0 0 3 .
[8] Peter Gregson Daniel Jones. The listening machine.
http://thelisteningmachine.org/.( V i s i t e do n
01/12/2015).
[9] Alexis Kirke and Eduardo Reck Miranda. A survey of
computer systems for expressive music performance.
ACM Comput. Surv., 42(1):3:1–3:41, December 2009.
[10] Trygve M. H. Reenskaug. Mvc, xerox parc 1978-79.
http://heim.ifi.uio.no/~trygver/themes/mvc/
mvc-index.html.( V i s i t e do n0 1 / 1 9 / 2 0 1 5 ) .
[11] Heroku. Object modeling in node.js with mongoose|
heroku dev center.https://devcenter.heroku.com/
articles/nodejs-mongoose,N o v e m b e r2 0 1 4 .( V i s i t e d
on 01/19/2015).
288
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 