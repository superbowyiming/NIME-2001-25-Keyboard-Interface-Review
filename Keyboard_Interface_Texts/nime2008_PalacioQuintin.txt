Eight Years of Practice on the Hyper-Flute:
Technological and Musical Perspectives
Cl´eo Palacio-Quintin
LIAM - Universit´e de Montr´eal - Montreal, QC, Canada
IDMIL - Input Devices and Music Interaction Laboratory
CIRMMT - Centre for Interdisciplinary Research in Music Media and Technology
McGill University - Montreal, QC, Canada
cleo.palacio-quintin@umontreal.ca
ABSTRACT
After eight years of practice on the ﬁrst hyper-ﬂute proto-
type (a ﬂute extended with sensors), this article presents
a retrospective of its instrumental practice and the new
developments planned from both technological and musi-
cal perspectives. Design, performance skills, and mapping
strategies are discussed, as well as interactive composition
and improvisation.
Keywords
hyper-instruments, hyper-ﬂute, sensors, gestural control,
mapping, interactive music, composition, improvisation
1. INTRODUCTION
Since 1999, I have been performing on the hyper-ﬂute [13].
Interfaced to a computer by means of electronic sensors and
Max-MSP software, the extended ﬂute enables me to di-
rectly control the digital processing parameters as they af-
fect the ﬂute’s sound while performing and allows me to
compose unusual electroacoustic soundscapes.
Until now, I mostly used the hyper-ﬂute to perform im-
provised music. Wishing to expand a repertoire for the
hyper-ﬂute, I began doctoral studies in January 2007 to
work on written compositions. Before developing a core
repertoire, I decided to review my experience with the in-
strument.
This article presents the original design of the hyper-ﬂute
and the learning experience of eight years of practice on it.
The performance skills and mapping strategies developed
over time now suggest new enhancements of the instrument.
Technological and musical issues in the development of a
new prototype of the hyper-ﬂute as well as a hyper-bass-
ﬂute will be discussed.
2. BACKGROUND
2.1 Why, Where and When
By the end of my studies in contemporary ﬂute perfor-
mance (Universit´e de Montr´eal – 1997), I was heavily in-
volved in improvised music and had started looking for new
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME08, Genova, Italy
Copyright 2008 Copyright remains with the author(s).
Figure 1: The hyper-ﬂute played by Cl´eo Palacio-
Quintin. Photograph by Carl Valiquet.
sonorities for the ﬂute in my own compositions. Already
familiar with electroacoustic music and with the use of the
computer, it was an obvious step to get into playing ﬂute
with live electronics. My goal was to keep the acoustic rich-
ness of the ﬂute and my way of playing it. The computer
would then become a virtual extension of the instrument.
During post-graduate studies in Amsterdam, I had the
chance to meet the experienced instrument designer Bert
Bongers [3]. In 1999, I participated in theInteractive Elec-
tronic Music Composition/Performance course with him
and themeta-trumpeter Jonathan Impett [9] at the Darting-
ton International Summer School of Music (U.K.). There,
I made my ﬁrst attempt at putting several sensors on my
ﬂute, programming a Max interface, and performing mu-
sic with it. Several months later, I registered as a student
at the Institute of Sonology in The Hague (The Nether-
lands) in order to build my hyper-ﬂute. The prototype of
the hyper-ﬂute was mainly built during the Fall of 1999
with the help of Lex van den Broek. Bert Bongers was a
valuable consultant for the design. He also made the main
connector from the sensors to the Microlab interface.
2.2 Original Design
2.2.1 Interface
The Microlab is an electronic interface that converts the
voltage variations from various analog sensors (between 0
and 5 volts) into standard MIDI data. It oﬀers 32 ana-
log inputs, a keyboard matrix of 16 keys and an integrated
ultrasonic distance measuring device. This interface was
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
293
Table 1: Sensors installed on the hyper-ﬂute
Sensors Parameter
1 Ultrasound sensors ﬂute’s distance to computer
3 Pressure sensors (FSRs) pressure: left hand and thumbs
2 Magnetic ﬁeld sensors motion of G# and low C# keys
1 Light-dependent resistor ambient light
2 Mercury tilt switches tilt and rotation of the ﬂute
6 Button switches discrete cues
originally designed and developed by J. Scherpenisse and
A.J. van den Broek at the Institute of Sonology. As a stu-
dent there, I had access to the schematics and was able to
build it myself.
2.2.2 Sensors
There is little free space to put hardware on a ﬂute be-
cause of the complexity and small size of its key mechanism.
Nevertheless it was possible to install sensors at speciﬁc
strategic locations. Table 1 shows an overview of all the
sensors installed on the hyper-ﬂute.
Inspired by Jonathan Impett’s meta-trumpet, I chose to
put diﬀerent types of electronic sensors on my ﬂute.“As
far as possible, this is implemented without compromising
the richness of the instrument and its technique, or adding
extraneous techniques for the performer – most of the ac-
tions already form part of conventional performance.”(page
148) [9]
The most important energy captors are proprioceptive
sensors. These directly relate to instrumental playing. A
performer is always aware of the action of her muscles on
the instrument and her physical position. Of course a well
trained musician is not really concious of these parame-
ters while performing. They become unconscious gestures
though always under her control. To collect gestural data,
a number of proprioceptive sensors have been installed on
the ﬂute.
Several analog sensors send continuous voltage variations
to the Microlab which converts them into MIDI Continu-
ous Controller messages. Ultrasound transducers are used
to track the distance of the ﬂute from the computer. An
ultrasonic pulsed signal is sent by a transmitter attached to
the computer, and is captured by the receiver attached to
the ﬂute’s footjoint. The Microlab calculates the distance
based on the speed of sound. Pressure sensors (Force Sens-
ing Resistors) are installed on the principal holding points
of the ﬂute (under the left hand and the two thumbs). Two
magnetic ﬁeld sensors (Hall Eﬀect) give the exact position
of the G# and low C# keys, both operated by the little
ﬁngers. A light dependent resistor is positionned on the
headjoint of the ﬂute. This photoresistor detects the varia-
tions of ambient light.
Other controllers used on the hyper-ﬂute send discrete
values : on/o ﬀ Midi Note messages. Two mercury tilt
switches are activated by the inclination (moving the footjoint
up) and the rotation (turning the headjoint outwards) of the
instrument. There are also six little button switches which
can also be considered pressure sensors, but which send two
discrete values (on/oﬀ) instead of continuous mesurements.
Two of them are located on the headjoint, and two are
placed close to each of the thumbs and can be reached while
playing.
3. LEARNING EXPERIENCE
When I built the hyper-ﬂute, I had little knowledge about
augmented instruments, and hardly any experience with
human-computer interaction. Several choices of design were
thus made because of technical considerations. Some of
these choices were arbitrary and made without overt musi-
cal considerations. However, most decisions turned out to
be quite pertinent. I will discuss design details and the use
of sensors in relationship with the physicality of ﬂute play-
ing. Finally, I will present some of my ideas on performance
skills and mapping strategies developed over the years.
3.1 Design & Sensors
When designing the hyper-ﬂute some sensors were chosen
simply because they were available. I just had to ﬁnd a
place to put them on the ﬂute. This was the case for the
ultrasound transducer and the light sensor. I also studied
the free space available on the instrument and looked for
what sort of sensor I could put there. Since the G# and
low C# keys are the only levers on the ﬂute with space
available under them, I installed the magnet sensors in those
two places.
Because it does not compromise the natural movements
of the ﬁngers and hands for instrumental playing, the ul-
trasonic range ﬁnder integrated into the Microlab interface
turned out to be one of the most useful controllers. The
same beneﬁts comes from the tilt switches which are acti-
vated without any interaction of the ﬁngers.
As there is no movement involved, pressure sensors (FSR)
are considered isometric. These sensors only capture muscle
tension. This made it easier to get used to performing with
them. A large FSR is installed under the left hand, which
holds the ﬂute by pressing it towards the chin. There is
a constant contact and a continual variation of pressure
on this point of the instrument while playing, though the
pressure is quite controllable.
Under the left thumb, a small FSR is placed on the B key.
As this key is used to play, it moves often and is sometimes
completely released. This limits the control of the sensor.
A third FSR is located under the right thumb holding the
ﬂute. There is a constant variation of the pressure on the
three sensors depending on what ﬁngering is being played
and how the instrument’s balance is kept (for example: if a
thumb if lifted, the two other holding points will get more
of the weight of the instrument). These pressure sensors
cannot be controlled without interacting with the playing
but they do not interfere with the normal motion of the
ﬁngers and hands. They capture natural gestures related
to the musical content performed.
The pressure sensors also interact directly with the but-
ton switches. Four of them are located close to the thumbs
and can be reached while playing. The respective thumb’s
pressure sensor is thus released when a button is used. The
left thumb cannot reach buttons without compromising the
ﬁngering, while the right thumb is freer. Like the two mer-
cury tilt switches, those buttons turned out to be very prac-
tical, even essential, to activate/desactivate various com-
puter processes and to scroll through menus during perfor-
mances. Two extra button switches, not easily reachable
while playing, are located next to the headjoint. In order
to perform without touching the computer, those switches
are often used to start and end a piece.
The magnet sensors give the exact position of the lever of
the G# and low C# keys. The small distance of the action
of the key is precisely mesured in 95 steps. It is possible to
play with the motion range of the keys and make diﬀerent
curves for the midi output with quite accurate control. This
is not a standard technique on the ﬂute and it aﬀects the
acoustics of the instrument.
Because it happened to be around at the time, a light
sensor was installed on the instrument. I expected to use
it with stage lighting. However, staging with a lighting rig
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
294
is quite uncommun when performing improvised electronic
music. I have used it only once in 8 years. Realistically,
I cannot control the ambient light myself, so this sensor is
not really relevant.
Over the years, the entire design of the hyper-ﬂute proved
to be quite robust. Everything still works as well as on the
ﬁrst day. The force sensing resistors need to be replaced
(more or less every 2 years) but all the other parts are still
the original ones. The Microlab interface is also very sta-
ble and reliable. Even as the MIDI protocol is becoming
obsolete and slow compared to new standards, the stability
of the interface has been a good help in developing perfor-
mance skills for the long term.
3.2 Performance Skills
The detailed physical control required to perform on tra-
ditional acoustic instruments takes time to learn. I spent
more than 15 years developing my instrumental skills. While
playing an acoustic instrument, all performers receive me-
chanical feedback cues via a variety of physiological and
perceptual signals. Haptic sensations include tactile and ki-
naesthetic perception. Kinaesthetic perception is the aware-
ness of the body state, including position, velocity and
forces supplied by the muscles. The auditory feedback is ob-
viously very important but the physical sensation of playing
comes before the perception of the sound.
While extending my ﬂute sound with computer process-
ing, I wanted to keep the same subtle control. It was obvi-
ous that I should use my already reﬁned instrumental skills
in order to control the sound processing parameters. How-
ever, in order to perform proﬁciently on the hyper-ﬂute,
many extra techniques needed to be developed.
Earlier I mentioned that the ultrasonic device and the tilt
switches were very useful because they do not compromise
natural movements. However, the movements they capture
are not normally necessary for ﬂute playing. The performer
is not trained to consciously notice them. But once these
sensors were linked to sound processing parameters, it was
very diﬃcult not to activate something without meaning to.
I had to learn to play completely motionless (which is very
unnatural for a performer) in order to attain the necessary
control.
In the case of the pressure sensors, they always react ac-
cording to the ﬁngerings played. It is almost impossible
to keep them completely stable, but they are very ﬂexible
and the motion of pressing them is natural. The maximum
values are reachable only with extreme pressure which does
not occur in normal playing although it can be used ex-
pressively. The process of learning to use those sensors has
not been too diﬃcult, as they are normal playing gestures
simply needing, at times, to be exaggerated.
The control of the little ﬁngers’ magnetic sensors was
much more diﬃcult to learn. Flutists are trained to push or
lift a key very fast as opposed to moving it slowly within its
motion range. After hours of practice, I trained my little
ﬁngers and can now control those sensors quite accurately.
Performing with some of the sensors installed on the hyper-
ﬂute was not always compatible with standard ﬂute tech-
nique and entailed a long learning process. Playing an ex-
tended instrument requires a new way of performing. This
should be kept in mind by designers of new interfaces. Few
performers are willing to put a large amount of energy and
time into learning to perform on a new instrument.
Experience showed me how much the interaction between
acoustic playing techniques and the motion captured by the
sensors is intimately connected. Musical gestures need to
be thought of as a whole. You cannot simply ask a ﬂutist
to play normally and add extra motions to be captured by
Figure 2: Example of multiparametric mapping of
inputs and parameters to control the acoustic ﬂute
sound
the sensors. All gestures need to be integrated in order to
achieve expressive performances.
Just like learning an acoustic instrument, it is necessary
to play on an electroacoustic instrument for a long period
of time before achieving a natural control of the sound. As
on any musical instrument, expressivity is directly linked to
virtuosity [7]. But in order for this to happen on the elec-
troacoustic instrument, the mappings of gesture to sound
must also remain stable.
3.3 Mapping Strategies
My ﬁrst attempts at controlling sound processing param-
eters with the hyper-ﬂute were made by directly coupling
each sensor to a speciﬁc parameter of sound processing.
This simpledirect mapping approach was soon changed. It
is almost impossible for a performer to think about many
diﬀerent parameters, each controlled separately but simul-
taneously. It implies an analytical cognitive mode of think-
ing which is confusing for human beings while performing a
complex task. Thinking in sequential order is very hard for
a player who is already busy playing an acoustic instrument.
Axel Mulder came to the same conclusion using a body-
suit with sensors, and trying to map each joint of the body
to control a single synthesis parameter.“This mapping ap-
peared to be very diﬃcult to learn. First of all, human move-
ments often involve the simultaneous movement of multiple
limbs. So, when the intent was to change one or more spe-
ciﬁc parameter(s), often other synthesis parameters were
co-articulated, i.e. also changed unintentionnaly.” (page
325) [12]
Researchers Hunt and Kirk have done experimental work
to compare diﬀerent types of interface mapping for real-time
musical control tasks. This research revealed that“complex
tasks may need complex interfaces”(page 254)[8], so the use
of a multiparametric interface seems to be the best choice on
the long-term in order to develop an interesting interactive
system. The holistic mode of thinking involves looking at
a perceived object as a whole. It relates to spatial thinking
and is much more appropriate for multi-dimensional gestu-
ral control.
An acoustic instrument is played in such a multipara-
metric way. “The resulting mapping of input parameters
to sound parameters in a traditional acoustic instrument
resembles a web of interconnections.” (page 235) [8] As il-
lustrated in Figure 2, the air pressure blown into a ﬂute,
which contributes to the pitch, also has an eﬀect on the am-
plitude and timbre of the sound. The pitch is also aﬀected
by other inputs (ﬁngerings, lip position). Each parameter
of the sound is aﬀected by diﬀerent inputs simultaneously.
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
295
Combinations of convergent and divergent mappings are
always experienced while playing an acoustic instrument. It
seems much more appropriate to control complex sound pro-
cessing parameters according to the same principles. These
highly non-linear mappings take substantial time to learn,
but further practice improves control intimacy and compe-
tence of operation.
Diﬀerent sound processing methods demand diﬀerent ways
of controling them. Mappings must be adapted for each
speciﬁc situation, and a lot of ﬁne tuning is necessary. I
experimented with diﬀerent combinations of direct, conver-
gent and divergent mapping, some being more suitable to
control speciﬁc sound processing patches. As my software
evolves for each new piece, no deﬁnite mapping is possible.
However, I try to keep as much consistency as possible in
the use of sensors, so that the precision of the control is
maintained for each performance.
4. INTERACTIVE COMPOSITION,
IMPROVISATION & PERFORMANCE
Joel Chadabe is one of the pionneers of real-time com-
puter music systems. In 1983, he proposed a new method
of composition called interactive composing, which he de-
ﬁned in the following terms: “An interactive composing
system operates as an intelligent instrument – intelligent
in the sense that it responds to a performer in a complex,
not entirely predictable way, adding information to what
a performer speciﬁes and providing cues to the performer
for further actions. The performer, in other words, shares
control of the music with information that is automatically
generated by the computer, and that information contains
unpredictable elements to which the performer reacts while
performing. The computer responds to the performer and
the performer reacts to the computer, and the music takes
its form through that mutually inﬂuencial, interactive rela-
tionship.” (page 144) [5]
From this point of view, the performer also becomes an
improviser, structuring his way of playing according to what
he hears and feels while interacting with the computer.
In most cases, users of interactive computer systems are
at once composer, performer and improviser. Due mostly
to the novelty of the technology, few experimental hyper-
instruments are built by artists. These artists mostly use
the instruments themselves. There is no standardized hyper-
instrument yet for which a composer could write. It is
diﬃcult to draw the line between the composer and the
performer while using such systems. The majority of per-
formers using such instruments are concerned with impro-
visation, as a way of making musical expression as free as
possible. Jonathan Impett also thinks that the use of com-
puters to create real-time music has profoundly changed the
traditional kinds of music practices.“In such a mode of pro-
duction, the subdivisions of conventional music are folded
together: composer, composition, performer, performance,
instrument and environment. Subject becomes object, ma-
terial becomes process.”(page 24) [10]
Using an interactive computer system, the performer has
to develop a relation with diﬀerent types of electroacoustic
sound objects and structures. These relationships consti-
tute the fundamentals of musical interaction. The computer
part can be supportive, accompanying, antagonistic, alien-
ated, contrasting, responsive, developmental, extended, etc.
All the musical structures included in a piece have diﬀerent
roles. Some aﬀect the micro-structure of a musical perfor-
mance, others aﬀect the macro-structure and many are in
between. The interaction between the performer and these
musical structures vary. The structures can also support
diﬀerent levels of interactivity between each other. We can
divide these structures in 3 distinct types:
• sound processing transforming the acoustic sound,
• sound synthesis,
• pre-recorded sound material.
On the hyper-ﬂute, I have focused on the development of
the ﬁrst type: transforming the ﬂute sound with live digital
processing. However, when looking for new extended ﬂute
sonorities, the process also leads to the integration of sound
synthesis.
In an improvisational context, the interactive computer
environment is designed to maximize ﬂexibility in perfor-
mance. The environnement must give the opportunity to
generate, layer and route musical material within a ﬂexi-
ble structure, like an open form composition. Ideally, the
computer environment would give the same improvisational
freedom the performer has developed with his acoustic in-
strument. Each performer has his personal repertoire of
instrumental sounds and playing techniques from which he
can choose while performing. This sound palette can be
very wide, and switching from one type of sound to another
is done within milliseconds. Of course, any interactive ges-
tural interface has a limited number of controllers. The
sound processing patches can only generate the sounds that
have been programmed (even if they include some random
processings). The freedom of the performer is somewhat
limited by the computer’s environment.
My long term goal is to develop aninteractive sound pro-
cessing palette that is as rich and complex as my instru-
mental one. I want to improvise freely and to be able to
trigger many diﬀerent processes at anytime, and this with-
out disturbing my ﬂute playing. Though there are still pro-
gramming issues to be addressed before achieving an ideal
environment, I have always felt more limited by the number
of controllers and buttons on the hyper-ﬂute. This has led
me to new developments on the instrument itself.
5. NEW DEVELOPMENTS
After eight years of practice, I am now very comfortable
playing the hyper-ﬂute. I have also developed a very good
knowledge of my musical needs in order to control the live
electronics while performing. Over the years, I found what
works best and what is missing on the instrument. So I
decided to make a new prototype which will feature some
new sensors. As I also perform on the bass ﬂute, an hyper-
bass-ﬂute is in development. The following sections brieﬂy
presents the planned design of those new hyper-instruments.
5.1 Hyper-Flute
To maintain the playing expertise I have developed over
the years, most sensors used since 1999 will be used in the
same physical conﬁguration, but will include technical im-
provements (ultrasound transmitter, magnetic ﬁeld sensors
on the little ﬁngers, and force sensing resistors under the left
hand and thumbs). There will be several more buttons on
the new prototype, located close to the right thumb which
is more free while playing.
Earlier I mentionned the necessity to have more sensors
which do not disturb the hands and ﬁngers while playing.
The new prototype is thus designed with a two axis ac-
celerometer placed on the foot-joint of the instrument. This
accelerometer gives information about the position of the
ﬂute (inclination and tilt of the instrument) in a continu-
ous data stream instead of the simple on/oﬀ switches used
previously.
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
296
Figure 3: Accelerometer and ultrasound transducer
mounted on a Bo-Pep
The present proprioceptive sensors on the hyper-ﬂute give
information about muscle actions that are not visible to
the audience (except for the ultrasound sensor and the tilt
switches working with the inclination of the instrument).
The use of an accelerometer will give more multidimensional
data about movements and position which are visible by the
auditors. This will help to correlate the amount of activity
of the computer with the physical activity of the performer.
The amount of data produced by the accelerometer greatly
increases the possibilities of multiparametric mapping and
permits the development of more complex musical struc-
tures. This will be very helpful to increase the number of
tasks while playing. For example, one can use the inclina-
tion to scroll through long menus of diﬀerent sound process-
ing modules or to choose between severalbuﬀers to record
in. This way, only one button is necessary to trigger many
diﬀerent tasks. As I am already aware of the instrument’s
inclination while playing (because of the tilt switches), it
is now easier to remember the physical position at various
angles.
Fastening the sensors on the ﬂute has always been prob-
lematic. I own only one (expensive) ﬂute and I do not
wish to solder anything onto it. Therefor I have been using
double-sided tape to attach the sensors to the ﬂute. This
way, the sensors can be taken oﬀ when the instrument needs
to be cleaned or repaired. But this is a tedious exercise and
there is always a risk of breaking them. I am now trying to
build the sensors on clips that can easily be attached and
removed. This will make it easier to transform any ﬂute
into a hyper-ﬂute, and will eventually give opportunities to
other performers to play my music.
A ﬁrst test was to use aBo-Pep for the accelerometer
and ultrasound transducer (as showed on Figure 3). These
plastic hand supports for the ﬂute are simply clipped on the
body of the instrument, and can be taken on and oﬀ in a
second. Some sensors can simply be applied on aBo-Pep,
while others will need to use a custom made clip.
5.2 Hyper-Bass-Flute
I am also developing a hyper-bass-ﬂute, a noticeably dif-
ferent instrument than the hyper-ﬂute. The bass ﬂute has
the advantage of being much bigger so there is more space
to attach sensors. Nevertheless, the weight of the instru-
ment limits the capacity of the thumbs to reach diﬀerent
sensors while playing. The new design of the sensors needs
to be diﬀerent than the hyper-ﬂute. Only the accelerome-
ter and ultrasound transducer can be installed on the bass
ﬂute as on the ﬂute. Compositional strategies will need to
be adapted for this instrument and a new period of learn-
ing will be necessary to perform with it. Even if many
controllers will be diﬀerent, I expect the learning process to
be much faster due to my experience with the hyper-ﬂute.
5.3 Interface
For both hyper-ﬂutes, I will replace the Microlab device
with a new interface using the Open Sound Control proto-
col. “OSC is a protocol for communication among comput-
ers, sound synthesizers, and other multimedia devices that is
optimized for modern networking technology. Bringing the
beneﬁts of modern networking technology to the world of
electronic musical instruments, OSC’s advantages include
interoperability, accuracy, ﬂexibility, and enhanced organi-
zation and documentation.This simple yet powerful proto-
col provides everything needed for real-time control of sound
and other media processing while remaining ﬂexible and easy
to implement.” [2]
This protocol will allow the transmission of diﬀerent types
of parameters with more resolution and velocity. This will
be achieved with fewer intermediary interfaces and will be
much faster. Data will go directly from one interface to
the computer through a USB connection. Previously, the
Microlab was plugged to a MIDI Interface then to the com-
puter.
A new ultrasonic range ﬁnder is being implemented on a
PSoC chip by Avrum Holliger at IDMIL. It has a much more
reﬁned resolution than the one used on the Microlab, which
was limited to 128 values by the MIDI protocol. This new
range ﬁnder will be directly linked to the main interface.
For the bass ﬂute, it is possible to install the complete
interface on the instrument. The hyper-bass-ﬂute will be
connected to the computer with a single USB cable. A
prototype is now in development using a Arduino-mini in-
terface [1] which is small enough to ﬁt on the instrument.
Wireless connection is not desirable because of its need for
power. A 9 volt battery would be too heavy to install on
the ﬂute.
5.4 Mapping Research Project
For my doctoral project, my compositions will aim to
optimize the mappings of my extended instruments in the
context of new computer music pieces. My ﬁrst intention
when building the hyper-ﬂute was to use the natural ges-
tures of the ﬂutist to control sound processing parameters.
However, as stated above, I was obliged to develop new
playing techniques to control some of the sensors.
In the Performance skills section, I mention that the ul-
trasound transducer, pressure sensors and magnet sensors
continually capture the natural movement of a performer.
It is a similar situation with the new accelerometer. Those
gestures are directly related to the musical material being
performed.
With the new prototype of the hyper-ﬂute, more infor-
mation from the natural gestures of the performer will be
usable. I would like to use these gestures to control the
computer so that the performer will not need to add too
many extra movements. To achieve this, I will study the
gestural data captured by the new hyper-ﬂute (and hyper-
bass-ﬂute) [15].
Instrumental music material will be written ﬁrst, then
performed on the hyper-ﬂutes. The performer will play
without taking notice of the sensors. All the gestural data
will be recorded together with the ﬂute sound. I will then
be able to analyse the gestural data in a speciﬁc musical
context. This analysis will guide the choice of mappings
between the sensors and the computer’s live processing pa-
rameters. The use of sensors will be precisely speciﬁed in
a musical context and will be directly related to the per-
former’s natural gestures. This should allow a more subtle
and expressive control of the sound processing than is pos-
sible in an improvised music context.
To explore the diﬀerences of motion between performers,
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
297
I will record other ﬂutists as well as myself. I expect other
ﬂutists will move more naturally then myself, as I am used
to playing with the sensors which react to any movement I
make.
6. MUSICAL PERSPECTIVES
After 8 years of practice, I consider the hyper-ﬂute as a
musical instrument in its own right. New technologies oﬀer
opportunities to enhance it but even with these improve-
ments, it will stay the same instrument. In addition to the
development of my improvisational environment, I want to
compose more written repertoire. I also hope to have other
composers do so as well. My most sincere wish is that even-
tually other performers will play the hyper-ﬂute. The mu-
sical perspectives are open-ended for the hyper-ﬂute, truly
an e wi n s t r u m e n tf o rt h et w e n t y - ﬁ r s tc e n t u r y .
7. ACKNOWLEDGMENTS
I would like to thank Marcelo Wanderley for his invalu-
able advice in my research and all the IDMIL team for their
great technical help. Sincere thanks to Elin S¨oderstr¨om and
Jean Pich´ef o rt h e i rw r i t i n gh e l pf o rt h i sp a p e r . M yd o c -
toral studies are supported by the FQRSC (Fonds qu´eb´ecois
de la recherche sur la soci´et´e et la culture).
8. REFERENCES
[1] Arduino-Mini.
http://www.arduino.cc/en/Main/ArduinoBoardMini,
visited January 2008.
[2] Open Sound Control.
http://opensoundcontrol.org/introduction-osc, visited
January 2008.
[3] B. Bongers. Physical interfaces in the electronic arts.
interaction theory and interfacing techniques for
real-time performance. In M. Wanderley and
M. Battier, editors,Trends in Gestural Control of
Music. IRCAM - Centre Pompidou, Paris, 2000.
[4] M. Burtner. The metasaxophone: concept,
implementation, and mapping strategies for a new
computer music instrument.Organised Sound,
7:201–213, 2002.
[5] J. Chadabe. Interactive composing: An overview. In
C. Roads, editor,The Music Machine: selected
readings from Computer Music Journal, pages
143–148. MIT Press, Cambridge-London, 1989.
[6] R. Dean.Hyperimprovisation: Computer-Interractive
Sound Improvisations.A RE d i t i o n s ,M i d d l e t o n ,
Wisconsin, 2003.
[7] C. Dobrian and D. Koppelman. The ’e’ in nime:
Musical expression with new computer interfaces. In
N. Schnell, F. Bevilacqua, M. J. Lyons, and
A. Tanaka, editors,NIME, pages 277–282. IRCAM -
Centre Pompidou in collaboration with Sorbonne
University, 2006.
[8] A. Hunt and R. Kirk. Mapping strategies for musical
performance. In M. Wanderley and M. Battier,
editors, Trends in Gestural Control of Music. IRCAM
- Centre Pompidou, Paris, 2000.
[9] J. Impett. A meta-trumpet(er). InProceedings of the
International Computer Music Conference, pages
147–149, San Francisco, 1994. International Computer
Music Association.
[10] J. Impett. The identiﬁcation and transposition of
authentic instruments: Musical practice and
technology. Leonardo Music Journal, 8:21–26, 1998.
[11] E. Miranda and M. Wanderley.New Digital Musical
Instruments: Control And Interaction Beyond the
Keyboard. AR Editions, Middleton, Wisconsin, 2006.
[12] A. Mulder. Towards a choice of gestural constraints
for instrumental performers. In M. Wanderley and
M. Battier, editors,Trends in Gestural Control of
Music. IRCAM - Centre Pompidou, Paris, 2000.
[13] C. Palacio-Quintin. The hyper-ﬂute. In F. Thibault,
editor, NIME, pages 206–207. Faculty of Music,
McGill University, 2003.
[14] M. Waisvisz. The hands, a set of remote
midi-controllers. InProceedings of the International
Computer Music Conference, pages 313–318, San
Francisco, 1985. International Computer Music
Association.
[15] M. Wanderley. Quantitative analysis of non-obvious
performer gestures. InGesture and Sign Language in
Human-Computer Interaction: International Gesture
Workshop, pages 241–253, 2003.
Proceedings of the 2008 Conference on New Interfaces for Musical Expression (NIME08), Genova, Italy
298