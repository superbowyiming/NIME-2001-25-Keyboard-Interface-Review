NAKANISYNTH: An Intuitive Freehand Drawing Waveform
Synthesiser Application for iOS Devices
Kyosuke Nakanishi
Tokyo Metropolitan University
6-6 Asahigaoka Hino-shi
Tokyo, Japan 191-0065
kyoro920@gmail.com
Paul Haimes
Tokyo Metropolitan University
6-6 Asahigaoka Hino-shi
Tokyo, Japan 191-0065
haimes@tmu.ac.jp
Tetsuaki Baba
Tokyo Metropolitan University
6-6 Asahigaoka Hino-shi
Tokyo, Japan 191-0065
baba@tmu.ac.jp
Kumiko Kushiyama
Tokyo Metropolitan University
6-6 Asahigaoka Hino-shi
Tokyo, Japan 191-0065
kushi@tmu.ac.jp
ABSTRACT
NAKANISYNTH is a synthesiser application available on
iOS devices that provides a simple and intuitive interface,
allowing users to produce sound loops by freehand drawing
sound waves and envelope curves. The interface provides a
simple way of interacting: the only input required involves
drawing two waveforms, meaning that users can easily pro-
duce various sounds intuitively without the need for com-
plex manipulation. The application’s interface comprises of
an interchangeable ribbon and keyboard feature, plus two
panels where users can edit waveforms, allowing users to
make sounds. This simple approach to the interface means
that it is easy for users to understand the relationship be-
tween a waveform and the sound that it produces.
Author Keywords
waveforms, digital synthesis, iOS, multi-touch, freehand draw-
ing
ACM Classiﬁcation
H.5.5 [Information Interfaces and Presentation] Sound and
Music Computing, J.5 [Arts and Humanities] Music, H.1.2
[User/Machine Systems] Human Factors.
1. INTRODUCTION
Most musical synthesisers have several parameters for pro-
ducing sounds. These synthesisers can manipulate sounds
in intricate ways, but their interfaces tend to be complex
— particularly in more traditional modular systems, e.g.
[5]. Therefore, users who wish to obtain proﬁciency with
these instruments need to invest signiﬁcant amounts of time
to learn how to use these devices, which may be a barrier
to entry for those who wish to explore sounds creatively
without the cognitive burden of learning a complicated sys-
tem. In this context, we are exploring freehand drawing
waveforms as an alternative to the manipulation of several
parameters.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’16,July 11-15, 2016, Grifﬁth University, Brisbane, Australia.
.
2. RELATED WORK
Over the past decade, smart phone applications have been
created to allow users to generate sounds through simple in-
terfaces manipulable through a phone’s touchscreen. Bloom,
a collaboration between musician Brian Eno and software
designer Peter Chilvers, was one early example [1]. De-
scribed as a combination of an instrument, composition tool
and artwork, Bloom allows “anyone to create elaborate pat-
terns and unique melodies by simply tapping the screen”
[1]. Control is another more recent application that utilised
a simple smart phone interface for use in real-time perfor-
mance and composition [9]. The multi-touch screen tech-
nology of smart phones means that several parameters can
be accessed while simultaneously composing or performing,
allowing for multimodal input [11].
In the ﬁeld of new interfaces for musical expression, there
are precedents of creating sounds based on freehand drawn
waveforms.
Iannis Xenakis foresaw the use of hand-drawn interfaces
as a way of controlling parameters for producing sound with
synthesisers. Xenakis’ UPIC (Unit´ e Polyagogique Informa-
tique du CeMaMu) system, developed in the 1960s and 70s,
“pioneered the use of graphical interfaces for music com-
position by introducing freehand drawing to control sonic
events” [12]. In the UPIC system, drawings created on a
tabletop device were used to control parameters such as
pitch and dynamics [12]. Similar techniques were also used
in the Fairlight CMI (computer musical instrument) syn-
thesiser, which utilised a light-pen for drawing waveforms
[2].
There are recent works that use similar techniques of con-
verting a freehand drawing waveform into a sound. Wavetable
[10] and Graph-Sono [4] are both musical interfaces that en-
able users to draw waveforms.
Wavetable used a tabletop interface and utilised tangible
interaction and multi-touch to allow users to interact with
the system. The application included a number of tools
including editing tools (eraser, copy, paste and gain), ef-
fects (delay, reverb and bit-crush) and ﬁle tools (recording,
opening and saving) [10].
Meanwhile, the Graph-Sono system (Fig. 1) used a video
camera to detect lines drawn by users on a piece of paper
[4]. The camera was connected to an Apple Macbook laptop
running Max/MSP, which generated sine-wave sounds [4].
Although these systems used the drawing of wave shapes
to generate sounds, such systems may not be easily acces-
sible to novice users, nor musicians wishing to easily incor-
143
Figure 1: The Graph-Sono system [4] uses a video
camera to detect lines drawn by users on a piece
of paper. The camera is connected to an Apple
Macbook laptop running Max/MSP, which gener-
ates sine-wave sounds.
porate them into live performances. However, the ability of
smart phone applications to provide a simple interface to
users — particularly those with little or no formal musical
training — means some of the functionality of complex sys-
tems such as UPIC and Graph-Sono can now be delivered
via an easy to use multimodal interface.
Figure 2: The application’s interface showing the
sound wave drawing area on the left and the en-
velope curve drawing area on the right. The bot-
tom part of the screen shows the keyboard control,
which is loaded by default.
3. OUR SYSTEM
NAKANISYNTH is an iOS application developed with open-
Frameworks — an open source C++ framework — [7] and
Pure Data [8].
The application runs on iPad, iPod Touch and iPhone
devices, provided that they are running iOS 6.1 or later.
The upper half of the screen is used for drawing two wave-
forms and the lower half is used to play sounds using either
a keyboard interface (Fig. 2) or a ribbon interface (Fig. 3).
By creating this application, we have removed the need
for complex controllers from the interface to make it more
simple and intuitive for novice users. This simplicity also
means that users learn to understand the eﬀect that a wave-
form has on the sounds generated. In the following sections,
we describe aspects of the system in further detail. A video
overview of the system is also available online [6].
3.1 Freehand drawing waveforms
Users can draw a waveform of any length in any position
within the two waveform drawing areas. The waveform on
the left side of the screen is a sound wave. The waveform on
the right side is an envelope curve. In both of the waveform
drawing areas, the value of the vertical axis means ampli-
tude and the value of the horizontal axis means time. The
drawn waveform data is interpolated with linear interpola-
tion based on the sample rate and the maximum length of
wave data used within the application, which is stored in
an array. A maximum of ﬁve waves’ data can be saved.
The saved wave shapes are shown to the side of both wave
drawing areas (Shown in ﬁgures 2 and 3). A frequency is
generated by resampling the array. It is easy to observe
the diﬀerences in sounds created by the drawn waveforms
because users can easily edit and change waveforms while si-
multaneously making sounds. Data from the two waveforms
is processed by Pure Data patches. The main Pure Data
patch has two arrays: the ﬁrst is for a cycle of sound and
the second is for the envelope curve. A second Pure Data
patch calculates the output from the two waveforms, along
with the inputed MIDI note, frequency of the envelope and
velocity.
3.2 Playing Sounds
Users can make a maximum of 10 sounds simultaneously by
touching the sound generation area, which is represented as
either a standard keyboard (Fig. 2) or a ribbon controller
(Fig. 3), depending on which option the user selects, with
the keyboard being the default option. The frequency in-
creases from the left to the right of this area. Users can also
change the frequency range with a green slider above the
ribbon/keyboard controller (Shown below the wave draw-
ing area on the left in ﬁgures 2 and 3).
In the ribbon controller mode, the frequency changes con-
tinuously so that users can make a beat sound or vibrato.
The volume becomes louder from the bottom to the top of
the playing sound area, which users can also adjust man-
ually. The cycle of envelope loops is adjusted with a blue
slider (Shown below the wave drawing area on the right in
ﬁgures 2 and 3). Therefore, the user can make a looped
melody and modulate it by shortening the cycle time. In
order to make it easy to imagine the relationship between
the waveform and the generated sound, a visualisation of
the waveforms’ sound output is shown above the drawing
area.
4. EARLY RESULTS OF USER TESTING
Although we are yet to conduct formal user testing, we have
exhibited the application at several events in Japan. During
these exhibitions we conducted informal user testing, where
observations and feedback from users suggested that it is a
useful and enjoyable device to use. The majority of peo-
ple — including children — who used it enjoyed producing
sounds with it.
In 2015 the app was released in Apple’s iTunes App Store
[3]. The feedback obtained from users who downloaded the
app through the iTunes App Store has been generally pos-
itive, with the application averaging 4.5 stars out of 5.
5. DISCUSSION AND FUTURE WORK
144
Figure 3: The application’s interface with the rib-
bon controller option selected, which is shown in
the bottom half of the screen. The green circles
signify touch events, which increase in size towards
the top of the ribbon controller area demonstrate
the increase in volume.
In this work, we focused on the intelligibility of sound syn-
thesis, in the hope that users can recognise a link between
the waveforms they have drawn and the sound that they
have produced. By creating a system that relies on a plat-
form as ubiquitous as iOS devices, we hope to bring waveform-
drawing synthesis to a broader audience, as previous at-
tempts have generally relied on more complicated systems
that require additional hardware.
In future versions, we may add functions such as mod-
ulation and other eﬀects, but will aim to retain the intu-
itiveness and simplicity of the current version. We will also
conduct further user testing when these changes are made
to verify that it has retained its ease of use. It is hoped that
this application will become a viable instrument for music
composition and live performance.
6. CONCLUSION
By creating a simple interface and only minimal parameters
for users to work with, we have created a software synth that
is intuitive and easy for musicians of any ability to use.
By relying on a simple interface for producing sounds,
we have created a very low learning curve for users. It is
hoped that this simple approach will facilitate the creativity
of users — with a particular focus on novice users, serving
as a way of educating and promoting the use of digital syn-
thesisers.
7. REFERENCES
[1] B. Eno and P. Chilvers. Generative Music: Creative
apps for the iPad, iPhone and iPod touch, 2012.
http://www.generativemusic.com/.
[2] J. Grant. The Fairlight Explained, 1984.
http://www.virtual-music.at/download/
fairlight/fairlight_cmi_explained.pdf.
[3] iTunes. iTunes Preview: NAKANISYNTH, 2015.
https://itunes.apple.com/us/app/nakanisynth/
id976936118?mt=8.
[4] S. Matsumura. Graph-Sono - Hand Drawing Sound.
In Proceedings of the 18th International Conference
on Artiﬁcial Reality and Telexistence , pages 356–357,
2009.
[5] Moog. The Moog Modular, 2015.
http://www.moogmusic.com/content/
moog-modular-synthesizers.
[6] NAKANISYNTH, 2016. http://nakanisynth.com/.
[7] openFrameworks. openFrameworks is an open source
C++ toolkit for creative coding, 2015.
http://www.openframeworks.cc.
[8] Pure Data. Pure Data, 2015.
https://puredata.info/.
[9] C. Roberts, A. Forbes, and T. Hollerer. Enabling
Multimodal Mobile Interfaces for Interactive Musical
Performance. In Proceedings of NIME 13, May 27-30,
2013, KAIST, Daejeon, Korea. , 2013.
[10] G. Roma and A. Xambo. A tabletop waveform editor
for live performance. In Proceedings of New Interfaces
for Music Expression , pages 249–252. NIME, 2008.
[11] A. Tanaka. Mapping Out Instruments, Aﬀordances,
and Mobiles. In Proceedings of the 2010 Conference
on New Interfaces for Musical Expression (NIME
2010), Sydney, Australia. , 2010.
[12] J.-B. Thiebaut, P. G. T. Healey, and N. B. Kinns.
Drawing Electro-acoustic Music. In Proceedings of the
International Computer Music Conference(ICMC-08,
Belfast), 2008. http://quod.lib.umich.edu/cgi/p/
pod/dod-idx/drawing-electroacoustic-music.pdf.
145