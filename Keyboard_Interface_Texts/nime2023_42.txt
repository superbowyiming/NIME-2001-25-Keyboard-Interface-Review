Reembodied Sound and Transducer-actuated Instruments in Refraction Interlude    Matthew Goodheart Rensselaer Polytechnic Institute   110 8th St. Troy, NY 12180  goodhm@rpi.edu   ABSTRACT “Reembodied sound” refers to the electroacoustic practice of projecting sound into resonating objects, thereby turning these objects into a kind of speaker. The practice, which typically uses an audio transducer attached to the surface of the object being resonated, lies in a middle-ground between loudspeaker-based music and augmented/actuated instruments, allowing practitioners to draw upon and fuse multiple paradigms of new and emerging technologies. This article examines Refraction Interlude, an interactive environment for solo performer and transducer-actuated metal percussion instruments. Building on a decade of reembodied sound research, the work combines augmented and actuated instruments, physical modeling, pre-recorded performer input, interactivity, and sound spatialization in a manner that facilitates adaptability to performer creativity and to the acoustic properties of the actuated instruments. The computational processes were minimized, designed to forefront the interaction and integration between these multiple domains.   Author Keywords Actuated instruments, human computer interaction, audio transducers, improvisation, reembodied sound, metal percussion, instrument-adaptable  CCS Concepts • Applied computing → Sound and music computing; Performing arts;  • Human-centered computing → Human computer interaction (HCI) → Interaction devices → Sound-based input / output;  • Hardware → Communication hardware, interfaces and storge → Sensors and actuators  1. INTRODUCTION Refraction Interlude is an interactive environment for performer and four transducer-actuated metal percussion instruments. The system is composed of multiple components including instrument augmentation and hybridization through the application of audio transducers to the metal percussion, digital instrument design in the construction of sound based on instrumental acoustics, and human-computer interaction. The focus of the work is on the integration of these elements in a  1 Examples of similar recent works include David Coll’s Hazardous Materials #1 in which hand-held transducers are pressed against the surface of frame drums [8]; Rama Gottfried’s spark in which transducers are placed unattached on piano strings [9]; and Nina Young’s Heart.throb and Sabrina Schroder’s Flatspin (Stircrazer 4) in which the transducers are attached to the heads of drums [10,11]. 
 
manner that centers reembodied sound techniques, so that the acoustic properties of the particular metal percussion instruments used in its realization are foundational elements. The overall design is based on the principle of “just enough,” drawing on each element in a relatively simple manner in order to allow the interaction between them to create interest and unpredictability. These interactions are constructed around a view of the improviser influenced by groups like the AACM, European Free Improvisation, and the Creative Music Studio. Central to this view is the notion that the performer is a foundational creative agent, bringing their own idiosyncratic approaches, sounds, and aesthetics to the work. [1,2,3] Through the incorporation of instrumental acoustics, the metal percussion instruments become foundational agents as well. As a whole, the work speaks to interrelations and hybridizations of identities – those of the performer and the instruments, each integrating elements of the other and responding in kind.  For performance and recording considerations, the name of the performer’s instrument is added to the title of the piece. For example, if the work is performed by a clarinetist, the work will appear on the program as Refraction Interlude: clarinet. 2. AUDIO TRANSDUCERS 2.1 Terminology and Design David Tudor’s Rainforest installation series (1968-1973) is generally cited as the initiation of the use of audio transducers to actuate resonant objects in an artistic context [4,5]. Since the early 2000s, the advent of easily available consumer-grade transducers, billed for use in home theaters, car stereo systems, retail displays, and multiple other applications [6,7], has accelerated their use in composition and performance,1 installation, and instrument design [12].2 These transducers go by a variety of names, including “sound exciters” [6], “surface transducers” [16], “surface speakers” [7], “vibration speakers” [17], “resonance speaker” [18], “contact speakers” [19], “tactile transducers” [6], and “bass shakers,” [20] though these latter two terms are often reserved for transducers designed for low frequencies. I have found that most artists simply refer to them as “transducers” for shorthand, and so will use the terms “audio transducers” or “transducers” for the remainder of the article.  2 Transducers are used in the design of many actuated, augmented, and hybrid instruments such Tom Davis’s Feral Cello [13], Jiffer Harriman’s Feedback Lap Steel [14], and Seth Seth Thorn’s Hybrid Violin [15], as well as the commercially available Yamaha Transacoustic Guitar and Transacoustic Piano, and the Steingraeber Transducer Piano. 

 Audio transducers are a deconstructed loudspeaker – essentially a loudspeaker without a cone or housing. As their marketing suggests, they are designed without a specific purpose in mind. As such, they are an incomplete tool, leaving it up to the user to construct their purpose. 2.2 Reembodied Sound The term “reembodied sound” was first coined by the composer David Coll in 2012 [21]. It refers to a related set of transducer-based practices, and accompanying aesthetic interests, that had arisen among a group of students and researchers at the Center for New Music and Audio Technologies (CNMAT) at UC Berkeley.3 The term is not universally used: The Finnish composer Otso Lähdeoja refers to transducer-based practice as “materially mediated sound diffusion” [22].4 I favor Coll’s terminology for describing my own work, in that it foregrounds the evolving relations of sound production and transformation. The resonated object is framed as an active agent within a multi-stage process: initial sound production – disembodied separation of sound from its source – reembodiment and accompanied new sound production – apprehension by the listener. This chain is central to my own aesthetic concerns and forms the basis for Refraction Interlude. REFRACTION INTERLUDE 2.3 History Refraction Interlude is based on an earlier sound installation called Listening Gong. Dedicated to Pauline Oliveros, the work uses a single actuated tam-tam. Playback of samples from a lecture by Oliveros into the transducer is triggered by input from an omnidirectional microphone placed within the exhibit space.5  Refraction Interlude was composed in 2019, building on the framework of Listening Gong. It was included as part of the multimovement work entitled Presences: suite for five performers and nine instruments, performed by the Broken Ghost Consort. It has received numerous performances and seen a recorded release as part of Presences [23].    
  3 This group included composers such as Evelyn Ficarra, Rama Gottfried, John MacCallum, Jess Rowland, Alexis Emillianoff, Heather Frasch, and myself, and researchers such as Adrian Freed and David Wessel. 4 These two terms arose within a few years of each other (2012 and 2018 respectively), from geographically distant communities (Berkely and Helsinki) which do not seem to have overlapped. 
3. PERFORMANCE SET UP 3.1 Transducer placement Metal percussion are non-linear instruments [24,25]. There is a tremendous amount of variation in their manufacture even in instruments of the same design, including differences in materials, hammering processes, and additional factors which influence their response [26]. While the nodal points of vibrational modes can be mapped [27], this does not necessarily aid in optimum transducer placement. Though placing a transducer directly on a node can produce an energetic response, the resonance of the instrument is significantly dampened by its weight. Alternately, placing the transducer on the outer edge of a gong or near the bell of a cymbal allows for the greatest amount of surface area vibrate freely, while weakening the response. For the purposes of this work, a favorable mix between response and resonance can be found in a band running within 2 – 3 inches around the bell of a 16 to 24-inch cymbal, and in a band running approximately 1/4 – 2/3 of the distance between the rim and the center of a gong. Smaller gongs, typically 16 inches and under, may require placing the transducer closer to the rim.   
  Transducers speak most effectively when the size and weight is matched to the instrument. For instruments under 18 inches the Dayton DAEX19CTs with an RMS power rating of 5W and 4Ohm impedance are used. For larger instruments, the Dayton Audio DAEX25FHE-4 with a 24W RMS power rating and 4Ohm impedance are typically used. While these transducers come with adhesive, this is removed and replaced by 3M VHB tape, allowing for quick application and removal between performances. Dayton has recently released versions of both transducers that include an “interchangeable hardware mount,” which are also effective.  Transducers are mounted on the rear face of the gong, and cymbals are mounted upside-down on the stands. The cymbals are then angled so that the bottom faces the audience, creating a dramatic visual presentation. 3.2 STAGE SET UP The performance setup is relatively simple: the performer plays into a microphone. This signal is sent to a computer for processing and signal generation, and the resultant signals are 5 This installation premiered at the ImproTech 2017 Workshop-Festival at the University of Pennsylvania, accompanied by a performance. A second mounting as a stand-alone installation took place at the New York Transduction Festival, Vol. 2 in 2018 at the Computer Music Center at Columbia University. 
Figure 1. Georg Wissel performing “Refraction Interlude: clarinet” at Cuba Cultur, Münster, DE. June 14, 2019.  
Figure 2. Grey areas show generalized optimal transducer attachment areas on cymbals and gongs.  
sent out through an interface to set of amplifiers, and out to the transducers which actuate the metal percussion.   
  The positioning of the instruments in the performance space is adaptable: the performer is usually center stage, and the metal percussion instruments can be positioned around the performance space in any configuration. In this way, Refraction Interlude can be considered site-adaptable piece of spatial music.   
  4. PRE-PERFORMANCE PREPARATIONS 4.1 Performer generated content As a preparation, the performer is asked to record a set of sound files, short improvisations (<2min) based on guidelines given by the composer. These guidelines fall into four sound categories: percussive, noise, sustained, and an extended technique of their own choosing. The purpose of these guidelines is to give simple and intuitive instructions, allowing the performer to bring their own creative ideas into the work. As an example, in the realization by clarinetist Georg Wissel, percussive sounds were based on key clicks, noise sounds were based on breath and smacking sounds, sustained sound were traditional held notes in a variety of registers, and the open choice consisted of multiphonics and multiphonic trills. The total time of all recorded material was approximately 10 mins.   These sound files are then compiled into a corpus for analysis and (ultimately) processing and playback in performance. The corpus is divided into 150ms grains, and each indexed grain is analyzed for its spectral centroid. The results of the analysis are saved as a list in a dictionary.  6 https://www.gnu.org/software/octave/index 
4.2 Metal percussion analysis Each metal percussion instrument is analyzed for its partial content. This analysis process consists of attaching a surface exciter to the instrument, playing pink noise through the exciter, and recording the results. The results are then analyzed in GNU Octave6 using a program called mg_px.m written for me by John MacCallum.7 The program allows for adjustments to the threshold, peak width, and cutoff frequencies parameters and, with the proper settings, the frequencies and amplitudes of a large number of the instrument’s partials can be derived. The results are saved in a text file and a graphic representation of the spectral envelope is produced as well. Each instrument’s frequency/amplitude pairs are then converted to a list and stored with an instrument designated key in a json dictionary.  
 4.3 Instrument Adaptability By storing the instrumental analysis within a dictionary to be drawn upon during computational processes, rather than hard-coding the data, the process are not tied to specific metal percussion instruments. This allows the use of different instruments for different realizations while still generating instrument-specific sound. This design I have termed “instrument adaptability.” 5. LIVE PROCESSING The interactive portion of the work is programmed in Max. The interaction parameters were kept simple, utilizing a few basic processes to balance performer control and systemic independence. Three pre-performance preparations are required: 1) a corpus of samples created by the performer, 2) a dictionary containing a spectral centroid analysis of the grains within the sample corpus, 3) a dictionary containing partial frequency content for each of the metal percussion instruments. Each metal percussion instrument is represented in Max as an independent channel to facilitate individual processing. Input from the performer is analyzed for both its spectral centroid and onset timing. This analysis then governs the selection of grains from the sample corpus. The output from grain playback is panned between the four internal instrument channels for individual processing. Within each channel, the output of the granular playback is split, with one iteration sent in a parallel process for resynthesis and filtering. A simple algorithm governs the evolving mix between the two streams of direct and resynthesized/filtered playback in all channels. The results are then sent to transducers attached to the metal percussion instruments to be sounded in the performance space.  
7 This program is not publicly available. 
Figure 3. Grey areas show generalized optimal transducer attachment areas on cymbals and gongs.  
Figure 4. Stage set-up for performance with piano at the ICMC conference in Limerick, IE. July 5, 2022  
Figure 5. Graphical representation of the spectral envelope between 0Hz and 1kHz of a 21-inch China cymbal produced by mg.px.m  
 5.1 Input Analysis There is a -30db threshold on the audio input stream. Crossing the stream triggers two functions: 1) a timer to keep track of the time since the last threshold cross, and 2) a real-time analysis of the input spectral centroid. The flow of analysis output has a variable time window dependent on the speed of the input. If there are less than 500ms between onsets, the window range for output is set between 60 - 133ms. If greater than 500ms, the output range is 800-1300ms. These response times allow the system to adjust to the speed and density of the playing. 5.2 Sample Corpus Grain Selection The results of the input analysis are then compared with the list of centroids from the sample corpus to determine the index of the closest match in the corpus. The spectral centroid was chosen as a point of comparison between the input and the corpus so that the response of the system would be grounded in an apparent relation to the performer’s input (brightness), while avoiding more direct imitative parameters such as pitch.  The length and crossfade between grains are determined by the input trigger timing analysis described above. Timing <= 500ms trigger a grain size of 180ms and a crossfade between grains of 10ms. Timings between 500 and 4000ms result in grains of 3800ms and crossfades of 100ms, while timing of over 4000ms trigger grains of 6000ms and a crossfade of 700ms. While the grains are of only three possible lengths, the fact that the corpus consists of multi-event samples causes the playback to sound varied, as multiple events and silences can take place within the playback of single grain. 5.3 Grain Playback Panning A simple algorithm governs the panning of grain playback between the four metal-percussion instrument-designated channels. For this virtual panning, a diamond model with each vertice representing an instrument allows for smooth vector-based amplitude panning. The panning takes place quickly, ranging between 500 - 1000ms per transition. The use of the diamond shape within this stage of the process is to facilitate smooth transitions of sound between instruments and does not refer to spatial placement of the instruments. 5.4 Resynthesis Within each instrument channel, the playback signal is analyzed, and sent to two parallel resynthesis processes. The first is a bank 
of sinusoids for direct resynthesis, the second to a resynthesis process adapted to the properties of the designated instrument of the given channel. Here the data stream is parsed into frequency and amplitude lists. Items from the frequency list are compared to the list of instrument partial frequencies from the stored dictionary, from which the closest matches are selected. The selected matched frequencies are then interleaved with the amplitude list and sent to a separate bank of oscillators. These two resynthesized signals are then combined, creating a hybrid signal which contains both the spectromorphology of the original grain and a modified spectral profile targeted to the resonant frequencies of the designated cymbal or gong.  
 5.5 Evolving Resonant Models Within each instrumental channel, the combined resynthesized signal is then passed to a bank of resonant bandpass filters. These filters are derived from subsets of the list of partial frequencies of the designated instrument stored in the dictionary (the same list used in the resynthesis process). For this process, the frequency content is trimmed to a list of the 50 strongest partials. 15 random frequencies are selected from this list, assigned amplitude and resonance/decay values, and subsequently sent to the bank of bandpass filters. This selection/generation process is repeated to create a second “target” list, and over the course of 18 seconds the resonant filters interpolate from the first list to the “target” list. Once the bandpass filters have completed the interpolation, a second “target” list is generated, and the process repeats.   This process continues throughout the piece, creating an evolving set of resonant models targeted toward the resonant frequencies of the designated metal percussion instrument. Stimulation of the filter bank by the resynthesized signal creates a shifting, continuous transformation of hybrid performer metal-percussion specific sound. 5.6 Mix A distribution-based algorithm governs the mix between granular playback and resynthesis/filtered signals, using a “origin-target” method similar to the resonant modelling with a 7800ms crossfade. The mix is global, in that each instrumental channel has the same mix between the two streams, although sound generation for each channel is independent. The use of global mix parameters facilitates the sonic unity of the reembodied sound system, in that each metal percussion instrument will have the same mix between the straight granular and resynthesized/filter playback, even though the sound each instrument produces will be different.  
Figure 5. Overview of the computational processes.  
Figure 7. Parallel resynthesis processes. 
5.7 Metal Percussion Actuation The final step in the process is the projection of sound into the metal percussion instruments through use of tactile transducers. The evolving synthesis processes detailed above create shifting and unpredictable responses due to the resonances and non-linear nature of the instruments. Not only do the instruments function as a filter that colors the sound from the transducers, but due to their non-linear responses some input frequencies trigger additional vibrational modes, generating complex timbral shifts not present in the original source sounds. As such, the performer not only interacts with the software-governed response times and synthesis processes, but with the idiosyncratic natures of the metal percussion instruments themselves, which thereby become generative agents. 6. CONCLUSION This project explores processes that combine the performer-as-creator ethos of free improvisation with the instrumental-acoustic based approach of reembodied sound. This form of reembodied sound practice requires synthesis process that rely on parameters that can be targeted toward specific real-world instruments. Creating processes that rely on pre-performance analysis of the spectral content of the excited instruments provides a valid basis for these processes, as well as making the work instrument-adaptable. Utilization of a performer generated sample corpus provides a parallel process for the performing instruments. By combining these processes with real-time performer input analysis, the output into the metal percussion incorporates performer-creator action while asserting agency of their own. As co-creators of the sonic environment, the metal percussion becomes a kind of “outerface” (as opposed to an interface), a responsive yet independent creative force with which the performer finds themselves in evolving relation. This evolving relation articulates a  central tenant of the reembodied sound framework of interrelated processes, in which each stage of sound source production, disembodiment,  reembodiment , and apprehension, are generative agents.   This kind of artist exploration is only made possible by the existence of audio transducers. As an incomplete tool, the technology facilitates the blending of augmented and actuated instruments, digital instrument design, music interaction, and spatial music into a wholistic system that does not fit neatly into any of these categories. Refraction Interlude is both a work of art and an instrument. The nature of transducers facilitates this tight integration between aesthetic expression and tool creation – they are inseparable from each other. Reembodied sound practices then, constitute a kind of assemblage in which multi-domain practices intersect and constitute a new wholeness. 7. Acknowledgments Thanks for institutional support from the Center for New Music and Audio Technologies at the University of California at Berkeley, the Computer Music Center at Columbia University, and Rensselaer Polytechnic Institute. Additional thanks to John McCallum for programming support, Garth Powell for supplying instruments, and Georg Wissel for performance feedback. 8. Ethics Statement This project adheres to NIME Ethical standards, and there are no known conflicts of interest. 9. REFERENCES [1] Lewis, G. E. (2009). A power stronger than itself: The Aacm and American experimental music. University of Chicago Press.  
[2] Smith, W. L., & Corbett, J. (2015). Notes (8 pieces): Source: A new world: Music: Creative music. Corbett vs. Dempsey.  [3] Cardew, Cornelius. (1971). “Towards an Ethic of Improvisation.” Treatise Handbook, London: Edition Peters, 1971.xvii-xxi [4] Collins, Nicolas. “Introduction: Composers inside Electronics: Music after David Tudor.” Leonardo Music Journal 14 (2004): 1–3 [5] Driscoll, John, and Matt Rogalsky. “David Tudor’s Rainforest: An Evolving Exploration of Resonance.” Leonardo Music Journal 14 (January 1, 2004): 25 [6] “Exciters & Tactile Transducers 101.” Fuber.com. Accessed January 31, 2023. https://www.daytonaudio.com/topic/excitersbuyerguide.  [7] Systems, Parthian. “Feonic FXPRO Audio Drive Case Studies for Retail: Up to 40% Sales Uplift. Attract Attention to Digital Signage & Interactive Retail Displays Museums, Galleries, Artworks and More.” Surface Speaker Examples. Retail, Museums, Art, Home. Accessed January 31, 2023. https://www.feonic.com/audio-case-studies.  [8] Coll, David. “Watch.” David Coll, composer & sound artist, November 3, 2018. https://davidcoll.wordpress.com/works/music-watch/.  [9] Rama, Gottfried. Spark (2015). Accessed January 31, 2023. http://www.ramagottfried.com/spark.html.  [10] Young, Nina. “Heart Throb.” Nina C. Young. Accessed January 31, 2023. https://www.ninacyoung.com/works/heart-throb.html.  [11] Schroder, Sabrina. Flatspin (Stircrazer IV) . Vimeo, 2023. https://vimeo.com/147859511. [12] Overholt, Dan, Edgar Berdahl, and Robert Hamilton. “Advancements in Actuated Musical Instruments.” Organised Sound 16, no. 2 (2011): 154–65. https://doi.org/10.1017/s1355771811000100.  [13] Davis, T. and Reid, L., 2020. Taking back control: Taming the Feral Cello. In: New Interfaces for Musical Expression 21-25 July 2021 Birmingham. [14] Harriman, Jiffer. (2015). Feedback Lapsteel: Exploring Tactile Transducers As String Actuators. Proceedings of the International Conference on New Interfaces for Musical Expression, 178–179. https://doi.org/10.5281/zenodo.1179076 [15] Thorn, Seth D, and Byron Lahey. “A Haptic-Feedback Shoulder Rest for the Hybrid Violin.” Proceedings of the 2019 International Computer Music Conference, 2019. [16] “Kit-19195 Sparkfun: Mouser.” Mouser Electronics. Accessed January 31, 2023. https://www.mouser.com/ProductDetail/SparkFun/KIT-19195?qs=t7xnP681wgXBIzNOtadEBg%3D%3D.  [17] Robot or human? Accessed January 31, 2023. https://www.walmart.com/c/kp/vibration-speaker.  [18] Yeqiang, L. (2020, May 15). Vibration speaker (resonance speaker). AIYIMA. Retrieved April 7, 2023, from https://www.aiyima.com/blogs/aiyima-audio/vibration-speaker-resonance-speaker [19] LeDonne, Robert D. 2007. Contact Speaker. US Patent 20070217644A1, filed March 5, 2007, and issued September 20, 2007.  [20] “Surround Sound Becomes a Tactile Experience with Bass Shakers.” Valencia Theater Seating, November 20, 2022. https://valenciatheaterseating.com/beginners-guide-to-bass-shakers/.  [21] Coll, David. 2012. “Investigating re-embodied sound.” Lecture, The Matter of Musical Experimentation: A Two-
Day Seminar, University of York, UK, May 8, 2012. Accessed January 5, 2023. [22] Lähdeoja, Otso. “Composing the Context: Considerations on Materially Mediated Electronic Musicianship.” Organised Sound 23, no. 1 (2018): 61–70. doi:10.1017/S1355771817000280. [23] Broken Ghost Consort (2021). Presences: suite for five musicians and nine instruments. Infrequent Seams. [24] Chaigne, Antoine, Cyril Touzé, and Olivier Thomas. “Nonlinear Vibrations and Chaos in Gongs and Cymbals.” Acoustical Science and Technology 26, no. 5 (2005): 403–9. https://doi.org/10.1250/ast.26.403.  [25] Touzé, Cyril, and Antoine Chaigne. “Analysis of Cymbal Vibrations: Lyapunov Spectrum and Route to Chaos.” The Journal of the Acoustical Society of America 106, no. 4 (1999): 2142–42. https://doi.org/10.1121/1.427326. [26] Kuratani, F, T Yoshida, T Koide, T Mizuta, and K Osamura. “Understanding the Effect of Hammering 
Process on the Vibration Characteristics of Cymbals.” Journal of Physics: Conference Series 744 (2016): 012110. https://doi.org/10.1088/1742-6596/744/1/012110. [27] McLaughlin, Scott. “Resonant Systems: Multiphonic Resonance Complexes in Sine-Wave Excited Cymbal Clusters.” Scott McLaughlin - Cymbals Resonant Systems. Accessed January 31, 2023. http://lutins.co.uk/ResonantSystems.html 10. Appendix Live performance by Georg Wissel, clarinet at Cuba Cultur, Münster, DE. June 19, 2019. https://www.youtube.com/watch?v=5rjZUT3B7rk  Studio recording by Georg Wissel, clarinet. Released on Presences: suite for five performers and nine instruments. Recorded 2019. Infrequent Seams 33. https://matthewgoodheart.bandcamp.com/track/interlude  