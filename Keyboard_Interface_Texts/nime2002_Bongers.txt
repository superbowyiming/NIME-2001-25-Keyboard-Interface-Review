A  S tr u c tu r e d  In s tr u m e n t D e s ig n  A p p r o a c h : T h e  V id e o - O r g a n
Bert Bongers and Yolande Harris
Metronom Electronic Arts Studio
Fundacio Rafael Tous d'Art Contemporani
Fusina 9, 08003 Barcelona, Spain
bertbon@xs4all.nl, yolandeh@terra.es
Abstract sound to be controlled live - film-maker Andrei Tarkovsky
writes 'time becomes the very foundation of cinema: as
sound is in music, colour in painting, character in drama'
[17]. The development of a tool for live video performance
takes this time based medium to another level,
incorporating a dynamic interpretation of space as discussed
in the secion on 'elastic narrative' below. Several
performances with the video-organ, as described in this
paper have helped us to assess the reasons, problems and
benefits behind the move towards performance and the live
possibilities of video and sound placed in space.
The Video-Organ is an instrument for the live performance
of audio-visual material. To design an interface we apply a
modular approach, in an attempt to split up the complex
task of finding physical interfaces and mappings to control
sound and video as generated by the computer. Generally,
most modules, or instrumentlets  as they are called, consist
of a human interface element mapped to a certain effect. To
describe the instrumentlets a design space is used
consisting of the parameters degrees of freedom, range and
precision. This paper is addressing the notion that
traditional approaches to composition are challenged and
changed in this situation, where the material is both audio
and visual, and where the design and development of an
instrument becomes involved in the process of performing
and composing.
INTRODUCTION
To develop a new instrument form from the ground up is
not an easy task [5]. In the case of traditional instruments
the shape and form factors of an instrument have always
been dictated by the physical process to be controlled, ie. a
vibrating string. The virtue of electronic instruments is at
the same time the problem: their total freedom to be shaped
in any form. It is only obvious to take human factors as a
starting point, which calls for analysis and study of human
motion and intent. Especially when attempting to include
the latter, human intent which in this case can be called
'composition' or at least 'inspiration', the issue gets
notoriously difficult and is usually unknowingly avoided.
In this paper we present a modular design approach, which
attempts to deal with the complexity by breaking down the
problem into smaller bits.
Figure 1: Part of the Video-Organ control surface
BACKGROUND
Using the computer as a tool to play, edit and even
synthesise image material is becoming increasingly
possible with the current state of processor power, memory
sizes and encoding / decoding algorithms. Video editing is
now catching up with audio editing and computer music,
however the real time or performance setting of video,
although increasingly common, lacks a tradition of live
performance. An interesting analogy can be made with tape -
music: electronic music was for decades a purely non -
performance idiom. Technological develop ments in the
eighties helped the re-discovery of live performance for
electronic music by pioneers such as Michel Waisvisz with
The Hands or the electronic glove of Laetitia Sonami, and a
similar transition seems imminent in video art. VJ's in
popular (club) culture are pioneering this issue, and other
initiatives often come from the field of electronic music,
for instance the Image/ine software developed at STEIM
(Studio for Electro-Instrumental Music in Amsterdam),
rather than from the field of video-art.
Interaction can be defined as 'mutually influential', that is,
both partners in the discourse (whether machine or human)
will have changed state, frame of mind, views, after the
interaction. Most interactions with computer systems
however are merely reactive , including so called interactive
art works. On the other extreme, computers offer many
possibilities to generate a world of their own under only
minimal control or influence of the user, and many
installation art works and instruments are based on this. In
order to make a truly interactive instrument, it seems to be
necessary to take a step back and develop the necessary
-reactive?- groundwork which will provide a base for
building up a more balanced, truly interactive level of
discourse between human (whether performer, composer, or
audience) and the art work.
Over the last one and a half years the authors have
collaborated on the development of the 'video-organ' - a
flexible instrument for the live performance of image and
sound. The aim is to create a gestural performance interface
to enable speed, direction and combinations of image and
There appears to be a close relationship between techno -
logical developments and the activity of bringing together
music and visual art. At the end of the 19th century when
electricity became more widespread 'colour organs' were
developed by the inventor A. W. Rimington as well as the
well known example of Scriabin's "Prometheus" [15]. In
the 1920s to 1940s several instruments were developed by
people like Thomas Wilfred, Kurt Schwitters and Oskar
Fischinger, who came from a film background - it was also
an active period of abstract films by for example Hans
Richter and painters influenced by music as can be seen in
the works of Wassily Kandinsky or Paul Klee. Currently,
the rapid pace of development of digital technologies play a
role in the intensified interest in the relationship between
the visual and the auditory.  Dick Raaijmakers [16] traces a
technological and artistic progression over the last century
towards a morphological  view of the previously distinct
disciplines of music (electronic sound), image (photo -
graphy) and architecture (liquid).
Traditional instruments are an important source of
inspiration and heuristics, knowing that the historical
development of such instruments including their
performing practice, institutionalised teaching, and
composing, happened at a very different pace. The
technology of our time develops at a high pace, and we can
build on the accumulated knowledge and skills over many
years. It is therefore reasonable to assume that the
instruments of our time (including the aspects mentioned)
can develop at a much higher pace, reaching a point where
instruments are usable within a few years rather than
centuries. However, even if all ergonomic considerations
have been satisfied in the design of the instrument, we still
have to work and perform with them for many years in
order to develop a proficiency to make relevant
communications. The simplicity of the approach towards
all material of the Video-Organ is a conscious reflection of
these ideas and is in constant development.
The Video-Organ is using sampled image and sound
material (one could call this video concrète ), rather than
synthesising such as in the case of colour organs from the
Ocular Harpsichord built around 1730 to more current work
such as the Dichromacord [8]. Although different from the
Video-Organ, these interesting cases investigate the
relationship between tonality in sound and the colour
spectrum of light, pitch/timbre/envelope versus
colour/hue/shape.
THE SET UP
The software and hardware, including the human interface,
evolved and changed over the course of the performances,
based on the Max programming environment on Apple
PowerMacs. Most image material is created using a DV
camera (Canon MX1), and video-editing and morphing
programs.With an instrument as described in this paper, it becomes
apparent that the previously distinct acts of composition,
performance, music, video, conflate into one. Moreover, we
assert that the actual building of the elements of the
instrument become an important part of the compositorial
process. This notion becomes increasingly common in this
field [2]
Hardware
Generally the performances take place using two Apple
PowerMac G4's and several projectors connected to the
monitor output, the images are Quicktime format clips. It
was found that with fast G4's a resolution of 640x480 and
full frame rate (25fps) is just about possible to play
correctly when using the Sorensen codec. To have the
highest possible frame rate is essential because we often
slow clips down to access the individual frames - much
like a flipbook.
APPROACH
To handle the complex situation of performing and
composing with audio-visual material we employ a
structured approach based on deconstructing it into separate
issues, and then building a new situation from the ground
up. The shape of the instrument itself has to emerge from
its parts, which we call instrumentlets . These
instrumentlets, or little instruments, are parts of the human
interface that perform only one or a few sensing tasks (but
can be highly sensitive with many different Degrees-of -
Freedom, DoF's, see below) and a certain shape. We
research the mapping between the manipulation of each
instrumentlet and sound and/or image,. The composition
therefore is built up from the stage of instrument building
and the collection of sounds and images. Closest to
traditional composition is the making of short clips,
consisting of image and sound. Each clip has its own
movement characteristics that suggest ways of controlling
them with gestures and actions, which are the design
parameters that shape an instrumentlet. Sounds, images,
and combined clips with sound and image form part of the
compositorial palette, as well as the sensors to capture the
performers gestures, and the ability to place the material in
architectural space. The composition builds up from these
elements. It is not technology driven, but rather interface
design and mapping become part of the compositional
process. The video-organ as an instrument is different from
traditional instruments in that it is being developed,
designed, built, and composed for all at once allowing
rapid iterations towards solutions in the process.
In the earliest performance, a PowerMac G4 with Media100
hardware was used enabling a high image quality.
However, it was found that the Media100 proprietary codec
(coding/decoding algorithm) was more suited to constantly
moving images rather than individual frame access and
progressions as needed in our situation. A workaround is to
disable the use of fields, so that frames are individual and
not relying on information of the next or previous frame.
By contrast, another performance (of the piece called
"BAT"), we used an iMac DV and a G3 PowerBook and
had to work with a considerably lower image quality.
Software
All sounds and images are played and manipulated from
the Max programming environment, according to mappings
defined in this environment as well. For sound
manipulation we are using MSP, and for images the
standard Movie object and sometimes the Movieplus object
written by David Rokeby, which has extended features for
image manipulation and is easier to use with multiple files.
Interfaces
To enable control over the sound and image material a
num ber of instrument parts, the instrumentlets were
developed, ranging from simple push sensors, turnpots and
sliders to more complex combinations. The human
interface will be described in more detail in the next
sections.
space has been the topic of several papers in this field
[11,13,18].
In most cases two Sonology MicroLabs were used to read
the signals from the sensors, in a customised version of the
general purpose interface box with 32 analog inputs, switch
matrix for 16 switches and one ultrasound distance
measuring channel. All values are sent to the computer in
MIDI note numbers and control change values of maximal
7 bits. Dedicated MIDI controllers are used too, such as a
2-octave keyboard and a touch/fader box, and several
(modified) USB input devices such as keyboards and a
drawing tablet.
The Design Space is described elsewhere in further detail
[7]. The issue of mapping is addressed in the
Instrumentlets section and on another level in the section
about Elastic Narrative.
Degrees of Freedom
The movement of each object in the three-dimensional
space can be described along the three axis (X, Y and Z, or
horizontal, vertical and back and forth) and its rotations
(around the same axis, called pitch, yaw and roll
respectively). An input device can be described in elements
each with their DoF's, for instance a mouse has two lateral
degrees of freedom generally defined as X and Y (left -
right and forward - backward respectively). Pressure sensors
are generally defined as being sensitive in the Z direction.
This is a somewhat arbitrary convention, a different
definition of the orientation of the orthogonal set could be
decided upon - this is also true for the rotational DoF's.
The descriptions here are based on the normal use of the
sensor, though in most cases the user can choose to move
along another axis.
There is no other visual interface than the images produced
during a performance. Only during development we use the
visual interface objects in Max.
THE PHYSICAL INTERFACE DESIGN SPACE
To structure the development and description of the many
elements and parameters in the Video-Organ instrument as
a multidimensional physical human interface, which is the
control surface that actually primarily matters to the user, a
design space is used consisting of the parameters described
in this section. This Physical Interface Design Space was
first developed to create a taxonomy of physical human
interfaces [3], and has been further developed over the last
years [5].
Range
The range of a physical sensor is measured in distance (in
millimetres, lateral movements) or number of degrees
covered (rotational movement), from zero; this is the case
of isomorphic or pressure measurement. Acceleration can be
measured in G (the unit of gravity).
The problem with making a categorisation of input devices,
as shown in the existing taxonomies [1, 9, 12], is the issue
of 'comparing apples and oranges' without having a notion
of the class of 'fruit'. This is because the developments in
input device technology for computers are still going on
and happening at a high pace (though not effectively
enough - we are still stuck with the mouse in most
applications!). In order to describe the interfaces the
solution proposed here is to split up the device and
describe it in its components - for example the mouse is a
combination of sensing of the lateral movements of the arm
(through a ball and rotational sensors), one or more
switches, and often a rotary dial. All these elements have
their own parameters. Another important issue is the notion
of feedback - the feel of an instrument or device. The device
can be free floating (gesture trackers), grounded (joystick on
the table), or contain active haptic feedback (tactile and
force feedback generated by the computer). Any action one
performs in the real world is guided by this feedback, and
therefore one cannot appropriately describe an interface
without taking this into account. The Physical Design
Space is based on
Precision
The precision of the range of a sensor is also dependent on
the electrical interface (eg. A/D converter resolution or
switch matrix) and can therefore range from two (a simple
switch on/off event) to 7 bits range in the MIDI protocol
(rather poor range of 128 steps) to the more desirable
precision for most analog readings of at least 10 bits.
The speed of the reading of the signal is mainly dependent
on machine factors such as A/D converter sample rate and
the lower level information processing in the computer's
operating system or specific MIDI drivers.
Haptic feedback
This parameter of the design space describes the 'feel' of a
controller, and is defined as either passive  (the mechanical
or inherent feedback from a physical object) or active  - this
refers to a palpable feedback generated by the computer
system through actuators. The feedback can be described in
the same parameters as the input, ie. resolution,
displacement range, degrees of freedom, etc, resulting in a
description of the force (or torque in the case of rotational
degrees of freedom) patterns or characteristics. This is
beyond the scope of the present article, but is described in
further detail elsewhere [4,7].
• description of both input and output, the interface as a two
way device
• breaking up the functions of the device into separate
sensors / actuators
• describing each of these elements along the dimensions of
the design space
• the design space consists of Degrees of Freedom, Range,
and Precision VIDEO-ORGAN INSTRUMENTLETSThe Physical Interface Design Space describes the physical
layer of the interaction between human and computer, on
top of which the higher layers can be described which
convey meaning  and intent  (semiotic, syntactic, semantic)
and the mapping  between content and action. The notion of
the importance of including this in the instrument design
In Table 1 a concise overview of the Instrumentlets deve -
loped and used is given, described in the parameters of the
Physical Instrument Design Space.
In the next sections some instrumentlets that illustrate the
design approach as outlined in this paper are described.
Table 1. An overview of the Instrumentlets There is one bigger knob which controls looping speed in a
sound sample between loop points set with two smaller
knobs, and in a later version two more potmeters evolved
which control filter parameters and a slide pot which
controls volume. The smallest one, the Knobbiebox,
contains just one pot, the movement of which is mapped to
playback speed of a clip. Several clips can be selected with
two buttons on the side of the box.
name                                      DoF's/amount                                 Range                            Precision [bits]                   connection         material
Squeezamin pitch, roll, Z 90°, 26mm 7 MIDI A
Knobbiesbox yaw/3 270° 7 MIDI A
Knobbiesbox+ yaw/5, Y 270°, 26mm 7 MIDI A
Knobbiebox yaw/1, Y/2 270°, 1mm 7, 1 MIDI V
Estrella Z 0 7 MIDI A/V
Presmorphs Z 0 7 MIDI V
The Turnamin 
Turnamin pitch, Z 360° ( mod 2 π ), 26mm 7 MIDI A/V
Tabletring X, Y 130mm, 90mm 9 USB A
The Turnamin (or Turn 'm In) is inspired by the use of
turntables by DJ's, spinning a little wheel influences the
playback speed of a sound sample ('scratching') and video
clip. It is based on a small motor which acts as a dynamo,
and generates a movement speed dependent voltage which
is fed into the Microlab. A few diodes limit and channel
the voltage (which reverses polarity depending on the
direction of turning) into two separate channels. It was
found that the range of the readout was a lot smaller than
the real vinyl thing (limiting factors are A/D conversion
and the MIDI protocol) so a small slide pot had to 'grow
on' to control a multiplication factor. Usually the sound
consists of some loud and clear drumbeats, which work
well when played very slowly and are more rhythmic when
played fast.
Faderbox Y/10 60mm 7 MIDI V
Keyboard Z/60 8mm 1 USB V
Keyboard Z/18 3mm 1 USB V
Keyboard Z/24, pitch/2, yaw/8 12mm, 180°, 270° 7 MIDI V
Accelring X ± 2G 7 MIDI V
Sliders Y 50mm 7 MIDI V
Pankeys Z/6 1mm 1 MIDI A
Stompin' Pedal pitch, roll, Z 60°, 22mm 7 MIDI A/V
Gesticulator X 1000mm 7 MIDI A/V
Drum 'n Bass Stick ptich, roll, Z/22 150°, 1mm 5, 1 USB A
The Squeezamin
To honour the oldest and still most widely known and
used electronic controller, the Theremin, we sometimes
contaminate this name with a word that describes the
properties of the instrumentlet. (Most names of the
instrumentlets are informal and appear a bit silly in an
academic paper.)
The Squeezamin allows for manipulating material in the
computer by squeezing sponge like material with sensors
embedded. It consists of two rigid plates of about 5cm x
5cm, a 3cm thick piece of foam in between, and four
sensors placed so that movement down is sensed (lateral
movement along one axis) as well as inclination of the top
plate (rotations around two other axis). These three DOF's
are usually mapped to volume and panning of four sound
tracks, and with switches different sound sets can be
selected.
Figure 4. The Turnamin
Sliders
For playing very short clips (about 5 seconds or 125
frames) we often use simple slide potentiometers, that
travel smoothly and play the clip just like the flick of
movement of the performer.
Figure 2. The Squeezamin
Knobbiesboxes
GesticulatorUsing an assembly of several potmeters, the
Knobbiesboxes are applied to control several closely related
parameters in a more precise way.
To allow for gestural control of a long clip with its sound
the ultrasound distance measuring channel of the MicroLab
is used. To overcome the limited range (128 steps on 1
meter) a simple algorithm is implemented that determines
when the speed  of the movement gets past a set threshold,
as an 'escape velocity' that makes the clip jump to another
range. This way there are two modes of interaction, small
range with high precision and long range, within the clip.
Tabletring
The Tabletring is an example of the use of an existing
computer input device with some modifications. It is based
on the hardware of the Wacom Graphire drawing tablet,
with a sensing surface of 9 x 13 centimetres tracking the
movement of a stylus. We use the coil from the stylus
which is the actual object that the sensing surface tracks
(and communicates its information through), which isFigure 3. The Knobbieboxes
placed in a ring on the finger (as shown in Figure 5) of the
player while the rest of the electronics were placed in a
bracelet. Using Richard Dudas 'wacom' object the move -
ment of the finger is read in 9 bit values in Max. These
values were used to control the parameters of a resonating
filter.
house, a large plastic inflatable bubble which the audience peered in
from the outside. Whole performance directed by architect Enric Ruiz
Geli, contribution commissioned by Metapolis Architects. Four
performances, in Mercat de les Flors theatre, Barcelona, September -
October 2001
Light Painting for Still Music , composition for Video-Organ and 6
instrumentalists, with visual score. Piece that explored differences of
movement and time in music and painting, for concert series called
"Painting Music" in 5th Cicle Internacional d'Influencies Musicals
Barcelona. Commission from Ensemble Barcelona Nova Musica,
Christiaan de Jong and 'Alternativa' Barcelona Film Festival in CCCB
(Centre Cultural Cuidad de Barcelona) November 2001
 
Metronom Meta-Orchestra , Video-Organ as member of multidiscplinary
group. Material: two specific images only, sounds - no context images,
January 2002, Festival Musica a Metronom, Barcelona
MediaEval , Solo Video-Organ composition for spatialised sound and
multiple screen video. The piece presents an elastic narrative that takes
the audience on a journey through landscapes of mountains, light, cities.
It begins with an image of a giant hand in the act of notating and the
journey is punctuated throughout with abstract notations. There is a
dialogue between examining the close-up human artefact of a score
present with musical instrumental sounds, and the landscape images with
environmental sounds. Performed at Festival Musica a Metronom
Barcelona, and at Alicante University Museum Experimental Music
Festival, January 2002
Figure 5. The Tabletring
PERFORMANCES
A number of compositions have been developed with the
video-organ, all in varying circumstances reflected in the
use of different material, making a total of eleven per -
formances usually by both authors. The first performances,
with the Meta-Orchestra [4], represent the video-organ in its
very early development stage and the role was as a member
of a larger group of improvisers of music and dance. The
subsequent performances have been "solos", one amongst a
series of solos with Metapolis Media House, and a final
one as a member of another Meta-Orchestra in Barcelona. In
the list below the performances are described in further
detail, including a description of the material used, in order
to give an idea of the flexibility of the instrument and an
impression of its practical performance development.
M O V E M E N T ,   S P A C E   A N D   E L A S T IC   N A R R A T IV E
The self-imposed limitations of the video-organ as an in -
strument enabled or forced us to focus on clarity and sim -
plicity in compositional ideas. By the later performances,
video effects are absent. The palette is a combination of the
original material - image, sound, image + sound clips - and
the manipulation possibilities of the instrumentlets. We
have confined this now to variable clip speed, variable
direction, access to individual frames, and most impor -
tantly a general play function that when played against
provides feelings of inertia. The palette is limited to one
image at a time, although rapid changes and cuts between
images are used. This model is roughly the same for the
sound manipulation providing a general compatibility and
consistency between the use of image and sound. The only
significant difference in treatment of image and sound
material is that the sound can be built up in a number of
layers simultaneously. In order to achieve sufficient depth
in the image we use multiple projections from different
sources to compose and play with tensions of similarity
and difference between the screens. The simplicity of the
set up enables us to focus on one key issue - movement.
Any placing together of image and sound raises questions
of audio-visual languages - how do the image and sound
effect each other, what is their common ground if any, and
how can these ideas be developed successfully in
performance? A significant common ground between sound
and moving image and their basic inherent parameters of
time and space, come together in the idea of "movement"
[10,16] - this is our main parameter. From the beginning to
the end of the composition and design process the concept
of movement holds the video-organ development together.
Movement is developed from the original material in the
following steps:
The Meta-Orchestra , Video-Organ as member of large multidisciplinary
group. Material: images of meta-orchestra rehearsals as context, various
other images. August 2000, Tweed Mill Dartington, England
The Meta-Orchestra  second public performance, Video-Organ as group
member. Material: images of meta-orchestra Tweed Mill performance,
rehearsals, fire. February 2001, Felix Meritis/De IJsbreker, Amsterdam
BAT , Solo Video-Organ. Using four large screens with mirroring and
spatialised sound the piece explored subliminal experiences of
movement, taking as a starting point the mysterious qualities of a bat
navigating by ultrasound at dusk. Composed and performed at Festival
Musica a Metronom, Barcelona, June 2001.
A  u  d  i  e  n  c  e
performertable with interfaces
computers
projector
screen
performer
speaker
wall
pillar
• movement of the camera/microphone or capturing movement within
the raw image
• pre-editing (before live editing) whether in morphs or simple
dissolve effects, and selection of clipsFigure 6. Performance lay-out of the piece "BAT"
• mapping of certain movements to gestures in the instrumentlets, for
example, a circular movement is mapped to a circular turning
gesture
Digital Day@Media House , Solo Video-Organ performance in
architectural project, as part of 60 minute show characterising a day in
the life of a media house, including cooks, kids, net artists and a dancer
in a full scale model where the audience walked through the 'house'. The
composition used video, models and sounds from the project - from
designs to construction - and put them together in the "work room" of the
• placing in space by movement between screens and speakers, for
example, by panning the sounds in relation to the four points of the
Squeezamin or moving the image between two screens
The issue of narrativity is unavoidable as, however abstract
the images and sounds are, when placed together in a
sequence they will be naturally understood by the audience
as in some way a "narrative". Through our experiences of
playing and composing with the video-organ in different
circumstances an approach towards structure and narrative
has naturally evolved, whereby the general direction and
sequence of events is notated before performance, leaving a
flexibility in precise timings and allowing a certain degree
of improvisation into the performances. This flexibility in
live performance is one of the main virtues of the video -
organ, encouraging an interaction with audience,
atmosphere, the two players. The narrative or journey that
the audience is taken through can be described as 'elastic'
narrative, giving a sense that the time, movement and
position in space can be stretched, spring back and be
malleably alive. To achieve and use this elasticity is the
performers task as much as it is central to the
compositional issues of gathering material and mapping to
the control surface.
REFERENCES
[1] Baecker, R.M and Buxton, W.A.S. The Haptic
Channel. In: Readings in Human-Computer
Interaction , Morgan Kaufman, San Mateo CA, 1987.
pp 357 - 365.
[2] Bahn, C.R. and D.Trueman, "Interface, Electronic
Chamber Ensemble. CHI Workshop New Interfaces
For Musical Expression  (2001)
[3] Bongers, A.J., "A Survey and Taxonomy of Input
Devices and Haptic Feedback Devices", Philips
Nat.Lab Technical Note TN-298/97. (1997)
[4] Bongers, A. J. "Tactual Display of Sound Properties
in Electronic Musical Instruments", Displays Journal ,
18/3. (1998)
[5] Bongers, A. J.,  "Physical Interaction in the
Electronic Arts : Interaction  Theory and Interfacing
Technology", Chapter in "Trends in Gestural Control
in Music"   CDROM, IRCAM Fr. (2000).
[6] Bongers, A.J., J.F. Impett and Y.C. Harris (ed.),
"HyperMusic and the Sighting of Sound", project
report and CD's for the European Commission (2001)
(available on line at www.Meta-Orchestra.net)CONCLUSION AND FUTURE PLANS
Throughout this paper we have mentioned that developing
a new instrument is not an easy task, and introduced a
structured and modular approach. By focusing on the
content  of the compositorial material rather then the
interface, we felt it was possible to perform with the
instrument even from its most rudimentary state. After
several successful performances we can conclude that this
approach is valid, also feeding back the experiences of
performing with the instrumentlets into the design process.
[7] Bongers, A.J., "The Physical Interface Design Space:
Towards a Quantitive Analysis of Human Interfaces.
(2002) [submitted]
[8] Conrad, D., "The Dichromacord - Reinventing the
Elusive Color Organ", Leonardo  32/5 (1999)
[9] Foley, J.D., V.L. Wallace and P. Chan, "The Human
Factors of Computer Graphics Interaction
Techniques", IEEE Computer Graphics and
Applications , 4/11. (1984) pp. 13-48
[10] Harris, Y.C., "Explorations in Movement, Towards
the Symbiosis of Architecture, Moving Image and
Music", MPhil Thesis, Cambridge University. (2000)
[11] Hunt, A., M.M. Wanderley and R. Kirk, "Towards a
Model for Instrumental Mapping for Expert Musical
Performance", proceedings of the International
Computer Music Conference . (2000)
[12] Mackinlay, J.D., S.K. Card and G.G. Robertson, "A
Semantic Analysis of the Design Space of Input
Devices, Human Computer Interaction  5. (1999) pp.
145-190
Figure 7. Clay models [13] Orio, N., N. Schnell and M.M. Wanderley, "Input
Devices for Musical Expression: Borrowing Tools
from HCI", proceedings of the CHI workshop New
Interfaces for Musical Expression . (2001)
It has become clear by now which elements may be
combined into shapes tending towards a more concrete new
instrument form, though many pieces of the puzzle are still
missing. Experiments with finding forms that reflect
anthropomorphic shapes, as shown in Figure 7, have been
conducted. Development of new instrumentlets is still
going on, also ones that are outside of the performance
model. Many demos have been given with the Video-Organ
as a development principle at the new Metronom Electronic
Arts Studio, also for groups of school children. This was a
valuable source of feedback, and we are currently working
on a stand alone instrumentlet (a subset based on the list in
table 1) for this purpose, with specially made content.
Another spin-off project uses a modified existing gestural
game controller to play sounds in an installation in
Barcelona.
[14] Paradiso, J., "New Ways to Play: Electronic Music
Interfaces", IEEE Spectrum  34/12. (1997) pp.18-30
[15] Peacock, K., "Instruments to perform color-music:
Two centuries of technological experimentation,"
Leonardo , 21 (1988), 397-406.
[16] Raaijmakers, D. "Cahier 'M', A Biref Morphology of
Electric Sound". Orpheus Institute, Gent Belgium,
(2000)
[17] Tarkovsky, A. "Sculpting in Time: Reflections on
Cinema", University of Texas Press, Austin Texas.
(1986)
[18] Vertegaal, R., T. Ungvary and M. Kieslinger,
"Towards a Musician's Cockpit: Transducers,
Feedback and Musical Function", proceedings of the
International Computer Music Conference . (1996)The modular approach to instrument design enables an
evolutionary development, yet is flexible enough to allow
more radical changes and improvements. The Physical
Inter face Design Space is useful for structuring the develop -
ments and suggests parameters of new instrumentlets.
[19] Video-Organ web site: www.meta-orchestra.net/video -
organ