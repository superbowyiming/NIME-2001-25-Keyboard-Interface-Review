The Daïs: A Haptically Enabled NIME for Controlling
Physical Modeling Sound Synthesis Algorithms
Pelle J. Christensen
Aalborg University
Copenhagen, Denmark
pjch17@student.aau.dk
Dan Overholt
Aalborg University
Copenhagen, Denmark
dano@create.aau.dk
Stefania Seraﬁn
Aalborg University
Copenhagen, Denmark
sts@create.aau.dk
ABSTRACT
In this paper we provide a detailed description of the devel-
opment of a new interface for musical expression, theda¨ıs,
with focus on an iterative development process, control of
physical models for sounds synthesis, and haptic feedback.
The development process, consisting of three iterations, is
covered along with a discussion of the tools and methods
used. The sound synthesis algorithm for the da¨ıs, a physical
model of a bowed string, is covered and the mapping from
the interface parameters to those of the synthesis algorithms
is described in detail. Using a qualitative test, the aﬀor-
dances, advantages, and disadvantages of the chosen design,
synthesis algorithm, and parameter mapping is highlighted.
Lastly, the possibilities for future work are discussed with
special focus on alternate sounds and mappings.
Author Keywords
NIME, Haptic feedback, Physical Modeling
CCS Concepts
•Applied computing→ Sound and music computing;
Performing arts;
1. INTRODUCTION
Theda¨ıs1 is a novel interface for musical expression de-
signed with multiple goals. The ﬁrst goal was to develop an
electronic instrument with multiple documented iterations.
The second goal was to produce a repository of code, CAD
models and build instructions, such that the build can be
replicated. The third goal was to create an interface suit-
able for controllingphysical modeling sound synthesis algo-
rithms. To avoid a common pitfall, the da ¨ıs was intended
to be easily reproduced by others by providing diagrams,
assembly instructions, and software through GitHub2.
A common way in commercial interfaces to interact with
virtual instruments is through knobs, faders and simple
push buttons (keys). These types of interactions are likely
preferred because of the easy of mapping them to a scalar
1da¨ıs is a middle English word referring to a platform in
a room where one would seat royalties or other digniﬁed
people.
2https://github.com/PelleJuul/dais
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’20, July 21-25, 2020, Royal Birmingham Conservatoire,
Birmingham City University , Birmingham, United Kingdom.
value, as well as because of aﬀordability. However, knobs
and buttons can be problematic in their expressiveness and
playability, especially when controlling multiple parameters
at the same time. Synthesis algorithms based on physical
modeling often have many parameters, and for more ad-
vanced models we might need to control multiple parame-
ters simultaneously in order to play the model at all. The
da¨ıs was therefore designed to provide intuitive control of
multiple simultaneous parameters.
2. CONCEPT / RELA TED WORK
The interface for the da¨ıs incorporates a design that cap-
tures the position and orientation of a disk suspended by
elastic string, as shown in Figure 1. The disk can be moved
around using hands and the full 3D description can provide
six simultaneous parameters — 3D rotation and position.
This style of interface may be interesting for multiple rea-
sons. First, one would be able to control up to six parame-
ters at once, suitable for physical modeling algorithms. Sec-
ond, a big disk aﬀords big movements of the arm and wrist,
which provides visual appeal forlive performances. Third,
we have not seen an interface of precisely this type before,
so it oﬀered a new and unstudied way of interacting with
sounds.
2.1 Haptic Feedback
The player-instrument system (preferably) features a tight
feedback loop where the actions of the performer is guided
by sensory cues received from the instrument. Though the
auditory modality is dominant in this interaction, the visual
and haptic channels are also important. It seems that the
haptic feedback channel is often overlooked when designing
electronic instruments. According to Cook this is one of
the main reasons for the lack of intimacy with electronic
instruments compared to acoustic ones [6].
Multiple studies have been performed on the importance
of haptics in acoustical instruments. Askenfelt and Jans-
son showed that the vibration in stringed instruments are
perceptible and that they might assist the player in into-
nating correctly, or in the case of the piano, help with tim-
ing[1]. In another study, Chafe showed that the sensation
of the vibrations of the string helps cello players produce
stable oscillations and that adding vibrotactile feedback to
an electronic instrument can make it easier to use[4].
This research suggests that the addition of haptics to
the da¨ıs might enable more precise and expressive playing.
Moreover, since the physical models are simulating vibrat-
ing systems, it would be interesting to actually feel those
vibrations.
Haptic feedback has been applied widely to interfaces
controlling physically based sound synthesis. An notable
project is The Cordis System from ACROE for which sev-
eral haptically enabled devices has been developed, one of
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
609
which is their Gesture Transducer, which uses a motor and
force sensor convert forces between the real and virtual
world [3]. A related project employed the Haply develop-
ment kit3, an open source interface for haptic feedback and
control [10].
For his PhiSEM project, Perry Cook constructed multi-
ple haptically enabled percussion instruments using phys-
ically informed synthesis [6, 5]. A small qualitative test
of an interface imitating a maraca showed that it provided
an “uncanny feeling of connection between gesture, sound,
and feel” , which Cook attributes to the haptics and syn-
thesis method [6]. Cook also comments onpassive haptics
in instrument design — the haptics that arise due to the
mechanical design of the instrument, present in the da¨ıs
through the suspended disk, which provides the sensation
of something pushing back at you.
2.2 Related Instruments
The Touch´ e by Expressive E4 features a sturdy base, upon
which sits a stadium-shaped plate which can be moved around
in a fashion similar to a joystick. The controller has no
sounds on its own, but can be used to control either soft-
ware synthesizers or eﬀects trough MIDI, or to send CV
voltages to analog synthesizers [8]. The main similarity be-
tween the Touch´ e and the da¨ıs is that they both allow sound
to be manipulated by moving a plate. Another instrument
similar to the da¨ıs is the Sensel morph 5. The main similar-
ity there is that it oﬀers an interface in the form of plate
that the player touches with their hands.
3. DEVELOPMENT PROCESS
3.1 T ools and Methods
For the mechanical design of the interface we use Open-
SCAD, an open source software solution for designing 3D
objects6. Unlike most other CAD software packages, Open-
SCAD uses a specially designed domain speciﬁc program-
ming language for describing geometry, with a standard li-
brary and functionality for building complex objects. For
the second iteration a custom PCB was designed, using Ki-
Cad — a free software suite for schematic and PCB design.
For the processor we chose the Bela platform, a small, single
board Linux computer (BeagleBone Black) with additional
hardware for audio IO.
Before using the Bela, audio algorithms were developed
in a desktop environment. For rapid prototyping and de-
bugging of algorithms the ﬁrst author developed thePre-
tentious Audio Library (PAL), which bundles mature open
source audio and GUI libraries in an easy to use package.
3.2 First Iteration
The goal of the ﬁrst iteration was to verify the eﬀectiveness
of our design idea. A picture of the ﬁrst build, made out
of cardboard and hot glue, can be seen in Figure 1. (a).
Disk position sensing was ﬁrst attempted with magnets and
Hall-eﬀect sensors. However, since the disk of the da¨ıs can
move somewhat freely, and because Hall-eﬀect sensors are
sensitive to angle as well as the distance, the measured value
is ambiguous.
Vibrotactile feedback of the disk was achieved using two
small vibration transducers: voice coils with an attached
mass that can be adhered onto objects to induce vibrations.
3https://www.robotshop.com/en/
haply-development-kit.html
4https://www.expressivee.com/buy-touche
5https://sensel.com/pages/the-sensel-morph
6https://www.openscad.org/
Figure 1: (a) 1st iteration (b) 2nd iteration (c) 3rd iteration
These were driven by the ﬁrst channel of an external class-
D stereo ampliﬁer, the second channel was used to drive a
small speaker for audio output. Testing this ﬁrst iteration
gave the impression that the suspended disk interface of-
fered a new and interesting way of interacting with sound,
because the interface aﬀorded a kind of ” wavey” movement
of the arm. The vibrotactile feedback deﬁnitely made the
disk more fun and intimate to interact with.
3.3 Second Iteration
The goal of our second iteration was to start working with
mechanical design, and ﬁnd an alternative to the Hall eﬀect
sensors. We decided to try optical proximity sensors, which
emit infrared light and measure how much is reﬂected. The
sensor used was a Broadcom APDS-9900 with 10 bits reso-
lution and a range of 10cm. Like Hall eﬀect sensors, it has
an exponential response, which can cause issues with noise
and resolution. This was mitigated by making sure that the
disk was not too far from sensor, and applying a low pass
ﬁlter to remove noise. The APDS-9900 has a maximal data
rate of 2.5ms per measurement, which is plenty for control
purposes. To capture the disk’s movement and orientation
in more detail, we detected: total depression (d) and rota-
tion around the x and y (horizontal) axes (the normal). To
calculate these it is necessary to detect three points on the
disk’s surface; to use three sensors.
A custom PCB was designed to ensure that the sensors
were placed at known positions. A multiplexer was used to
allow reading of all three sensors on the same I2C bus of a
Bela. The sensors were calibrated to obtain measurements
in millimeters by ﬁtting sensor values to known distance
values. Testing showed the computation of thed value was
quite reliable. The computation of the normal was very sen-
sitive to noise and errors in calibration, which made map-
ping of the rotation parameters nearly unusable.
We also experimented with mounting a piezo on the disk
to capture more expressive interactions at audio rate, en-
abling the performer to use more nuance, e.g., scratching
gestures on the surface. However, the vibrations of the disk
were picked up by the piezo resulting in a feedback loop. We
tried to solve this problem using a custom feedback suppres-
sion algorithm, but when that did not work optimally, we
opted to instead move the piezo to the side of the instru-
ment (an alternative interaction). Otherwise, interacting
with this prototype was similar to the ﬁrst iteration.
The synthesis algorithm (an early version of the ﬁnal one)
felt well suited for the interface and the vibrotactile trans-
ducers worked well with the stiﬀer ﬁberboard.
3.4 Third (ﬁnal) Iteration
Since the solution with three proximity sensors was too
noisy for the disk surface normal calculation, another sen-
sor setup was developed. To measured we now use a single
proximity sensor, which eleminated the need for the mul-
tiplexer and the custom PCB, simplifying the design, soft-
ware, and calibration routine.
For measuring the angle of the plate we use an Bosch Sen-
sortec BNO055 IMU (intertial measurement unit), a sensor
which includes the accelerometer/gyroscope sensor fusion
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
610
algorithms internally. It can be conﬁgured with I2C com-
mands, after which orientation and linear acceleration can
be read at a rate of 100Hz. The chip is mounted to the disk
with a piece of foam to isolate it from vibrations. The IMU
provides orientation data (we use rotation around the x and
y axis - the z axis currently unused).
We wanted this ﬁnal iteration to look more ﬁnished. The
aesthetic of the exterior aﬀects how we perceive and inter-
act with an instrument [9]. Therefore, electronic interfaces
should also signal that they are instruments for creating art
by having a beautiful, or at least nice, complete exterior.
This meant having a nice way to mount all the sensors,
route the wires, and a place to hide the Bela and ampliﬁer
board.
The new design is a ﬁfteen-sided polygon (pentadecagon)
as shown in Figure 1 (c). The construction is divided into
two compartments: the lower compartment, which contains
the electronic innards; and the upper compartment where
the proximity sensor is mounted. Five of the wall pieces
are lower than the others to make room for the wrist of the
performer. The disk is mounted via elastic string, which iso-
lates the vibrations of the disk from the piezo microphone.
4. AUDIO ALGORITHM
The sound synthesis algorithm used is a physical model of a
bowed string. The method used isﬁnite diﬀerence schemes ,
which is a way of numerically solving partial diﬀerential
equations. For a through introduction to ﬁnite diﬀerence
methods and the models and notation used here see [2].
The model we use is described by the diﬀerential equation
utt = c2uxx − 2σ0ut + 2σ1utxx − Fbφ(vr), (1)
where u = u(x, t) is the displacement of the string at point x
and time t, c2 is the wave speed determining the frequency
of oscillation, σ0 is general damping, and σ1 is frequency
dependent damping. The last term models the bow. Fb is
the force at which the bow is pressed onto the string. The
functionφ(vr) determines the frictional force applied to the
string based on the relative velocity between the string and
the bow.vr is computed by
vr = ut − vb, (2)
where vb is the speed of the bow. φ can be deﬁned in mul-
tiple ways, see e.g. [2, chapter 4.3], but we decided on
φ(vr) = sign( vr)(ǫ + (1 − ǫ)e− α |vr |) (3)
as it was the most playable of the ones we tried.
To implement Equation (1) it is discretized as follows:
δttun
l= c2δxxun
l− 2σ0δt·un
l+ 2σ1δt− δxxun
l− Fbφ(vr), (4)
and Equation (2) as
vr = δt− u − vb. (5)
5. P ARAMETER MAPPING
Fb is mapped exponentially to the depression the disk of
the da¨ıs — a nice analogy to pressing a bow onto a string,
especially combined with the haptics of the elastic string.
vb is mapped to the tilt towards the player, which is max-
imally a = 15 deg. For some playing techniques we need a
mapping that allows vb = 0 exactly, so when the disk is
tilted away from the player, which means that a ≤ 1 deg,
then we set vb = 0. The mapping of vb is unlike a real bow
because the disk does not need to be moved continuously.
To approach that, one map the angular velocity of the disk
tovb.
Figure 2: Test setup with the da ¨ıs, keyboard and speaker.
c2 is mapped to the most recent pitch played on an exter-
nal MIDI keyboard — a crude way of providing pitch control
of our physical model, but very convenient to implement.
Stability of the physical model is ensured by bounding the
value ofc2. c2 is also mapped to the side-to-side tilt of the
disk to allow vibrato and pitch bends.
The pitch mapping causes an artiﬁcial, instant change in
pitch which does not suit the organic sound of the string
model. A better pitch mapping would be to introduce a
fretting model that would introduce extra damping some-
where on the string such as in [11].
6. EV ALU A TION
To evaluate, a semi-structured interview was performed to
see if test subjects would be able to understand the param-
eter mapping, what they thought of the expressiveness of
the instrument, and to get feedback regarding the choice of
sound and mapping as well as ideas for alternative sound
designs and interactions.
6.1 T est Setup and Participants
Each interview started with one participant entering the
room where the da¨ıs was set up with an external speaker
and a keyboard on the right as in Figure 2. Audio of the in-
terview was recorded for future reference and notes regard-
ing answers and interaction were taken in a spread sheet.
The test then proceeded with the questions reported in the
following section. The test was performed with six par-
ticipants, all students within a music technology masters
program. Each round took around ten to twenty minutes
depending on how talkative the participant was.
6.2 Results and Discussion
What are your immediate thoughts when seeing this thing
[the da¨ıs]? / How do you think it is played? Four subjects
expected to move the disk while controlling the pitch of the
sound using the keyboard. One subject suggested a drum-
like interaction and the last subject thought the tension of
the elastic string might be a parameter. Two subjects found
the instrument visually appealing.
What kind of sound do you think it will make? Two sub-
jects expected a percussive sound. One expected a sus-
tained sound. The other subjects expected digital sound.
The subjects were also asked to do a verbalsound sketch
[7]. Almost all subjects were reluctant to do this and no
substantial observations came of their answers.
After playing the da ¨ıs without instruction: How did it feel
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
611
to play the instrument? Two subjects played the instrument
in a drum-like manner, while four other participants imme-
diately pressed down the plate and were confused it did not
make any sound right away. Subjects were mostly not able
to ﬁgure out the mapping by themselves. One subject cor-
rectly identiﬁed the pitch bend mapping, and that pressing
down and tilting towards/away would change the sound.
What kind of sound did it make? Most subjects described
the sound as acoustic, string-like sound. One described the
sound as bell-like. No participants directly suggested a vio-
lin or cello sound which might be due to the visual diﬀerence
to traditional bowed string instruments.
After being instructed on how to play the da ¨ıs: How did
it now feel to play the instrument? No subjects seemed to
have trouble understanding the mapping. Three subjects
immediately utilized the mapping and explored the sonic
possibilities of the instrument. Two subjects were able to
get some quite musical results — playing phrases and using
the pitch bend. Some subjects mostly tapped the piezo
mic to produce sounds. All subjects expected to be able to
tap and then modulate the sound using the disk, but were
disappointed because pressing down the disk kills the sound
produced by tapping. One subject developed a technique of
tapping on the side and then pressing the disk to produce
a sustained tone with a strong attack, which is a technique
we had not thought of previously. Most subjects described
the playing experience as fun, enjoyable and intuitive once
they got a hang of the mapping.
Can you think of any other sound or interaction style that
would suit the interface? Many subjects suggested that a
percussive sound could be suitable and that moving the disk
around should modulate the sound. One suggested that a
wind instrument sound could also be nice to try.
Do you have any other comments on the instrument or
the testing procedure? One subject would like to see the
da¨ıs and the keyboard being built together into one object
or maybe with continuous pitch control instead. It seems
that multiple of the subjects were afraid to break something
since most of them refrained from pressing the disk down
very far. One subject commented that they liked the big,
performative movements aﬀorded by the interface.
Only one subject commented on the haptic feedback and
stated that they really liked it. When inquired about the
haptic feedback all subjects stated that they noticed it but
did not actively think about it and that they really liked it.
The fact that the subjects did not comment on the vibrotac-
tile feedback by themselves can be interpreted in two ways:
either the vibrotactile feedback does not aﬀect the playing
experience enough to merit a comment, or, the haptic feed-
back enriches the experience in such a natural way that you
don’t even think about it. We are inclined to think the
second interpretation is the right because the interaction is
similar to an acoustical instrument where vibrations of the
instrument body is a natural part of the combined experi-
ence of playing the instrument.
6.3 Summary of Evaluation
The chosen mapping scheme is not at all intuitive and new
users would have a hard time ﬁguring it out without in-
struction. This is because the mapping was not designed to
be intuitive, but to aﬀord expressive playing once you know
how it works. The short amount of time the test lasted for
was not enough to learn the mapping properly. The exte-
rior of the da¨ıs suggests a percussive sound to some and an
abstract synthy sound to others. The actual bowed string
sound was not suggested by any test participants. The vi-
sual similarity to a drum has an impact on what users expect
of the interface; a mapping to a percussive sound would be
a natural choice for a new mapping. Once introduced to
the mapping users found the instrument fun, enjoyable and
natural to play. Because of the mapping and the physical
modeling algorithm, new playing techniques and timbres
can be discovered, and the instrument gets a life of its own,
sometimes producing unexpected sounds. Finally, the hap-
tic feedback adds another natural dimension to the playing
experience and made the playing more fun and expressive.
7. CONCLUSION
Since some subjects were afraid to damage the interface,
more time should be spent on the design to make the da¨ıs
more sturdy. In an extended evaluation, one could instruct
participants in how much abuse the da¨ıs is able to take,
since it is more robust than it may appear. In the future,
it will be interesting to test diﬀerent sounds and mappings.
A takeaway from the test is that a mapping to a percussive
sound would be good, using the contact microphone as a
trigger and the disk to manipulate the sound. One colleague
suggested that granular synthesis would be a good choice
since the disk could be used to navigate though a sound
space in an intuitive way.
8. REFERENCES
[1] A. Askenfelt and E. V. Jansson. On Vibration
Sensation and Finger Touch in Stringed Instrument
Playing.Music Perception: An Interdisciplinary
Journal, 9(3):311–349, Apr. 1992.
[2] S. Bilbao. Numerical Sound Synthesis . John Wiley &
Sons, Ltd, Chichester, UK, Oct. 2009.
[3] C. Cadoz, A. Luciani, J. Florens, C. Roads, and
F. Chadabe. Responsive Input Devices and Sound
Synthesis by Stimulation of Instrumental
Mechanisms: The Cordis System.Computer Music
Journal, 8(3):60, 1984.
[4] C. Chafe. Tactile Audio Feedback. In Proc. Intl.
Computer Music Conf. , Tokyo, 1993.
[5] P. R. Cook. Physically Informed Sonic Modeling
(PhISM): Percussive Synthesis. In Proc. of The
International Computer Music Conference , 1996.
[6] P. R. Cook. Remutualizing the Musical Instrument:
Co-Design of Synthesis Algorithms and Controllers.
Journal of New Music Research, 33(3):315–320, Sept.
2004.
[7] D. Rocchesso, G. Lemaitre, P. Susini, S. Ternstr ¨om,
and P. Boussard. Sketching sound with voice and
gesture.ACM Interactions , 22(1):38–41, Jan. 2015.
[8] E. Simon. Improved haptic controller, eu patent
ep3129981b1, 2018.
[9] L. Turchet. Smart Musical Instruments: Vision,
Design Principles, and Future Directions. IEEE
Access, 7:8944–8963, 2019.
[10] J. Villeneuve and J. Leonard. Mass-interaction
physical models for sound and multi-sensory creation
: Starting anew. InProceedings of the 16th Sound and
Music Computing Conference , pages 187 – 194,
Malage, 2019.
[11] S. Willemsen, N. Andersson, S. Seraﬁn, and S. Bilbao.
Real-time control of large-scale modular physical
models using the sensel morph. InProceedings of the
16th Sound and Music Computing Conference , pages
151 – 158, Malaga, May 2019.
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
612