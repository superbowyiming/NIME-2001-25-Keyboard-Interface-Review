Parameterized Melody Generation with Autoencoders and
Temporally-Consistent Noise
Aline Weber, Lucas N. Alegre
Institute of Informatics
Federal Univ. of Rio Grande do Sul
Porto Alegre, Brazil
{aweber, lnalegre}@inf.ufrgs.br
Jim Torresen
Department of Informatics; RITMO
University of Oslo
Oslo, Norway
jimtoer@iﬁ.uio.no
Bruno C. da Silva
Institute of Informatics
Federal Univ. of Rio Grande do Sul
Porto Alegre, Brazil
bsilva@inf.ufrgs.br
ABSTRACT
We introduce a machine learning technique to autonomously
generate novel melodies that are variations of an arbitrary
base melody. These are produced by a neural network that
ensures that (with high probability) the melodic and rhyth-
mic structure of the new melody is consistent with a given
set of sample songs. We train a Variational Autoencoder
network to identify a low-dimensional set of variables that
allows for the compression and representation of sample
songs. By perturbing these variables with Perlin Noise—
a temporally-consistent parameterized noise function—it is
possible to generate smoothly-changing novel melodies. We
show that (1) by regulating the amount of noise, one can
specify how much of the base song will be preserved; and
(2) there is a direct correlation between the noise signal and
the diﬀerences between the statistical properties of novel
melodies and the original one. Users can interpret the con-
trollable noise as a type of “creativity knob”: the higher it
is, the more leeway the network has to generate signiﬁcantly
diﬀerent melodies. We present a physical prototype that al-
lows musicians to use a keyboard to provide base melodies
and to adjust the network’s “creativity knobs” to regulate
in real-time the process that proposes new melody ideas.
Author Keywords
Variational Autoencoders, Perlin Noise, Melody Genera-
tion
CCS Concepts
•Applied computing→Sound and music computing;
•Computing methodologies→Neural networks;
1. INTRODUCTION
In recent years we have seen a growing interest in the appli-
cation of machine learning techniques to perform music gen-
eration, processing, and analysis. A few examples include
automated music composition, genre classiﬁcation [6], and
music tagging and recommendation [2]. Automated music
generation techniques, in particular, have been extensively
studied in the context of methods capable of composing new
songs based on a set of sample melodies. One interesting ap-
plication of music generation relates to constructing compu-
tational systems for helping musicians explore a richer space
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’19, June 3-6, 2019, Federal University of Rio Grande do Sul,
Porto Alegre, Brazil.
of possibilities when composing—for instance, by suggesting
a few possible novel variations of a base melody informed
by the musician. In this paper, we propose and evaluate a
machine learning technique to generate new melodies based
on a given arbitrary base melody, while ensuring that it
preserves both properties of the original melody and gen-
eral melodic and rhythmic characteristics of a given set of
sample songs.
Previous works have attempted to achieve this goal by
deploying diﬀerent types of machine learning techniques.
Methods exist, e.g., that interpolate pairs of melodies pro-
vided by a user [9]. We, by contrast, extend this idea to
generate an arbitrary number of new melodies based on
one that is provided by the user; it retains a parameterized
amount of the original song’s properties while still sounding
similar to a set of sample songs. Other existing techniques
identify compressed representations of melodies and then
produce new ones by randomly tweaking such representa-
tions [10]. The resulting melodies are consistent with songs
in a training set but are not smooth variations of a given
reference song. Other works require, e.g., that each bar
of a given base song be discretized into a ﬁxed number of
intervals [12]. Our approach allows for the generation of
arbitrary continuous, real-valued note durations when pro-
ducing new melodies. Furthermore, our method also diﬀers
from these techniques by enabling for both novel note dura-
tions and pitches to be generated by independent processes.
In this paper, we propose a method to autonomously gen-
erate new melodies that are variations of an arbitrary base
melody. These are produced by a neural network that en-
sures that (with high probability) the melodic and rhyth-
mic structure of the new melody is consistent with a given
set of sample songs. This is achieved by training a Varia-
tional Autoencoder (VAE) to identify a set of latent vari-
ables that describe melodic phrases drawn from a set of
songs. This process indirectly discovers patterns both in the
pitches and note durations of such melodies. Then, given a
base melody provided by the user, our method perturbs its
corresponding latent variables by applying Perlin Noise—
a temporally-consistent parameterized noise function— to
generate smoothly-changing novel melodies. We show (1)
that by regulating the amount of noise, it is possible to spec-
ify how much of the base song will be preserved; and (2)
that there is a direct correlation between the noise signal
and the diﬀerences between the statistical properties of the
novel melodies and the original one. Users of our system
can interpret the controllable noise as a type of “creativity
knob”: the higher it is, the more leeway the network has
to generate signiﬁcantly diﬀerent melodies. We also present
a prototype of a physical system that allows musicians to
use a keyboard to provide base melodies and to regulate (in
real-time) the process of generating novel melody ideas.
174
2. BACKGROUND
We ﬁrst describe two key concepts required to understand
our proposed technique: Variational Autoencoders and Per-
lin Noise. Section 3 introduces a method combining these
ideas in a way that achieves the previously-described goals.
2.1 Variational Autoencoders
An Autoencoder (AE) is a type of unsupervised learning
algorithm based on neural networks. Its goal is to learn
a more eﬃcient—more compact—representation of a given
set of high-dimensional inputs; for instance, of human face
images. Such a new representation, called a latent represen-
tation, allows for an AE to reconstruct any possible input
solely based on its latent representation. Identifying a com-
pressed representation of information is generally possible
because, e.g., even though diﬀerent images of human faces
may have diﬀerent pixel values, they also share similari-
ties: there are patterns in the distance between eyes and
the mouth, in the possible types of skin colors, etc. An AE
is essentially a neural network tasked with learning how to
produce as outputs the same inputs that it receives; an AE
processing a particular picture of a cat, for instance, will
be successful if it can propagate it through its layers and
provide the same picture as output.
Mathematically, an AE is a learned function ϕcomposed
of two parts: an encoder ϕenc mapping an input vector X
to another vector, z, called a latent representation ofX; and
a decoder ϕdec mapping z to an output vector ˆX. Ideally,
ˆX should be as close as possible to the original input X.
When this is possible, the latent vector z deﬁnes a set of
numbers providing an alternative representation ofX. If the
dimensionality of z is smaller than that of X, we say that z
is a compressed (more compact) representation of X. In an
AE, z is often a set of variables representing the high-level
characteristics of X. One interesting property of AEs is
that after latent representations z have been identiﬁed, one
can use them to generate novel synthetic data similar to the
data used to train the network. To do so, one can generate
random vectors z(specifying, e.g., a random combination of
eye color, skin color, etc.) and have the decoder ϕdec map
z to the corresponding predicted ˆX—e.g., a new picture of
a human face.
One diﬃculty with the above-described process is that in
a regular AE, one does not know a priori the typical range
or magnitude of values taken by latent vectors z. Its ele-
ments could be, e.g., numbers from 1 to 10 or from 3000
to 10000. This means that when generating novel data, it
is hard to know the range of random values to be used to
create random z vectors. A variant of AEs known as Varia-
tional Autoencoders (VAEs) [5] solves this problem. VAEs
are neural networks similar to AEs but trained under a con-
straint that the values of z for typical inputs X should have
a known and pre-deﬁned form; for instance, they should be
distributed according to a zero-mean, unit-variance Gaus-
sian. This allows for the generation of random vectors z
with values consistent to the ones observed during training,
thus facilitating the use of AEs to generate novel data—e.g.,
novel human faces, similar to (but diﬀerent than) the ones
seen during training. Mathematical details of how VAEs
are trained are beyond the scope of this paper but can be
found in the literature; e.g., [5].
2.2 Perlin Noise
Several applications require the generation of random num-
bers to, for instance, automatically generate textures, arti-
ﬁcial/synthetic pictures, and music. Consider, for example,
a game where terrain (mountains) needs to be generated
on-the-ﬂy to create a new landscape every time the game
is played. One way of doing that is to pick random heights
for each mountain. A simple method of generating random
numbers (also known as noise) for this purpose is to draw
them from a Gaussian or Uniform distribution. However,
this kind of noise does not always provide realistic results—a
terrain where each mountain has its height determined in-
dependently by drawing from a Gaussian distribution looks
like the random landscape in Figure 1a. To address this
problem, [7] introduced the idea of Perlin Noise, a type
of gradient-based noise function widely used for generating
visual eﬀects in computer graphics. Its main property is
that consecutive random numbers drawn from this function
are similar—they are said to be consistent with the ran-
dom numbers generated immediately before it. If random
mountain heights are drawn from a Perlin Noise function,
for instance, the height of nearby mountains will not be
completely independent and random but will be similar,
resulting in a sequence of heights that is spatially consis-
tent. A terrain where each mountain has height determined
by drawing numbers from a Perlin Noise function looks like
the landscape in Figure 1b. By comparing these landscapes,
one can observe how Perlin Noise creates random patterns
that look more realistic than those generated by completely
random noise functions—in particular, because the space
consistency provided by Perlin Noise results in mountain
heights that vary smoothly over the space.
In this paper, we use Perlin Noise not to generate noise
that is spatially-consistent, but temporally-consistent—i.e.,
random numbers that vary smoothly over time. Having a
notion of temporal consistency is important in automated
music generation to ensure that the properties of newly-
produced melodies change smoothly over time, thereby re-
sulting in an idea of continuity along the melody. Con-
cretely, we will use Perlin Noise as a disturbance applied
to the latent representations of diﬀerent parts (windows) of
a base melody, to generate new melodies that are smooth
variations (over time) of the original one.
(a) (b)
Figure 1: Landscape generated using (a) random Gaussian
elevations; and (b) Perlin Noise (adapted from [11]).
3. PROPOSED METHOD
We now present a method that combines VAEs and Perlin
Noise to generate new melodies that are smooth variations
on a given melody. It allows the user to control how diﬀerent
the new melody will be w.r.t. the base one while still sound-
ing similar (both in terms of pitch and note durations) to
the general structure of songs in a training set. The high-
level characteristics of pitches and note durations of the
original melody are ﬁrst represented as low-dimensional la-
tent vectors computed by two independently-trained VAEs.
They are then smoothly transformed over time by applying
temporally-consistent Perlin Noise to the corresponding la-
tent vectors. Applying the decoder functions of each VAE to
the perturbed representations results in a novel, smoothly-
varying melody based on the original one. Because we com-
175
pute latent representations with VAEs trained over a set of
sample songs, the decoded melody also retains the general
properties of the songs in the training set.
Our method receives as input a base melody described
as a sequence of notes, where each note is annotated with
its corresponding pitch and duration. To generate a new
melody, our technique analyzes sequences of L consecutive
notes—called windows—of the original melody. We give
the pitches and durations of notes within each window as
inputs to two diﬀerent VAEs. One VAE is specialized in
modeling sequences of pitches that commonly occur in the
training set, while the other is specialized in modeling se-
quences of note durations that typically occur in that set.
The VAEs ﬁrst compute the latent representation of pitches
and note durations within a window. Perlin noise (varying
smoothly over consecutive windows of notes) is then applied
to the latent representations. When these perturbed latent
representations are given to the decoders, time-consistent
variations of the original melody are produced as a function
of the magnitude of the noise applied at each window.
3.1 Neural Network Architectures
Our technique makes use of two feed-forward fully-connected
VAEs responsible for modeling the characteristics within a
window: ϕP , for note pitches; and ϕD, for note durations.
•ϕP solves a classiﬁcation problem: givenLnote pitches
within a window, the k-th output of ϕP (1 ≤k ≤L)
is a vector containing the probabilities of note kbeing
each one of the possible 12 pitches in the chromatic
scale. Pitches within an input X are one-hot encoded,
resulting in inputs to ϕP with ( L×12) dimensions.
Both the encoder ( ϕP
enc) and decoder ( ϕP
dec) of ϕP
are composed of fully-connected layers with ReLu ac-
tivations. They are connected through a latent space
layer zP . The output layer of ϕP is followed by a
Softmax layer to ensure that the ﬁnal values form a
probability distribution over the possible pitches. The
overall architecture of ϕP is depicted in Figure 2a;
•ϕD solves a regression problem: given L note dura-
tions within a window, the k-th output of ϕD (1 ≤
k ≤L) is the predicted real-valued duration of note
k. Both the encoder ( ϕD
enc) and decoder ( ϕD
dec) of
ϕD are composed of fully-connected ReLu activated
layers connected by a latent layer zD. The output of
the decoder is followed by a linearly-activated layer so
that the outputs of ϕD can be arbitrary real-valued
durations for each note within a window. The overall
architecture of ϕD is depicted in Figure 2b.
As previously discussed, it is possible to train a VAE in
a way that enforces that latent vectors z have values within
some pre-deﬁned distribution or range. Here, we enforce
that the latent spaces of ϕP and ϕD are constrained to
follow a Gaussian distribution N(0,0.1). ϕP is trained by
analyzing a large number of windows of L pitches drawn
from some set of sample songs; ϕD is trained similarly but
over windows of L note durations. For more details on the
construction of the dataset, see Section 4.1.
3.2 Temporally-Consistent Noise
As discussed in Section 3, we wish to use a Perlin Noise func-
tion to generate random numbers that are time-consistent;
these will be used to perturb the latent representations of
note pitches and durations, thus resulting in new melodies
that diﬀer from the base one in a way that varies smoothly
over time windows. Let g(t) be a standard Perlin Noise
function as deﬁned in [7]: it takes as input a time (in ar-
bitrary units) and returns a time-consistent pseudo-random
Dense (192 neurons)
Dense (150 neurons)
µ (25 neurons) σ (25 neurons)
Sample      (25 neurons)
Dense (192 neurons)
Dense (150 neurons)
(ﬂatten)
(reshape)
Softmax
Input      : 16 variables
Dense (128 neurons)
µ (64 neurons) σ (64 neurons)
Sample      (64 neurons)
Dense (128 neurons)
Dense (128 neurons)
Dense (128 neurons)
Output     : (16 x 12) variablesˆP i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
(a) (b)
Output      : 16 variablesˆD i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
Input     : (16 x 12) variablesP i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
D i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
'
P
enc
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
'
P
dec
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
'
D
enc
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
'
D
dec
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
z
P
i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
z
D
i
<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>
Figure 2: Network architectures of (a)ϕP ; and (b) ϕD, both
for the case of window length L= 16.
number. We deﬁne a variant of this function that is parame-
terized by two adjustable variables, αand β. In particular,
we deﬁne our parameterized temporally-consistent Perlin
Noise function f(t; α,β) by ﬁrst using αas a scaling factor
applied to the input of g: random values are then drawn
from g(tα), instead of from g(t). Larger values of α result
in a less smooth sequence of random values. Secondly, we
analyze a given sequence of values drawn from this modiﬁed
noise function and rescale their magnitudes to the interval
[−β,+β]. Larger values of beta imply a higher-amplitude
sequence of random values. Intuitively, the α and β hyper-
parameters can be used to adjust how fast and by how much
(w.r.t. a given base melody) the diﬀerent note windows in
a new melody may change. In particular, α regulates the
smoothness of noise over time; i.e., how fast the melody
being generated may change over time. See Figure 3a for
an example. The β hyper-parameter, on the other hand,
regulates the amplitude of the random numbers being gen-
erated; it determines how much noise may be applied to
each window being analyzed. See Figure 3b for an example.
3.3 Generation of Novel Melodies
After training both VAEs, we generate novel melodies by
applying temporally-consistent Perlin Noise to the latent
representation of note pitches and durations within each
window of the base melody. We denote the base melody,
composed of N notes, as X = [(p1,d1),..., (pN,dN)], where
pk is the pitch of the k-th note and dk is its the dura-
tion. We ﬁrst divide the melody into non-overlapping win-
dows of size L, where the i-th window is deﬁned as Xi =[(
p(i,1),d(i,1)
)
,...,
(
p(i,L),d(i,L)
)]
, where p(i,k) = piL+k is
the k-th note pitch in window i and d(i,k) = diL+k is the
corresponding note duration. Our method splits each ele-
ment of that window into its two components, resulting in
a vector of pitches, Pi, and a vector of durations, Di.
The encoder ϕP
enc of ϕP receives pitches Pi as input (for
each window i of the melody) and computes their latent
representation, zP
i . It then perturbs zP
i with Perlin Noise
εP
i = f(i; αP ,βP ), thus obtaining ˜zP
i . The decoder ϕP
dec
maps ˜zP
i to Pnew
i —a vector containing the new pitches gen-
erated for that time window. A similar process is also used
to produce note durations: ϕD
enc receives as inputDi, gener-
ates zD
i , and applies noise εD
i = f(i; αD,βD) to that latent
representation. The resulting perturbed latent vector ˜ zD
i
is given as input to the decoder, ϕD
dec, which produces the
176
new durations Dnew
i for that window. Elements of Pnew
i
and Dnew
i are merged to form Xnew
i , a vector containing
the new melody for the i-th window. The above process
iterates over all length- L windows of the original melody,
repeatedly appending each newly-generated window Xnew
i
to the novel melody Xnew. See Algorithm 1 for details.
Algorithm 1Generation of Novel Melodies
Let X = [(p1,d1),(p2,d2),..., (pN,dN)] (Base melody)
Xnew ←[ ] (Initializes new melody)
for ifrom 1...⌈N/L⌉do (Iterates over melody windows)
Let Xi =
[(
p(i,1),d(i,1)
)
,...,
(
p(i,L),d(i,L)
)]
Let Pi =
[
p(i,1),...,p (i,L)
]
(Current window pitches)
Let Di =
[
d(i,1),...,d (i,L)
]
(Current window durations)
εP
i ←f(i; αP ,βP ); εD
i ←f(i; αD,βD) (Perlin Noise)
zP
i ←ϕP
enc(Pi) (Current latent pitches)
˜zP
i ←zP
i + εP
i (Noise-perturbed latent pitches)
Pnew
i ←ϕP
dec(˜zP
i ) =
[
pnew
(i,1) ,...,p new
(i,L)
]
(New pitches)
zD
i ←ϕD
enc(Di) (Current latent durations)
˜zD
i ←zD
i + εD
i (Noise-perturbed latent durations)
Dnew
i ←ϕD
dec(˜zD
i ) =
[
dnew
(i,1) ,...,d new
(i,L)
]
(New durations)
Xnew
i ←
[(
pnew
(i,1) ,d new
(i,1)
)
,...,
(
pnew
(i,L),d new
(i,L)
)]
Xnew ←Xnew ∪Xnew
i (Appends window to new melody)
end for
4. EXPERIMENTS
In the following experiments, we analyze three main prop-
erties of our method: 1) the eﬀect of α in the generated
melodies, demonstrating that it regulates the temporal con-
sistency of the new melody w.r.t. the original one; 2) the
eﬀect of β in the generated melodies, showing that exists
a direct correlation between noise magnitude and the dif-
ference between statistical properties of novel melodies and
the original one; and 3) the advantages of using temporally-
consistent noise to perturb the latent variables describing a
given base melody, compared to na ¨ıvely perturbing them
using i.i.d. noise drawn from a Gaussian or Uniform distri-
bution. We also describe a physical prototype that allows
musicians to use a keyboard to provide base melodies and
adjust the noise hyper-parameters, thus regulating (in real-
time) the generation of noise-parameterized melody vari-
ations that retain the general compositional properties of
songs in a training set.
4.1 Training Process
We trained the two VAEs described in Section 3.1 using
a dataset of classical songs in the MIDI format. As we
are interested in processing melodies (monophonic note se-
quences), not harmonies, we selected only MIDI tracks that
were composed of 85% or more of individual notes. We
preserve only the highest note in each eventual remaining
chords. This resulted in a set of 197 MIDI tracks in the
key of C major. A training set was then constructed by ex-
tracting windows of L= 16 notes from the selected tracks.
This was performed in a sliding-window fashion: the ﬁrst
window of a track included the notes (1,..., 16); the second
window, notes (2 ,..., 17); and so on. The complete re-
sulting dataset contained 131613 windows. As discussed in
Section 3.3, each training window is a vector Xi composed
of L tuples of pitches and durations. To obtain the data
needed to train each VAE, each window Xi in the dataset
was split into its components: a vector of pitches, Pi, and a
vector of durations, Di. The set of all Pi’s constituted the
training set of ϕP , and the set of all Di’s constituted the
training set of ϕD. All note durations were standardized—
a pre-processing step to help stabilize the training of neu-
ral networks. The datasets of ϕP and ϕD were split into
training and validation sets according to an 80%-20% pro-
portion. The parameters of both VAEs were optimized by
the gradient descent method Adam [4] with a 0.001 learning
rate. Training resulted in a 97% validation reconstruction
accuracy for ϕP and a validation loss of 0.5283 for ϕD.
4.2 Novel Melody Analysis
We start our analyses by quantifying how eﬀective our method
is in generating new melodies that are variations of a base
melody. We ﬁrst introduce a distance metric for quanti-
fying how diﬀerent a window of pitches Pi (from the base
melody) is when compared to the pitches generated by our
networks. Concretely, we wish to evaluate how diﬀerent
a window Pnew
i =
[
pnew
(i,1) ,...,p new
(i,L)
]
(produced by ϕP ) is
w.r.t. the original window Pi =
[
p(i,1),...,p (i,L)
]
. We ﬁrst
note that the VAE trained to predict note pitches, ϕP , pro-
duces as output a probability distribution over the most
likely pitches to appear at each location k within a window
i; i.e., it produces Prob(pnew
(i,k) = C), Prob(pnew
(i,k) = C#), and
so on. This implies that the VAE produces a stochastic set
of notes; note pitches actually outputted by ϕP are drawn
from the above distribution.
To deﬁne a distance metric d(Pi,Pnew
i ) that compares
how diﬀerent a newly-generated melody Pnew
i = ϕP (Pi) is
w.r.t. a given base melody Pi, we ﬁrst note that if a trained
VAE has high accuracy, the pitch pnew
(i,k) with highest proba-
bility at a location kwithin the new window will match the
true pitch p(i,k) at the corresponding position in the original
input window Pi. Based on this observation, one can deﬁne
a distance metric between Pnew
i (the melody produced by
the network) and Pi (the original melody being perturbed)
by computing the log-likelihood of the stochastic output of
ϕP generating the original base melody exactly even when
the latent space is perturbed by noise:
d(Pi,Pnew
i ) = −log
( 16∏
k=1
Prob(pnew
(i,k) = p(i,k))
)
(1)
= −
16∑
k=1
log
(
Prob(pnew
(i,k) = p(i,k))
)
(2)
When the VAE produces an output Pnew
i that is precisely
equal to the original input Pi, d(Pi,Pnew
i ) is minimized:
the noise applied to the latent pitch representation did not
change the original melody at all. As the VAE model pro-
duces melodies Pnew
i that diﬀer signiﬁcantly from the orig-
inal one, the metric d increases. We now use the above-
deﬁned metric to quantify the eﬀect of diﬀerent values of α
in the generated melodies, demonstrating that it does regu-
late the temporal consistency of the novel melody w.r.t. the
original one (Figures 3a and 3c). We also analyze the eﬀect
of diﬀerent β’s in the generated melodies, showing that a
direct correlation exists between the noise level and the dif-
ferences between the statistical properties of novel melodies
and the original one (Figures 3b and 3d).
To conduct the experiments that follow, we selected 20
random 16-note melody windows from the training set. Each
one was used to create a song composed of 200 repetitions of
that window. We perturbed the latent representations ac-
cording to diﬀerent parametrizations α and β of the Perlin
177
Noise function. Afterward, we used the proposed distance
metric d to quantify how diﬀerent each perturbed window
was w.r.t. the original one, as a function of the amount of
Perlin Noise being applied at that time. 1
Time
Perlin Noise Magnitude
= 0.100
= 0.050
= 0.025
(a)
Time
Perlin Noise Magnitude
= 0.30
= 0.25
= 0.20
 (b)
0 50 100 150 200
Time (window)
0
10
20
30
40Avg. Distance to Original Melody
= 0.100
= 0.050
= 0.025
(c)
0 50 100 150 200
Time (window)
0
10
20
30
40
50Avg. Distance to Original Melody
= 0.30
= 0.25
= 0.20
 (d)
Figure 3: Sample Perlin Noise functions constructed by se-
lecting (a) diﬀerent α values; and (b) diﬀerent β values.
Distance metric over time between novel melodies and the
original one for (c) diﬀerent values of α; and (d) diﬀerent
values of β. Note how the distance/similarity between new
melodies and the original one isdirectly correlated(and thus
regulated) by the corresponding Perlin Noise function.
Figures 3a and 3b show a few sample Perlin Noise func-
tions f(t; α,β) constructed by selecting diﬀerent values ofα
and β. Note how (as expected from the discussion in Section
3.2) α regulates the smoothness of the noise function over
time, and βregulates the magnitude of the noise. Figures 3c
and 3d show the eﬀect that diﬀerent values ofαand β(when
used to parameterize the noise applied to the melody la-
tent representations) have on the distance d(Pi,Pnew
i ) be-
tween newly-generated melodies and the original one. Here,
the vertical axis depicts the average distance between the
newly-generated windows constructed by our network and
the corresponding original ones. In particular, we can com-
pare Figures 3a and 3c and observe that the higher we set α
(thereby allowing the amount of noise applied to each win-
dow to vary more rapidly), the faster d(Pi,Pnew
i ) changes
along the melody. Note also how the smoothness and gen-
eral form of the Perlin Noise function (for diﬀerent values
of α) is directly correlated with (and therefore regulates)
the smoothness and general form of the distance function
d(Pi,Pnew
i ). This can be observed by noting how changes
to the Perlin noise over time cause a direct and proportional
change to the distance metric dat the corresponding times.
These observations imply that α does indeed regulate how
rapidly the similarity between the novel melodies and the
original one might change over time. A similar analysis can
be performed by comparing Figures 3b and 3d: by chang-
ing β, one can regulate how much of the original melody
will be preserved in the newly-generated melodies. When
β = 0.30, for instance, the novel melodies can diﬀer w.r.t.
the original melody up to 4 times more than whenβ = 0.20.
Hence, besides allowing for random melody generation in a
temporally-consistent way, our proposed method also suc-
cessfully introduces a parameterized way of controlling how
1Interested readers can listen to sample generated melodies
at https://www.youtube.com/watch?v=k_z54dV4WoI.
diﬀerent the novel melodies may be over time.
0 50 100 150 200
Time (window)
0
20
40
60Avg. Distance to Original Melody
(a)
0 50 100 150 200
Time (window)
0
5
10
15
20
25
30Avg. Distance to Original Melody (b)
Figure 4: Distance between novel melodies and the original
one when noises are sampled from na ¨ıve distributions: (a)
Gaussian N(0,β/2); and (b) Uniform U[−β,+β].
Finally, we now show that na ¨ıvely perturbing the latent
variables that describe a given base melody using i.i.d. noise
drawn from a Gaussian or Uniform distribution does not re-
sult in the properties that we previously achieved with Per-
lin Noise. Figures 4a and 4b show the average distance be-
tween newly-generated melodies and the original one when
their latent representations are perturbed by noises drawn,
respectively, from N(0,β/2) and from U[−β,+β]. In both
cases, we used β = 0.25 so that it is possible to compare
these results with the ones in Figure 3d. Using a Gaussian
noise distribution (Figure 4a) produces melodies composed
of highly-variable windows of pitches, often followed by a
sequence of windows almost identical to the original ones.
This results in new songs with harsh transitions between
subsequent windows and no control over the smoothness of
the generated melodies. Using a Uniform noise distribu-
tion (Figure 4b) produces melodies with even more abrupt
variations over time. These experiments conﬁrm that, dif-
ferently from was achieved when using Perlin Noise, it is not
possible to use Gaussian or Uniform noise distributions to
control the temporal consistency of the melodies being gen-
erated by the networks, and that a temporally-consistent
and smooth parameterized noise generator is needed.
Pitch Creativity (α)
Pitch Creativity (β)
Time Creativity (α)
Time Creativity (β)
   Notes              
             Time                            
Original Melody New Melody
   Notes              
             Time                            
Figure 5: Graphical interface of our system prototype.
Besides performing the computational analyses above, we
implemented a physical prototype of our system that allows
musicians to use a keyboard to provide base melodies and
to adjust the network’s “creativity knobs” (αand β) to reg-
ulate, in real-time, the process that proposes new melodies
that retain general compositional properties of the songs in
the training set. This prototype is depicted in Figure 5. Our
prototype is composed of a standard MIDI keyboard con-
troller that allows musicians to play and generate melodies
in real time. The resulting MIDI ﬁles are pre-processed ac-
cording to the steps in Section 3 and are given as input
to our method. By using the graphical interface in Fig-
ure 5, users can tweak the values of αand β for each of the
autoencoders—the one generating variations in pitch and
178
the one changing note durations. The resulting melodies
can be heard in real-time. This system may be used as an
aid in the composition process: a musician provides a ba-
sic melody idea to the system, which then produces a given
number of variations of it. The musician selects the one that
is preferable and may feed it back to the system to obtain
more novel ideas/variations, thereby exploiting the system’s
proposed variations to help in the composition process.
5. RELATED WORK
Other approaches exist that generate melodies based on
applying transformations to the latent representation of a
given melody. Most of these either perform simple trans-
lations of the latent vector or apply operations that inter-
polate between existing melodies; e.g., MusicVAE [9, 10].
Our method, by contrast, uses a more general and user-
adjustable noise function to determine how the latent rep-
resentation of a base melody will be modiﬁed over time.
This allows the user to directly regulate how similar the pro-
duced melody will be w.r.t. the original one while retaining
the general characteristics of songs in the training set. [12]
also shares with us the objective of generating melodies,
but it does not aim at producing interesting variations of a
user-provided base melody. Instead, they generate melodies
that are compatible with a given chord progression; this re-
quires the use of a pre-deﬁned rule-based system so that
Gaussian noise (when added to the latent variables) pro-
duces feasible melodies. Other methods exist that generate
melodies by exploiting properties of VAEs; e.g., [3]. Here,
the authors introduce a technique that samples from the
latent space, but that is limited in that each (ﬁxed-length)
portion of the melody being analyzed must be discretized
into a pre-deﬁned maximum number of ﬁxed-duration notes.
Our method, by contrast, can represent, process, and gener-
ate sequences of notes with arbitrary real-valued durations,
since we deploy independent VAEs networks to model se-
quences of pitches and sequences of real-valued note lengths.
[1] presents many diﬀerent methods for music generation
based on deep learning techniques, including autoencoders.
These methods diﬀer from ours in that they usually per-
turb points in the latent space by random sampling or by
interpolating previously-speciﬁed latent vectors, instead of
applying temporally-consistent noise that ensures smooth
melodic changes. Finally, the idea of using Perlin Noise as
a tool to generate new melodies has been studied before;
e.g., in [8]. This work diﬀers from ours in that they do
not use Perlin Noise to modify a given melody. Instead,
they use it to construct something akin to an eﬀects pedal
that applies Perlin Noise directly to an analog sound signal,
thereby obtaining a parameterized sound eﬀect generator.
6. CONCLUSIONS AND FUTURE WORK
We have introduced a method to generate new melodies by
perturbing the latent representation of a base melody with
temporally-consistent gradient noise. This ensures that the
characteristics of the novel melody are smoothly transformed
over time. Furthermore, the discovery of latent represen-
tations via variational autoencoders makes it possible for
the generated melodies to retain general properties consis-
tent with songs in a training set. We proposed the use
of a parameterized type of Perlin Noise and showed that
there is a direct correlation between the noise signal and
the diﬀerences between the statistical properties of the novel
melodies and the original one. Users of our system can in-
terpret the controllable noise as a type of “creativity knob”:
the higher it is set, the more leeway the network has to
generate signiﬁcantly diﬀerent melodies. We also brieﬂy
discussed a physical prototype that allows musicians to in-
teract in real-time with our system via a MIDI keyboard
and a graphical interface.
Our method currently analyzes and processes melody win-
dows, one at a time, to produce smooth variations over a
base melody. However, it does not take into account how
the original song was split into bars. Exploiting this type of
information, as well as heuristics regarding typical note du-
rations within a bar, may allow us to generate new melodies
that more closely retain higher-level rhythmic properties of
the original song. We also wish to study, as future work, the
possibility of extending our system to use recurrent VAEs;
these may be able to more directly model the relation be-
tween successive notes instead of dealing with disjoint se-
quences of neighboring-note windows. Finally, we are also
interested in extending the mathematical and statistical
analyses discussed in Section 4.2 and performing a human-
centered evaluation to measure how eﬀective our method
may be in facilitating the composition process.
7. ACKNOWLEDGMENTS
This work was partially supported by FAPERGS under
grant no. 17/2551-000; by The Research Council of Nor-
way, project no. 240862 (Engineering Predictability with
Embodied Cognition project); as part of the Collaboration
on Intelligent Machines (COINMAC) project no. 261645;
and by Centres of Excellence scheme, project no. 262762.
8. REFERENCES
[1] J.-P. Briot, G. Hadjeres, and F. Pachet. Deep learning
techniques for music generation-a survey. arXiv
preprint arXiv:1709.01620, 2017.
[2] D. Eck, P. Lamere, T. Bertin-mahieux, and S. Green.
Automatic generation of social tags for music
recommendation. In Advances in Neural Information
Processing Systems 20, pages 385–392, 2008.
[3] G. Hadjeres, F. Nielsen, and F. Pachet. Glsr-vae:
Geodesic latent space regularization for variational
autoencoder architectures. In 2017 IEEE Symposium
Series on Computational Intelligence, pages 1–7, 2017.
[4] D. P. Kingma and J. L. Ba. Adam: A method for
stochastic optimization. In International Conference
on Learning Representations, 2015.
[5] D. P. Kingma and M. Welling. Auto-encoding
variational bayes. CoRR, abs/1312.6114, 2013.
[6] T. L. Li, A. B. Chan, and A. Chun. Automatic
musical pattern feature extraction using convolutional
neural network. In Proc. Int. Conf. Data Mining and
Applications. sn, 2010.
[7] K. Perlin. An image synthesizer. ACM Siggraph
Computer Graphics, 19(3):287–296, 1985.
[8] A. Popov. Using perlin noise in sound synthesis. In
Proceedings of the Linux Audio Conference, 2018.
[9] A. Roberts, J. Engel, and D. Eck. Hierarchical
variational autoencoders for music. In NIPS
Workshop on Machine Learning for Creativity and
Design, 2017.
[10] A. Roberts, J. Engel, C. Raﬀel, C. Hawthorne, and
D. Eck. A hierarchical latent vector model for
learning long-term structure in music. arXiv preprint
arXiv:1803.05428, 2018.
[11] Scratchapixel Website. Using perlin noise to create a
terrain mesh. [Online; accessed 2019-01-23].
[12] Y. Teng, A. Zhao, and C. Goudeseune. Generating
nontrivial melodies for music as a service. arXiv
preprint arXiv:1710.02280, 2017.
179