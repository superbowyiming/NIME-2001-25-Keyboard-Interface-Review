Creating a Real-Time Responsive Handbalancing Interface with
HANDâ˜…CS
Linnea Kirby
linnea.kirby@mail.mcgill.ca
CIRMMT
McGill University
Montreal, Quebec, Canada
Christiana Rose
CirqueIT, LLC
Denver, Colorado, USA
christiana.lauren.rose@gmail.com
Jeremy Cooperstock
CIRMMT
McGill University
Montreal, Quebec, Canada
jer@cim.mcgill.ca
Marcelo M. Wanderley
CIRMMT
McGill University
Montreal, Quebec, Canada
marcelo.wanderley@mcgill.ca
Figure 1: Performer handbalancing with HAND â˜…CS.
Abstract
This paper introduces HANDâ˜…CS, a new interface for interdisci-
plinary expression for music, movement, and light. Our interface
augments a pedagogical interface for hand-balancing, Haptics-
Assisted iNversions Device (HAND), and transforms it into one
for artistic expression. It draws upon Lickliderâ€™s concept of man-
computer symbiosis, specifically the commensalism form of sym-
biosis. HANDâ˜…CS strives to embody a performance apparatus
and system with symbiotic connectivity between performer and
interface. This paper discusses the inspiration and background
for such a system pulling from the fields of human-computer
interaction (HCI), music technology and new interfaces for mu-
sical expression (NIME), and circus arts. In addition, it defines
the design and implementation, evaluation of the prototype of
HANDâ˜…CS, and future work.
This work is licensed under a Creative Commons Attribution 4.0 International
License.
NIME â€™25, June 24â€“27, 2025, Canberra, Australia
Â© 2025 Copyright held by the owner/author(s).
Keywords
Circus, Handbalancing, Inversions, Sonification, Performance,
Music Technology, Symbiosis
1 Introduction
Interactive interfaces for musical expression often are inspired
from musiciansâ€™ perspectives. As circus artists, musicians, and
technologists, we present HAND â˜…CS, a new interdisciplinary
interface for expression that utilizes micromovements within
handbalancing to generate music and light. Born from an inter-
face originally designed for handbalancing pedagogy, we describe
HANDâ˜…CSâ€™s inspiration, design, and performance application.
Using the SensingTex mat 1 as an input controller, HANDâ˜…CS
harnesses the pressure array data to approximate the outlines and
weight distribution of the hands and then maps the resulting co-
ordinates to sonic parameters in MaxMSP.2 In addition, wearable
myoelectric sensor and LED augmented sleeves visualize the con-
stant effort and microadjustments necessary to maintain even the
most â€œbasicâ€ of handstands. We provide a historical background
1https://www.sensingmat.cloud/
2https://cycling74.com/products/max
NIME â€™25, June 24â€“27, 2025, Canberra, Australia Kirby et al.
on music and circus technology intersections, describe the tech-
nical design of each component of HANDâ˜…CS, demonstrate and
evaluate the current prototype interface with a performer, and
discuss future directions. HANDâ˜…CS is yet another interface in
the making that embodies our personal motivation and interest
in "making the ordinary extraordinary through clever, subtle,
and deliberate attention to the integration of movement, music,
interactive technology, and light" [20].
2 Background and Inspiration
2.1 Guiding Philosophy
HANDâ˜…CS is inspired by and named after Lickliderâ€™s concept of
man-computer symbiosis where in his words, two organisms be-
come â€œheavily interdependentâ€ and â€œconstitute not only a viable
but a productive and thriving partnershipâ€ [12, 4]. More specifi-
cally, HANDâ˜…CS takes inspiration from the commensalism form
of symbiosis, where one organism benefits from a relationship
and the other is unaffected [1].
Lickliderâ€™s two main goals for man-computer symbiosis in-
clude integrating computing machines into technical problem-
solving and real-time thinking processes [12]. While Licklider
mainly focuses on applications with concrete solutions â€” such as
military tactics and hypothesis testing â€” and fixates on human
language communication between human and computer, more
artistic applications and alternative modes of communication are
left unexplored.
Sonami and Fiebrink embodied the artistic application side
of Lickliderâ€™s man-computer symbiosis through Spring Spyre, an
instrument meant for symbiotic connectivity with its performer
by means of machine learning (ML) [4, 5]. Notably, not only has
Sonami stated that she actively collaborates with the instrument
itself, but it took her over four years before feeling like she be-
gan to understand how to allow the instrument to inform her
own creativity. Though this prototype of HANDâ˜…CS does not
yet include ML framework in its mapping, conceptually we are
designing HANDâ˜…CS to inspire interactions and behavior that
foster a similar type of creative dialogue between instrument, per-
former and audience. Furthermore, we plan for HANDâ˜…CS to be
an interface that evolves over a longer period of time, including
the incorporation of ML, as discussed in Section 5.
HANDâ˜…CS is multimodal, combining music, handbalancing
and light, and also draws inspiration from Salterâ€™s entangled
concepts on performance. Salter distinguishes performance from
other forms of information-sharing through seven characteristics:
(1) an interest in enaction or doing, (2) real-time,
dynamic processes over static objects or represen-
tations, (3) engagement with the temporal moment
of the present, (4) embodiment and materiality, (5)
immanent experience, (6) the effect of both human
and nonhuman presence, and (7) transmutation and
reconstitution. [21, xxiii]
Circus arts often engage these characteristics in performance. By
incorporating sensors into the performance space, and designing
for interaction, performers would be able to better embody these
characteristics and explore new entanglements between these
elements. HANDâ˜…CS creates a space to play and engage in these
concepts.
2.2 Pioneering Circus Tech
In recent years, there has been a small but growing trend to-
wards interdisciplinary circus research. Relevant works have
often focused within the discipline of juggling. These include
Reynolds et al. â€™s responsive environment for the acclaimed Flying
Karamazov Brothers [6] using ultrasonic trackers, accelerometer-
embedded gloves, and augmented juggling clubs [ 17], Willier
and Marqueâ€™s technique for jugglers to control sound generation
through electromyography (EMG)â€“though their method didnâ€™t
support real-time sonic generation [ 24], and Leischner et al. â€™s
approach using juggling balls augmented with accelerometers,
gyroscopes, and WiFi sensors to generate musical output [ 11].
Within the NIME community, Ã–zcan and Ã‡amcÄ± further explore
ideas of playfulness and failure using modified tennis balls with
a microprocessor, six degrees of freedom sensor and Bluetooth
module to generate music [26].
Ground and aerial based movement circus disciplines have
also explored integrating technology and sonic elements. No-
tably, Elblaus et al. presented three proofs of concept for different
modes of sonic interaction within circus arts concluding that
â€œthe use of interactive sound in circus is an excellent way to add
new channels of expressivity to the communication between
performer and audienceâ€ [3]. The second authorâ€™s Sonic Aerialist
eLecTrOacoustic system (SALTO), an interactive musical system
developed for dance trapeze [18] embodied Elblaus et al. â€™s quote,
giving the dance trapeze artist control over musical expression
through an armband containing eight EMG sensors, and a nine
degrees of freedom (DoF) inertial measurement unit (IMU).
3 Towards Creating an Expressive
Interdisciplinary Performance Interface
Ideally, once a handbalancer has mastered control over their bal-
ance from training with HAND, their weight distribution can be
used as input control for a performance apparatus. By intention-
ally shifting weight, the performer could control different events,
including sound, visual, or light. HANDâ˜…CS allows for real-time
expression over these audio-visual elements that are typically
fixed and cued in circus performance giving the performer almost
full creative control. Freeing the performance from strict cues,
allows the artist to eschew the mental overhead of focusing on
strict timing and choreography. They can better engage with and
direct the performance as it currently is developing in the space.
To achieve this, HANDâ˜…CS is compromised of several hardware
and software components: 1) the mat (apparatus), 2) HAND (base
software), 3) Sound software, 4) Myoelectic LED Compression
Sleeves.
3.1 The Mat
The mat used for HAND â˜…CS is the SensingTex pressure mat
platform â€œHealth Mat Dev Kit 1.9â€ and can be seen in Figure 1.
The mat, which feels similar to a thin yoga mat, has a sensing area
of 480 mm by 480 mm containing a 48Ã—48 array of 6 mm diameter
sensors with a 10 mmsensor element resolution [22, 23].
SensingTex provided demo software to view pressure distri-
bution, as well as a simple Python [ 16] skeleton program for
obtaining pressure data from the mat sensors.
3.2 The Base Software
The base software was originally written for the Haptics-Assisted
iNversions Device (HAND), a handbalancing pedagogy device
created by the first author able to correct handbalancing posture
HANDâ˜…CS NIME â€™25, June 24â€“27, 2025, Canberra, Australia
in real-time and can be found in Appendix B. In each cycle, the
software first runs a k-means algorithm on the data obtained
from the mat to determine which points belong to each hand.
Using these points, it calculates the current center of pressure be-
tween the hands, as well as the ideal center of pressure (assuming
equal pressure across all points). Then, the software determines
a correction vector from the current center of pressure to the
ideal center of pressure.
For HANDâ˜…CS, the software added a feature to calculate hand
boundaries. After determining the points belonging to each hand,
the four points containing the most positive and most negative ğ‘¥
and ğ‘¦ values for each hand are saved. Additionally, the HAND
software can provide a real-time visual representation of the pres-
sure matrix (see Figure 2). While useful for the pedagogical tool
and early iterations, a pressure-map visualization of a handstand
is not particularly interesting on its own for performance and
could instead be a hindrance to the performer or distraction to
the audience [2]. For the first prototype of HANDâ˜…CS the correc-
tion vectors and hand boundary data were used to trigger and
shape sound events.
Figure 2: Visualizing the pressure matrix data. Purple indi-
cates no pressure; red indicates highest pressure.
3.3 Sonifying the Handstand
The base software for HAND was augmented to send the Sens-
ingTek data to Cycling â€™74â€™s Max software via Open Sound Con-
trol (OSC) messages. The second author then developed the mu-
sical interface in Max. After the OSC messages are received in
Max they are then scaled to control two different sound modules
and a stereo pan slider.
At this early stage, the musical interface elements of HANDâ˜…CS
draw inspiration from previous creative endeavors and the sec-
ond authorâ€™s journey as a keyboardist, composer and improviser,
combined with the challenge of designing interfaces that convey
the broad strokes and subtle details of performance in circus arts.
Additionally, the concepts of dynamic and static balance, ideas
of effort, pressure and perception are ideas under constant explo-
ration in her work [ 19]. The sonic elements of HANDâ˜…CS also
draw inspiration from Anthony Braxtonâ€™s idea of "springboards
of musical activity" [25] to create sonic pallettes that performers
can start and jump off from.
3.3.1 Mapping. The mapping structure aims to highlight the
internal constant presence and energy in static and dynamic
Table 1: Overview of sonic mapping for demo version
Variable Effect
ğ‘‰ ğ‘¥ Pan right
ğ‘‰ğ‘¦ Pan left
Left Hand (ğ¿) Percussive & organ sound
ğ‘¥ğ‘šğ‘ğ‘¥ğ¿ Playback controller
ğ‘¥ğ‘šğ‘–ğ‘›ğ¿ Pitch
ğ‘¦ğ‘šğ‘ğ‘¥ğ¿ Time stretch
ğ‘¦ğ‘šğ‘–ğ‘›ğ¿ Selection length
Right Hand (ğ‘…) Organ sounds
ğ‘¥ğ‘šğ‘ğ‘¥ğ‘… Playback controller
ğ‘¥ğ‘šğ‘–ğ‘›ğ‘… Pitch
ğ‘¦ğ‘šğ‘ğ‘¥ğ‘… Time stretch
ğ‘¦ğ‘šğ‘–ğ‘›ğ‘… Selection length
Hand in trigger zone Plink tones with reverb and delay
movements, as well as the creative dialogue between the per-
former and the apparatus. A descriptive outline of the mapping
is shown in Table 1. The sonic palette consists of both digitally
generated and pre-composed and recorded sound files.
The correction vector used in HAND is used as a stereo pan
controller, where the vectorâ€™s ğ‘¥ coordinate corresponds to pan-
ning right, and the ğ‘¦ coordinate corresponds to panning left to
communicate shifting balance. Each cycle, the four points con-
taining the minimum and maximumğ‘¥ and ğ‘¦ values for each hand
are used as hand boundaries. These points are then used within
two primary sound modules: one module focuses on sonifying
the data into discrete sound features while the second module
focuses on continuous manipulation of a sound feature. When a
hand boundary hits a defined threshold, percussive sounds in the
plink module and time selections for each of the pre-composed
soundfiles are triggered in the Max patch. The second module
uses the hand boundaries to manipulate pitch and time-stretch
parameters of those pre-composed sound files. The range of max
ğ‘¦ and min ğ‘¥ values for each hand boundary map to pitch and
time-stretch parameters.
It should also be noted the first iteration of mapping is catered
for demonstration purposes, and will be expanded upon in future
work. For this first prototype the thresholds for triggering are
arbitrary. This was intended to give a base and wide variety of
possibilities as we were learning the range of the SensingTek Mat
with one handbalancer. Similarly, the mapping range and scaling
is also in the early stages with the same intention of creating
a base range to allow for a wider set of sonic possibilities. This
range was implemented after observing the sensor output from
HAND with our performer for the demonstration video. With
continued development the ranges will be fine tuned to achieve
more precision and expressivity over the generation triggers and
manipulation ranges.
3.3.2 Sound Modules. The plink module, inspired by the My-
oWare sensors and pedagogical application of HAND, expands
the orbits module from Hollow [19], "The orbits module uses eight
impulse generators (click ) passed through a resonant bandpass
filter (reson~) as well as white and pink noise (noise~, pink~)
through a resonant lowpass filter (lores~)" [ 19]. The eight im-
pulse generator are triggered when different thresholds are met
within the eight boundary points. After they pass through the
NIME â€™25, June 24â€“27, 2025, Canberra, Australia Kirby et al.
resonant lowpass filter these tones are then sent through a preset
pan mixer, delay and reverb modules from Maxâ€™s BEAP3 library.
The parameters of which could be sensor-controlled in future up-
dates. They function as a constant reminder of the ever-present
energy and rhythm of the hands, much like a metronome guiding
a musician. This rhythmic quality was deliberately selected for
the demonstration to accelerate the performerâ€™s learning process
of the interface.
The pitch shift and time-stretch module controls four pre-
composed sound files, including percussive, melodic and har-
monic layers made from metal, wooden, and electric organ record-
ings. The idea behind merging these elements was to give per-
formers a dynamic soundscape they could control with percussive
and fluid changes. The pre-composed sound files are loaded into
Maxâ€™s playlist~object and the speed and pitchshift messages are
modified by the sensor data to stretch and manipulate the pre-
composed material. Although the interface only processes data
from the hand pressure on the SensingTex mat, it empowers the
artist to use their whole body for expressive purposes. As the per-
former moves their legs, they are instantly making adjustments
to their balance and pressure in the hands which is reflected in
the boundary data. By leveraging parameters like stereo panning,
time stretching, and pitch shifting, thereâ€™s a dynamic link be-
tween the leg sweeps or the sharp bends and precise movements
of knees and feet.
3.3.3 Interaction paradigms. This initial sonic springboard aims
to set the stage for multiple types of interaction and interpretation
from both performer(s) and audience. By embracing the concept
of commensalism in symbiotic relationships [12] [1], these sonic
elements can be set in motion by the performer or autonomously
continue without further intervention. This approach grants the
performer flexibility and control â€” whether theyâ€™re up in the air
initiating sound or descending to transition seamlessly into a
grounded move off the mat, the soundscape can persist or pause.
This creates opportunities for extended durations of sound or
silence, as well as moments of dynamic interaction.
3.4 Lighting Up the Handstand
In addition to sound, we wanted to add a visual component that
wasnâ€™t reliant on projection and aided our intent of enhancing
audience understanding of the effort required to maintain even a
â€œbasicâ€ handstand. For HANDâ˜…CS we also added MyoWare muscle
sensors to collect and visualize muscle activation data [13]. The
MyoWare sensors are surface electromyography (sEMG) sensors,
which monitor changes in electrical activity of the muscle groups
they are placed over [14]. Using a compression sleeve, conductive
ribbon, snaps, and conductive thread, and referencing a tutorial
from the MyoWare website, 4 a pair of reusable performance
sleeves were built to each accommodate two MyoWare muscle
sensors with LED shields â€” LED displays built specifically to
snap onto the MyoWare sensors [9]. When activated, the target
muscles trigger the LED shield to activate a number of LEDs
corresponding to the intensity of the signal received (Figure 3).
3.4.1 Placement of the MyoWare muscle sensors.To allow for
some flexibility, four muscle groups were prepared as placement
sites for the MyoWare muscle sensors: two on the upper arm, and
two on the lower arm. Both the flexors and extensors of both the
3https://docs.cycling74.com/userguide/
4https://myoware.com/project/conductive-fabric-electrodes/
Figure 3: Demonstration of the MyoWare muscle sensors
with LED displays. At rest (left), not enough muscle acti-
vation is detected to light up any LEDs. When flexing the
wrist (middle), all ten LEDs light up, indicating significant
muscle activity. After the initial activation (right), the mus-
cle signal is not as strong, so fewer LEDs are on.
upper and lower arm were targeted, as they control the bending
and straightening of the elbow and wrist joints [10].
See Figure 4 for a diagram of conductive ribbon and snap
placement for each of the four targeted muscle groups.
Figure 4: Placement of MyoWare muscle sensor electrodes
and LED displays. Note that the conductive ribbon is sewn
on the inside of the sleeves. The snaps are sewn on the
outside using conductive thread to maintain conductance.
The LED displays are mounted to the MyoWare muscle
sensor, which are in turn mounted to a cable shield.
3.4.2 Construction of the sleeves. The sleeves were constructed
using commercial compression sleeves as the base. Each My-
oWare sensor requires three electrodes in the form of snaps. In
order to create reusable electrodes, three strips of conductive
ribbon per muscle group were sewn onto the inside of the com-
pression sleeves, using the placement determined in Section 3.4.1.
To preserve elasticity, alternating cuts were made on the sides
of the ribbon (see Figure 5 for a visual). Snaps were then sewn
onto the other side of the sleeve using the same placement as
the ribbon using conductive thread in order to maintain conduc-
tivity. Note that before using the sleeves, the conductive ribbon
must be moistened to ensure conductance, similar to consumer
electronics such as the Garmin HRM-Dual heart rate monitor [7].
Four pouches were created to hold the MyoWare muscle sen-
sors with both the LED shield and cable shield attached. A rectan-
gle cutout was created to maintain visibility of the LED displays
HANDâ˜…CS NIME â€™25, June 24â€“27, 2025, Canberra, Australia
and was reinforced with thread to ensure the sensors would not
be able to slip out of the pouch through the display window.
Snaps were sewn to the pouches on the top inside rim as closures
and to the back as attachment points to the sleeves. Because both
the lower and upper arm each have two muscle group options,
two display sites were determined to ensure consistency.
Note that the sleeves are sided, as the muscle groups in the
left and right arm are mirrored. See Figure 4 for a visual of the
LED placement and Figure 5 for a detailed diagram of the sleeve
construction.
Figure 5: Overview of the MyoWare sleeves.
4 Development and Demonstration
Before collaborating with a performer, we scaled the data range
mapping in Max using our body weight and sample data to estab-
lish a base sonic palette. We then asked a trained handbalancer
to test and demonstrate HANDâ˜…CS, as shown in Figure 1. Note
that while the different leg positions provide visual interest, the
ideal center of pressure between the hands remains the same,
as evidenced by the consistency of the shoulder and torso posi-
tion. Additionally, we further tuned the mapping ranges after a
rehearsal with the performer. The demo video URL can be found
in Appendix A.
4.1 A Tool for Improvisational Performance
The demo video exemplifies HANDâ˜…CSâ€™s potential and ease-of-
use for a handbalancer. By generating music in real-time, the
performer is able to fluidly improvise without fear of missing a
musical cue or confusing an accompanyist. That being said, it is
important to keep in mind that HANDâ˜…CS in its current state is
a prototype, not a final product.
We asked our performer to write a statement on her experience
trying out HANDâ˜…CS:
I feel like it was pretty quick to get a gauge of
the music side of things and I didnâ€™t quite have
enough facility to play around with huge differ-
ences in weight shifts, but it was super satisfying
to feel like the music automatically went with my
movements and brought a different element, like
a conversation almost. I had a lot of fun trying to
figure out the responsiveness of the lights related to
the MyoWare â€” I know part of that is related to my
perspective and my area of expertise. Iâ€™m already
super curious about what muscles are firing in a
handstand and then to have a visual representation
related to muscles on/off and speed and change of
contraction was very neat to explore and definitely
something I could feel myself leaning into more.
It was nice to be able to â€œproveâ€ the technology â€”
testing out arm movements just standing and then
comparing the lights in a handstand and I was ex-
cited to be surprised at which was the dominant
muscle based on arm position.
You know, I think I was maybe fully tuning the mu-
sic out while I was doing it/it was not very loud if I
recall correctly, but looking at the video it definitely
feels like I had something to be in conversation with
â€” which even if I donâ€™t understand it or feel like
I can â€œcontrolâ€ it is a very fun thing to have as a
performer. I feel like in my artistic experiences it
is so rarely about control and more about inter-
play, play, and response. Authentic conversation
whether with other artists, space, props, music is a
really magical experience. You can definitely be in
conversation with fixed music too, but even more
enriching, engaging, and exciting when the music
talks back to you.
Based on her feedback, 1) the sonic elements must be amplified
further in the performance or rehearsal space and 2) interest is
perhaps better directed not as a controller/interface but more
as a collaborator. The addition of ML, similar to Sonami with
Spring Spyre, could bolster creativity and lend itself to more of a
collaborator role. However, significant time would be needed for
an artist to become not only comfortable with HANDâ˜…CS, but to
create a true symbiosis with it.
5 Future Directions
Thus far HANDâ˜…CS is a promising new interdisciplinary inter-
face for musical and visual expression of circus based movements
â€“ beginning with hand-balancing. The interface offers a play-
ground to explore Salterâ€™s seven characteristics of entanglement
in performance as described in Section 2.1. The initial creative
work with HANDâ˜…CS provided a demonstration and proof of
concept. Both developers and performers are still iterating and
learning what the system is capable of. There is great potential
to keep developing the interface as its only in its infancy. There
are several areas of continued development weâ€™re considering at
this time; performance, accessibility, expressivity and additional
collaboration.
NIME â€™25, June 24â€“27, 2025, Canberra, Australia Kirby et al.
5.1 Works in Progress
The first performance piece with HANDâ˜…CS is currently in de-
velopment. Stasis is a 3-minute-long piece exploring themes of
perception, effort and stillness through hand-balancing, light,
and sound. Stasis exposes the effort required to perform a â€œbasic
handstandâ€ and asks the audience to contemplate ideas about
equilibrium and the subtle shifts needed to maintain a stillness
â€“ though not an effortless one. Jensenius et al. has previously
explored this concept â€” though with upright performers â€” to
research and develop micromovement-based performances [8].
In Stasis, an expert handbalancer will perform a straight body
handstand with minimal movement; almost imperceptibly shift-
ing their weight to trigger sound and light events. Through
weight shifting, the performer will also be involuntarily firing
various arm muscles, thereby triggering different visual events.
Stasis offers a deeper exploration into the expressivity of the
sonic elements of HANDâ˜…CS. By removing the handstand itself
from the equation, the audience would be left to engage only
with its creations. We plan to premiereStasis in late 2025 or early
2026.
5.2 Porting to an Open Source Language
Environment
As mentioned in Section 3.3, the current iteration of HANDâ˜…CS
uses Cycling â€™74â€™s Max, a proprietary and moderately expensive
software system. Furthermore, users of proprietary software are
held hostage to vendor upgrades, which are often mandatory,
and often break existing codes written using previous versions
of the software.
A more attractive alternative, when available, is the use of
open source software. For example, Pure Data [ 15] is free and
open source, and would allow for easier collaboration with, and
innovation from, other circus artists and musicians. Max was
chosen initially due to the second authorâ€™s familiarity with the
software.
5.3 Augmenting for More Expressive Range
and Collaboration
As a prototype, the preliminary design of HANDâ˜…CS was kept
simple for feasibility of execution, but there is significant poten-
tial for improvement.
5.3.1 Finer control and more complex modulation.Future itera-
tions of HANDâ˜…CS could expand the HAND algorithm sensitivity
to include individual finger control, more nuanced mapping and
more precise threshold settings for the sonic elements and further
expanded visual patterns generated from the muscle activity on
the sleeves. Expanding the algorithm sensitivity to use individual
finger control could improve responsiveness, musical range, and
expressivity. Additionally, incorporating co-modulation of sound
and light events through both the muscle activation and pressure
data could provide more meaning to an audience.
As outlined in Section 4.1, the addition of ML could elevate
HANDâ˜…CS such that the apparatus could learn from the per-
former and vice versa, while facilitating a more dynamic and
organic method for calibration. The dynamic aspect of an ML ap-
proach to calibration encourages an expressive dialogue between
the performer, apparatus and creative output in real-time. It also
can be adapted to extend to more than one type of media, e.g.,
music, lighting, and graphics.
5.3.2 Collaboration. Furthermore, expanding the contextual range
to include not only solo handbalancing, but groups of inter-
changing handbalancers or partner acrobatics acts would amplify
HANDâ˜…CSâ€™s adaptability. Weight limit is of no immediate con-
cern, and supporting a broader range of performance contexts
would benefit HANDâ˜…CSâ€™s performance potential, allowing for
more diverse types of performances.
6 Ethical Standards
This work does not have any potential conflicts of interest. Our
performer was a willing collaborator and agreed to be filmed.
There are no other ethical concerns of note.
Acknowledgments
Thank you to Molly Barger for agreeing to be our performer. We
would also like to thank CIRMMT for the use of the SensingTex
pressure mat platform.
References
[1] [n. d.]. Commensalism. https://www.merriam-webster.com/dictionary/
commensalism
[2] Marion Cossin. 2024. Integrating Technology with Circus Performances. (20
09 2024). SOCS Colloquium.
[3] Ludvig Elblaus, Marie-Andree Robitaille, Maurizio Goina, and Roberto Bresin.
2014. Modes of Sonic Interaction in Circus: Three Proofs of Concept. In
ICMC|SMC|2014 (Athens, Greece, 2014-09-14). 9. https://doi.org/10.5281/
zenodo.850964
[4] Rebecca Fiebrink. [n. d.].Wekinator | Software for real-time, interactive machine
learning. http://www.wekinator.org/
[5] Rebecca Fiebrink and Laetitia Sonami. 2020. Reflections on Eight Years of
Instrument Creation with Machine Learning. InNIMEâ€™20 (Birmingham, United
Kingdom, 2020-07-21). 237â€“242. https://www.nime.org/proceedings/2020/
nime2020_paper45.pdf
[6] FKB 2LLC. [n. d.]. The Flying Karamazov Brothers . https://www.fkb.com/
[7] Garmin, Ltd. 2019. HRM-Dualâ„¢. https://www8.garmin.com/manuals/
webhelp/hrm-dual/EN-US/HRM-Dual_QSM_EN-US.pdf
[8] Alexander Refsum Jensenius, Kari Anne Vadstensvik Bjerkestrand, and Victo-
ria Johnson. 2014. How still is still? Exploring human standstill for artistic
applications. International Journal of Arts and Technology 7, 2 (2014), 207.
https://doi.org/10.1504/IJART.2014.060943
[9] Jimblom and El duderino. [n. d.]. Arduino shields v2 . https://learn.sparkfun.
com/tutorials/arduino-shields-v2/all
[10] Wynn Kapit and Lawrence M. Elson. 1993. The anatomy coloring book (2nd ed
ed.). HarperCollins College Publishers.
[11] VojtÄ›ch Leischner and Pavel Husa. 2023. Sonification of a juggling perfor-
mance using spatial audio. Proceedings of the ACM on Computer Graphics and
Interactive Techniques 6, 2 (12 08 2023), 1â€“6. https://doi.org/10.1145/3597619
[12] J. C. R. Licklider. 1960. Man-Computer Symbiosis. IRE Transactions on Human
Factors in Electronics HFE-1, 1 (3 1960), 4â€“11. https://doi.org/10.1109/THFE2.
1960.4503259
[13] MYOWARE by Advancer Technologies. [n. d.]. Introducing MYOWARE 2.0 .
https://myoware.com/
[14] Jaeu Park, Jinwoong Jeong, Minseok Kang, Nagwade Pritish, Youngjun
Cho, Jeongdae Ha, Junwoo Yea, Kyung-In Jang, Hyojin Kim, Jumin Hwang,
Byungchae Kim, Sungjoon Min, Hoijun Kim, Soonchul Kwon, ChangSik John
Pak, HyunSuk Peter Suh, Joon Pio Hong, and Sanghoon Lee. 2023. Impercep-
tive and reusable dermal surface EMG for lower extremity neuro-prosthetic
control and clinical assessment. npj Flexible Electronics 7, 1 (25 10 2023), 49.
https://doi.org/10.1038/s41528-023-00282-z
[15] Miller Puckette. [n. d.]. Pure Data . https://puredata.info
[16] Python Software Foundation. [n. d.]. Welcome to Python.org . https://www.
python.org/
[17] Matthew Reynolds, Bernd Schoner, Joey Richards, Kelly Dobson, and Neil
Gershenfeld. 2001. An immersive, multi-user, musical stage environment. In
Proceedings of the 28th annual conference on Computer graphics and interactive
techniques (2001-08). ACM, 553â€“560. https://doi.org/10.1145/383259.383324
[18] Christiana Rose. 2017. SALTO: A System for Musical Expression in the Aerial
Arts. 5. http://homes.create.aau.dk/dano/nime17/papers/0058/paper0058.pdf
[19] Christiana Lauren Rose. 2020. Pressure Zones: A body of works exploring
improvisational interdisciplinary performance through an interactive system .
Masterâ€™s thesis. Dartmouth University. https://www.proquest.com/docview/
2416289172/B1652424BF434CE1PQ/1?accountid=12339
[20] Linnea Kirby Christiana Rose. [n. d.]. CirqueIT. https://cirqueit.wixsite.com/
index
[21] Chris Salter. 2010. Entangled: Technology and the Transformation of Perfor-
mance. The MIT Press. https://doi.org/10.7551/mitpress/9780262195881.001.
0001
HANDâ˜…CS NIME â€™25, June 24â€“27, 2025, Canberra, Australia
[22] Sensing Tex, Sl. 2017. PRESSURE MAT Technical Description.
[23] Sensing Tex, Sl. 2024.Pressure Mat Dev Kit 2.0 - Sensing Tex . https://sensingtex.
com/product/pressure-mat-dev-kit-2-0/
[24] Aymeric Willier and Catherine Marque. 2002. Juggling Gestures Analysis for
Music Control. In Gesture and Sign Language in Human-Computer Interaction ,
Ipke Wachsmuth and Timo Sowa (Eds.). Vol. 2298. Springer Berlin Heidelberg,
296â€“306. https://doi.org/10.1007/3-540-47873-6_31 Series Title: Lecture Notes
in Computer Science.
[25] Nate Wooley. [n. d.]. SA16: The Anthony Braxton Issue. http://archive.
soundamerican.org/sa_archive/sa16/index.html publisher: Sound American.
[26] Zeynep Ã–zcan and AnÄ±l Ã‡amcÄ±. 2024. Juggling for Beginners: Embracing and
Fabricating Failure as Musical Expression. In Proceedings of the International
Conference on New Interfaces for Musical Expression . 138â€“141. https://doi.org/
10.5281/zenodo.13904807
A HAND â˜…CS demo video
https://youtu.be/ePlOPTdzB7A
B GitHub repository
https://github.com/linneakirby/HAND
The HANDâ˜…CS Python module and Max patch are located within
the HAND repository.