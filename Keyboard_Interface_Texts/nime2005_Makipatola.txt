Experiments with Virtual Reality Instruments
Teemu Mäki-Patola 
MSc, researcher 
Laboratory of Telecommunications 
Software and Multimedia, Helsinki 
University of Technology 
+358 9 451 5849 
tmakipat@tml.hut.fi 
Juha Laitinen, Aki Kanerva 
Research assistant 
Laboratory of Telecommunications 
Software and Multimedia, Helsinki 
University of Technology 
+358 50 {353 1243, 544 8673} 
{jmlaitin,aki.kanerva}@tml.hut.fi 
Tapio Takala 
Professor 
Laboratory of Telecommunications 
Software and Multimedia, Helsinki 
University of Technology 
+358 9 451 3222 
tassu@tml.hut.fi 
 
ABSTRACT 
In this paper, we introduce and analy ze four gesture-controlled 
musical instruments. We briefly discuss the test platform designed 
to allow for rapid experim entation of new interfaces  and control 
mappings. We describe our desi gn experiences and discuss the 
effects of system features such as latency, resolution and lack of 
tactile feedback. The instruments use virtual reality  hardware and 
computer vision for user input, and three-dimensional stereo 
vision as well as simple deskt op display s for providing visual 
feedback. The instrument sounds are sy nthesized in real-time 
using physical sound modeling. 
Keywords 
Musical instrument design, virtua l instrument, gesture, widgets, 
physical sound modeling, control mapping. 
1. INTRODUCTION 
Physical sound modeling is an ac tive research area. Real-time 
implementations of these m odels m ake it possible to alter any 
parameter of the model while play ing, offering more freedom for 
lively perform ances. This  creates  a need for controllers whose 
input flexibility matches the control com plexity of the sound 
models. Virtual reality (VR) input technology, such as data gloves 
and location/orientation trackers with gesture analy sis, is one way 
of offering several natural degrees  of freedom. We have created 
several musical instruments that use this approach. See our project 
web site for additional information and videos [20]. 
An article by  Paradiso [13] and a book edited by  Wanderley and 
Battier [17] offer a good introduction to existing electronic 
interfaces and gestural controllers . M any of them  have been 
created during the last few decades , even a few com mercial ones 
[23], [26] . However, there has been little research on virtual 
reality interfaces for sound control [1], [8] , [10] . The interfaces 
presented in this paper are perh aps m ore “instrument-like” than 
most of the other virtual reality  interfaces, which have been more 
of the type of interactive sound environments or interactive filters. 
There are few quantitative studies that com pare sound control 
interfaces [16], [19]. Also, the importance of parameter mappings 
has only lately become a topic of active study  [3] , [4] . On the 
other hand, there is a lot of literature on how to design musical 
interfaces [2], [5], [9], [10], [18]. However, more research on the 
effects of individual interface properties  is needed. F or instance, 
virtual reality interfaces are bound to differ from  classical 
instruments, as the medium and its properties are fundamentally  
different. VR technology  introduces some latency , cannot easily 
simulate tactile feedback, and is lim ited in both spatial and 
temporal resolution. For instrum ent design in VR, it is important 
to know the effects of these properties. Only  then can we find out 
the strengths and weaknesses of the approach, and give 
suggestions on the kinds of interfaces it is well-suited for. 
Our analy ses of the pres ented instruments have  two emphases. 
First, we state our design experi ences and research on the above 
concepts. Second, we use Sergi Jorda’s theoretical framework [6] 
to evaluate the potential and expressiveness of the instruments. 
The presented instrum ents utili ze two interaction approaches: 
gestural control, and interac tion with virtual objects called 
widgets. In this context, gestur al control means control by  body  
motion without directly  interacting with any  phy sical or virtual 
objects. Virtual widgets are computer graphic objects the users 
can interact with. For example, a Virtual Xylophone instrument 
includes mallet and plate widgets. 
2. HARDWARE 
Most of the instruments were created in a Cave-like virtual room , 
called EVE, in our laboratory  [22] . Visualization of the virtual 
environment is back projected ont o three walls and a floor of a 
three-by-three meters large cube-shaped room. The users perceive 
the visualization three-dimensionally through active stereo shutter 
glasses. Virtual objects  can be created to be perceived at any 
distance around the user. The rendering is done by  an SGI 
InfiniteReality Onyx2 running an IRIX 6.5 operating system. 
User input comes from data gl oves (5DT) and a magnetic motion 
tracker (Ascension Technologies MotionStar). MIDI devices can 
also be us ed. The m otion tracker has six se nsors. It sa mples the 
three-dimensional location and orientation of each s ensor at a rate 
of 100Hz. The spatial resolution is about 1cm/2 degrees. Both 
gloves measure the user’s finger flexure and return one integer 
number for each finger defining how m uch the finger is bent. The 
gloves are not able to simulate tactile feedback. 
 
Permission to m ake digital or  hard copies of all or  part of this wor k for 
personal or classroom use is granted without fee provided that copies are 
not m ade or  distr ibuted for  pr ofit or com mercial advantage and that 
copies bear this notice and the full citation on the fir st page.  T o copy  
otherwise, or republish, to post on ser vers or  to r edistribute to lists,  
requires prior specific permission and/or a fee. 
Conference’04, Month 1–2, 2004, City, State, Country. 
Copyright 2004 ACM 1-58113-000-0/00/0004…$5. 00. 
EVE’s sound system consists of 15 loudspeakers surrounding the 
cube-shaped room, behind the screen walls. Vector Based 
Amplitude Panning [14]  is used to make the sounds originate 
from any desired direction. 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
11
Two of the instruments were also implemented in a desktop 
environment. Their user interface is  based on a web cam era and 
computer vision technology, and they run on a typical Linux PC. 
3. INSTRUMENT INTERFACE SYSTEM 
The m ain goal of the ins trument interface s ystem was  to make 
prototyping quick and easy. Any input from  any interface can be 
easily mapped to any  parameter(s) of the controlled sound 
model(s). Next, we describe the main components of the system. 
- Input devices. The sy stem can use a variety  of input devices, 
such as data gloves, magnetic trackers, computer vision and MIDI 
controllers. Input is acquired fro m input devices at a high and 
constant rate, allowing for precise rhythmic control of sound.  
- Input processing . The user interface com ponents can access all 
collected input data, and process it in any  way to extract desirable 
features, such as gestures, and produce any  number of new output 
values/events. For instance, a xy lophone plate component may  
send its distance from  a m allet, collision speed with the m allet, 
location of the collision in 3D space, and so on. 
- Control parameter mapping module . The parameters from the 
interface components control the param eters of the sound models 
through a mapping module. The mappings can be created using a 
visual mapping editor application. 
- Control language. We developed an expressive control language 
that contains MIDI as a subset . As phy sical sound models can be 
computationally heavy , contro lling was made possible over a 
network. 
- Visual feedback. The performer is surrounded by  a 270 degree 
virtual reality view, able to produce any  kind of stereoscopic 
computer graphics. 
- Collision detection and other object interaction . We now use a 
simple collision detection library [21] but we are in the process of 
integrating a more physically realistic one [25] to the system. 
- Sound models or MIDI synthesizer. A plug-in based software 
called Mustajuuri [24]  is used as a DSP platform. Each sound 
synthesis model is converted into a Mustajuuri plug-in. As MIDI 
is a subset of our control language, MIDI sy nthesizers can be 
controlled as well. However, in the instruments presented here, 
we did not use commercial MIDI devices, but concentrated on VR 
interfaces and physical sound models instead. 
3.1 Mapping the Control Parameters 
Control parameter mapping m eans routing the us er interface 
parameters into the control parameters of the sound model(s). Our 
configurable mapping module allows any  amount of input 
parameters to influence any amount of output parameters. 
 
Figure 1. Screen shot of a mapping from the mapping editor. 
The mappings are created as node trees (the Nx+y node in the 
figure performs a scaling and offsets the input signal). 
For ease of use, we m ade a graphical editor for creating and 
editing the control mappings. Its visual node tree gives the user a 
clear view of each mapping and of the parameters in it. 
4. ANALYSIS FRAMEWORK 
In Jorda’s analy sis framework [6], efficiency  and learning curve 
are the main concepts. The fram ework is  aim ed at analy zing 
professional instruments, and does not directly  take into account 
user satisfaction or the appeal of the experience. Our ins truments 
were mostly show cases inspecti ng possibilities, designed to offer 
inspiring experiences. Despite this, we use the fram ework in our 
analysis, as  it offers  one us eful perspective. We also share our 
design experiences in the analysis part. 
4.1 Efficiency 
The efficiency  of a m usical ins trument measures the relation of 
musical output complexity to control input complexity. This value 
is scaled with perform er freedom , which repres ents how m uch 
control the performer has over the output. Although a CD player 
produces great m usic, its  perform er freedom  is  low, as  the user 
can do little to influence the m usic. The equation param eters are 
not easily quantifiable, and we will only  attempt to estimate them 
in our analysis. 
tyutComplexiControlInp
reedomperformerF*lexityoutputComp=encyinstEffici  (1)  [6] 
4.2 Learning curve 
Learning curve represents the behavior of efficiency  as a function 
of practice tim e. Sim ple instrum ents, such as a Tibetan Singing 
Bowl, will likely  not allow one to  find new levels of expression 
after years of play ing in the way  a piano or a violin will. A good 
balance between challenge, boredom and frustration is sought [6]. 
Classical already established instruments are accepted to have a 
long learning curve. People are also accustom ed to notice 
subtleties in their playing. However, new instruments are likely to 
require a shorter learning curve in order to become accepted. 
5. INSTRUMENTS 
We will now describe four instrum ents created with the system 
presented earlier. Each description is followed by  design 
experiences and a short analysis of the instrument in question. 
5.1 Virtual Xylophone 
The Virtual Xy lophone interface consists of a user-definable 
number of virtual xylophone plates and two virtual mallets. In his 
hands, the performer holds two magnetic sensors, from which the 
VR mallet visualizations extend. The m allets inherit the location 
and orientation of the sensors and cast shadows on the plates. 
Collisions are detected between the virtual m allets and the 
xylophone plates, controlling the corresponding DSP objects 
accordingly. Hit velocity  is m apped to the am plitude of an 
impulse sent to the sound m odel. The hit location on each plate 
affects the decay  tim e of the sound . The instrum ent allows a 
polyphony of several sound models. 
On the left s ide of the perform er, the interface dis plays a large 
piano keyboard. The performer can grab key s with their left data 
glove. This creates a new xy lophone plate of the chosen note, 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
12
attached to the us er's hand. The note nam e, s uch as  C5, is  als o 
displayed above the plate. The pe rformer can then move the plate 
to any  place and orientation in 3D space. Grabbing and moving 
existing plates is also possible. The arrangement can be saved and 
loaded. Figure 2 shows a performer playing a complex layout. 
The possibility of moving plates and creating new ones allows the 
user interface to be cus tomized even for the needs  of individual 
pieces of music. Chords can be constructed by piling plates on top 
of each other. Short passages can be m ade by  s tacking plates  
further apart, play ed with one swift motion that goes through 
them all. Thus, difficult passages can be made easier to play. 
 
Figure 2. A performer playing the Virtual Xylophone. By 
moving and creating new plates, the performer has created a 
custom interface for playing a particular musical piece. 
5.1.1 Analyses 
Because both the m allets and the plates  exis t only virtually, the 
mallets can pass through the plates without resistance or recoil. 
While at first this seem ed to bother som e users, it elicited a new 
playing s tyle, where plates  coul d be struck from both sides by  
moving hands on sm ooth, elliptical paths. This is m arkedly 
different from traditional Xy lophone play ing, and allows for 
playing with continuous motion.  
Due to the tracker’s limited spatial resolution, the m otion paths in 
playing the Virtual Xy lophone are larger than with a normal 
xylophone. There is a latency  of approximately  60ms between 
striking and hearing the sound. Yet, without tactile feedback, the 
sound and visualization, both delay ed, are the only  cues of the 
exact collision time. Thus, the delay is not so noticeable. 
The Virtual Xy lophone is effectivel y a different instrument than 
the traditional xy lophone. Plates can fill the entire space around 
the user, and are relatively  large. M oving between them  is not as 
fast as with a traditional xy lophone. The sound is more flexible, 
and the configurable user interface offers possibilities to m ake 
playing easier, and for making effects, s uch as  patterns  and 
chords. The instrum ent is intuitive to any one fam iliar with the 
traditional xylophone. 
Based on initial feedback from  early  user tests, the configurable  
interface cons iderably increas ed the appeal of the ins trument. 
Making chords and short sequen ces was  s een to be very  
rewarding. The people testing it were excited and inspired. 
The efficiency of the Virtual Xy lophone can be altered by  
creating different lay outs of plates. The performer has real-time 
control over three parameters: amplitude, note and decay time. All 
are controlled by  varying the sty le of striking. The performer can 
also create chords  and s equences and im prove the interface by  
placing the plates optimally for each musical piece. This increases 
the efficiency considerably. Yet it also interferes with learning, as 
each interface is different and usable mostly for a particular piece. 
We do not y et know if some layout would be a good general 
solution. Possibly some core of the layout could stay the same. 
The learning curve of the instrument is gentle in the beginning. 
The concepts of play ing and m odifying the interface are eas y to 
grasp. The instrument is likely  to stay interesting for a long time. 
Unlike with the traditional xy lophone, this does not come so 
much from acquiring kinaesthetic  mastery , but from discovering 
the possibilities of the interface, and from  better understanding of 
how individual interfaces should be designed. 
5.2 Gestural FM Synthesizer 
The Gestural FM Sy nthesizer is an evolution of the somewhat 
famous Therem in ins trument [26] . The original Theremin, 
invented in 1919, is considered to be the first gesture controlled 
instrument. It is play ed by moving one's hands in the air near two 
antennae. The right hand controls the pitch of the instrument’s 
simple sine wave oscillator. Moving the hand closer to the 
antenna increas es pitch, and m oving it away decreases it. 
Amplitude is controlled by  moving the left hand relative to a 
loop-shaped antenna located on the left side of the instrument. 
Playing the original Theremin is  difficult, and requires  perfect 
pitch hearing, because the only feedback is  aural. F inding notes  
on the instrument requires recognizing them through hearing. The 
basic sine wave sound also limits expressiveness. 
We created the Ges tural F M S ynthesizer in the s pirit of the 
original Theremin. For a m ore interesting sound, we replaced the 
sine wave with a preconfigured FM synthesizer. The sound was 
set to simulate brass with infinite sustain for a continuous sound. 
The instrument is play ed with data gloves. The pitch of the sound 
is controlled by moving the right hand up and down, and the 
amplitude by opening and closing the fingers of the right hand - 
as if the performer was letting the sound out from his hand. When 
closed, the left hand’s relative position alters the tim bre by slight 
changes in modulation indexes. When the left hand is open, the 
modulation indexes remain unchanged. 
The instrument also offers a vis ualization of a musical scale as a 
vertical piano key board. A thin line is projected from the 
performer’s hand to the current pitch on the keyboard. The pitch 
is continuous; the keyboard is only a visual aid. 
5.2.1 Analyses 
As a result of the visual feedback, users found it much easier to 
find particular notes on the Gestural FM Synthesizer, compared to 
the original Therem in. We suspect that the vis ual feedback is  of 
considerable help in early  rehearsal of the instrum ent. Its 
importance is likely  to lessen as the perform er learns  the 
instrument better and comes to rely on their inner presentation. 
With continuous sound instrument s, system latency  is less 
noticeable than with percussion instrum ents. We conducted a user 
test of the just noticeable latency on a normal Theremin [12]. The 
first latency  that the subjects statistically noticed was 30ms. 
However, they  were not certain of their answers until latencies 
reached the length of 60 to 70m s. W e als o tes ted how latency 
affects play ing accuracy , and tim e to reach des ired notes  on a 
Theremin and on a Virtual Reality  Theremin [11]. The study 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
13
suggested that latencies up to 60ms do not impair play ing these 
instruments. Mostly, this is because the ins truments do not offer 
tactile feedback. As a function of latency , tim e to reach notes 
increased roughly five times as much as the introduced latency. In 
light of these tests, virtual rea lity interfaces seem  feasible for 
controlling continuous sound, desp ite the latency . With better 
hardware the latency can also be considerably reduced. 
As mentioned, a traditional Therem in is a difficult instrum ent. 
The few virtuoso performers have y ears of intensive rehearsal 
behind them. The learning curve of the Gestural FM Synthesizer 
is likely  to be s teeper in the beginning as  the vis ual feedback 
supports learning. Seeing where the absolute notes are makes it 
possible to play the instrument without perfect pitch hearing. 
Vibrato is easy to produce on the traditional Theremin by  waving 
the fingers of the pitch hand. However, becaus e of the low 
resolution of our tracker, the required motion is slightly larger on 
our instrument. As a result, it is more tiring to produce vibrato. 
Thus, we added the option of automatic halftone vibrato, switched 
on when the performer moves the right hand away from their 
body. After a threshold, the distance alters the vibrato speed 
between 4Hz and 8Hz. This opti on can be switched off at the 
performer's preference. The pitch s cale can be modified, but 
because of the limited resolution, should not be too small. 
The performer controls four pa rameters with the hand motions. 
Pitch and amplitude are controlled more than modulation indexes, 
which alter the timbre of the sound.  However, all of them can be 
controlled in real-time. Total mastery of controlling all parameters 
fluently and m eaningfully is likely  to require a lot of practice. 
Because of the extra sound param eters, the instrument efficiency 
is likely to be higher than that of a traditional Theremin. 
5.3 Virtual Membrane 
The Virtual Membrane is an interface built around a sound m odel 
of a rectangular membrane [15] , supported at the edges. Its 
physical properties, such as dimensions, tension, damping and a 
few other material properties, can be modified in real-tim e. It is  
visualized as a simple, textured rectangular plate matching the 
physical dimension parameters of the sound model. 
 
  
Figure 3. Playing the Virtual Membrane with a waveform 
visualization and playing the exhibition version. 
The user interacts with the m embrane with two m allets, one in 
each hand. The hit location is  mapped to the excitation point of 
the sound model. Hitting different locations produces a different 
timbre. Hit velocity is m apped to the am plitude of the im pulse 
given to the excitation location. Plate dimensions, tension and the 
speed of the sound in the material are controlled by  virtual slider 
widgets. These are mapped directly to the parameters of the sound 
model. The sliders can be moved by touching them with a virtual 
mallet held in the perform er’s left hand. It is possible to quickly  
“scratch” the sliders even when the sound is playing after a hit. 
The membrane allows the user to experiment with a highly  
flexible sound model. Its sound is  realistic, and the material 
options range from leather to wood, to metal and bey ond. The 
sound propagation can also be visualized as an animated wave 
grid on the virtual plate (see Figure 3). This  vis ualizes how the 
impulse propagates on the surface of the membrane. 
5.3.1 Analyses 
Because of the flexible and realistic sound m odel the instrum ent 
attracts ones interest. Test users rem ained fascinated with the 
instrument for a long tim e. Also, hitting a leather membrane a 
hundred square feet in size produces a magnificent, thunder-like 
sound. The user really feels that he is making something happen. 
The ability to alter physical parameters while the plate is vibrating 
opens up interesting possibilities that are not readily  available in 
the real world. The material can be continuously  changed while 
waves are travelling on it. F or instance, tension can be low as the 
user hits the membrane, resulting in a sound of so low a frequency 
that it is not audible. Increasing the tension after the hit makes the 
waves faster and creates powerful, rumbling sounds. Thus, the 
attack can be omitted, and only the decay used for special effects. 
Another version of the membra ne, based on a web camera and 
displayed at a science exhibition, uses one slider for interpolating 
between predefined m aterial s ets. The interpolation is also 
visualized by  morphing the drum plate material on screen. 
Another s lider is  us ed for m odifying decay  length (material 
friction), and a third one for modify ing the size of the plate. Each 
slider can be “scratched” while playing. 
In the VR room version, the pe rformer has simultaneous real-time 
control over four parameters, impulse amplitude, two-dimensional 
hit location, and one of four s ound model parameters in the form 
of sliders. Additionally, the other three parameters can be 
controlled by focusing on a different slider 
It is not clear what a performer could learn to do when practicing 
the instrument for a long tim e. In addition for using it as a drum  
with a large sound scale it offers  possibilities for interesting and 
uncommon sound effects. Currently  it could be used as an 
atmospheric sound generation tool for movies, as an example. 
Tactile feedback could be added to the interface by  using a MIDI 
drum set for inputting amplitude a nd hit location. The MIDI drum 
could then be expanded with gest ural control of the material 
parameters. Tactile feedback w ould allow for faster and m ore 
accurate play ing. M aterial param eters could be controlled with 
feet, similar to a kettle drum , or with body location. This would 
free both hands for drumming. W ith a M IDI interface the 
efficiency of the instrument would be that of a normal drum 
expanded with larger control complexity and performer freedom 
in the form of additional control of the sound. 
5.4 Virtual Air Guitar 
Playing an air guitar is to imitate rock guitar gestures along music 
without the actual phy sical instru ment. It is more showmanship 
than musical performance and does not require real musical skills. 
Nevertheless, the musical component is alway s present. The goal 
of our Virtual Air Guitar (VAG) project was to create a more 
interactive vers ion of the experience, s omething that the users 
could control instead of just acting along with. The Virtual Air 
Guitar [7] is actually playable. It has an electric guitar sound and 
is controlled with guitar playing gestures performed in the air. 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
14
We have made two versions of the Virtual Air Guitar. One is 
implemented in the virtual room , and the other one on a generic 
desktop Linux PC with a web cam era interface. Both versions use 
the dis tance between the us er's hands to determine pitch, and 
plucking is done by moving the right hand in a strumming motion. 
The pitch scale can be either fre t-based or continuous. Slides are 
made by  moving the left hand along the imaginary  guitar neck, 
and vibrato is produced by shaking the left hand. 
The instrument uses an exte nded Karplus-Strong sound model, 
which is tuned to match a Stratocaster guitar. The sound goes 
through a simulated tube amplifier and effects chain to produce 
the distorted electric guitar sound. 
In order to achieve a realistic result that sounds like real guitar 
playing, we implemented a guitar control language, and created a 
software component that is c ontrolled with this language. The 
component controls the six strings of the sound model, and keeps 
track of the s tate of each s tring and the perform er. The controller 
understands elementary  play ing techniques such as hammer-ons, 
pull-offs, vibrato, fret-based slides and mute. The component then 
modifies the parameters of the sound model accordingly. 
The Virtual Air Guitar interface offers different play  modes such 
as free play , pentatonic s cale play , strumming and rock solo 
modes. The des ktop vers ion becam e a major attraction in 
Finland’s largest science exhibition. For more information on the 
Virtual Air Guitar, see our comprehensive article about it [7]. 
 
Figure 4. Left: rocking on with the VR version of the Virtual 
Air Guitar. Right: computer vision view of the desktop ver-
sion. A web camera interface tracks the yellow gloves and uses 
gesture recognition to detect plucks, vibrato, slide and mute. 
5.4.1 Analyses 
From the selection of instrume nts we have created,  the VAG has 
proven to be the m ost popular. It is thrilling for people to produce 
guitar music out of thin air. Als o, the electric guitar is  a “ cool” 
performance instrument. Who hasn't dreamed of being a rock star, 
even for a short while? 
The VAG is made so that even an unskilled performer can map 
his motion intensity  into relatively  good sounding rock guitar 
playing. A built in scale quantization m akes sure only  notes or 
chords that fit well together are produced. 
Naturally, the quantization gets in the way  of play ing actual 
songs. However, without quantiza tion, finding specific notes is 
difficult, because there is only  air between the hands . The right 
hand’s m otion als o caus es the dis tance to fluctuate. Thus, we 
cannot use too accurate a scale, even though the location of the 
right hand is lowpass-filtered. On  the exhibition version, the two 
play modes include the four chords of the intro to Smoke on the 
Water, and a more free solo mode on a pentatonic minor scale. 
The performer controls pitch and volume. In most of the modes 
this control is lim ited to a predefined s cale. Thus , the actual 
efficiency of the instrument is not near that of a norm al guitar. 
However, the VAG is an entertainment device rather than a 
professional instrument. And as such it functions well. We have 
witnessed users of all ages  play  the instrument. They have been 
curious and enthusiastic about it and have almost alway s walked 
away with a smile on their face. 
As the VAG was designed to be playable for a beginner its 
learning curve is  likely  to flatten already after a short practice. 
However, as it supports the playing, even a beginner can have lots 
of fun with it and experience what it is like to play a rock guitar. 
6. DISCUSSION 
Virtual reality  is a different m edium com pared to our physical 
world. Replicating interfaces of traditional instrum ents in virtual 
reality may not bring about useful  results unless we are extending 
existing instrum ents with additional control. Fam iliarity with a 
real-world counterpart helps to  grasp the concept and supports 
playing in the beginning. However, it may not be easy to find new 
ways of playing, which is needed as the instruments are different. 
Users of traditional instrum ents m ay not even be interested in 
using virtual replicas, as the orig inal ones work fine. Instead, we 
should discover the kinds of interfaces that are best suited for the 
VR medium. For instance, the presented Gestural FM Synthesizer 
extends the expressiveness of the Theremin, and is not hampered 
by latency or lack of tactile feedback. For making instruments in a 
new medium the properties of the m edium and their m usical 
effects should be well understood in order to work around the 
limitations and utilize the strengths.  
The efficiency  of our ins truments is  reduced by low spatial and 
temporal resolution, as well as by  latency . Low resolution 
requires larger motion paths. Togeth er with latency , it makes fast 
and accurate playing difficult. However, these properties could be 
improved with existing technology . Lifting the limitations would 
also allow for faster instruments with vast possibilities. 
One of the m ost promising features of virtual reality  interfaces is 
the potential for visualization. Unlike in phy sical instrum ents, 
where visual feedback can only directly correspond to the physics 
of sound production, such as vibrating strings, virtual instruments 
allow for intelligent visual feedback. A phy sical instrum ent's 
appearance is static, but a virtual ins trument can modify its visual 
features in real-tim e. F or exam ple, the Gestural FM Synthesizer 
includes visualization that helps users reach correct pitches. 
In addition to providing visual cues, visualization could be used to 
teach the us er how to play  an ins trument. F or exam ple, the FM 
Synthesizer could include a scrolling musical score that marks the 
correct position of the hand in the air. 
In addition to teaching m usic, features of the instrum ent sound 
can also be vis ualized. F or exam ple, a virtual widget could 
change color and texture according to the properties of the sound 
it is linked to while play ing. Visualizing sound has been studied 
already, but there is little resear ch available on visualizing the 
interpreted concepts that human s make of sounds. For example, 
the term "brightness" is subjective. The m eanings of thes e terms 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
15
and how to visualize these con cepts could offer interesting 
knowledge for computer instrument design. 
In addition to adding to the usability of an instrument, 
visualization can also affect the entire performance. Playing a 
musical instrument is not an isolated act, but most often placed in 
the context of a concert or other performance. The performer is 
not only producing music, but creating a multimedia show for the 
audience. By visualizing sounds in an aesthetically appealing 
way, artists are given more possibilities for engrossing the 
audience. At the time of writing, music visualization based on 
audio analysis is gaining increased  attention, especially in the 
music style of electronica. These are most often abstract patterns 
that change color and shape to the rhythm and spectral content of 
music. By linking visualization to the instruments that produce 
the sound, it can be made much more detailed, reacting to changes 
in individual sounds. Basically, this generalizes the concept of 
mapping to cover the routing of c ontrol parameters also to the 
visualization as well as to the sound model. 
7. CONCLUSIONS 
In this paper, we have presented a virtual reality software system 
designed for making musical inst ruments. Four instruments 
created with the system were presented. The instruments were 
analysed against a framework that considers the learning curve 
and efficiency of an instrument. Each instrument’s analysis was 
also accompanied with our design experiences. The effects of 
several interface properties were discussed in relation to design of 
musical instruments using novel input and output hardware. 
8. ACKNOWLEDGMENTS 
This research was supported by  Pythagoras Graduate School 
funded by the Finnish Ministry of Education and the Academy of 
Finland and by an EU IST program (IST-2001-33059) [20]. 
9. REFERENCES 
[1] Choi, I. A Manifold Interface for Kinesthetic Notation in 
High-Dimensional Systems. in Trends in Gestural Control of 
Music. Battier and Wanderley, eds., IRCAM, Centre George 
Pompidou, Paris. 2000. 
[2] Cook. P. Principles for Designing Computer Music 
Controllers. NIME Workshop – CHI, 2001. 
[3] Hunt, A., Wanderley, M., Paradis, M. The Importance of 
Parameter Mapping in Electronic Instrument Design. In 
Proceedings of the Conference on New Interfaces for 
Musical Expression (NIME), 2002. 
[4] Hunt, A., Wanderley, M., Kirk, R. Towards a Model for 
Instrumental Mapping in Expert Musical Interaction. Proc. 
of the International Computer Music Conference, 2000. 
[5] Hunt, A. Radical User Interfaces for Real-time Musical 
Control. PhD Thesis, University of York UK. 
[6] Jordà, S. Digital Instruments and Players: Part I – Efficiency 
and Apprenticeship. Proceedings of the Conference on New 
Interfaces for Musical Expression (NIME04), Hamamatsu, 
Japan, 2004. 
[7] Karjalainen, M., Mäki-Patola, T., Kanerva, A., Huovilainen, 
A. and Jänis, P. Virtual Air Guitar. Proc. AES 117th 
Convention, San Francisco, CA, October 28-31, 2004. 
[8] Lanier, J. Virtual Reality and Music. 
http://www.advanced.org/jaron/vr.html (visited 20.1.2005) 
[9] Machover, T. Instruments, Interactivity, and Inevitability. 
Proceedings of the NIME International Conference, 2002. 
[10] Mulder, A. (1998) Design of Virtual Three-Dimensional 
Instruments for Sound Control. PhD Thesis, Simon Fraser 
University. 
[11] Mäki-Patola, T. and Hämäläinen, P. Effect of Latency on 
Playing Accuracy of Two Continuous Sound Instruments 
Without Tactile Feedback. Proc. Int. Conference on Digital 
Audio Effects (DAFx'04), Naples, Italy, 2004. 
[12] Mäki-Patola, T. and Hämäläinen, P. Latency Tolerance for 
Gesture Controlled Continuous Sound Instrument Without 
Tactile Feedback. Proc. International Computer Music 
Conference (ICMC), Miami, USA, November 1-5, 2004. 
[13] Paradiso, J. Electronic Music Interfaces: New Ways to Play. 
IEEE Spectrum, 34(12), 18-30, 1997. Later expanded as an 
online article, 1998.  (visited 20.1.2005): 
http://web.media.mit.edu/~joep/SpectrumWeb/SpectrumX.ht
ml  
[14] Pulkki, V. Spatial sound generation and perception by 
amplitude panning techniques. PhD thesis, Helsinki 
University of Technology, Espoo, Finland, 2001. 
[15] Trautmann, L., Petrausch, S., Rabenstein, R. Physical 
Modeling of Drums by Transfer Function Methods. Proc. 
Int. Conf. on Acoustics, Speech & Signal Processing 
(ICASSP), Salt Lake City, Utah, May 2001. 
[16] Vertegaal, R. Eaglestone, B. Comparison of Input Devices in 
an ISEE Direct Timbre Manipulation Task. Interacting with 
Computers 8, 1, pp.113-30, 1996. 
[17] Wanderley, M., Battier, M. Eds. Trends in Gestural Control 
of Music. Ircam - Centre Pompidou - 2000. 
[18] Wanderley, M. Performer-Instrument Interaction: 
Applications to Gestural Control of Music. PhD Thesis. 
Paris, France: Univ. Pierre et Marie Curie - Paris VI, 2001. 
[19] Wanderley, M., Orio, N. Evaluation of Input Devices for 
Musical Expression: Borrowing Tools from HCI. Computer 
Music Journal, 26:3, pp. 62-76, Fall 2002. 
[20] ALMA project instruments page (visited 13.4.2005):                                       
http://www.tml.hut.fi/~tmakipat/alma/almawebisivu/HUTT
MLIndex.html  
[21] ColDet Library website. (Visited 20.1.2005) 
http://photoneffect.com/coldet/ 
[22] EVE home page. http://eve.hut.fi/ (Visited 30.1.2005) 
[23] I-Cube website. (Visited 18.1.2005) 
http://infusionsystems.com/catalog/index.php 
[24] Mustajuuri DSP software webpage (Visited 19.1.2005) 
http://www.tml.hut.fi/~tilmonen/mustajuuri/ 
[25] Open Dynamics Engine (Visited 20.1.2005) http://ode.org/ 
[26] Theremin info pages. (Visited 19.1.2005) 
http://www.theremin.info/  
[27] The Yamaha Miburi System (visited 20.1.2005) 
http://www.spectrum.ieee.org/select/1297/miburi.html 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
16