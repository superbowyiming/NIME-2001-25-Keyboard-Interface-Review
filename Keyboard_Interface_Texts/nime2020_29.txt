EXP ANDING ACCESS TO MUSIC TECHNOLOGY
Rapid Prototyping Accessible Instrument Solutions For
Musicians With Intellectual Disabilities
Quinn Jarvis-Holland
PCC Adaptive
Instruments Project
705 N Killingsworth St
Portland, OR 97217
quinnjarvisholland@gmail.com
Crystal Cortez
PCC Adaptive
Instruments Project
705 N Killingsworth St
Portland, OR 97217
crystalquartez@gmail.com
Station
∗
PCC Adaptive
Instruments Project
705 N Killingsworth St
Portland, OR 97217
lnathan.gammill@pcc.edu
Francisco Botello
PCC Adaptive
Instruments Project
705 N Killingsworth St,
Portland, OR 97217
franciscobotelloungson@gmail.com
ABSTRACT
Using open-source and creative coding frameworks, a team
of artist-engineers from Portland Community College work-
ing with artists who experience Intellectual/Developmental
disabilities prototyped an ensemble of adapted instruments
and synthesizers that facilitate real-time in-key collabora-
tion. The instruments employ a variety of sensors, send-
ing the resulting musical controls to software sound gener-
ators via MIDI. Careful consideration was given to the bal-
ance between freedom of expression, and curating the pos-
sible sonic outcomes as adaptation. Evaluation of adapted
instrument design may diﬀer greatly from frameworks for
evaluating traditional instruments or products intended for
mass-market, though the results of such focused and indi-
vidualised design have a variety of possible applications.
Author Keywords
NIME, proceedings, rapid prototyping, adaptive, inclusive,
music, controller, MIDI, microcontrollers, creative coding,
intellectual disabilities
CCS Concepts
•Human-centered computing→ Interface design pro-
totyping; Accessibility design and evaluation meth-
ods;•Applied computing → Sound and music comput-
ing;
1. INTRODUCTION
Innovative Digital Musical Instruments (DMIs) and audio
eﬀects can often bypass some presumed necessary abilities
or talents. For example, autotuning algorithms allow un-
∗N. Gammill
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’20, July 21-25, 2020, Royal Birmingham Conservatoire,
Birmingham City University , Birmingham, United Kingdom.
trained vocalists to hit perfect pitches and “all in one” mix-
ing eﬀects hide a variety of complex DSP tasks behind a
single “macro” knob. New technology that is widely acces-
sible consistently gains traction and constantly reshapes the
professional landscape of music. Increasingly, even highly
technical professions such as mastering have been challenged
by AI assisted DSP.[3]
To remain hopeful in this fast changing climate it is im-
portant that we harness and celebrate the ways that these
technologies increase access to, and facilitate hyper-localized
design and musical expression for under-served populations
including people with intellectual/ developmental disabili-
ties.
The idea for a project building adapted instruments was
brought forth by Daniel Rolnik (an outsider art critic and
curator, who at the time was working with Portland Art and
Learning Studios [7]) and Dan Wenger1 at PCC. The Port-
land Community College Cascade campus interactivity lab 2
provides an audio-oriented makerspace and classes teach-
ing microcontrollers and programming to musicians. In
contrast to more traditional engineering environments, the
lab has empowered many queer, trans, and neuro-divergent
artists to utilize electronics and programming. The group
of engineers - experienced students and lab techs -all em-
ploy a variety of code, sensors, microcontrollers, and midi
devices in their own respective installation art and musical
performances.
In the initial meeting Mx Rolnik discussed possible out-
comes for the project as well as the needs and current equip-
ment of the artists with cognitive disabilities at PALS. Many
artists at the Studios enjoyed making music, but due to the
layout of the Studios and the traditional non-adapted in-
struments (piano, guitar, drumkit) at their disposal, “jam-
ming” could become cacophonous. It was suggested that
the instruments in question could somehow connect to each
other as artists frequently collaborated in the open-ﬂoor
Studios space. For more consonant improvisation the de-
signs could employ adaptive constraints to pitch and tim-
ing. Mx Rolnik also gave the team a sense of the ways some
artists at PALS prefer expressing themselves (eg, vocally-
loud, vocally quiet, small sign language, large gestures).
1PCC Cascade Campus Arts & Professions Dean
2part of the Music and Sonic Arts program [8]
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
149
Notably, diﬀerent individuals with the same diagnosis had
vastly diﬀerent preferred range of movements and methods
of self expression. This observation is at odds with design
processes that rely on broader user-demographic data or
designing for all people with a speciﬁc intellectual disabil-
ity. The initial meeting gave the team a chance to learn
important information relating to disability rights, people
ﬁrst language, and the social model of disability-
A social model of disability presents the individual as
needing access to support and assistance in the same way
as any other person. For example, in a lecture hall chairs are
provided for the audience to sit. These are not considered
aids or specialist devices; they are normalised as part of the
expectation that people would become tired if they had to
stand up for any length of time. Chairs are provided for
the comfort and assistance of the audience in a completely
unremarked way. Yet if someone needs special access to a
room or venue this can sometimes be presented as a diﬃ-
culty and problematised. The principles of inclusive design
recommend that it is not only people who walk into a venue
who should be considered but all members of society includ-
ing wheelchair users and parents with prams/buggies. [5]
The parallels that can be drawn to considerations for DMI’s
are rather clear. In order to best care for people in our com-
munities who experience disabilities we must think of them
in the design of things that will be in use in their community.
Failing to make accessible designs, according to the social
model, is equivalent to building in impairment. “A person
might be very musical and enjoy the violin, saxophone, or
piano. But if the individual does not have the use of both
arms, he or she is unable to play these instruments. And
like buildings, instruments are created by human artisans.
So, from a social model perspective, because these instru-
ments are not designed with a person with an impairment
in mind, they prevent some individuals from making mu-
sic with them even though their impairments do not inhibit
them from enjoying music.” [1] Abramo makes an excellent
point here applying the social model of disability to instru-
ment design. However the example given doesn’t encom-
pass the variety of experiences and impairments faced by
people with disabilities, particularly those experiencing in-
tellectual disabilities. Abramo’s example does not challen ge
the prevalence of “meritocracy” and a marked penchant for
valuing skill or virtuosity above self-expression. Intellec-
tual disabilities do not inhibit people who experience them
from enjoying music, or desiring to play music. Aspects
that may be inhibiting are having a vast number of con-
trols, requiring precision rhythmic timing, reading quickly,
or having too many pitches to choose from. Coincidentally,
these are also aspects of an instrument’s design that inhibit
non-musicians, children, and the elderly. In turn- building
interfaces with the intent of avoiding impairments can serve
multiple populations.
2. PROJECT P ARAMETERS
The team was given a hardware parts budget of 400 USD
to build the control surfaces and to design a standalone
application(s) in Cycling 74’s Max 8. Keeping a relatively
small budget for developing four instruments, and designing
freely redistributable applications were both important con-
siderations in order for the project to possibly seed similar
eﬀorts at other makerspaces or studios. The most impor-
tant project parameters were decided on as follows:
• 400 budget for parts and materials
• Four or more controllers connected to their respective
sound generating softwares
• Utilize a variety of sensors
• Empower artists artists with musical expression, choice
• Intuitive and sonically inspiring for skilled or non-
disabled musicians
• Consonant or in-key improvisation
• Facilitate some form of quantized rhythm as well as
real-time playing
• Sounds complement each other when played together
• Low-cost materials
• Documentation of designs for DIY re-use
Control surfaces were developed using PJRC’s Teensy
and Adafruit’s Feather uControllers with the arduino IDE.
Both microcontrollers have excellent diagrams, open source
schematics, libraries, and other resources online. In particu-
lar PJRC’s website has extensive documentation and guides
on using usbMIDI in projects. For instruments that bene-
ﬁt from being wireless, the Feather was the preferred dev
board due to the built-in (and simple to program) atwinc
WIFI chip. MAX/msp was chosen for developing the sound
generation side of the instruments- in part due to the en-
gineers’ familiarity but also because of the immediacy of
prototyping with visual and object oriented programming.
Routing patches in MAX saves time compared to writing
the same DSP operations in openFrameworks, faust, or
other lower level languages. Having visual representations
of operations and signal ﬂow is particularly useful for mem-
bers of the team who are visual artists and visual learn-
ers. While not open source, MAX can publish executables
of a patch, allowing for free distribution of our completed
programs. While this has worked for the project param-
eters with PALS, fully open source alternatives should be
explored for further development of the software.
3. PROTOTYPES AND CLIENT INTERAC-
TION
The team visited the studio space at PALS to meet some
of the artists as well as staﬀ members who were excited
about getting new instruments. This served as a time to
brainstorm possible form factors and to observe the chal-
lenges they currently face with non-adapted instruments.
The team spoke with artists there as well as staﬀ facilita-
tors who all had a variety of perspectives and ideas about
new instrument solutions. Using the parameters given at
the beginning of the project, communicating with artists
and staﬀ clients, and making our own observations, proto-
type controllers using various sensors were developed for the
clients to play and for the engineers to get a better sense
of what kinds of movement and interaction the clients pre-
fer to use. These controllers, paired with sound generation
patches in software, quickly found their place as DMI’s.
3.1 Initial Designs
Auto Scaling Touch Synth
A box with small stripes of etched copper on a PCB panel
that sends note-on messages when touched. The sound gen-
eration is done in a MAX patch with a simple Karplus/Strong
implementation to give a plucked string sound.
Interactive Drum Sequencer
A GUI and set of sensor pads that control a drum machine,
tempo, and modulation. The software GUI shows accompa-
nying visual feedback of the 4 sensor pads and shows when
they have activated one or committed to a sequence. The
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
150
drum sounds were samples triggered in MAX that would be
aﬀected more intensely or less intensely with ﬁlters depend-
ing on how hard the pad was pressed. With a computer
vision script, the computer camera tracked bodies moving
to the left or right to speed up or slow down the sequence.
This was represented by a slider on the screen as well as
shown in a video of themselves in the camera embedded in
the GUI.
Rotation Detecting Headband Synth
An accelerometer embedded in a wearable headband, pro-
grammed to track the rotation and movement of the head.
Sound generation is done in a MAX with a subtractive syn-
thesizer, letting the controller move the notes of a synth
up and down in accordance to the degree of rotation of the
head left or right. Moving the head side to side triggered
an added harmony, and up and down added more or less
reverb to the synth.
Handheld 9 Degrees Of Freedom Controller
Compact and wireless handheld device containing a 9 De-
grees of Freedom sensor detecting orientation and rotation
of the device. The handheld controller drives the same
sound generating patch as the headband synth, but with
up and down tilting controlling the pitch in glissando, and
left and right tilt controlling ﬁlter cutoﬀ.
Xbox Kinect Air Harp
An Xbox Kinect IR camera feed is interpreted in a Process-
ing sketch to detect human bodies. The Processing sketch
provides visual feedback from what its camera is capturing,
through a colored ﬁlter, to the GUI. The rectangular area
captured by the IR camera is divided into columns and rows
of cells. A detected body part moving from one cell to an-
other triggers a note-on at a speciﬁc pitch, sent as OSC to
MAX - triggering a guitar-like Karplus Strong patch
3.2 Evaluation & Observations
There is a vast amount of study into HCI and DMI eval-
uation, however most traditional or commercially oriented
frameworks do not serve minority populations such as peo-
ple with intellectual disabilities. For this application the
team opted for a mostly unstructured client-instrument in-
teraction time. Thought was given to the project goals of
musical expression, immediacy, performance, and collabora-
tion. Making calls on whether an instrument performance is
bad or good is challenging when working with a population
that subverts traditional concepts of intellectual hierarchy.
For most instruments, those traditional values are a central
focus above accessibility.
“[...] there may be many perspectives from which to view
the eﬀectiveness of the instruments we build. For most per-
formers, performance on an instrument becomes a means of
evaluating how well it functions in the context of live music
making, and their measure of success is the response of the
audience to their performance. Audiences evaluate perfor-
mances on the basis of how engaged they feel by what they
have seen and heard. When questioned, they are likely to
describe good performances as ” exciting” ” skillful” ” musi-
cal.” Bad performances are ” boring,” and those which are
marred by technical malfunction are often dismissed out of
hand. If performance is considered to be a valid means of
evaluating a musical instrument, then it follows that, for the
ﬁeld of DMI design, a much broader deﬁnition of the term
” evaluation” than that typically used in human-computer
interaction (HCI) is required to reﬂect the fact that there
are a number of stakeholders involved in the design and
evaluation of DMIs.” [6]
With instrument prototypes functioning, the team and
clients met again to beta test and collect observations. For
the testing, the instruments were connected to a mixer and
PA and initialized to the same key and scale. The engineers
then stepped back to observe what sorts of interactions arise
naturally. Later the engineers took a more hands-on posi-
tion with the artists to collect feedback and to explore any
intended uses of an instrument the clients had not harnessed
yet. Artists preferred to play all of our instruments collec-
tively. While the sound modules were diﬀerent there wasn’t
enough sonic space between the sounds we chose for each
synth to be fully distinguishable from other instruments in
the room, especially because all sounds are emitted from
the same speakers.
The artists had vastly varying levels of comfort with each
instrument, as with general expression. The most outgoing
and physically energetic artists tended to appreciate the
instruments using sensors that captured a lot of movement,
such as the air harp or 9DOF controller. Artists who were
more physically or socially reserved gravitated to the more
predictable and involved instruments - the drum sequencer
and touch synth Notable observations by instrument:
Auto Scaling Touch Synth
• Thin copper strips are too small an area for artists to
easily “return” to a desired pitch
• Sonic range of such a small number of keys is too lim-
ited
• Note-on triggers from the controller, and the string-
pluck sound module are momentary, sustained key
presses do not yield sustained sound
• Very immediate and repeatable action to sound reac-
tion
Interactive Drum Sequencer
• The artists were very engaged with the visual feedback
program and were interested in seeing their own image
transformed by the choices they were making while
playing the instrument.
• The artists were utilizing the face tracking speed con-
trol by moving their bodies left to right but it became
obvious that the group of artists preferred collabora-
tive creation with the instrument, and the camera’s
computer vision was not programmed to track multi-
ple faces and send erroneous control messages.
• Diﬀerent ranges of dexterity and styles of playing be-
came apparent. Some artists preferred lightly touch-
ing the pads while others preferred to push hard on
them. The sensors in the pads did not allow for this
range of pressure application.
Headband Synth
• The artists took to playing this instrument intuitively,
however there wasn’t an obvious connection between
action and sound reaction aside from turning the head
left and right and the parameters they were control-
ling.
Handheld 9DOF Controller
• Most artists were very comfortable coming up and
grabbing the instrument.
• Since it was small enough to be comfortably held they
were able to really move it around freely and discover
all kinds of sounds.
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
151
Kinect Air Harp
• Popular with more physically energetic or expressive
artists.
• Artists enjoyed visual feedback from the camera feed.
3.3 Revision & Further Development
After client interaction and testing the team shared obser-
vations, and made revisions where appropriate. Roughly
halfway through the process from initial tests to ﬁnal work-
ing prototype, the team hosted another testing session with
their in-development instruments. This allowed another
round of client observation, and strengthened the iterative
feedback loops between the artists and instruments, and the
observations and design. These are the revisions and devel-
opments for each instrument:
Auto Scaling Touch Synth
• Pluck sound generation patch layered with a poly-
phonic multi- oscillator synthesizer with sustain and
diverse set of possible timbres (programmed into pre-
sets)
• Blank PCB panels hand painted and etched to yield
organic-edged keys
• Keyboard expanded from 12 to 24 keys by wiring the
etched PCB panels to two MPR121 cap-touch sensing
chips
• Added two momentary buttons to cycle the scale and
cycle through presets
• Enclosure designed to house the electronics of the key-
board, to be milled out of plywood
Interactive Drum Sequencer
• Replaced the piezo sensors with pressure sensors that
could detect a greater range of touch
• Sample drum sounds were replaced with synthesized
drum sounds in Max which provided a wider range of
parameters to manipulate
• Added a button to switch between eﬀected and dry
drums
• Removed the face tracking component and replaced it
with a touch capacitive strip on the sequencer itself to
set the rate of the sequence, avoiding multiple player
bug
• Laser cut an enclosure that could be either a handheld
or table top instrument
Headband Synth
• Added the capability to change the key and scale of
the synth to ﬁt with other instruments that might be
playing as well.
• Added preset options to provide a range of sounds
• Exaggerated the eﬀects associated with the up and
down and side to side movement of the head to make
interaction clearer
• Developed a GUI in Max for artists to adjust presets,
scale, and key
Handheld 9DOF Controller
• Upgraded the 9-DOF sensor to one that allowed for
more reliable reading of heading (yaw).
• Developed a smaller form factor to allow for it to be
attached to the hand and allow for more people to use
the instrument.
• Installed a battery and wiﬁ chip to make the device
un-tethered
Kinect Air Harp
• Reﬁned note-on detection to produce fewer clustered
triggers
• Updated the index of note values to include the same
scales and keys as the other instruments
• Re-arranged the scale degree placement in the cam-
era’s rectangular grid to a more musically and har-
monically rewarding pattern
One of the most apparent observations from visiting the
PALS Studios was that playing music can be diﬃcult when
many people with various non-adapted instruments are play-
ing together in close quarters. To play “in sync” with other
musicians can require lightning fast perception and solid
timing. Fortunately, computers are well adept at both tasks:
our team built a master sync patch in max that sends OSC
over wiﬁ, any other instruments plugged into laptops run-
ning their app can connect to the same wiﬁ network in order
to receive the OSC. The primary OSC message controls the
variable that picks a particular scale on the “scaler” pre-
build in max. Rigid mapping of each instrument to the
same scale does restrict a lot of possible musical choices -
however the simple setup seemed eﬀective almost immedi-
ately. For additional musical variation, the OSC reception
can be turned oﬀ to manually select a scale. This is notable
for vastly increasing the variety of possible harmonies and
intervals intervals between two diﬀerent modes.
4. CONCLUSIONS
The instruments after revisions and further development,
began to serve their intended purposes well. More notably,
the sound generators all compliment each other nicely, with
certain instruments providing higher notes and others ﬁll-
ing in the lows. If there were no budget or time restraints it
would be valuable to repeat the testing and revision phases
with various client groups. With all the instruments locked
to the same key “wrong” notes are mostly omitted. While
this diminishes an individual player’s choices and freedom,
it also serves to empower the player by lessening the fear of
failure, and increasing the musical cohesion of the player’s
resulting sounds with the group’s. While it’s important to
consider the freedom in failure and limitlessness, restrictions
may not be opposite to creative expression. As explored by
Thor Manussen in “Designing Constraints:” “Margaret A.
Boden deﬁnes constraints as one of the fundamental sources
for creativity: ” [F]ar from being the antithesis of creativity,
constraints on thinking are what make it possible. . . .
Constraints map out a territory of structural possibilities
which can then be explored, and perhaps transformed to
give another one” (Boden 1990, p. 95). For Boden, the con-
tinuity of cultural constraints constitutes the possibilityto
evaluate creative work, or to recognize ideas as creative. All
cultures are founded on constraints,- they are the rule-sets
that maintain dynamic unity.” [4] Magnussen and Boden
both make a compelling case for constraints empowering
expression and creativity here. In this project the great-
est success was found in letting artists with disabilities play
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
152
the instruments together and the result being a tradition-
ally “in tune” and sonically pleasing tapestry. When eval-
uated against the primary goals, the instrument ensemble
designed through this project was a success as it empow-
ers players with the unity of collaborative music making
regardless of ability or musical knowledge. Further, the
hyper-localized rapid development process used can be ap-
plied to similar design issues for minority populations that
traditional design processes and evaluation frameworks may
not serve.
4.1 Links
Documentation and project ﬁles will be made available on
github:https://github.com/pccadaptiveinstrumentsteam/
PCC-Adaptive-Instruments-Project .
5. ACKNOWLEDGMENTS
Thanks to ACM SIGCHI and NIME for allowing us to mod-
ify templates they have developed. Our thanks to Daniel
Rolnik, the artists and assistants we worked with at PALS,
Dean Wenger, PCC, Jesse Mejia, Darcy O’neal, Paul Stof-
fregen, and Cycling 74[2]
6. ETHICAL ST ANDARDS
Funding for the project - 400 USD, came from Cycling 74,
the publisher of MAX/MSP[2]. To avoid conﬂict of inter-
est we have avoided claiming their product as better than
another, or comparing various commercial products, but
rather share why it was useful to us. We also acknowledge
that the commercial nature of the software is a drawback
and are working on - and encourage others to develop -
fully open source platforms. Engineers were compensated
for hours spent meeting and building the instruments as
PCC “casual hire” at 15 USD an hour. Mx. Rolnik led a
discussion for the team around best practices for interact-
ing with the clients with disabilities without disrespecting,
overlooking, or misinterpreting their reactions. Mx. Jarvis-
Holland also provided information on “People First Lan-
guage” [9] and the social model of disability to inform the
team’s vocabulary and to encourage a musical rather than
medical approach.
7. REFERENCES
[1] J. Abramo. Disability in the classroom: Current trends
and impacts on music education. Music Educators
Journal, 99(1):39–45, 2012. JSTOR
www.jstor.org/stable/41692695.
[2] Cycling 74. https://cycling74.com/.
[3] J. Drake. Ai and music. Sound On Sound,
15(5):795–825, November 1993.
[4] T. Magnusson. Designing constraints: Composing and
performing with digital musical systems. Computer
Music Journal, 34(4):62–73, November 2010. JSTOR
www.jstor.org/stable/40962941.
[5] J. F. Noone. The applications of mainstream music
technology to facilitate access to creative musical
experiences for people with disabilities., June 2018.
https://193.1.102.136/bitstream/handle/10344/
7570/Noone_2018_Applications.pdf.
[6] S. O’Modhrain. A framework for the evaluation of
digital musical instruments. Computer Music Journal,
35(1):28–42, 2011. JSTOR
www.jstor.org/stable/41241705.
[7] PALS. Portland art and learning studio.
https://portlandartandlearningstudio.com/.
[8] PCC. Creative coding program.
https://www.pcc.edu/programs/
music-and-sonic-arts/creative-coding.html.
[9] K. Snow. Disability is natural, 2009.
https://www.disabilityisnatural.com.
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
153