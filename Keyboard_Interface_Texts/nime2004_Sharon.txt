The Stranglophone: Enhancing Expressiveness In
Live Electronic Music
Michael Eyal Sharon
New York University - Interactive
Telecommunications Program (ITP)
721 Broadway, 4th Floor
New York, NY 10003, USA
212-998-1880
ms1671@nyu.edu
ABSTRACT
This paper describes the design and on-going development of
an expressive gestural MIDI interface and how this could
enhance live performance of electronic music.
Keywords
gestural control, mapping, Pure Data (pd), accelerometers,
MIDI, microcontrollers, synthesis, musical instruments
1. INTRODUCTION
The Stranglophone was developed to investigate alternative
strategies for gestural mapping and expressive control of
electronic music. The goal was to develop a controller for
electronic musicians that could allow them to “step away”
from their laptop screens, but retain and possibly enhance the
playability of their custom software instruments. The hardware
was assembled using readily available electronic components,
plastics and wood. The software was written in Miller
Puckette’s Pure Data (pd) graphical programming
environment.
2. BACKGROUND
Live electronic music performance has at various times been
criticized by the inability of it’s performers to effectively
convey emotion and expression using most of the commonly
available interfaces such as a laptop, keyboard, mouse. This
inability to effectively communicate emotionally with the
audience can be directly related to a lack of direct gestural
performance in a traditional laptop musician’s repertoire [1].
A gesture can be defined as a body movement which conveys
information [2]. An expressive instrument can be broadly
defined as one that uses an effective mapping scheme and
overall metaphor to convey the feeling of the musician to the
audience [3]. In traditional acoustic sound generation, the
sound is generated by a visible physical gesture of the
performer which members of the audience can observe and then
create a natural association or mapping of that gesture to the
creation of that sound. In electronic music performance, the
absence of a clearly identifiable gesture can create ambiguity
in the audience’s perception of the creation of the sound and
can lead to an otherwise stellar performance being unfairly
branded as “emotionless” or (even worse) “dull”.
Compounding this problem, gestures traditionally associated
with the performing of electronic music such as the movement
of a mouse or typing on a computer keyboard are small, subtle
and general movements which do not suggest any specific
expression or association with sound creation.
Although many genres of electronic music may not require a
“lead” or “performative” aspect to their performance, there are
many instances where an alternative interface could create a
superior live performance by effectively bridging the gap
between the physical gesture of the performer and the
expectations of the audience. Although there has recently been
renewed commercial interest in gestural controllers for
electronic music performance, such as the IR sensors found in
Roland’s MC-505 Groovebox[4] or in Alesis’ AirFX [5]
controller, these products are typically used to how a sound is
effected or parameters of those effects and are not intended to
function as primary controllers or sound generators.
The Stranglophone was developed to function as a dual
purpose device – both as primary interface for sound
generation as well as a configurable effect modifier.
3. DESIGN
The initial design, prototyping and development of the
Stranglophone took place during Gideon D’Arcangelo’s NIME
class [6] taught at the Interactive Telecommunications
Program at NYU. The initial version was then presented at the
NIME03 conference in Montreal. The second iteration of this
design which will be presented is currently being developed at
NYU.
The design criteria for the project included portability, ease of
use, the potential for virtuosity and a clear relationship
between the gestures required to play the instrument and the
sound produced. The name was derived from the words
“strings” and “angles” which were the primary effectors in the
initial design.
3.1 HARDWARE
Figure 1. Stranglophone sphere controller and black box
The Stranglophone platform consists of two elements – a
spherical handheld controller and a black box. The handheld
controller consists of an L-shaped block of wood mounted
inside a clear plastic sphere. The “back” of the instrument has a
Cat-5 jack, as well as a single Analog Devices ADXL202
accelerometer mounted on top. Four small sliding
potentiometers are mounted in the base of the wood and
attached to springs on the back. These sliding pots have
custom handles which are attached to guitar strings with key
rings at the ends.
The sensor information from the Stranglophone sphere is sent
via a Cat-5 cable to the black box controller which contains a
BX-24 microcontroller, power connectors, two indicator LEDs
(power and MIDI traffic) and a single MIDI jack. The BX-24
processes the sensory data and sends it out as MIDI note-on
messages on five separate channels.
3.2 Software
The software is written in PD and it consists of synthesis,
effects and amplification modules. The accelerometer in the
sphere sends tilt data to PD which uses this to generate a base
frequency. In the initial version, tilting the sphere right
produced lower-pitched sounds while tilting it left produced
higher pitched sounds. This basic sound is controlled and
shaped by the four sliding potentiometers which affect a
number of user assignable parameters.
4. PERFORMANCE
The Stranglophone was initially performed at the avant garde
jazz club Tonic in NYC as part of Gideon D’Arcangelo’s
NIME03 [6]. I have since performed it as part of a
demonstration at NYU as well at a freeform jam session at a
club called the Frylab. At these performances, the audience
reacted most positively to the instrument when I stood up and
performed it expressively, like a saxophone or guitar player.
5. ONGOING DEVELOPMENT
The next steps in this project are to redevelop the handheld
interface to make it more ergonomically sound, as I felt that
holding the sphere for long periods of time could be
uncomfortable. I would also like to make the sphere
completely wireless so as to improve its potential range of
possible gestures. I am also considering switching to
Microchip’s PIC series of microcontrollers as they are lower
cost and more flexible than the Basic X controller.
6. ACKNOWLEDGMENTS
I would like to thank Gideon D’Arcangelo for his invaluable
help and assistance, the entire NIME 2003 class for their
advice and innovation and the rest of the ITP community for
their support.
7. REFERENCES
[1] J. Rovan, M. Wanderley, S. Dubnov, and P. Depalle,
``Instrumental gestural mapping strategies as
expressivity determinants in computer music
performance       .,''    in Proceedings of the Kansei - The
Technology of Emotion Workshop, (Genova - Italy), Oct.
1997.
[2] Wanderley, M. and M. Battier, 2000. Trends in Gestural
Control of Music. (Edition électronique.) Paris: IRCAM.
[3] Gadd, A. and Fels, S, MetaMuse: Metaphors for Expressive
Instruments in Proceedings of the 2002 Conference on
New Instruments for Musical Expression (NIME-02),
Dublin, Ireland, May 24-26, 2002
[4] Roland MC-505 Groovebox (D-Beam controller)
http://www.rolandus.com/products/details.asp?CatID=4&      
SubCatID=0&ProdID=MC-505    
[5] Alesis AirFX
http://www.alesis.com/products/airfx/about.html    
[6] D’Arcangelo, Gideon, New Interfaces for Musical
Expression Workshop, Spring 2003, Interactive
Telecommuications Program, New York University.
http://stage.itp.nyu.edu/nime/nime03.htm      