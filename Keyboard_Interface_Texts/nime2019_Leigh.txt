Exploring Human-Machine Synergy and Interaction  on a Robotic Instrument Sang-won Leigh MIT Media Lab  75 Amherst St MA 02142, United States sangwon@media.mit.edu 
 Abhinandan Jain MIT Media Lab  75 Amherst St MA 02142, United States abyjain@media.mit.edu 
 Pattie Maes MIT Media Lab  75 Amherst St MA 02142, United States pattie@media.mit.edu  ABSTRACT This paper introduces studies conducted with musicians that aim to understand modes of human-robot interaction, situated between automation and human augmentation. Our robotic guitar system used for the study consists of various sound generating mechanisms, either driven by software or by a musician directly. The control mechanism allows the musician to have a varying degree of agency over the overall musical direction. We present interviews and discussions on open-ended experiments conducted with music students and musicians. The outcome of this research includes new modes of playing the guitar given the robotic capabilities, and an understanding of how automation can be integrated into instrument-playing processes. The results present insights into how a human-machine hybrid system can increase the efficacy of training or exploration, without compromising human engagement with a task.  Author Keywords Robotic Instrument, Human-Robot Interaction CCS Concepts • Human-centered computing → Human computer interaction (HCI) → Interaction devices 1. INTRODUCTION Musicians have long experimented with novel ways to generate sounds using different techniques and implemented systems. Widely used tools include slide bars, different picks, trilling devices, and so on. Famous experimental guitarists such as Fred Frith and Derek Bailey utilized objects such as violin bows, paper clips, or chains to create interesting sounds in their compositions. Musicians have also developed new instruments in order to support novel sonic expressions. The Pikasso guitar [8] was used by Pat Metheney and allowed for a novel blend of sounds and musical styles with its additional strings. Paolo Angeli’s Prepared Sardinian Guitar is “an 
orchestra-instrument with 18 cords, a hybrid between guitar, baritone, violoncello and drums, gifted with hammers, pedals, some propellers at variable speed” [9]. Angeli himself explains that the instrument “improvises and composes unclassifiable music, suspended between free jazz, folk noise and minimal pop.”  This research began with the aim to explore the use of robotic mechanisms that further increase the expressivity of the guitar. The system, like the Prepared Sardinian Guitar, supports a wide array of novel genres of sound (figure 1). The critical difference is that the control is computation-driven. The three actuation mechanisms added–solenoid hammering, electromagnetic bowing, and motorized tremolo–all play along with the physical setup of the guitar such as the strings and pickups, channeling their actions through the existing sound-generation pipeline. This way, we could let the musicians’ established knowledge and experience on the guitar naturally translate to this novel system, as well as preserve the chance to encounter happy accidents originating from the physics of the guitar.  In order to test the system’s efficacy in performance, composition, or training, we gave our robotic guitar to several musicians and music students. The guitar can be configured to take different levels of agency, from fully automating the robotic plucking to a user directly controlling the actuators by fretting. This realizes a hybrid human-machine performing where a portion of control responsibility are offloaded to machines. Through that, we open a discussion on degrees of machine intervention on creative tasks, and make a qualitative inquiry into how such forms of human-robot interaction will impact users. It is a prevalent notion that offloading to machines will reduce human agency, more so in creative domains. We investigate how much of that view holds in musical contexts, and the varied ways machines can make positive impacts on creative processes.  Our case study presents evidence of the robotic guitar improving the efficacy of learning and composing; as well as a musician and the machine collaboratively making complex patterns that are not viable by regular means. The findings also answer some of the questions stemmed out from previous research done with beginner guitar players in the context of early stage pedagogy. Furthermore, our study indicates several subroutines as to how the use of robotic augmentation brings with it opportunities beyond making a task viable. These subroutines are illustrated through interviews and recording sessions with professional musicians, with the focus on three main aspects: Embodied Augmentation addresses how an augmentation system coming to form and action impacts its user. Leveraging the physicality of the guitar, robotic assistance demonstrates stronger presence and allows users to physically intervene in the sound generating process.  Shared Agency – the computational and embodied assistance turns a single-person task into a collaborative performance. The shared agency between human and robot may affect the person in different ways beyond the newly afforded capabilities. Expanding Musicianship – is of main concern of collaborative robotic instruments. However, how will it specifically expand upon conventional practice in both sonic and technical sense? 
Figure 1: Our system integrates several sound-generating mechanisms and can interface with external MIDI systems. 
437
2. RELATED WORKS 2.1 Robotic Musicianship Research in robotic musicianship [14][19] explores systems that automate musical instruments, sometimes utilizing them in musical practices or performances, as in the Orchestrion [2] or AUTOMATICA [10]. Robots for guitars include LEMUR [1], Z-Machines [5], and instruments by Maywa Denki [6]. These systems use mechanical actuation to create novel musical expressions that are different from or impossible to create by regular means. However, these robots are designed to be fully autonomous and replace human musicians.   2.2 Augmented Performance Researchers and practitioners have been aiming to extend the control modalities of existing musical instruments via sensing and post-processing. Hyperinstruments [23] was a series of expanded musical instruments, giving extra ability and finesse to virtuosic performers. Among those, the Hypercello recognizes signal such as finger position/pressure, wrist angle and bow position, that drive its sonic outcome. The design of the instruments centered around the natural musical activity of a musician, while augmenting the instrument’s sonic output. Gong et al. [17] integrated capacitive touch sensing onto the top of a ukulele for sonic control, Freed developed a similar system for fingerstyle guitar players [7], and Newton and his colleagues explored how musicians augment their instruments with electronics [25][26]. Keyboards also have been designed with extra control modalities through the use of soft material [3][21]. Leslie utilizes real-time EEG signals to generate sounds that blend into her flute performances [24]. From a signal processing perspective, these systems afford means to post-process the output of existing musical instruments or add sound elements outside the sonic dimension of those instruments. 2.3 Augmented Learning Researchers also studied the possible impact of sonic post-processing in learning or improvising. Piano systems created by Yuksel et al. assist in a musician’s performance within the sonic dimension of the piano [31][1]. Their BRAAHMS system assists a musician’s improvisation with a harmonic addition or removal without compromising the general direction of the music played. However, the form of machine intervention stays minimal such as adding a note. The augmentation of musical expression is nearly non-existent, so is a more dyadic interaction between human and machine.  
2.4 Collaborative Instruments Cooperative musical machine is a category of systems introduced by Barton and his collaborators [12][20], where human and machine controls both contribute to a machine-generated music. Relatively few examples have been reported, while the majority of existing works focused on the piano [4][28][29][30]. The prosthetic robotic arm for drumming [13][18] has two drumsticks attached–one is controlled by a human drummer and the other by a machine. For the guitar Ogata and colleagues [27] created automatic hammering apparatus that replaces the plucking hand on the guitar. Cyther [12] is self-tuning zither that can be played by a human performer. It presents itself as a rare example of a collaborative instrument where both human and machine contributes to the same music while affecting the action by each other. The history of cooperative instruments is outlined comprehensively in the paper [12]. Our system belongs to this category, where the robotic actuators are capable of taking initiative in actuating the strings.  3. REVISITING PREVIOUS WORK In previous research [22], a fretting augmentation device was used with novice users in the context of learning. The tested system offloads fretting actions on the guitar, as opposed to the plucking version presented in this paper. Please refer to the paper for further details. The use of the fretting augmentation was found to make three major impacts on the course of learning: 1) decomposing a task and helping focus on one part of the task [16], 2) demonstrating how a successfully accomplished musical piece sounds, and 3) making the task more engaging and motivating experiments. Much like side-wheels for a bicycle, automating parts of fretting eased the users into focusing on more musical aspects of the given task. A majority of the users immediately started focusing on rhythm when they engaged with the robotic device, where only three out of eighteen participants were able to identify rhythm as needed to be practiced before engaging with the fretting augmentation. Additionally, being enabled by the robot, the users showed a strong tendency to engage in unguided explorations such as trying a new strumming technique or playing songs that were not part of the given task. These observations hint at key qualitative functions that an augmentation, or an augmented state, could bring to the course of learning. Expanding the research to a broader context of performing or composing on musical instruments, it will be possible to get a more generalized insight into how an augmented state can impact creative processes. However, it is worth noting that observing such qualitative factor is difficult since every musician develop their own methodologies and means to find inspiration. Therefore, we aimed to give the fullest freedom to the musicians participating in our study.  4. SYSTEM The hardware (figure 2) consists of four major components: fret detection, pick actuation, coil actuation, and motor actuation. We used a Fender by Squier MIDI guitar for fret detection; fretting events are recognized by sensors embedded in the guitar neck and sent to an Arduino microcontroller mounted inside the guitar. The Arduino also controls six individual push-type solenoids used for hammering actuation, six electromagnet coils that create oscillating magnetic fields for sustained actuation, and six high torque motors that can produce tremolo picking actuation. For maximizing the temporal resolution of the control, an FPGA  parallelizes the control.  The solenoid actuation is triggered by a pulse for duration of between 2–5 microseconds for varying effects of pick strength (the higher the duration, the higher the strength); the coil actuation is accomplished by generating square waves with specific periods; and the motors use PWM input for varying speed of rotation (figure 3). A proximity sensor (APDS 9960) recognizes whether a musician’s hand is placed on the plucking area, in order to determine whether she or he is plucking the guitar or leaving it to the control software.  
Figure 2: Two versions of our hardware design for inside mount (left) and outside mount (right).   
Figure 3: Hardware and command signals for the system.  
438
The control of the robot is driven by MIDI messages generated by our software, or ones from external MIDI software processed by our software. The main source of control commands is the fret detection; fretting actions on a guitar string can be directly mapped to its corresponding actuators as well as predefined MIDI patches for more sophisticated actuation patterns. Additionally, the software provides functions to automatically generate rhythm patterns through 1) randomization, 2) statistical analysis of existing music or 3) Euclidean rhythm generation.  5. CASE STUDY We collaborated with several musicians with diverse backgrounds from places such as Berklee College of Music or New England Conservatory, and asked them to use our prototype. We first let the musicians spend enough time in different configurations, and revisited the recordings to discuss new discoveries and experiments we would like to pursue – that were conducted in follow-up sessions. We videotaped and observed the entire sessions, and after all the sessions were completed, we interviewed the musicians about their general thought process, frustrations, and new ideas. The aim of the case study was to learn how they would to use the system as part of their composing or performing processes, and explore any new ideas they come up with. The details of these experiments will be discussed in the individual studies, but a consistent observation across different musicians was that they attempted to combine their own control over the guitar strings and make the resulting sound much more complex than a simple combination of human fretting and machine plucking. Some of them commented on the automated pattern as something they were inclined to respond to, or compared it to playing with another musician.  5.1 The Extension of the Hand The first impression of these studies was that the system allowed for freer maneuvers on the guitar compared to their regular practices. There were also new combinatorial patterns of playing multiple notes, such as chord shifts in which the individual notes were played with slightly different timings. Typically, the timing of a note being played is determined by the right hand, where minute movements of the left hand don’t result in explicit sound. However, when the robotic plucking is initiated on a fretting event, every small gesture made with the fretting hand could be heard. Gerard, who has been a guitarist for more than 15 years, showed a unique style in using the system. He would play as he normally did on the guitar, but the solenoids would respond to his left-hand movement and hit the strings–filling in the gap between his own strokes. This resulted in two effects: the notes he played become more percussive and accentuated, and often the solenoids were actuated off-beat, making the overall rhythm more sophisticated. He compared these novel effects to the invention of distortion, a radical tonal shift that led to new genres of rock music. He explained that the solenoids in our system introduce a “mechanical, rhythmical, or percussive shift” by adding novel rhythmic elements to his own guitar playing.  “It (the robotic guitar) is a mechanical shift, because I feel distortion is in a way a digital shift or a tonal shift, and this is more of a rhythmic shift. It makes the guitar more percussive, which it already kind of is, but you can play it as a more voiced instrument or you can play as a percussive instrument when you're just hitting the strings. It's more percussive, and this accentuates that percussiveness.” He further adds that this percussiveness, the new mechanical property, added to his guitar playing enables a new connection between his movement and its resulting sound, which he calls “synaptic connection.” This is a particularly interesting remark. The immediate response one gets from touching an instrument makes it almost synaptic, and as the response becomes more novel, it may 
inspire the musician and reveal new opportunities. The overall process of playing this instrument, as he describes it, happens through continuous and accidental discoveries. He makes certain moves, and the resulting sounds become sample points, that help him understand what can be repeated or adapted to something more interesting.  Tina, a Berklee College of Music student majoring in film-scoring and a guitar player, makes a similar remark. She spent most of her experiment with the solenoid module, summarizing her experience as “initially felt nervous,” but soon found out that the mechanical and sonic novelty of the system could be an exciting way to boost musical creativity. She started the experiment in the manual control mode, only using her fretting hand to play some chord progressions. Later, we switched on the automatic plucking and let her play on top of it. After about 20 minutes, we listened to the recording and she said "this is actual music.” She notes that the instrument creates sounds that are unexpected and somehow freeing when compared to a regular guitar, with which she has a good understanding of how certain movements would result in specific output sounds. “When you play the actual guitar, you know what the outcomes are going to be. And there is a room for mistake, like you press a note, and it will be not the note you are expecting. But it will sound good and you are like ‘ok, it's good’–you are making music. (But here) Everything is unexpected, you don't know what is going to come next. The chords are unpredictable because there is so much texture and unpredictability, unless you sequence it to the rhythm that you want.” With Jose, a performing student from Berklee, we discovered how to create microtone effects on the guitar. This purely resulted from an accident, when we temporarily disconnected the guitar from the computer and made sensing/control all work on the microcontroller inside the guitar. The communication delay was reduced to its minimum, and the string actuation changed. When the robotic plucking action happens together with Jose’s picking, the solenoids hit the string and add pressure against it. This adjusts the pitch of the string briefly, pushing it up slightly. It also changes the texture of the attack sound into something that may be heard in traditional instruments like the banjo. The banjo creates its characteristic “twang” when plucked as a result of the instrument’s particular physical properties. The mechanical interference of the solenoids in our system could emulate that effect, loaning the guitar another instrument’s properties. 5.2 Usability in Exploration Oscar, who is a sound effects designer and a guitarist, joined the experiment with the motivation of turning the mechanical sound into new sound effects. He spent most of his time using the coils and motors, which he explains challenges one major limitation of the guitar: sustain. Although we currently have several analog and digital solutions to enhance the sustain of the guitar, Oscar explains, (still) the guitar does not afford “droney” sounds. E-bows are a very accessible solution to making bowing effects on the guitar, but only one string can be actuated at a time. Oscar’s attempt to break from such constraints shows clearly in the 
Figure 4: Gerard using the MIDI controller to automate rhythm and adding melodic element on top of it.  
439
experiments where he used all six strings at the same time, using DC motors and electromagnetic coils to make drone sounds.  “More what interested me right away was just the sound of the guitar itself, and how it separated itself from a traditional guitar—at that separation while still having a familiarity. This is a guitar you know what to do and know how to play but there was all the weird stuff going on with the actuators. In my experience, the things that drew me to it the most was not the playability of it, but rather the sonic quality and how it differentiated itself.” These observations are consistent with Gerard’s, who remarked on the sonic shift happening within the system. Oscar similarly focused on the novel sonic property of the system, but he also made a noteworthy comment on the usability of the system. He said the system is intuitive to use and allows for more musical exploration than manually generating sounds in other ways. He explains that his experience with similar experiments was “mostly concerned about how to make a good sound” as opposed to “experimenting with the sound.” Since our system uses the fretboard as the control interface, his past experience and knowledge of the guitar could translate naturally to this experiment. Thereby, he could carry out a more structural, or compositional, exploration using the system.  “I think the most obvious one is just how easy it is, playing the guitar with a mechanical thing. It's just like I just press the frets and it start going. I think both of those encourage different types of exploration–by having something that is easier to play, it'll be a lot more exploration as to what's going on the fretboard as opposed to what's going on the mechanical part. I would say that your instrument encourages a more musical exploration…  Obviously, the playability is going to change your inclination of what you're going to play on something. The type of melody that a guitar player would compose oftentimes ends up being different than the type of melody you typically see a piano player compose or a saxophone player, or the instruments themselves tend to encourage a certain style.” His thoughts reach even further into the usability of an instrument facilitating specific styles of musical expression. In the field of ethnomusicology, it is widely discussed how instruments guide the musical output in designated directions [11]. Our system uses the fretboard as their control interface, with the intention of making it more intuitive to use. However, this design choice seems to have a broader impact, allowing for a natural understanding of how to use the systems, as well as facilitating a higher-level approach to exploring musical ideas.  5.3 Shared Agency Gerard also commented on the automated plucking patterns driven by MIDI patches he wrote. He compared that with using a drum machine, except there is a critical difference in that the system has a more melodic element that encourages his response to it. “In a way, it becomes a drum machine–it's similar to playing along with a drum machine but more. Listening back, I don't really know exactly what I was playing…it was a weird sensation, almost an out-of-body experience. But at the same time, it allows me to do like a counter-melody that sounds like two guitars. There is a difference in the way that I'm wanting to react.” The choice of the phrase “out-of-body experience” is quite interesting since it implies a sensation of losing agency, but the machine is not totally agonistic to his intention. He is still willing to respond to it and make melodies on top of the automation, and even 
has control over the automated rhythms since he can “trigger” it in different ways. There will be more findings on shared agency in later case studies. The takeaway here is that it becomes a motivating factor. The counter-melodies Gerard could create, the iteration of the automated rhythm and his own melodies forming different synergies each time, allows for an exploratory maneuver only possible through this meshed configuration.  Andres, a proficient jazz performer, further experimented with learning through automation. He mentioned a common behavioral trait of music students, who continuously, and almost automatically, train their finger movements even when they are not playing an instrument. He asked me to automate a plucking pattern while he practiced on top of it. An interesting discussion point Andres brought up is that the automation is not only used for offloading a part of the learning task, but also allows him to feel it. We discussed it further since the feedback aspect was not something that we intended in the design of the system. His next comments provided a detailed perspective: “It feels like training wheels. You could get the sound and hear mindlessly, and you have to get back on it if you lose it which is in that regard is very different from metronome–because the metronome is very indifferent to what you're doing. It takes no input but something like this, if you're off by an eighth note, you'll hear it. It teaches you in a way that a metronome doesn't teach you; it's nudging you to do it right.” Interestingly, but not too surprisingly, this remark is identical to what was discussed in the beginner user study. However, Andres adds that the resulting sound responds directly to his movement and mistakes. When he makes a mistake or deviates from the intended course of practice, then the sound he hears immediately shows that. This feature is particular to the setup where both the robot and the musician contribute and coordinate to make sounds. This could also explain why we see beginner subjects in the beginner experiment [22] showing increased engagement and motivation in practicing. The shared agency over the guitar makes the learning process “not indifferent” to how a learner is performing.  After this discussion, we decided to try something Andres may struggle to learn in a short period of time: we tested polyrhythm patterns across different strings, one with four against five, the other seven against three–both reasonably complex. After approximately two minutes of Andres using the polyrhythm plucking pattern together with his fretting, he demonstrates the pattern with his plucking hand.   “I feel a difference, because I would not have been able to do that–it would have taken me 10 minutes to write it out, and then 10 more minutes to do it very slowly. I'm already feeling what it would feel like to play correctly. You can make metronomes play basically this, but then you go to the guitar, you can't do it. But feeling these strings actually move on your fingers it elicits some response where you can kind of map whatever fingers you are using to the feeling on your fingers…  You saw how long it took, maybe three minutes of listening to it being done. Because I already know what it feels like, I could just imagine what it feels like because the guitar already did it for me.”  He confirms that even though he was not directly playing the notes, he was able to feel what it’s like to play correctly. He was also surprised by the amount of time it took—he said that it would typically take more than 30 minutes or an hour to reach the level he was able to reach within 2–3 minutes. It would be hard to pinpoint a single reason that made this possible, but some of the crucial ones would include how much agency he shared with the robot and how he felt the demonstration right through his fingertips.  
440
5.4 Adaptation and Inspiration The most noteworthy thing about Oscar’s experiment was that he tried to push the system to its limit. He focused on finding different limits within the system and discovering its unexpected yet repeatable behaviors. At one point during the session, he concurrently activated more than four coils and four motors at the same time, which exceeded the power capacity of our circuit. The motors then started moving at variable speeds, the intensity of the electromagnetic actuation fluctuated, and the solenoids accidentally activated—presumably due to the power instability. He kept forcing the system into these erratic behaviors and trying to guide them purposefully. “As soon as I picked it up my first thought was like ‘okay, let me break it’.’ I’m not literally breaking it, but trying to see what are the limits of what I can do, where are its limits at; like what's the highest fret I can play at, how many strings can I play it once, and see some of the limitations of the machine. Once I figured all that out, it was about seeing how I could use those limitations to get something interesting out of it…” We were able to find sounds that were not envisioned in the system’s original design. We were most intrigued by Oscar’s use of an excessive amount of power—even as the designers, we didn’t expect the robot’s resulting behaviors. Notably, the solenoids were turned off in the settings of the software, but they were still triggered erroneously. The resulting random pattern, combined with other controlled sound coming from bowing and tremolo picking, sounded ambient and poetic in a sense that it was happening as a result of an error. But the error pattern was not truly random; Oscar understood, to an extent, what the erratic behaviors were going to be: “The hammering was triggered based on like pressing the string but if you just held it for a really long time, a lot of times I would just false trigger again, just from like subtle movements. I was like ‘I am just going to sustain this to see how it starts playing itself,’ and be like ‘oh, I liked this reaction, let me push it further.’ I would say this experimental exploration is pretty free in the sense that I'm just poking stuff randomly, but there is an order to the randomness.” Oscar’s ability to bring order to the randomness presents an example of how we could extend our agency over a system that we do not directly and transparently control. He was able to adapt and extend his to understanding of the instrument to the boundary cases. Tina describes such an experience of using the robotic guitar as “learning,” and mentions that the robot nudges a musician to adapt to the different texture of sound and experience. “You are learning with it. You are learning to adapt to the instrument as you keep playing it. You would play a chord, you know how to play the chord, but it will feel different. So, you have to change…  I think it would also make the guitar more accessible to world rhythm, or world music. I was raised in Ukraine, and instruments like Balalaika, have specific tone that comes out as a bounce, and it (the robotic guitar) felt like it. And with this, you could even make it even more, it sounds different and I have never heard it before…” She adds that this instrument hints at a way to challenge today’s compositions, where “the creativity is regulated.” This highlights again the importance of the mode of expression afforded by an instrument, which Oscar also referenced in the previous study. However, there is a subtle difference in what Tina says, in that she emphasizes the musician’s role. She emphasizes the “feedback” in that the instrument takes a more inspiring role. Tina’s perspective 
could be seen in her remark about how the newly afforded sound texture. would support “world music.” These two roles may complement each other and will be a worthy future research topic, but how exactly would these roles take part in a musical exploration? 5.5 Real-time Improvisation with Robot Can we apply this to the context of improvisation? Andres and us decided to take an interesting twist in the automated sequence—we used randomized plucking patterns made by the robot, then Andres would improvise on top of it. This was what he said after a few minutes of improvising with the robot:  “Oh wow, that's crazy, it feels so much like I'm playing with another person. I see it more applied to free jazz or free improvised music. The main difference is the harmonic; you're not following the correct chords or harmony, you're just kind of improvising much like I was doing. And this is really exciting because it feels so much like I had the same feeling of spontaneity and excitement, as I did when playing with another musician just now. I was so surprised when by the things that would happen and I tried to respond in ways–much like I would have done when playing with someone else.”  We steered the randomization software while Andres played the guitar. The software directly adds the randomized pattern to an Ableton Live sequence. Andres, while having zero control over the automation, he was able to improvise along. It was also notable that he kept transitioning between controlling the sound himself and letting go, and the transition was not binary. At times, he would lead the transition by plucking and fretting across the entire fretboard, or listen to what the robot is playing by pausing his hands. In between transitions, he also mixed his chords and other actions with the robotic plucking to create more dynamic sounds. The continuous switch between automation and directive control is something only possible because both human and robot are working in the same physical space.  6. DISCUSSION 6.1 Role of Augmentation The two main features identified were enabling and providing novelty provided by the systems. Enabling means that the system could help users go beyond their skill levels or physical aptitude, allowing for freer explorations. This was observed in the previous experiment with beginners [22], which was consistent with more skilled guitar players as well. The musicians immediately tried complex maneuvers that are otherwise difficult or impossible. Moreover, the novelty of the sound coming from the system seemed to inspire them. If enabling concerns an extension purely on the skill level, novelty augments the stylistic or aesthetic component in its expression. From the remarks on the specific discovery of the sonic distortion that makes the guitar possess the timbre of a Middle eastern instrument; to the new correspondence between the hand’s movement on the fretboard and the resulting percussive attack, which creates a new “synaptic connection,” the study clearly shows how a native expression pattern that augments the human movement opens new opportunities in exploration. 6.2 Conforming to Regular Practice There was also a crucial takeaway from the session with Oscar that helps us understand the design of user interface for computational instruments. It hints at how retrofitting the interface to a user’s past experience could be helpful. Not only does it allow for a faster adaptation to the novel system, but it also enables a more structural approach toward using or exploring with the system. It could also explain the nearly immediate and sustained change in the subjects’ behaviors when they started using the fretting augmentation in the guitar-learning experiment [22] . 
441
It is also important to note that the retrofitting element in the design goes hand in hand with the novelty factor afforded by the system. It has been mentioned by many of the subjects, a notable one being Tina. Since the way this instrument is played heavily relies on the tonal configuration of the fretboard, she applied her knowledge of how certain movements on the guitar would result in corresponding sounds. 6.3 Forming Synergy with an Augmentation There also have been other second-order effects from using the systems. The fretting augmentation [23] could function as side-wheels and a demonstration for successfully playing a guitar phrase needing both hands. This also held true for more skilled players. Andres, after using the automated polyrhythm plucking pattern, was able to drastically reduce the time necessary to learn the rhythm pattern–from 30 minutes to 3 minutes. It was also critical that he had a great deal of agency over the sound. He was not simply listening and observing what the robot played; he was combining the automated plucking with his own fretting actions where he could feel the strings being actuated, and his mistakes would immediately result in incorrect sounds.  These examples of automation suggest a more independent mode of operation by machines than the manual control cases. However, the human users were not fully removed from the task. They had to engage in the task deeply, play along with the robots, where their decisions and actions would lead to immediate consequences. It could be said that shared agency is another critical element in engaging the user and making a stronger impact on the course of learning. It also leaves room for a user to influence the final sound. Andres, improvising with the randomly generated plucking patterns by the robot, says it feels like playing with another person. However, when we look more closely into what he does, he creates a much more complex synthesis of what he is playing himself and what the robot is playing–suppressing the sound made by the robot, fully surrendering the guitar to the robot, or combining what he is playing with what the robot is doing. This may read as an opposing view to the way technologies are often viewed as a crutch [15]. However, these case studies led to design examples that prove otherwise. Could we then use these functions to construct more and more opportunities to achieve greater outcomes? 7. CONCLUSION In this paper, we presented a longitudinal study with musicians using our robotic guitar. The experiments and interviews with the musicians suggest that the mechanical augmentation of their guitar play enabled them to engage in more complex maneuvers on the guitar, that is consistent with a previous study with novice users. Other findings presented ways that such a system can take a role as a companion. Automated control of the robotic guitar lends itself as side-wheels or a demonstration for training musical patterns beyond a user’s skill level. Hinted at by the study with beginners, a highly skilled musician supports this feature by reducing the length of training required to play complex polyrhythm patterns from thirty minutes to three minutes. The shared agency over the robot may have improved the efficacy of their practice routine; the system makes it more engaging and gives them a more holistic sensation of playing as opposed to being replaced and guided through it. Furthermore, we observed that the close intermeshing of human and robot movements may lead to a broader spectrum of expression. They provide an insight into the possible ways that such systems will facilitate enhanced training or exploration, and the lessons lend design strategies for future systems where human and machine engage closely in a task. 8. REFERENCES [1] LEMUR. http://www.singerbots.com [2] Orchestrion. http://www.patmetheny.com/orchestrioninfo [3] Seaboard. https://roli.com/products/seaboard/grand-stage [4] Yamaha Disklavier. http://www.disklavier.com [5] Z-machines. http://yurisuzuki.com/design-studio/z-machines  [6] Denki, M. http://www.maywadenki.com 
[7] Freed, A. Augmented Electric Guitar. http://www.adrianfreed.com/content/augmented-electric-guitar [8] Linda Manzer. Pikasso Guitar. http://www.manzer.com [9] Paolo Angeli. Prepared Sardinian Guitar. http://www.paoloangeli.com/en/prepared-sardinian-guitar/ [10] Stanford., N. AUTOMATICA. https://nigelstanford.com [11] Aho, M. (2016). The Tangible in Music: The Tactile Learning of a Musical Instrument. Taylor & Francis.  [12] Barton, S. Prihar, E. Carvalho, P. (2017). Cyther: A human-playable, self-tuning robotic zither. NIME 2017.  [13] Bretan, M., Gopinath, D., Mullins, P., & Weinberg, G. (2016). A Robotic Prosthesis for an Amputee Drummer.  [14] Bretan, M., Weinberg, G. (2016). A survey of robotic musicianship. Communications of the ACM, 59(5). [15] Clark, A. (2004). Natural-Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence. Oxford University Press. [16] Frederiksen, J. R., White, B. Y. (1989). An approach to training based upon principled task decomposition. Acta Psychologica, 71(1–3), 89–146. [17] Gong, N.-W., Zhao, N., Paradiso, J. A. (2012). A Customizable Sensate Surface for Music Control. NIME 2012. [18] Gopinath, D., Weinberg, G. (2016). A generative physical model approach for enhancing the stroke palette for robotic drummers. Robotics and Autonomous Systems, 86.  [19] Hoffman, G., Weinberg, G. (2010). Synchronization in human-robot Musicianship. In 19th International Symposium in Robot and Human Interactive Communication. IEEE.  [20] Kemper, S. Barton, S. (2018). Mechatronic Expression: Reconsidering Expressivity in Music for Robotic Instruments. NIME 2018. [21] Lamb, R., Robertson, A. (2011). Seaboard: A New Piano Keyboard-Related Interface Combining Discrete and Continuous Control. NIME 2011. [22] Leigh, S., Maes, P. (2018). Guitar Machine: Robotic Fretting Augmentation for Hybrid Human-Machine Guitar Play. NIME 2018. [23] Machover, T., Chung, J. (1989). Hyperinstruments: Musically intelligent and interactive performance and creativity systems. [24] Makeig, S., Leslie, G., Mullen, T., Sarma, D., Bigdely-Shamlo, N., Kothe, C. (2011). First Demonstration of a Musical Emotion BCI. ACII 2011.  [25] Newton, D., Marshall, M. T. (2011). The augmentalist: Enabling musicians to develop augmented musical instruments. TEI 2011. [26] Newton, D., Marshall, M. T. (2011). Examining How Musicians Create Augmented Musical Instruments. NIME 2011. [27] Ogata, T., Weinberg, G. (2017). Robotically Augmented Electric Guitar for Shared Control. NIME 2017. [28] Tomita, Y., Barber, G. (1996). New technology and piano study in higher education: getting the most out of Computer-Controlled Player Pianos. British Journal of Music Education, 13(02), 135.  [29] Xiao, X., Ishii, H. (2011). MirrorFugue: communicating hand gesture in remote piano collaboration. TEI 2011.  [30] Xiao, X., Tome, B., Ishii, H. (2014). Andante: Walking Figures on the Piano Keyboard to Visualize Musical Motion. In NIME (pp. 629–632). [31] Yuksel, B. F., Afergan, D., Peck, E. M., Griffin, G., Harrison, L., Chen, N. W. B., … Jacob, R. J. K. (2015). Braahms: a novel adaptive musical interface based on users’ cognitive state. NIME 2015. [32] Yuksel, B. F., Oleson, K. B., Harrison, L., Peck, E. M., Afergan, D., Chang, R., Jacob, R. J. (2016). Learn Piano with BACh: An Adaptive Learning Interface that Adjusts Task Difficulty Based on Brain State. CHI 2016. 
442