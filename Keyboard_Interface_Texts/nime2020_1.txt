Reinventing the Noisebox: Designing Embedded
Instruments for Active Musicians
John Sullivan
Input Devices and Music
Interaction Lab; CIRMMT
McGill University
Montréal, Canada
john.sullivan2@mcgill.ca
Julian V anasse
CIRMMT
McGill University
Montréal, Canada
julian.vanasse@mail.mcgill.ca
Catherine Guastavino
Multimodal Information Lab;
CIRMMT
McGill University
Montréal, Canada
catherine.guastavino@mcgill.ca
Marcelo M. Wanderley
Input Devices and Music
Interaction Lab; CIRMMT
McGill University
Montréal, Canada
marcelo.wanderley@mcgill.ca
ABSTRACT
This paper reports on the user-driven redesign of an embed-
ded digital musical instrument that has yielded a trio of new
instruments, informed by early user feedback and co-design
workshops organized with active musicians. Collectively,
they share a stand-alone design, digitally fabricated enclo-
sures, and a common sensor acquisition and sound synthesis
architecture, yet each is unique in its playing technique and
sonic output. We focus on the technical design of the in-
struments and provide examples of key design speciﬁcations
that were derived from user input, while reﬂecting on the
challenges to, and opportunities for, creating instruments
that support active practices of performing musicians.
Author Keywords
embedded acoustic instruments, digital fabrication, partic-
ipatory design
CCS Concepts
•Applied computing → Sound and music comput-
ing; Performing arts; •Human-centered computing →
Participatory design;
1. INTRODUCTION
The NIME design community, well known for the proliﬁc
creation and investigation of new digital musical instru-
ments (DMIs) and interfaces, is complemented by a commu-
nity of creative practitioners who actively use and explore
new technologies in performance. In fact many wear both
hats, actively participating in the design of new instruments
as an integral part of their compositional and creative prac-
tices.
However, researchers have found that most new DMIs
experience limited real-world use in musical performance or
production, if any at all. A variety of reasons for this have
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’20, July 21-25, 2020, Royal Birmingham Conservatoire,
Birmingham City University , Birmingham, United Kingdom.
been oﬀered. It may be an intentional choice by designers,
who may develop instruments as research probes not meant
for musical practice [12]. With DMIs that would be put
into musical service, issues of build quality and reliability
are frequently cited as limiting factors for their adoption
into sustained real-world use [14]. Furthermore, lacking es-
tablished instrumental techniques, adequate forms of musi-
cal notation, and canonical repertoires [7], new instruments
face steep challenges towards their widespread adoption [9],
highlighting the importance of shared communities of prac-
tice for their success [8, 16].
1.1 T owards Design for Performance
In a previous work, we had conducted an online survey to
poll musicians about their use of new instruments and tech-
nologies [15]. We found several elements of the respondents’
performance practice that inﬂuenced the instruments and
technologies respondents were willing to work with. For one,
the survey indicated that more frequent performers are less
likely to use non-commercial DMIs in their practice and are
more reliant on software and oﬀ-the-shelf controllers than
those who perform less frequently in public. Additionally,
while [12] previously found that performers from the NIME
community are usually closely involved with designing and
building the DMIs they use, we found that this is much less
common outside of NIME-based practices (including those
not involved with academic research and who perform in
more popular music styles).
Based on these ﬁndings we were interested to develop
instruments that can support the demands of active and
professional performance through the involvement of expert
musicians from diverse performance communities during the
design process. The result was the development of three new
DMIs that are based on an existing platform for standalone
instruments called the Noiseboxes [13], with design features
that came from early user feedback and a set of co-design
workshops.
There were three main reasons for redesigning an existing
instrument instead of starting from scratch. First, the ba-
sic standalone design establishes some useful constraints in
terms of size, interaction capabilities, and fabrication mate-
rials and methods, ensuring that new designs are manage-
able in scope and can be reliably produced with available
resources. Second, our previous development cycles oﬀer a
reliable and tested base to build oﬀ of, while still remaining
open and ﬂexible to support new creative designs. Lastly,
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
5
the Noisebox was originally designed as a research probe for
previous projects and was never used in authentic public
performance, which now presents an opportunity to rethink
the design process towards these ends.
2. THE NOISEBOXES
The original Noiseboxes were conceived out of practice-
based research and development ofembedded acoustic in-
struments, deﬁned by [2] as “an embedded musical instru-
ment that provides direct sound output” . Each carries out
its own computation onboard with a Raspberry Pi or simi-
lar single-board computer and produces sound via onboard
ampliﬁcation and mounted loudspeakers, while integrated
sensors provide user control of sound synthesis parameters.
The instruments are fully standalone with the inclusion
of an internal rechargeable battery. One of our original aims
with the Noisebox design was to imbue a digital instrument
with some inherent qualities of conventional acoustic in-
struments that may be missed on a DMI. For one, onboard
sound production and battery power make for immediate
playability with no need for connections, conﬁgurations or
additional hardware to get started. For another, a stan-
dalone instrument combines input device and sound pro-
duction into one cohesive unit, reversing a deﬁning attribute
of DMIs (the decoupling of control from sound production
[11]).
2.1 T echnical Design
Two distinct versions of the Noisebox were developed, yield-
ing multiple copies of each.
The ﬁrst version (top and right of Fig. 1) utilized the
Satellite CCRMA framework for embedded instruments [3],
comprised of a Raspberry Pi for sound synthesis, onboard
mapping and general system functions, an Arduino Nano
microcontroller for sensor acquisition, and a custom Linux
(Raspbian) distribution. Mapping and audio programming
was done in the Pure Data visual programming language.
Sonically, the instrument functions as a “drone box” . It
is comprised of a polyphonic FM synthesizer with embed-
ded sensors mounted on the laser cut enclosure to control
the number of voices and their frequencies. An internally
mounted inertial measurement unit (IMU) modulates var-
ious timbral parameters with the instrument’s movement
and orientation. An enhanced model was also produced
that included delay and reverb eﬀects and sound presets
that could be interpolated.
A second version (bottom and left of Fig. 1) of the in-
struments was constructed the following year. The instru-
ments functioned similarly to the v1 instruments, and also
included a base model and an enhanced model equipped
with additional eﬀects. However the underlying architec-
ture was redesigned to use an early version of thePrynth
framework for embedded instruments 1 [5]. Similar to Satel-
lite CCRMA, Prynth uses a Raspberry Pi as the process-
ing base, but utilizes purpose-designed PCBs with an in-
tegrated Teensy 3.2 microcontroller for sensor acquisition,
and uses the SuperCollider programming language for on-
board audio processing and mapping. An appealing feature
of Prynth is that it runs a web server that can be accessed
through any network-connected browser, which hosts a Su-
perCollider code editor and provides convenient access to
management and conﬁguration options.
3. USER FEEDBACK AND WORKSHOPS
For the second version of Noiseboxes we ran a small pi-
lot study that explored how performers appropriate new
1https://prynth.github.io/
Figure 1: The Noiseboxes, v1 & v2 (Clockwise from
top left): v1 initial prototype, v1 ﬁnished, v1 with
additional controls and audio FX, v2 with additional
controls and FX, a quartet of v2 instruments.
instruments and develop personalized playing styles. Fol-
lowing the basic format of a previous study by [18], four
participants were given an instrument to take home for one
month. After two weeks they returned individually to re-
port on their progress and give a short demonstration per-
formance with the instruments. At the end of the month
they returned as a group to give another performance, this
time with a small invited audience in attendance. Feedback
was collected from the participants and audience members
through interviews, questionnaires and a round table dis-
cussions.
While much of the feedback — and study — concerned
the participants’ engagement and development with the in-
strument, some key issues with the instrument design were
identiﬁed that would disqualify it for real-world use. Over-
all sound quality was lacking, mostly because of the inex-
pensive small onboard speakers but also due to component
failure in one of the instruments. Participants and audi-
ence members alike recommended more interesting sound
synthesis, greater variety of sounds, and better user con-
trols for sound parameters. Performers reported noticeable
latency between user input and sound, which made the in-
strument feel unresponsive and diﬃcult to control. Finally,
while the “retro” aesthetic of the instrument was appreci-
ated, the sharp corners of the laser cut acrylic enclosure and
location/placement of controls made the instrument uncom-
fortable and diﬃcult to play.
3.1 Co-Design W orkshops
In addition to the feedback from the pilot study, we solicited
design input through a set of co-design workshops with ex-
pert musicians that were run in a parallel project. Inspired
by the “magic machine workshops” described in [1], we en-
gaged participants in open exploration of interface design
through nonfunctional prototyping.
Ten performers who maintain an active public perfor-
mance practice participated, divided in two sessions. Dur-
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
6
Table 1: Emergent workshop themes that were implemented into i nstrument designs
Theme Keybox Stringbox Tapbox
1. Flexible performance
modes
synth, looping & eﬀects modules;
external audio input
note-based & sequencer
performance modes
dual synthesis
modes
2. Extending existing
instruments
piano-style keyboard & classic
subtractive synth format
guitar-style strings &
grid-based control surface
cajon-style input
device
3. Embodied interaction and
playful engagement
ergonomic rounded corners &
recessed touch keyboard
asymmetrical shape &
physical strings
entire instrument
is controller
ing the workshop they crafted nonfunctional DMI proto-
types from primitive crafting materials (posterboard, paper,
sticks, glue, etc.) based on an initial creative design prompt.
Presentations and discussion followed, yielding novel ideas
for interaction and high-level suggestions for for the design
of new instruments they would be willing to use in their
own performance practice.
While the workshops are part of ongoing research that
will be documented in a separate report, we show exam-
ples of three emergent themes that led to speciﬁc design
implementations (shown in Table 1):
1. Supporting ﬂexible performance modes with multi-
function controls, audio and control signal routing and
modular functionality.
2. Extending or taking inspiration from existing instru-
ments by incorporating conventional elements into new
designs.
3. Encouraging tangible, embodied interactions and play-
ful engagement with textures, shapes and materials
that possess unique physical and tactile properties.
4. REINVENTING THE NOISEBOX
With our current project, we wanted to reinvent the Noise-
box to address some of the issues with the originals and
create new unique, highly playable, stable instruments that
would be appealing to performers and robust enough to
withstand the rigors of real-world performance. The result
is three new instruments that combines feedback and lessons
learned from the original designs with new ideas from the
workshops, while employing a reﬁned set of digital design
and fabrication tools (Figure 2).
4.1 Instrument 1: The Keybox
Our ﬁrst instrument in this series is a radical departure
from the largely inharmonic noise-based timbres of the past
versions. The Keybox (Fig. 3) is a two-oscillator poly-
phonic subtractive synth featuring a Moog-style ﬁlter, am-
plitude envelope, eﬀects section, and looper with external
audio input. It is equipped with an onboard OLED dis-
play, four multifunction rotary encoders, 8 buttons and a
20-note piano-style capacitive touch keyboard. Whereas the
previous Noiseboxes were best at producing dense swarm-
ing chaotic drones, the Keybox is immediately easy to play,
control and understand, while the multifunction encoders,
buttons and display provide access to a host of parameters
for deep modulation and sound design.
Figure 3: The Keybox.
The Keybox utilizes the same computing hardware as its
predecessor, utilizing a Raspberry Pi 3 Model B+ for au-
dio processing and general system function, and a Teensy
3.2 microcontroller for sensor acquisition. However the soft-
ware framework has been redesigned from the ground up.
Sensor data is encoded into bytes on the Teensy using the
Consistent Overhead Byte Stuﬃng(COBS) protocol. This
facilitates the eﬃcient and reliable transfer of data with
minimal latency [4], which has shown substantial improve-
ment over previous versions. On the Raspberry Pi, all audio
Figure 2: CAD renderings of three new Noiseboxes (L-R): the Keybox, Stringbox, and Tapbox.
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
7
synthesis and control mapping is done in the SuperCollider
language, which receives the incoming sensor data from the
Teensy via hardware serial port.
The onboard display functions separately from the rest
of the instrument processing. When parameters change,
new data is sent as OSC (Open Sound Control) messages
from SuperCollider to a Python script which updates the
display. The display can move between several pages of
grouped parameters with the two red buttons to its left.
Each page displays eight parameters, mapped to the four
rotary encoders and four buttons to the right.
The fabrication of the Keybox and the other new instru-
ments are revised as well. The enclosures are constructed
from a combination of 3D printed frame assemblies and laser
cut panels. While the instruments are mostly true to their
“box” names, our updated fabrication materials and meth-
ods permit rounded, smooth edges for a more ergonomic
feel, and allow for the possibility of alternate shapes, angles
and more freeform designs.
While the Keybox is ﬁnished and playable at present, we
continue to make incremental enhancements. In its current
form it lacks direct sound output, but an updated version
of the enclosure will include onboard ampliﬁcation, which
is a feature of the other instruments. Additionally, we have
installed an IMU that will map movement of the instrument
to user-selectable sound parameters.
Figure 4: The Stringbox.
4.2 Instrument 2: The Stringbox
The Stringbox (Fig. 4) is a digital synthesizer inspired by
the form and function of a ukelele. The primary function
of the instrument is a physically modeled string synthesizer
that can be played in traditional guitar (or ukelele) fashion.
Four strings provide an excitation source through pluck-
ing or picking (or alternately by bowing, rubbing, scraping,
etc.) A 4x8 matrix of soft elastomer buttons sit on the short
neck of the instrument that can be pressed as ﬁngerings
on a fretboard to determine the pitch of the correspond-
ing string’s note. A simple implementation of the Karplus-
Strong synthesis algorithm provides a string-like sound, and
the instrument can be played as one would play a ukulele.
Alternately, a separate mode can fully reconﬁgure the in-
strument, and while the physical and visual similarities may
remain, it becomes totally diﬀerent. In this mode, the 4x8
grid can function as a sequencer, with diﬀerent pages to de-
termine sound sources and synthesis algorithms, sequences
and arpeggios, while the strings can be manipulated by the
user to modulate the corresponding audio track.
Figure 5: Detail of the Stringbox piezoelectric
pickup assemblies.
The core hardware of the Stringbox is the same as the
Keybox. A Teensy 3.2 receives sensor data, encodes it with
the COBS protocol and sends it to SuperCollider running
on a Raspberry Pi for audio synthesis and processing. Sim-
ple string pickups (Fig. 5) are made from small piezoelectric
elements sandwiched between rigid discs of laser cut acrylic
and mounted in a ﬂexible 3D-printed housing. The pickup
design is based on the approach developed by [6], where
string excitation is carried to the coupled piezo, which out-
puts a corresponding voltage that is passed to an analog
input of the Teensy. Two types of data are extracted from
the piezo input: an event trigger with corresponding veloc-
ity (as with the initial pluck of a string), and a continuous
data stream resulting from the vibration of a plucked string
or sustained excitation (as in the case of a bowed string or
other string interaction).
An onboard speaker gives the player an option for direct
sound output, while audio can also be routed through a
parallel audio output jack. An embedded IMU allows for
further modulation of sound parameters though the move-
ment of the instrument. While the hardware is ﬁnalized,
additional features are planned for the instrument includ-
ing additional synthesis models and deeper functionality of
the sequencer module.
4.3 Instrument 3: The T apbox
The third and ﬁnal instrument in the series is a digital per-
cussion instrument (Fig. 6). Each face of the rectangular
instrument consists of a discrete panel that “ﬂoats” on rub-
ber washers attaching it to the instrument frame. Five of
the faces are equipped with large piezoelectric elements held
ﬂush to the inside of each panel, each mapped to a diﬀerent
voice of the embedded synthesizer. The instrument can be
played by drumming, tapping, knocking and rubbing the
various surfaces of the instrument and exciting the diﬀer-
ent synth voices. The ﬁnal face is equipped with two small
speakers, USB ports for charging and reprogramming pur-
poses, and a volume/multipurpose slider.
An IMU is embedded within the instrument which, in ad-
dition to modulating synthesis parameters based on move-
ment, serves a particular function. There are two synthesis
modes mapped to the instrument which can be selected and
mixed based on the absolute orientation of the instrument.
In its normal upright state, signals from the piezoelec-
tric elements are each routed to a physical model of an
N-segmented tube. This produces a unique bell-like tone
for each interaction. Rotation and movement of the instru-
ment changes the parameters of the virtual tube segments,
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
8
Figure 6: Top view of the Tapbox.
modulating the frequency, duration and timbre of the tones.
When the instrument is rotated into an upside-down ori-
entation, the controls are mapped to a synthesized drum set,
with each face a trigger for a diﬀerent drum or cymbal. As
with the ﬁrst mode, movement and rotation can modulate
the drum sounds in diﬀerent interesting ways. Addition-
ally, with the instrument held near the midpoint between
the two, the modes are cross-faded proportional to the angle
of orientation.
The synthesis method for the ﬁrst mode processes audio
directly from the piezoelectric element, whereas the second
uses a layer of feature extraction to trigger sample playback
when an onset (strike or slap, etc.) is detected. These
two methods are extreme ends of a continuum, and any
combination of feature extraction and audio signal can be
mapped to synthesis parameters.
The technical design and hardware of the Tapbox is a
slight departure from the architecture of the Keybox and
Stringbox, as it is built on theBela2 platform which runs
on a BeagleBone Black single board computer. Percussion
tasks demand considerably lower latency than other more
legato instrumental gestures. Selected for its exceptionally
low latency and readily accessible audio rate signal acquisi-
tion [10], the Bela proved to be the ideal platform to bring
this instrument to fruition.
As with the other instruments, we continue to explore
ways to improve the Tapbox. We are experimenting with
various preparations of the panels, applying several mate-
rials to bring a diversity of textures into instrument play.
Rippled hot glue, felt, tree bark and glued pebbles can dec-
orate the sides of the instrument that provide the performer
with an array of surfaces to hold, rub, or strike. On the ﬂat
felt, hand slapping is eﬀective; on the pebbles rubbing like
a g¨uiro is a possible playing technique.
5. CONCLUSIONS AND FUTURE WORK
From the original Noiseboxes, we hope these instruments
are improved in several ways. First, we have addressed
a number of design and performance issues that our pilot
study identiﬁed. High quality loudspeakers and overall im-
proved hardware design provide better sound quality, while
the instruments employ a range of synthesis and eﬀects pro-
cessing methods, making for more interesting and diverse
sounds. Improved software design and the use of a highly
eﬃcient data encoding algorithm have decreased latency to
2https://bela.io/
within acceptable bounds for adequate control intimacy [17]
on the Keybox and Stringbox, while for the percussive ges-
tures of the Tapbox we have opted to use the ultra-low
latency Bela platform. And with the use of CAD design
and fabrication tools, we have crafted more ergonomic and
playable interfaces.
Along with these improvements, ideas generated from
workshops with expert musicians have resulted in unique
design choices that will make the instrument interesting and
engaging to use.
This project is still ongoing, and an important future step
will be to organize a longitudinal user study with the instru-
ments that will allow us to evaluate if and how the informed
design choices support the instruments’ use in performance
and to assess prolonged engagement over extended periods
of time. Furthermore, the instruments’ software continues
to be updated with new features and reﬁnements to the
existing sounds and controls.
6. CLOSING REMARKS
The three instruments presented here represent a dedicated
focus on the development of new digital musical instru-
ments that will be both appealing and robust for long-term,
engaged use in real-world performance practice. We be-
lieve that our user-driven approach may ultimately lead to
greater uptake and longer term use that many DMIs cur-
rently experience.
Our work here is only one part of the equation towards
bringing DMIs into more active performance practices. For
one, there is a distinction to be made between commer-
cially available instruments which beneﬁt from industrial-
production technologies and non-commercial DMIs designed
and constructed using readily available maker tools and
technologies. Furthermore, it is vital to acknowledge the
important role that community-building plays in the devel-
opment of performance practices around new instruments.
However, with this and related work, we hope to pro-
mote design processes involving performers to best meet
their needs in terms of DMI use and performance within
and beyond NIME, with new tools and new techniques to
support ever-evolving musical practices.
7. ACKNOWLEDGMENTS
The authors would like to extend our thanks to K. An-
dersen, A. McPherson and F.Morreale, and Messrs. Lepri,
Harrison and Armitage for fruitful discussions and input
leading to the research and development documented here.
Thanks also to Yves M´ ethot and the Centre for Interdisci-
plinary Research in Music Media and Technology for fabri-
cation assistance and facilities. Finally, thanks to Patrick
Cowden for continued work and collaboration on these and
ongoing instrument designs.
8. ETHICAL ST ANDARDS
The user research described in Section 3.1 has been ap-
proved by the McGill University Research Ethics Board,
File #188-0918.
9. REFERENCES
[1] K. Andersen and R. Wakkary. The magic machine
workshops: Making personal design knowledge. In
Conference on Human Factors in Computing Systems,
Glasgow, Scotland, 2019.
[2] E. Berdahl. How to Make Embedded Acoustic
Instruments. In Proceedings of the International
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
9
Conference on New Interfaces for Musical Expression ,
pages 140–143, London, United Kingdom, 2014.
[3] E. Berdahl and W. Ju. Satellite CCRMA: A Musical
Interaction and Sound Synthesis Platform. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 173–178,
Oslo, Norway, 2011.
[4] S. Cheshire and M. Baker. Consistent overhead byte
stuﬃng. IEEE/ACM Transactions on Networking ,
7(September):159–172, 1997.
[5] I. de Almeida Soares Franco. A Framework for
Embedded Digital Musical Instruments . Dissertation,
McGill University, 2019.
[6] J. Harrison, R. H. Jack, F. Morreale, and
A. Mcpherson. When is a Guitar not a Guitar ?
Cultural Form , Input Modality and Expertise. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 299–304,
Blacksburg, VA, USA, 2018.
[7] C. Mamedes, M. Rodrigues, M. Wanderley,
J. Manzolli, D. H. L. Garcia, and P. Ferreira-Lopes.
Composing for DMIs - Entoa, a Dedicate Piece for
Intonaspacio. InProceedings of the International
Conference on New Interfaces for Musical Expression ,
London, UK, 2014.
[8] A. Marquez-Borbon and P. Stapleton. Fourteen Years
of NIME : The Value and Meaning of ’Community’ in
Interactive Music Research. InProceedings of the
International Conference on New Interfaces for
Musical Expression, pages 307–312, Baton Rouge, LA,
USA, 2015.
[9] A. P. McPherson and Y. E. Kim. The problem of the
second performer: Building a community around an
augmented piano.Computer Music Journal ,
36(4):10–27, 2012.
[10] A. P. McPherson and V. Zappi. An environment for
submillisecond-latency audio and sensor processing on
beaglebone black. In138th Audio Engineering Society
Convention, pages 965–971, Warsaw, Poland, 2015.
[11] E. R. Miranda and M. M. Wanderley. New Digital
Musical Instruments: Control and Interaction Beyond
The Keyboard. A-R Editions, 2006.
[12] F. Morreale and A. McPherson. Design for Longevity:
Ongoing Use of Instruments from NIME 2010-14. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 192–197,
Copenhagen, Denmark, 2017.
[13] J. Sullivan. Noisebox : Design and Prototype of a
New Digital Musical Instrument. In Proceedings of the
International Computer Music Conference , pages
266–269, Denton, TX, USA, 2015.
[14] J. Sullivan and M. M. Wanderley. Stability,
Reliability, Compatibility: Reviewing 40 Years of
DMI Design. InProceedings of the 15th Sound and
Music Computing Conference , pages 319–326,
Limassol, Cyprus, 2018.
[15] J. Sullivan and M. M. Wanderley. Surveying Digital
Musical Instrument Use Across Diverse Communities
of Practice. InProceedings of the International
Symposium on Computer Music Multimedia Research ,
Marseille, France, 2019.
[16] O. Vallis and A. Kapur. Community-Based Design:
The Democratization of Musical Interface
Construction.Leonardo Music Journal ,
21(May):29–34, 2011.
[17] D. Wessel and M. Wright. Problems and Prospects for
Intimate Musical Control of Computers. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 11–14,
Seattle, WA, United States, 2001.
[18] V. Zappi and A. P. McPherson. Dimensionality and
Appropriation in Digital Musical Instrument Design.
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 455–460,
2014.
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
10