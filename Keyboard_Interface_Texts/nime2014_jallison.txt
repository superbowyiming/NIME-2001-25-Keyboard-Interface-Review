Pitch Canvas: Touchscreen Based Mobile Music
Instrument
Brad Strylowski
George Mason University
7320 Spriggs Ford Court
Manassas, VA
bstrylow@gmu.edu
Jesse Allison
Louisiana State University
340 East Parker
Baton Rouge, LA 70803
jtallison@lsu.edu
Jesse Guessford
George Mason University
417 Performing Arts Building
Fairfax, VA
jguessfo@gmu.edu
ABSTRACT
Mobile music applications are typically quite limiting to mu-
sicians, as they either attempt to mimic non-touchscreen
interfaces or do not oﬀer enough control. Pitch Canvas is
a musical interface that was built speciﬁcally for the touch-
screen, with the goal of providing the same depth and ﬂexi-
bility as traditional instruments without virtualizing an ex-
isting physical interface. Pitches are laid out in a hexag-
onal pattern that allow for easy scale, chord, and arpeg-
giation patterns, while notes are played by touch but sus-
tained through continuous movement. Pitch bends can be
achieved by passing through the space between the notes, al-
lowing slides, slurs, and rudimentary vibrato. The current
implementation of the interface runs only on Apple iPad
tablet computers using libPd [1] to convert user interaction
into audio. An iPad overlay oﬀers physical feedback for the
circles as well as the pitch bend area between the circles.
A performable version of the application has been built,
though several active developments allow alternative sonic
interpretation of the gestures, enhanced visual response to
user interaction, and the ability to control the instrument
with multiple devices.
Keywords
NIME, Mobile Music, Touchscreen instrument, geometric
pitch space
1. INTRODUCTION
Performing musicians rely primarily on traditional musi-
cal interfaces, which have the advantage of several cen-
turies of developments and improvements resulting in a well-
understood acoustic manner of producing sound. Over the
past few years, the ownership and computing power of tablet
computers has expanded, and with the relative ease of ap-
plication development, has inspired a new generation of mu-
sical interfaces. Music applications built for tablet comput-
ers have tended to oﬀer a touchscreen-simpliﬁed version of a
traditional interface, as musically-trained users will already
understand how to control these interfaces. Examples in-
clude Ocarina by Smule1 and Garageband by Apple2.
1http://www.smule.com/apps
2http://www.apple.com/ios/garageband/
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’14,June 30 – July 03, 2014, Goldsmiths, University of London, UK.
Copyright remains with the author(s).
Fewer music applications explore the touchscreen and its
aﬀordances for interaction as the foundation for a new inter-
face, developed independently of established instruments,
although a number of frameworks allow for this. The urMus
project at the University of Michigan enables novel touch
based paradigms through openGL and lua scripting [3], as
does the MoMu Toolkit from Stanford [2]. Both projects en-
able development of unique touch experiences, however in-
struments developed on them still tend to gravitate towards
dividing the screen up into key-like regions for interaction.
Inspired by such mobile applications that do break the key-
press paradigm, Pitch Canvas is an attempt at developing a
musical interface similar in depth, ﬂexibility, and playability
to traditional music interfaces but based on quintessential
touchscreen interactions.
2. MOTIV ATION & PRIOR WORK
Atau Tanaka outlined the existing paradigm of mobile in-
strument design at the 2010 NIME conference. In his paper,
the concept of aﬀordances inherent in Mobile devices was
elucidated as “objects that project perceived possibilities of
usage” [9] based on ideas developed by Gibson [4] and Nor-
man [7]. When applied to touch based tablets, we ﬁnd that
many apparent touch interactions such as key-press inter-
actions are quite deceptive. It appears as an aﬀordance due
to the visual division of the screen, however the tactile re-
sponse from pressing the screen is the same anywhere on the
device even though the sonic response may change. As an
extension of this, a dragging motion quite natural to touch
screens often produces a similar disconcerting eﬀect: that
of moving across visual elements without any link to touch,
speed, or sound. As a result of the disconnect, constant
visual focus must be placed on the screen and interaction
with the device can occlude the interface.
Emerging from these ideas, two main motivations, non-
skeuomorphism and full user control, inspired the original
conception of the Pitch Canvas interface. Touch events have
a distinct evolution - the initial touch, hold and/or drag,
and release. As natural elements of a touchscreen interface,
these became the building blocks for musical interaction.
To take advantage of this, Pitch Canvas was envisioned as
a hybrid application, fully functioning both as a musical
instrument and as a drawing interface. As a natural mode
of touchscreen interaction, drawing oﬀers an intuitive way
to dictate a musical gesture, and has a potential application
as a means of communicating a musical score with a user
unfamiliar with reading music.
2.1 Non-skeuomorphism
Contrary to many mobile music applications which attempt
to digitize the physical aspects of traditional interfaces, Pitch
Canvas was intended as a non-skeuomorphic instrumental
interface. Popular music applications often provide virtual
Proceedings of the International Conference on New Interfaces for Musical Expression
171
representations of established traditional interfaces, as seen
in GarageBand’s suite of virtual guitars, keyboards, and
drums. Borrowing an interface design allows many users
instant familiarity with the manner of controlling the in-
strument, but without the physical components of their
predecessors, in migrating to a touchscreen can lose playa-
bility. As such, very few musicians attempt to use these
mobile applications in recordings and performances, ﬁnd-
ing the physical interfaces easier to control. With the goal
of creating an instrument that could take full advantage of
the touchscreen’s unique properties, Pitch Canvas was de-
signed as a non-skeuomorph, which with original structure
and mapping would sacriﬁce potential user familiarity for
touchscreen compatibility. The representation of notes as
circles requires less precision than other virtual controller
shapes (such as guitar strings), and the hexagonal layout
allows easy transitions between notes in the same key.
Because the device represents discrete notes in circles that
can be pressed like piano keys, many ﬁrst-time users com-
pare the interface instrument to a keyboard interface op-
timized for the touchscreen. However, as a touchscreen-
based instrument, Pitch Canvas makes use of the potential
for expression through sustained gestures and internal tim-
bre and ﬁlter control, which are more diﬃcult to replicate
in real time on a keyboard and require supplementary con-
trols such as knobs and x-y pads. The current version of the
application limits the audible presence of key-press control,
and the planned directions for the project explore the po-
tential of artistic expression through highly location-speciﬁc
gestural interaction.
2.2 Full User Control
The second motivation shaping the design of Pitch Canvas
was providing the user with full control of produced audio.
Most mobile music applications that avoid borrowing as-
pects of actual interfaces can compensate for the conversion
to a touchscreen by delegating many musical functions to
the computer, as with repeating patterns and loops. Many
of these applications, such as Reactable Mobile [5], have
been well designed and can oﬀer the user an intuitive way
to create music, but presets impose limitations on the real-
time manipulation of audio. In order to restore user con-
trol of audio in real time, Pitch Canvas was constructed
to delegate as little audio production as possible, generat-
ing sound only when user interaction explicitly demands a
speciﬁc type of note. The resulting interface plays much
like traditional physical instrumentation, and with all au-
dio conﬁgured in real time can prove useful to performing
musicians.
Musix [8], by Brett Park and David Gerhard, and Hex
Player [6]are examples of non-skeuomorphic iOS instruments.
They utilize hexagonal-based pitch structures to experiment
with multiple harmonic layouts or geometric pitch spaces.
The instruments oﬀer several synthesized sounds and many
customizable settings, and employ a touch-based interac-
tion. However, performing with the instruments tend to be
key-press interaction oriented and appear to have limited
gestural control over the resulting sound beyond pitch and
amplitude.
3. IMPLEMENTATION
3.1 Device
The initial version of the interface was developed for Apple’s
iPad for a variety of reasons. Using a commercially pro-
duced tablet guaranteed reliable hardware, allowing time
to be spent on designing and programming the instrument.
Apple’s tablet computers claim the largest market share,
oﬀering a wide potential user base. Also, future research
will explore inter-device implementations of the instrument,
which would require using Wi-Fi, Bluetooth, or OSC con-
nections, all oﬀered on the iPad. Finally, by using a com-
mercial tablet with a large market share, Pitch Canvas oﬀers
a costless musical instrument to a large number of people
unlikely or unwilling to commit money towards purchasing
a traditional instrument.
Figure 1: Pitch Canvas hexagonal pitch map.
3.2 Interface Design
The Pitch Canvas interface consists of numerous circles,
each of which contains a certain note, arranged in a hexago-
nal layout (Figure 1). Currently, all notes are within a user-
speciﬁed major key with pitch increasing vertically, and the
vertical pattern repeats every other column. The circles
are arranged so that circles adjacent on either side will be
only a whole or half tone in distance, and vertically ad-
jacent circles will be a major or minor third. The trian-
gular areas between the circles carry pitches intermediate
to those of the bordering circles, allowing for slides, bends,
and slurs between adjacent notes. When the program is
ﬁrst run, pitches are mapped to each pixel on the screen,
and as the user plays a gesture, the corresponding pitches
are sent to a Pure Data patch that generates the audio.
In addition to mapping the frequency of a note to circle
location, Pitch Canvas maps volume to gesture speed and
feedback and overdrive to horizontal position. The layout
of the interface allows many common harmonic relation-
ships to be played easily, including scales, triads, arpeggios,
and trills. Simple major and minor chords can be played
as three vertically-adjacent circles, and inversions can also
be constructed without diﬃculty. In addition, the interface
enables the guiding of a melody through diﬀerent tones and
eﬀects throughout its lifetime, and facilitates polyphony and
Proceedings of the International Conference on New Interfaces for Musical Expression
172
counterpoint when controlled with two hands.
3.3 Audio Production
The application produces audio in a Pure Data patch, using
libPd. The Objective-C code sends control messages to the
patch on twenty-four diﬀerent channels. The patch contains
eight audio channels for eight ﬁnger multitouch gestures,
accepting three values on each channel: the midi number
of the gesture’s current circle, the velocity of the gesture,
and the gesture’s current horizontal location. These three
values are then mapped to the frequency, gain and harmonic
activation, and the overdrive and feedback amounts. Each
new gesture ﬁrst initiates a synthesized attack and then
transfers to a feedback loop with overdrive thus providing
tap-responsive and motion-responsive styles of performance.
3.4 Haptic Feedback
The initial design plan oﬀered the user one method of iden-
tifying interaction, requiring the user to watch themselves
play the instrument to ensure their ﬁngers aligned as in-
tended with the circles. This is less than ideal, as it limits
the ability of a musician to sightread or communicate with
other musicians while maintaining control of the interface.
Furthermore, because ﬁngertips have size comparable to the
size of the circles, there is a recognizable uncertainty as to
where exactly the ﬁnger is on the screen. This makes bound
motion diﬃcult to control, and when using continuous ges-
tures for polyphony, synchronizing transitions becomes dif-
ﬁcult. A custom iPad case was developed to improve usabil-
ity by providing haptic feedback, oﬀering an intuitive mode
of identifying notes(Figure 2). A laser cutter was used to
cut circles out of a sheet of paper in a pattern based on
the interface’s structure. After attaching the paper to the
tablet, the interface was placed beneath a vellum overlay
which would allow a recognizable border without demand-
ing too much eﬀort to transition to a new circle. The paper
and vellum layers did not cause a noticeable interruption
in interaction with the controller. The haptic feedback of-
fered by the overlay decreases uncertainty in touch location
relative to the interface and eases the demand on visual
attention.
Figure 2: Haptic feedback iPad overlay.
4. PERFORMANCE
Pitch Canvas was used in the compositionRecombinant Fea-
tures by Jesse Allison and performed by the Laptop Orches-
tra of Louisiana in November 2013 (Figure 3). The compo-
sition was a quartet piece of four Pitch Canvases at diﬀerent
tunings and transpositions. It was a structured improvisa-
tion in two parts taking advantage of the rhythmic qualities
of the tap-based attack, and the organic sound of swirling
multiple ﬁngers slowly across the canvas.
Figure 3: LOLs PerformingRecombinant Features.
5. FUTURE DIRECTIONS
In August, a stable version of the interface was completed
that could perform capably even on the ﬁrst generation
iPads. This version has been submitted to Apple for review,
and should soon be available through their app store. Sev-
eral additional aﬀordances are also planned as the subject
of future research, which should improve the instrument’s
virtuosity and performance capabilities.
5.1 Trans-Device Instrumentation
User control will be expanded by allowing multiple devices
to connect to the same interface using OSC via Bluetooth or
Wi-Fi. Additional devices would allow multi-modal inter-
action, and the planned developments would aﬀord greater
speciﬁcation of audio in real time. One mode, currently in
development, would record the current gesture and redraw
the gesture at the location of the next touch. This would
oﬀer repeated use of a visual pattern, and the retransla-
tion at a diﬀerent sonic location would allow exploration
of harmonic relationships. A second mode will allow ap-
plication and speciﬁcation of multiple audio settings and
eﬀects, oﬀering binary and fader controls beyond the scope
of single-device interaction. A third mode will enable real-
time redesigning of the interface’s visual aspects, enabling
simultaneous artistic expression in both the sonic and visual
dimensions. With the iPad’s pre-existing screen-sharing ca-
pabilities, this will allow greater artistic expressibility and
further audience immersion. These multi-modal improve-
ments were mainly designed with the goal of improving per-
formance value, as the incorporation of multiple devices will
oﬀer a stronger real-time control of the instrument.
5.2 Visual Feedback
Since conception, visual feedback has been one of the cor-
nerstones of the Pitch Canvas interface as gesture-based in-
teraction translates easily to drawing. The primary goal in
improving visual feedback is enabling drawing as the sim-
ple visual record of previous gestures. Although a drawing
capability has been implemented in the current app frame-
work, the redrawing processes greatly overwhelms the syn-
thesis capabilities of the device, rendering the application
unusable as either a drawing or musical application. Opti-
mization will need to occur to make drawing images a viable
mode of interaction.
Once this functionality exists, visual representation will
be expanded across multiple modes, also allowing eﬀects
such as the brightening of a circle upon initiation of a touch
or a diﬀused, fading trail behind a gesture. With a sec-
ond device, the visual representation of the gesture can be
controlled in real time, allowing an aesthetic expression of
Proceedings of the International Conference on New Interfaces for Musical Expression
173
audio. With an established visual expression of speciﬁc set-
tings, linking subtle audio eﬀects to graphics parameters
such as texture and brush type, complex compositions can
be communicated to other musicians in a natural and un-
derstandable manner. As a real-time drawing application,
Pitch Canvas will encourage compositions arranged both
visually and musically, and as a translator of a visual ges-
ture to a sonic expression, will also encourage gesture-based
improvisation. Implementation of drawing will oﬀer multi-
ple dimensions for artistic expression, and with pre-existing
haptic feedback, Pitch Canvas will become a more robust,
performable interface.
5.3 Degrees of Freedom
The current Pitch Canvas interface utilizes three degrees
of freedom: circle/pixel location mapped to pitch, gesture
speed mapped to gain and level of harmonics, and hori-
zontal location mapped to feedback and overdrive. Further
degrees of freedom, including proximity to center of circle,
curvature, direction, and tablet orientation will be mapped
to the tone of the produced note and further audio eﬀects.
As the relation between gesture and audio will become much
more precise, custom settings to control how degrees of free-
dom are mapped to aspects of the audio will be explored.
Currently, the two degrees of freedom requiring function-
based relationships, velocity and horizontal location, relate
linearly to their audio attributes. In future iterations, the
mapping function, such as parabolic or exponential relation-
ships, will be speciﬁable improving playability in a variety
of performance situations. Finally, the scale of the relation-
ship will be adjustable so that small deviations from the in-
tended gesture have a variable signiﬁcance on the produced
audio.
6. CONCLUSION
Built in a non-skeuomorphic approach to mobile music ap-
plications, the Pitch Canvas interface oﬀers users complete
control of produced audio through touch gesture. The cir-
cular interface takes advantage of the touchscreen layout,
and the positioning of the notes allows users to easily cre-
ate many musical patterns. Although the ultimate goal of
the project was to create a new interface that musicians
will ﬁnd useful, user feedback has suggested that the in-
strument also has applications in multiple ﬁelds, including
music education and therapy. As a free instrument requir-
ing no physical components, Pitch Canvas will be available
to thousands of users wishing to explore an interest in mu-
sic. With trans-device interaction, audio visualization, and
more degrees of freedom, Pitch Canvas will oﬀer musicians
more speciﬁc, real-time control, and will hopefully prove
useful when composing, recording, and performing.
7. ACKNOWLEDGMENTS
Thanks to Shantanu Thatte, Hali Dardar, Dr. Brygg Ullmer,
and the LSU Tangible Visualization Lab for assistance in
constructing the haptic overlay.
This material is based upon work supported by the Na-
tional Science Foundation under award OCI-1263236 with
additional support from the Center for Computation & Tech-
nology at Louisiana State University. Continued develop-
ment in the spring of 2014 has been funded by the Oﬃce of
Student Scholarship, Creative Activities, and Research at
George Mason University.
8. REFERENCES
[1] P. Brinkmann, P. Kirn, R. Lawler, C. McCormick,
M. Roth, and H.-C. Steiner. Embedding pure data
with libpd. In Proc Pure Data Convention 2011, 2011.
[2] N. J. Bryan, J. Herrera, J. Oh, and G. Wang. Momu:
A mobile music toolkit. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Sydney, Australia, 2010.
[3] G. Essl and A. M ¨uller. Designing mobile musical
instruments and environments with urmus. In New
Interfaces for Musical Expression, pages 76–81, 2010.
[4] J. J. Gibson. The ecological approach to visual
perception. Routledge, 1986.
[5] S. Jord` a, G. Geiger, M. Alonso, and M. Kaltenbrunner.
The reactable: exploring the synergy between live
music performance and tabletop tangible interfaces. In
Proceedings of the 1st international conference on
Tangible and embedded interaction, pages 139–146.
ACM, 2007.
[6] A. J. Milne, A. Xamb´ o, R. Laney, D. B. Sharp,
A. Prechtl, and S. Holland. Hex player—a virtual
musical controller. 2011.
[7] D. A. Norman. The psychology of everyday things.
Basic books, 1988.
[8] B. Park and D. Gerhard. Rainboard and musix:
Building dynamic isomorphic interfaces.
[9] A. Tanaka. Mapping out instruments, aﬀordances, and
mobiles. In Proceedings of the International Conference
on New Interfaces for Musical Expression, pages 15–18.
New Interfaces for Musical Expression, 2010.
Proceedings of the International Conference on New Interfaces for Musical Expression
174