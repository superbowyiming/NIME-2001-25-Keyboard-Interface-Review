Considering Audience's View Towards an Evaluation 
Methodology for Digital Musical Instruments  
 
Jerônimo Barbosa, Filipe Calegario, 
Veronica Teichrieb, Geber Ramalho 
Centro de Informática 
Universidade Federal de Pernambuco (UFPE), Brazil 
{jbcj,fcac,vt,glr}@cin.ufpe.br 
 
 
Patrick McGlynn 
An Grúpa Theicneolaíocht Fuaime agus Ceoil Dhigitigh, 
National University of Ireland, Maynooth, 
Ireland. 
patrick.j.mcglynn@nuim.ie 
 
ABSTRACT 
The authors propose the development of a more complete Digital 
Music Instrument (DMI) evaluation methodology, which provides 
structured tools for the incremental development of prototypes 
based on user feedback.  This paper emphasizes an important but 
often ignored  stakeholder present in the context of musical  
performance: the audience. We demonstrate the practical 
application of an audience focused methodology through a case 
study ( ‘Illusio’), discuss the obtained results and possible 
improvements for future works. 
Keywords 
Empirical methods, quantitative, us ability testing and e valuation, 
digital musical instruments, evaluation methodology, Illusio 
1. INTRODUCTION 
The use of computers in the design of novel musical instruments 
has led to an extraordinarily rich body of work throughout its 
relatively brief histor y. Notable examples include MUSIC and 
Radio Baton, by Max Mathews in 1957, The Hands, by Michel 
Waisvisz in the 1980s, Hyperinstruments, by Tod Machover in 
1986, The Buchla Lightning and others [12] [19]. 
In recent years, the amount of research concerning Digital 
Musical Instruments ( DMIs) has increased dramatically, as 
illustrated by [13] and [14]. DMIs can be defined as a music -
making system which consists of a separate control interface and 
sound generator connected via a mapping strategy [11]. While 
acoustic musical instruments are limited by their physical means 
of sound -production, this separation (referred to as ‘control 
dislocation’ [13]) allows DMI designers to explore the process of 
interaction with more freedom. 
The growing interest in DMIs has prompted a large number of 
questions that are still open. One of the most relevant is: how can 
these systems be objectively evaluated?  
The importance of this question becomes clear when we observe 
the nature of DMI development – experimentation and ongoing 
development of prototypes are integral parts of the process.  
As mentioned by Wood [20], a critical ingredient for designing 
systems that are well -suited to their goal is "understanding 
potential users". Focused observation techniques and detailed 
analysis of the resulting data are essential characteristics of any 
good development process. Structured evaluation methods can 
increase the efficiency of user feedback and provide better tools 
for its analysis. Despite its importance, research about the i ssue is 
sparse [18].  
Table 1. Analyzing NIME conference publications from 2009, 
to 2011 based on a similar study made by Stowell et al.[18] 
 
Taking into account the proceedings of the International 
Conference on New Interfaces for Music Expression (NIME) of 
the last 3 years, despite perceiving a percentage increase in the 
last year, the number of papers employing formal evaluation 
methodologies is still low. The lack of any standard approaches of 
evaluation makes it difficult to compare results.  
One aspect that must be considered in an evaluation process is 
the variety of stakeholders involved in the use, conception, 
perception and even commercialization of a musical device.  
As mentioned by O’Modhrain in [14], this leads us to a more 
generic concept of evaluation, where the whole process should 
consider:  
 Performer's view - How effective is the relationship 
between performer and device? Does it  allow the 
performer to reach all of his musical intentions?  
 Audience's view - Is the relationship between performer 
and device established in such a way that those observing 
the performance might be affected sensitively? 
 Manufacturer's view - How effective is the system from a 
commercial perspective? 
An initial step towards a structured DMI evaluation 
methodology examined the performer’s view [2]. Our current 
research aims to develop  further this methodology by focusing 
upon the audience's view. In doin g so, we expect both 
perspectives to complement each other and , therefore, permit a 
more thorough investigation into DMI evaluation. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. 
NIME’12, May 21-23, 2012, University of Michigan, Ann Arbor. 
Copyright remains with the author(s). 
 
 
2. AUDIENCE'S VIEW 
The phenomenon of control dislocation has undoubtedly 
introduced, even within a century which has seen “the  great 
opening-up of music to all sounds” [22], one of the greatest 
challenges to the ideological boundaries of music in history [17]. 
The rapid advance of digital technology over the last 30 years has 
allowed musicians to transgress the boundaries of phys ical cause-
and-effect, at least from the observers’ perspective, and adopt the 
computer as the “interpreter between our physical body and the 
sound production" [17].  
It has been perspicaciously observed  that we cannot simply 
transplant our understanding of spectatorship from the domain of 
acoustic musicianship to that of digitally -mediated performance 
[9]. Accordingly the creation of meaningful and perceivable 
connections between human action and sound has been identified  
as a key point for making a performance convincing for the 
audience [14]. The ability to evaluate the extent to which an 
audience can understand these connections would prove a 
valuable asset to DMI designers.   
According to Davis [4], a performance ecosystem comprises 
four parts: the inst rument – an artifact that is manipulated  to 
produce music; the performer – an agent who directly interacts 
with the instrument; the listener (referred -to here as ‘the 
audience’) - who watches the interaction and has an indirect 
relationship with the instru ment; and the environment – the place 
where the performance takes place. 
In traditional human-computer interaction (HCI) design, there is 
no equivalent to the audience as defined above. HCI design 
models focus almost exclusively upon the direct user of the  
system. In DMI research, this has led to a predominance of 
performer-centered design and an insufficient treatment of the 
audience. 
An early approach by Hsu et al . [10] evaluates DMIs based on 
both performer and audience perspectives. A quantitative 
experiment was performed, with questionnaires applied in loco, in 
order to rate the experience of both parties during performance. It 
is important to note that the questionnaires took into consideration 
the audience’s subjective, artistic judgment of the music 
performed, based on their previous experience of similar pieces.  
This brought variables to the evaluation process that did not 
concern the instrument itself – such as the skills of the performer 
and the artistic quality of the music.  
The most comprehensive research regarding the audience's view 
of DMIs has been conducted  by Fyans, Gurevich and Stapleton 
[8] [9] [6]. Their experiments were constructed according to a set 
of interaction design guidelines (developed by Bellotti et al [3]) 
that focus upon the c ommunicative aspects of interaction. This 
approach, which was inspired  by analysis of human -human 
communication directing the design of systems, was inserted  in 
the context of communication between a performer and the 
audience, mediated by DMIs. It is based on five questions:  
 Address (hereafter called Cause comprehension) - "How 
does the spectator know that the performer is in direct 
communication with the system?" - concerns the 
audience's understanding of which input actions are 
possible to the user; 
 Attention (hereafter called Effect comprehension) - "How 
does the spectator know that the system is responding to 
the performer?" – concerns the audience's understanding 
of which output the system is generating; 
 Action (hereafter called Mapping comprehension) - "How 
does the spectator think the user controls the system?" - 
concerns the audience's understanding of how the mapping 
between input actions (cause) and output results (effect) 
functions;  
 Alignment (hereafter called Intention comprehension) - 
"How does the spectator know that the system is doing the 
right thing?" - concerns the audience's understanding of 
the intentions of the performer; 
 Accident (hereafter called Error comprehension) - "How 
does the spectator know when a mistake occurs?" - 
concerns th e audience's understanding of mistakes made 
by the user and the system. 
Fyans et al.  focused exclusively upon two of these questions 
(Intention and Error) to build a model of audience's error-
comprehension based on the concept of mental models [6]. This 
model was subsequently used  for conducting qualitative 
experiments [8].  
 
Figure 1. Reeves et al. [16] taxonomy for classifying musical 
interactions according to audience's view 
A markedly different approach to the issue of DMIs and 
audience perception was adopted  by Reeves et al. [16]. Their 
method classifies musical interactions according to the audience’s 
perception of the relationship between input manipulations and 
audio output (Figure 1). Interactions are classified  according to 
four categories – hidden, partially revealing, transforming and 
amplified – defined as follows:  
 Secretive - Represents when interaction tends towards 
hiding to the audience both performer's input 
manipulations and the instrument's output effects; 
 Suspenseful - Represents when interaction tends towards 
revealing to the audience the performer's input 
manipulations but hiding the instrument's output effects; 
 Magical - Represents when interaction tends towards 
hiding to the audience the performer's input manipulations 
but revealing the instrument's output effects; 
 Expressive - Represents when interaction tends towards 
revealing both performer's input manipulations and the 
instrument's output effects. 
Besides providing design guidelines for reaching each o ne of 
these categories, this work is also important as a pioneering 
attempt to analyze how the audience perceives new musical 
interfaces.  

 
3. SCOPING 
Before introducing our methodology proposal, it is essential to 
untangle some concepts from previous works in order to provide a 
clearer view of the context.  
The process of perceiving a musical performance is a complex 
phenomenon that is intrinsically linked  to social, cultural, 
technical, perceptual and emotional relationships [9]. This implies 
several different possible approaches. 
Due to scoping reasons, we decided to focus the present work 
on the audience's understanding of how a DMI works - as well as 
the kind of interaction it employs - since: (a) it engages with 
communicative and cognitive issues , which are understood to be 
sensitive in this context [9] ; (b) we believe that it can be 
objectively measured, as suggested by previous attempts [6].  
After defining the study focus, we needed to select which 
variables were most important to consider regarding the 
stakeholders involved in performance ecosystem. 
 About the performer: we decided that the skill of the 
performer using the system is not important for the 
audience to understand the functioning of the DMI and 
omitted this concern from the study. We reasoned  that the 
most important here was the performer desire of using the 
instrument as a tool for expressing his musical intentions - 
no matter if this attempt is successful (what should have 
led to a  correct understanding by the audience) or not 
(what should have led to a sensation that the performer has 
made a mistake). Additional information can be found in 
previous works [7]; 
 About the artistic result: unlike the Hsu and Sosnick 
approach [10], we assumed that judgments about artistic 
quality of the performance deals with subjective aspects 
(e.g. taste) that exceed the understanding of how the DMI 
works and was not considered  in the present study. 
However, w e admit that analyzing the perceived artistic 
result could have given different insights that could be 
useful for instrument improvement. Additional 
information can be found in previous works [15][5]; 
 About the audience: the personal background (cultural, 
social, emotional) of e ach audience member may  also 
affect our focus. We are trying to consider this aspect in 
the present work by using the concept of the target  
audience, used in others areas as advertising, produ ct 
design and game design [1]. Another sensitive variable 
regarding the audience is its previous knowledge about 
what the performer is about to perform, as it can directly 
influence the comprehension about how the DMI works 
[8]. We decided to focus only on participants with no 
previous knowledge about the functionality  of the DMI, 
nor previous information about the performer's intention. 
It is important to highlight that this work does not attempt to 
judge the ‘best’ available instruments today. Rather, we have 
focused upon developing a structured methodology for DMI 
analysis and comparison that will enable users to assess the 
suitability of a DMI for a given performance context. In this way, 
we assure that the research can be of benefit in a wide variety of 
circumstances – whether evaluating the suitability of different  
instruments for a particular performance (e.g.  selecting an 
interface for an art installation aimed at children) or choosing 
between different prototypes of the same instrument (e.g. " this 
version had these improvements compared with the older one").  
4. EVALUATION METHOD PROPOSAL 
Drawing from previously described attempts, this work proposes a 
synthesis of techniques that provide a more robust evaluation of 
the audience's understanding of how a DMI works. 
This proposal consists of three steps: audience profi ling, data 
collection and data visualization. The techniques, involved in each 
step, are described as follows. 
4.1. Audience Profiling 
This first step collects information about candidates participating 
in the experiment and compares it to the target audience p rofile of 
the instrument. With this , the most suitable candidates are 
selected. 
A questionnaire approach was chosen for this process due to the 
easily measurable data produced. Questions cover personal topics 
such as their relationship with technology and music, their age, 
and whether they play a musical instrument. Besides these 
suggestions, we encourage more detailed questions to be created 
according to the specificity of the target audience profile. 
4.2. Data collection 
The second step has the purpose of collecting data from the target 
audience about their comprehension of how the DMI works. 
We adopt Fyans et al. approach in which a performance using 
the DMI is recorded  and the video is exhibited  to the audience. 
However, instead of structured interviews, a questionnaire is 
presented to each viewer after the video. The questions are based 
on the human -human communication aspects presented by Fyans 
et al. and Bellotti et al., as follows:  
 Cause comprehension - "Which part of the performer's 
body (or yet, which t echnological device) was used  to 
interact with the system?"; "How understandable are the 
actions made by the user for interacting with the system?”; 
 Effect comprehension - "Did the system provide enough 
audiovisual information for the audience to understand 
what is happening between the user and it?”; 
 Mapping comprehension - "How clear is the relationship 
between the user's actions and the resulting sound?”; 
 Intention comprehension - "How successful was the user 
to express himself using the system? "; "Was the user's 
intention well understood?”; 
 Error comprehension - "Were the system's errors perceived 
(e.g. technical problems and software bugs)?"; "Were the 
user's errors noticeable?”. 
 The other part of the questionnaire focuses on the relationships  
between the cause and the effects of the instrument and aims to 
classify the type of interaction provided by the instrument 
according the taxonomy developed by Reeves et al.  Although it 
does not concern the understanding of how the DMI works itself, 
we believe that Reeves' scale could be useful for researchers as a 
categorization tool by pointing out a context the DMI can be 
inserted.  
This classification is based on the results of two questions: (a) 
“how would the participant classify the performer's acti ons for the 
functioning of the system? ”, (b) “how would the participant 
classify the system's response to the user's actions?”. 
It is important to highlight the difference between perceived 
understanding and actual understanding: while the former 
concerns what people think they understand about a given subject, 
the second concerns what they actually comprehend – subjects are 
 
often inaccurate in estimating their real levels of understanding 
and usually consider themselves above-average [21]. 
To solve this pr oblem, we propose the use of open questions in 
support of the scale-based answers and allow participants to write 
their own answer. Thus, it is possible to verify whether the 
participant properly understands how the system works - in others 
words, the accuracy of the answers - by examining the congruence 
between the users’ responses and the manner in which the DMI 
actually functions. 
We elected to use online questionnaires and online videos of 
performances. A local approach would have difficulties 
concerning the schedules of the participants , the place and it 
would add a more time-consuming step to the research. On the 
other hand,  the online format of the study allowed us to reach a 
larger number of people that could answer the questions 
asynchronously from other participants. 
4.3. Data visualization 
The last step of our process aims to show the information in a 
clear and intuitive manner, helping us to visualize and analyze the 
results.  
  
Interaction model: -- 
Figure 2. Example of an empty DMI datasheet 
In order to do this, we used a datasheet consisting of a radar 
chart that plots the level of audience comprehension concerning 
the DMI (Figure 2). A brief description of the DMI is also 
included along with its classification and features. The chart is 
based upon a numerical average - ranging from 0 to 1 - of the 
results of the questionnaire, with each axis regarding Bellotti's 
communication aspects [3].  
5. THE PROTOTYPE 
In order to validate and refine the proposed evaluatio n 
methodology we developed a prototype - hereafter called Illusio - 
and performed some case studies, as described in the following 
sections.  
  
Figure 3. The Illusio interface 
It is important to highlight that the intention of this paper is not 
to defend the quality of this particular application, but instead to 
explain the process by means of this example. 
The system Illusio - shown in Figure 3 - is a DMI that allows 
collaborative control of real -time recorded loops through 
relationships between sketches and sounds. It combines a multi -
touch interface with the interaction metaphor of guitar pedals 
using a multi -touch surface and a "guitar pedal" (a modified old 
computer keyboard). Developed in Processing 1, openframeworks  
2and Open Sound Control 3, Illusio was created  initially for 
experienced musicians, focusing on providing a one-man-band 
interaction in the context of music performances. 
At first, the system shows a white screen in the multi -touch 
surface, where users can only draw rough sketches.  Once 
completed, these sketches can be edited, grouped, removed and 
associated with real time recorded sounds - loops, recorded with 
one or more instruments via the pedals audio input.  
 
Figure 4. User interacting with illusio 
Once loops are recorded and associated to sketches, they can be 
manipulated (played, stopped and processed) in real -time via the 
multi-touch surface. 
The system is shown  in use above (Figure 4) , and a 
demonstrative video is also available online 4. 
6. EXPERIMENT 
Regarding the audience profiling stage, 80 participants were 
contacted by e -mail and were asked to  answer a profile test.  
Among them, 47 were selected due to their accordance with the 
target profile: people with close relation to technology and music 
(scored 3,  4 or 5 in a 1 to 5 scale) and who play a musical 
instrument. 
In the data collection step, the selected participants were then 
contacted by e -mail, asked to watch a  video from a performance 
with the prototype and to answer an online questionnaire as 
presented in the previous section.  
6.1. Cause Comprehension Degree 
Concerning the question s related to the cause comprehension 
degree, it can be seen that the majority of the participants 
considered they understood the user's actions, where 46% marked 
4 and 35% marked 5 in a scale from 1 to 5 (where 1 is "Did not 
understand" and 5 is "Completely understood").  
Besides, taking into account a list of body parts and also a list of 
interaction devices in the questionnaire, the majority indicated the 
actual body parts and devices us ed during the performance, 
indicating a match between perceived and actual understanding. 
The calculat ed average value related to this axis was 3,83 in a 
scale from 0 to 5. 
                                                                 
1 http://processing.org 
2 http://openframeworks.cc 
3 http://opensoundcontrol.org 
4 http://jeraman.info/illusio 

 
6.2. Effect Comprehension Degree 
Considering the effect comprehension degree  questions, 68% of 
the participants marked 3 or 4 (35% ans wered 3 and 33% 
answered 4) in a scale from 1 to 5 (where 1 is "I do not agree" and 
5 is "I completely agree") and only 13% marked 5.  
This result shows us that the system's output effects were not 
evaluated by the audience as well as the user’s actions. It could be 
an interesting point in a future redesign of the instrument. The 
calculated average value related to the effect axis was 2,91. 
To measure whether the participant actually understood the 
system’s effe cts, an open question was used: “Describe in few  
words how does the system work ”. The written answers well 
described the effects of the system.  
6.3. Mapping Comprehension Degree 
In respect of the mapping comprehension degree, 41% of the 
participants marked 4 and 35% marked 5 (considering a scale 
from 1 to 5, where 1 is "Did not understand" and 5 is "Completely 
understood"), which shows that the mapping was considered well 
understood by the majority of the audience.  
The open question used in the Mapping degree was the same 
used in the Effect degree. However, we could perceive here that 
only a few participants mention ed what the user did for reaching 
system’s outputs, fact that has hindered the accuracy measure of 
mapping results. The calculated average value related to this axis 
was 3,8. 
6.4. Intention Comprehension Degree 
When the issue is the intention comprehension degree, 79% of the 
participants marked 4 or 5 in a scale from 1 to 5 . However, once 
again, the usage of open questions did not help to verify  the 
accuracy of these results, as the answers were very abstract and 
confusing, hindering any attempt to match positively or negatively 
perceived and actual understanding. The calculated average value 
related to this axis was 3,87. 
6.5. Error Comprehension Degree 
When as ked whether the system correctly responded the user's 
actions, concerning the error comprehension degree, 30% of the 
participants marked 5 and 59% marked 4 in the 1 to 5 scale, what 
seems suitable as the system and the performer actually presented 
only a few errors during the performance.  
However, when a yes-no question was asked about the system's 
and the performer's errors, 74% of the participants answered that 
they have not perceive d any error, which may indicate that the  
system does not highlight occurrences of error s to the audience .  
This fact also seems to justify why participants thought the system 
properly responded the user's actions. 
The average value related to the error axis was 2,67, calculated 
using an average between the first mentioned scale-based question 
and the percentage of the participants that answered Yes on the 
yes-no questions related to error – and who has correctly justified 
it in the open questions. 
6.6. Instrument Classification and Datasheet 
Finally, classifying the instrument according to Reeves' 
taxonomy, two scale-based questions were asked about cause and 
effect subject. T he scale ranged in a continuum between 1) 
“Invisible/absent” to 5) “ complex/excessive”, representing 
respectively "hidden" and "amplified" in the scale pr oposed by 
Reeves. The final average values were, respectively, 3,17 and 3,39 
(between "partially revealing" and "transforming" according 
Reeves), which led to classify the instrument interaction model as 
expressive.  
According to these results, the Illusio datasheet was created, as 
shown in Figure 5.  
  
Interaction model: Expressive 
Figure 5. Illusio datasheet 
It is important to highlight that all above mentioned data is 
available in Internet 5 as references for other works, including the 
video demo used in the data collection step 6. 
7. DISCUSSION 
The proposed approach was considered successful since it showed 
to be a practical and structured method that has provided useful 
information towards a more reliable analysis and comparison of 
DMIs.  
Although the results and the final datasheet provided som e 
information about how it would be possible to improve the 
prototype, it is important to notice that it would be more 
meaningful if we had compared them with datasheets of different 
DMIs (or yet of different versions of Illusio prototypes), what 
would provide a more objective way to analyze and compare 
them. However, as a work in progress and always aiming the 
methodology improvement, we believe that the developed 
research generated a set of relevant results that will be discussed  
as follows. 
Considering t he profiling step, it is thought that deepening the 
profiling techniques and focusing on a more specific audience 
profile could give the process more refined results about the 
instrument. 
Assuredly, the most problematic step was the data collection 
step. R egarding it, although the open questions used in the 
questionnaire provided good parameters for reducing the gap 
between perceived and actual understanding it did not properly 
work for all cases due to the fact some answers were abstract or 
superficial, which did not give any hint about how to match both 
understandings.  
On the other hand, this usage has brought a collateral positive 
point as comments, suggestions and criticisms received that could 
be used for future phases of instrument re-design. 
It is al so important to highlight that due to the scope of our 
approach on analyzing only the audience’s understanding about 
the instrument being evaluated , we could have lost important 
information that is related the other aspects (aesthetic, cognitive) 
not studied in this paper. Thus, the more effective we wanted this 
evaluation process to be, the more analyzed and considered these 
aspects should also be. 
                                                                 
5 http://www.cin.ufpe.br/~fcac/nime2012 
6 http://youtu.be/CAiVWvVFaqI 

 
Finally, inside the data visualization phase, concerning the 
classification according to Reeves et al. taxonomy of instrument 
interaction models, it is important to consider that as it is based on 
relations of two different variables (cause and effect)  - a 2 -
dimension graph - we lose potential useful information due to the 
labeling process that reduces it to a one-dimension information. 
Thereby, a DMI labeled as “Secretive” , and that presents a high 
cause-effect correlation could be classified in the same group 
which contains another “Secretive” DMI with a more pondered 
average, which is much closer to other labels.  
8. CONCLUSION 
This work presented an approach for evaluating DMIs considering 
the audience’s view, based on the combination of ideas and 
methods of previous works, aiming to contribute towards building 
a more complete and deeper generic DMI evaluation 
methodology. 
Despite its application generated useful results regarding DMI 
evaluation in the context of audience's view and about how to 
improve the Illusio system itself, it is important to highlight that 
the method is still under continuous development. Thereby, other 
case studies and iterative reapplications are necessary - including 
case studies comparing different versions of Illusio and comparing 
Illusio with well -known DMIs - what would allow us to make 
systematic analysis and comparison of them. 
A critical point for improvement in this study is the technique 
used for reducing the gap between actual and perceived 
understanding, which was not suitable for all  cases. For future 
work, we plan to change from asking questions to presenting 
statements that will be considered  true or false by the audience - 
following the Likert scale [23]. With this approach, we believe the 
results will be more objective , and the actual understanding could 
be closely reached. 
Regarding the ongoing research about a more generic evaluation 
methodology [2], we also propose to merge the approach 
presented here with the early approach that considered the 
performer's view, providing a more complete and deeper way to 
visualize the DMI, enriching the whole evaluation process.  
Finally, it is necessary to highlight the importance of this 
evaluation process as a phase in the cycle of user-centered design 
of a DMI, where user (both performer and audience) feedback is 
constantly used to improve the system.  
9. ACKNOWLEDGMENTS 
The Illusio was developed with the partial support of Rumos Itaú 
Cultural Cybernetic Arts. 
10. REFERENCES 
1. E. Adams, Fundamentals of Game Design, 2nd Edition. New 
Riders, (2010). 
2. J. Barbosa, F. Calegario, F. Magalhães, V. Teichrieb, G. 
Ramalho, and G. Cabral, “Towards an evaluation 
methodology for digital music instruments considering 
performer’s view: a case study,” in Proceedings of 13th 
Brazilian Symposium on Computer Music, 2011. 
3. V. Bellotti, M. Back, W. K. Edwards, R. E. Grinter, A. 
Henderson, and C. Lopes, “Making sense of sensing systems: 
five questions for designers and researchers,” in CHI  ’02 
Proceedings of the SIGCHI conference on Human factors in 
computing systems, (2002). 
4. T. Davis. "Towards a Relational Understanding of the 
Performance Ecosystem". Organised Sound, 16, (2011). 
5. J. Freeman and M. Godfrey. “Technology, real-time notation, 
and audience participation in flock,” in International 
Computer Music Conference, (2008). 
6. A. Fyans and M. Gurevich, “Spectator understanding of error 
in performance,” in Proceedings of the 27th international 
conference extended abstracts on Human factors in computing 
systems, p. 3955, (2009). 
7. A. Fyans and M. Gurevich. “Perceptions of Skill in 
Performances with Acoustic and Electronic Instruments,” in 
NIME  ’11 Proc. of the 2011 Conference on New interfaces 
for musical expression, no. June, pp. 495-498, (2011). 
8. A. Fyans, and P. Stapleton. “Examining the spectator 
experience,” in NIME  ’10 Proceedings of the 2010 
conference on New interfaces for musical expression, pp. 1-4, 
(2010). 
9. M. Gurevich, A. Fyans. Digital Musical Interactions: 
Performer–system relationships and their perception by 
spectators. Organised Sound, 16(02), pp.166-175. (2011). 
10. W. Hsu and M. Sosnick. “Evaluating interactive music 
systems: An HCI approach,” Proceedings of New Interfaces 
for Musical Expression (NIME), pp. 25-28, (2009). 
11. J. Malloch, D. Birnbaum, E. Sinyor, and M.M. Wanderley, 
“Towards a New Conceptual Framework for Digital Musical 
Instruments,” Proceedings of the 9th International Conference 
on Digital Audio Effects, p. 49–52, (2006). 
12. M. Marshall, “Physical interface design for digital musical 
instruments,” Ph.D. Thesis. McGill University, (2010). 
13. E. Miranda and M. Wanderley.  New Digital Musical 
Instruments: Control and Interaction beyond the Keyboard. A-
R Editions, (2006). 
14. S. O’Modhrain. “A framework for the evaluation of digital 
musical instruments,” Computer Music Journal, vol. 35, no. 1, 
pp. 28–42, (2011). 
15. J. Radbourne, K. Johanson, H. Glow, and T. White. “The 
Audience Experience: Measuring Quality in the Performing 
Arts,” International Journal of Arts Management, vol. 11, no. 
3, pp. 16-29, (2009). 
16. S. Reeves, S. Benford, C. O’Malley, and M. Fraser. 
“Designing the spectator experience,” Proceedings of the 
SIGCHI conference on Human factors in computing systems - 
CHI  ’05, p. 741, (2005). 
17. W. Schloss, “Using Contemporary Technology in Live 
Performance: The Dilemma of the Performer,” Journal of New 
Music Research, vol. 32, no. 3, pp. 239-242, (2003). 
18. D. Stowell, A. Robertson, N. Bryan-Kinns, and M.D. 
Plumbley, “Evaluation of live human–computer music-
making: Quantitative and qualitative approaches,” 
International Journal of Human-Computer Studies, vol. 67, 
(2009). 
19. M. Wanderley, “Instrumentos Musicais Digitais: Gestos, 
Sensores e Interfaces,” Em Busca da Mente Musical, Editora 
da Universidade Federal do Paraná, (2006). 
20. L. Wood, “Semi-structured interviewing for user-centered 
design,” interactions, vol. 4, p. 48–61, (1997). 
21. C. Dyer, “Research in Psychology: A Practical Guide to 
Methods and Statistics”. Malden, MA: Oxford: Blackwell 
Publishing (2006). 
22. Chadabe, J. The Past and Promise of Electronic Music. 
Prentice Hall, New Jersey, 1997. 
23.Page-Bucci, H. The value of Likert scales in measuring 
attitudes of online learners. 
http://www.hkadesigns.co.uk/websites/msc/reme/likert.htm , (2003). 