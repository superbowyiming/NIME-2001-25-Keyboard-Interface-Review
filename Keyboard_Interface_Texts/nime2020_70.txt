 
 
Force dynamics as a design framework for mid-air musical 
interfaces
Anders Eskildsen 
Music and Sound Knowledge Group 
Aalborg University 
Aalborg, Denmark 
aes@hum.aau.dk 
 
Mads Walther-Hansen 
Music and Sound Knowledge Group 
Aalborg University 
Aalborg, Denmark 
mwh@hum.aau.dk 
 
ABSTRACT 
In this paper we adopt the theory of force dynamics in human cog-
nition as a fundamental design principle for the development of 
mid-air musical interfaces. We argue that this principle can provide 
more intuitive user experiences when the interface does not provide 
direct haptic feedback, such as interfaces made with various ges-
ture-tracking technologies. Grounded in five concepts from the the-
oretical literature on force dynamics in musical cognition, the paper 
presents a set of principles for interaction design focused on five 
force schemas: Path restraint, Containment restraint, Counter-
force, Attraction, and Compulsion. We design and describe an ini-
tial set of examples that implement these principles using a Leap 
Motion sensor for gesture tracking and SuperCollider for interac-
tive audio design. Finally, the paper presents a pilot experiment that 
provides initial insight into how users experience their interaction 
with the interface, including ratings of the interface’s intuitiveness 
and ability to provide musical inspiration. 
 
Author Keywords 
Force dynamics, Image schemas, Leap Motion, Gesture-based in-
struments, Multimodality, SuperCollider. 
CCS Concepts 
• Human-centered computing → Interaction design → Interaction 
design process and methods → User interface design • Applied 
computing → Sound and music computing 
1. INTRODUCTION 
Recent technological advances in fine-grained motion tracking 
have enabled experimentation with new mid-air musical interfaces, 
where the user interacts with the interface by means of free move-
ment without wearable sensors. The theremin was invented by the 
Russian physicist Léon Theremin exactly one century before this 
paper was written, but technologies like Microsoft’s Kinect and the 
Leap Motion controller in particular appear to have reinvigorated 
the interest in experimental gesture-based interfaces for musical ap-
plications, as evidenced by recent literature [1,3,8,15,18,27]. 
In this paper we use the semantic category – force dynamics – as 
the basis for a theoretical framework to be used in the design of  
musical interfaces where direct haptic feedback is absent. 
Furthermore, we present an interface design ed to exemplify and 
evaluate the usefulness of the design framework. 
The aim is to explore how force concepts – used by Leonard 
Talmy [20] to organize meaning in language and subsequently by 
George Lakoff [11] and Mark Johnson [9] to account for fun da-
mental structures of human imagination (image schemas) that gov-
ern thought and language – can serve as a framework to organize 
meaning in sound and, ultimately, a framework to intuitively inter-
act with sound in mid-air.  
While impact force is usually recognized as an important dimen-
sion in the design of tactile interfaces for musical interaction, force 
is a neglected aspect in the design of interfaces with no direct feed-
back through the tactile senses. Proceeding from the idea that sound 
is a multimodal phenomenon, we explore how one may design non-
haptic interfaces where mid-air gestures activate structural features 
of tactile force in the user’s mind. Such forms of activation are gen-
erated through a combination of gestures and sounds based on force 
schemas. 
This study is particular interested in how the bodily effort in-
volved in a performance with mid-air instruments can be effectively 
reflected when combined with sounds designed with specific inher-
ent force characteristics. The goal is to increase cognitive pro-
cessing fluency in such a way that intuitive interaction and mean-
ingful improvisation are possible. When sounds and gestures are 
designed and combined appropriately, the interface allows for 
faster processing (cognition is time-pressured, see [7]), construc-
tive stimulation of the user’s imagination (unlocks new opportuni-
ties that are only imaginable during interaction), and for more pre-
cise gestural control.  
In order to explore specific sound/gesture combinations we de-
signed an interface using the Leap Motion controller for motion 
tracking and SuperCollider for sound generation and mapping of 
gestures to sounds. The design of sounds and gestures was based 
on five concepts: Path restraint, Containment restraint, Counter-
force, Attraction, and Compulsion.  The choice of these particular 
force schemas was based on previous studies of force dynamics in 
music production [21,22,23] and what we found to be a manageable 
selection of interaction patterns for the interface. 
In this paper we thus investigate how design principles grounded 
in the theory of force dynamics can facilitate designs of intuitive 
and musically inspiring interfaces based on gesture-tracking tech-
nologies such as the Leap Motion. Based on a pilot experiment in 
which we examined how users experience our interface we provide 
an initial evaluation of the interface design and discuss possible fu-
ture directions in force dynamics-based design of mid-air musical 
interfaces. 
2. THEORETICAL FRAMEWORK 
Interacting with sound in an improvised performance is a process 
that involves the mental processing of the heard sound (mind 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
361
 
 
external stimuli) and the act of imagining new sounds (mind inter-
nal process). Gibbs and Matlock [6] have shown that physical bod-
ily simulation aids both the processing of external stimuli and rea-
soning about imagined future scenarios. In such cases the body is 
used as a vehicle to ‘off-load’ mental computation onto the world. 
For instance, when experiencing a tight sound (see [24] for a dis-
cussion of the use of cognitive metaphors in reasoning about sound 
quality), one may feel the need to squeeze one’s hands, and when 
imagining a looser sound to succeed the tight sound one may feel 
the need to loosen up the squeezed hands. Also, these hand gestures 
allow the user to visually ‘consult’ the interface to reason about the 
heard sound and to make informed decisions about how to continue 
the interaction.  
Accordingly, imagining and perceiving sound should not be seen 
as detached cognitive processes, but as integrated elements in the 
dynamic involvement with a sound interface. Ideally, interfaces 
should be structurally coupled with the mind's internal cognitive 
processes in such a way that they ‘extend’ the cognitive work onto 
the performance environment – the gesture zone (e.g. the idea of 
extended cognition as presented by Clark and Chalmers [5]).     
The interface presented in this paper builds on the idea that our 
cognitive system is inherently multimodal. When we hear a sound 
that activates specific mental force schemas, we activate the same 
neurons as we would activate if experiences with similar force 
schematic structures were perceived through other senses (see 
[12]). Similarly, when interacting with a sound interface with no 
haptic feedback entering through the tactile senses (as opposed to 
cognitively activated haptic feedback that emerges as a result of 
appropriate stimulation of other senses), the user may still experi-
ence and make sense of the interaction through force schemas 
learned from previous sensory-motor interactions with the world.  
The aim is not to achieve a representational match between a 
specific gesture and the music the gesture generates (e.g. creating 
Mickey Mousing effects where hand movements are used to repre-
sent different forms of sonic qualities and sequences). Moreover , 
our approach is different from studies that propose interfaces based 
on emulations of traditional musical instruments or recording 
equipment – such as a piano keyboard [8,1,18], drum pads [8], or a 
mixing console [15]. 
In the framework we argue for here, both sounds and interactions 
should be designed and coupled – structurally – through the same 
set of force schematic mental patterns. The user does not experience 
these patterns directly, but experiences through them – and this is 
what allows for the activation of sensorimotor experiences that do 
not represent anything particular in the external world, but are ex-
periences that are governed by  cognitive schemas embodied 
through previous recurring sensorimotor experiences – in this case, 
previous force dynamic interactions that lead to embodied force 
schemas. 
Several scholars have presented accounts of force structures in 
music material – e.g. tonal tension, stability and instability, and 
contraction and expansion (see [16,4,13]) – and, further, how these 
structures are an essential part of the aesthetic appreciation of music 
(see [17]). Following these studies, we suggest that mid-air inter-
faces built on force schemas may not only prove more intuitive to 
use, they may also be perceived as more musically meaningful 
when venturing beyond imitation of traditional musical interfaces. 
3. INTERFACE DESIGN 
Inspired by Johnson’s theory of force schemas [9], five strategies 
for the mapping of hand position/movement to musical sound (and 
vice versa) were devised, focusing on Path restraint, Containment 
restraint, Counterforce, Attraction, and Compulsion. 
In practice, several of these force schemas are often combined in 
the perception and conceptualization of sound and music. The sche-
mas are presented here separately for the purpose of clear delinea-
tion of the design principles discussed in this paper.  
3.1 Path Restraint (Blocking) 
Music moves in time. For most people in the English-speaking 
world, it is common to understand time as something which flows 
from left to right on a mental timeline – future is on the right side 
and past on the left [26]. This way of structuring time presumably 
arises from the direction of our writing system (language and mu-
sical notation) [14]. 
The metaphor of musical movement has been studied extensively 
by Johnson and Larson [10], who argue that listeners make sense 
of musical progression as a force – grounded in the experience of 
physical forces – that causes musical material to change from one 
state to another. Like other forces, musical progression can be 
blocked by means of a barrier. 
An interaction design based on the concept of path restraint im-
plies that the user should be able to introduce a barrier which would 
continuously interrupt the flow of sound, yet, the sound continues 
to exert its inherent force on the barrier. Conversely, removing the 
barrier should enable the music to continue its movement along the 
force vector from left to right. 
  
 
Figure 1. Removal of restraint schema (from [9]). 
 
Figure 1 illustrates how this principle was implemented in Example 
1: A musical sequence (F1) will be in motion before any hands enter 
the interaction zone. For this example, a sample of pre-recorded 
sound is played back by means of granular synthesis, a technique 
which allows for flexible manipulation of audio material. Grains 
are generated from the recorded sample based on a pointer which 
moves through the sample such that the perceived playback tempo 
corresponds to that of the original recording. Placing the right hand 
(F2) in the interaction zone stops the progression of the pointer, ef-
fectively blocking the musical development. The continuous cy-
cling of similar audio grains illustrates the continued exertion of 
force against the barrier. When the hand leaves the interaction zone, 
the pointer resumes normal playback rate due to removal of the bar-
rier. 
3.2 Containment Restraint (Compression) 
Figure 2 illustrates the containment restraint schema that was used 
to design Example 2: A generative algorithm produces musical 
notes distributed within a certain space of possibilities. The size of 
the space within which the algorithm operates is determined by the 
distance between the two hands – keeping hands far apart corre-
sponds to a big space, while putting hands closer together com-
presses the space into a smaller one. The size of the space is illus-
trated by constraining the algorithm parameters towards weighted 
randomization with regard to four key parameters: Pitch, rhythm, 
timbre, and reverberation. 
Pitch: An unconstrained space (i.e., no hands) yields a broad 
range of possible pitches on a pentatonic scale, spanning severaloc-
taves. Constraining the space narrows the range of possible pitches 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
362
 
 
by limiting the distance between possible scale degrees and operat-
ing within one octave only. Rhythm: Without constraint, there is a 
low tempo and a high chance of generating rests in the musical al-
gorithm, leading to a rhythmically sparse soundscape. Constraining 
the space leads to a higher tempo, fewer rests, and more rapid note 
envelopes, effectively producing a more intense and rhythmically 
complex soundscape. Reverberation: A big container leads to 
longer reverberation decay times, and smaller containers lead to 
shorter decay times. The notes thus appear to reverberate in a phys-
ical space which changes corresponding to container size. 
 
 
Figure 2. Containment restraint schema. 
 
3.3 Counterforce 
Counterforce involves counter movement of a sound with an inher-
ent force tendency that is opposite that of the hand movement. 
Thus, the movement of hand may either push the sound in the op-
posite direction of its inherent force vector or the hands may pull 
the sound away from its stable position (stretch). 
 
 
Figure 3. Counterforce schema. 
 
Example 3 implements counterforce (see Figure 3) by establishing 
an equilibrium in the form of a regular mid-tempo drumming 
groove which is played at normal playback rate when the right hand 
is present in the centre of the interaction zone. Moving the hand up 
or down removes a portion of the frequency spectrum with a low or 
high pass filter, respectively, whereas moving the hand left or right 
decreases or increases the playback rate. Toward the extreme hori-
zontal edges of the interaction zone, the playback rate begins to 
change chaotically, rendering the musical groove increasingly un-
stable. Removing hands from the interaction zone restores the equi-
librium, allowing the counterforce to push back  and restore the 
playback rate and frequency spectrum of the original sample. 
3.4 Attraction 
The attraction schema can be realised as a magnetic force that at-
tracts the sound to an object (a hand) entering the interaction zone. 
When a hand activates the attraction schema the sound changes 
gradually until it reaches the position of the hand.  
 
 
Figure 4. Attraction schema (from [9]). 
 
Example 4 (see Figure 4) implements attraction in the sense that 
when a hand (B) is detected in the interaction zone, a new sine tone 
is generated (A) and it moves gradually toward the hand’s position, 
where horizontal position corresponds to the placement of the 
sound in a stereo field and vertical position corresponds to pitch. 
When hands leave the space, the force of attraction is no longer 
present, and the tone descends in pitch before disappearing co m-
pletely. 
3.5 Compulsion 
Compulsion involves the movement of something by external 
force. As opposed to the counterforce force schema, compulsion 
involves interaction with sounds without inherent force tendencies. 
When the force exerted on the sound by the hands is removed, the 
sound will remain in a state of rest.   
 
 
Figure 5. Compulsion schema (from [9]). 
 
Example 5 (see Figure 5) implements compulsion by allowing the 
user to affect the settings of a simple FM synthesizer by moving 
his/her right hand around the XY-plane of the interaction zone. 
Moving the hand along the Y-axis positions the frequency of the 
carrier (i.e. pitch) from low to high. Moving the hand along the X-
axis affects modulator to pitch ratio when the hand is moved to the 
right of centre and changes the modulation index when the hand is 
left of centre. Richer and noisier timbres are generated at the far 
extremes on either side. When the speed of the hand’s movement 
crosses a certain threshold, the pitches begin to shift randomly, and 
the timbre becomes increasingly rough. This latter aspect of the 
sound design is meant to provide sonic feedback when the hand is 
in motion, i.e. a form of sonic friction. 
4. TECHNICAL IMPLEMENTATION 
As a well-known piece of hardware within the NIME community, 
the Leap Motion was selected for this study as an accessible way to 
develop a prototype interface for tracking hand motion and posi-
tion. Originally developed for virtual reality applications, the Leap 
Motion hardware was released in 2012 and has subsequently been 
used in many experimental interfaces for computer music. The sen-
sor tracks the position of a user’s arms, hands, and fingers in 3D 
space by means of infrared cameras. In this way, impressive preci-
sion and low latency is achieved (see [18,27] for further discussions 
of the controller and its limitations).  
The interactive sound design for the examples was developed in 
SuperCollider, a free and open source platform for audio synthesis 
and algorithmic composition [19]. Built for interactivity with a 
strong library of generative algorithms and a highly capable audio 
synthesis and signal processing engine [25], SuperCollider was a 
fruitful platform for this project. 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
363
 
 
As an interface between the Leap Motion controller and Super-
Collider, a small JavaScript application was developed in order to  
pull data from the sensor using the Leap Motion JavaScript API and 
communicate with SuperCollider using the OSC protocol. This ap-
proach has the advantage of working natively in a web browser, 
which afforded rapid prototyping of a simple graphical interface by 
means of which users could switch between examples and get some 
minimal feedback on the sensor’s ability to ‘see’ their hands. 
Some previous studies of interface design involving the Leap 
Motion controller employ different imaginative forms of visual 
feedback [3,1]. Since the intention of the present study was to test 
how interface intuitiveness can be achieved primarily through au-
ditory rather than visual feedback, we did not provide visual feed-
back about the position or movement of the user’s hands . Visual 
feedback was used only to indicate whether or not the controller 
was able to “see” the user’s hands, in the form of an on-screen hand 
symbol which lights up when the hand is registered as present 
within the interaction zone (see figure 6 and 7). 
 
 
Figure 6. Example 1 is activated. Hand is not present in the in-
teraction zone. 
 
 
Figure 7. Example 1 is activated. Hand is present. 
 
We decided to highlight the presence of hands in the interaction 
zone with visual feedback to deal with the instability of the sensor 
toward the extreme edges of the interaction zone, which in initial 
tests proved confusing for users with no previous experience with 
the controller. Since the examples differed regarding the number of 
hands the user should use to navigate the interface, simple text in-
structions for each example in the graphical user interface were pro-
vided at the bottom of the user interface screen.  
5. EVALUATION 
A pilot study was conducted with 11 participants (a ll male – eight 
current musicology students and three former musicology students) 
to provide an initial indication of intuitiveness and general evalua-
tion of the user experience. 
5.1 Procedure 
Each participant was given a brief introduction to the interface and 
was then allotted five minutes to play with the interface without 
researcher interference. Participants were then asked to fill out a 
survey which involved rating each example on whether the exam-
ple was intuitive to use and whether the example was musically in-
spiring. Ratings were given on a five-level Likert scale ranging 
from “Strongly disagree” to “Strongly agree”. Subsequently, par-
ticipants were asked to evaluate their general experience of using 
the interface by checking five items on a list of 10 descriptors (six 
positive and four negative descriptors) subtracted from the Mi-
crosoft Desirability Toolkit [2]. Throughout the experiment, and 
while filling in the survey, participants were able to switch freely 
between the five examples to explore the interaction possibilities of 
the interface. The sessions were recorded on video. After each ses-
sion, a short debriefing was conducted during which participants 
were invited to express their impressions in their own words. 
5.2 Results and discussion 
Generally, participants found much to praise in the presented ex-
amples of interaction patterns and corresponding sounds. In the 
evaluation session participants reported the interface to be captivat-
ing, entertaining, inspiring, and intuitive (figure 10). 
 
 
Figure 10, Word cloud for general descriptors of the interface. 
 
The analysis of the video-recorded sessions supports the reported 
viewpoint that the interface was entertaining and inspiring. Still, 
none of the participants operated all five examples as intended in-
side the allotted five minutes.     
Example 1 (Path restraint) had – from a design perspective – the 
simplest interaction pattern as it only responded to whether the right 
hand was present or not. Some participants reported that the inter-
action pattern reminded them of DJ’ing (scratching). This associa-
tion is consistent with the design principle in example 1 – to inter-
rupt the flow of music – and it explains why all participants blocked 
the flow of music by placing their fingers on a virtual surface (palm 
of right hand facing down) rather than blocking the flow of music 
by using the hand as a barrier (palm of right hand facing left). Oth-
ers, however, found this example unintuitive – presumably, be-
cause the interaction was too simple compared to their expectations 
– nothing happened when they moved the right hand forth and back 
within the interaction zone.   
Example 2 ( Containment restraint ) was rated as both the least 
intuitive and the least musically inspiring among the five examples. 
Most participants attempted to operate the interface example with 
one hand at a time (despite the instructions displayed on the screen 
to use both hands) and those who used both hands simultaneou sly 
primarily attempted to adjust different parameters with each hand. 
Two participants appear to move their hands together and apart to  
control the sound after having played around with this specific ex-
ample for more than 90 seconds. This suggests that this example – 
where the coordination of hand movements is essential for the 
working of the interface – requires more time to learn.       
Example 3 ( Counterforce) and example 4 ( Attraction) were re-
ported as the most intuitive and most musically inspiring examples. 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
364
 
 
The video shows that most participants acquire some control with 
the interface examples and start to make music within 30 seconds 
or less. However, in both cases most participants only operate the 
examples in one dimension (the vertical dimension). 
In comparison to example 3 and 4, fewer participants found e x-
ample 5 (Compulsion) to be intuitive or musically inspiring, yet, all 
participants appear to adjust the sound on both the X and Y- axes 
within approximately 30 seconds.   
 
 
Figure 8. Survey results: “The example was intuitive”. 
 
 
Figure 9. Survey results: “The example was musically inspir-
ing”. 
 
During debriefing, five participants reported that they experienced 
latency in some of the examples (e.g. in example 4) or that the in-
terface occasionally did not respond properly to the hand move-
ments. This perceived latency/unresponsiveness of the system (the 
technical latency of the system is only a few milliseconds and un-
likely to be perceivable) suggests that the instrument’s gradual 
changes to timbral and spatial qualities in some cases clashes with 
the user’s expectations – expectations presumably embodied in pre-
vious experiences with interfaces built on a direct representational 
match between movement and sound or other interfaces/instru-
ments with instant response sounds.  
6. CONCLUSION AND PERSPECTIVES 
In this paper we argued that force dynamics may serve as a mean-
ingful design principle for more intuitive and inspiring mid-air in-
terfaces for musical expression. Users’ self-reported assessments of 
intuitiveness based on five minutes of interaction with the interface 
can only provide limited insight into the general usability of the 
particular interface developed here. Based on the findings in the 
pilot study, however, we suggest that the proposed design princi-
ples allow for the development of interfaces which function as cog-
nitive extensions in such a way that the user can ‘consult’ the 
interface in a meaningful way to make informed decisions about 
future actions.  
To make detailed observations and user ratings of the five differ-
ent interaction patterns, this study tested each pattern separately. 
The pilot test shows that examples built on the simplest and most 
complex interaction patterns (Path restraint and Containment re-
straint) were found to be the least intuitive and musically inspiring. 
The remaining three examples – where the user interacts with the 
sound by moving one hand at a time on the X and Y-axes – were 
more intuitive and inspiring. Interestingly, example 4 (Attraction), 
where some participants experienced latency, was rated as the most 
musically inspiring while example 1 (Path restraint) was intuitive 
yet relatively uninspiring. 
We suggest that the influence of visual cues and user expectations 
needs to be addressed further in future experiments. While the au-
ditory material may evoke force dynamic patterns in the user’s 
mind, there is, in fact, no visual and physical restraint in free air . 
This may lead to cognitive dissonance between the basic meaning 
of the force schemas and the perceived nature of the physical ma-
terial in the interaction zone (air), resulting in a lack of perceived 
intuitiveness. 
It is assumed that a stronger link between abstract metaphorical 
thinking (how the user makes sense of the timbral and rhythmic 
variations in the auditory material) and concrete actions (the hand 
gestures) will make the interface both more intuitive and musically 
meaningful. For this reason, we plan to experiment with different 
sound designs and to introduce perceptual stimuli from other mo-
dalities, e.g., implement visual cues to support the basic force sche-
mas, in future versions of the interface. Since multiple force sche-
mas are often combined in everyday cognition, we also plan to test 
how different combinations of force patterns in the same interaction 
zone can strengthen the aforementioned link. Combining path re-
straint and compulsion such that the user can restrain the music’s 
inherent motion while changing other musical aspects through hand 
gestures, for instance, could draw upon the relatively high intuitive-
ness of example 1 and extend the interactive and sonic possibilities. 
Future research into the effect of user expectations on perceived 
intuitiveness of mid-air interfaces could address the relationship be-
tween the user’s background, expectations, and his/her experience 
of the interface. The participants in this study shared a background 
in musicology, but including participants with other backgrounds 
could provide valuable insight into the usability and potential ap-
plications of mid-air musical interfaces across contexts. 
7. ACKNOWLEDGMENTS 
Our thanks to participants in the experiment and to the Music and  
Sound Knowledge Group (MaSK) at Aalborg University for fund-
ing the project. Thanks to Mark Grimshaw-Aagaard and two anon-
ymous reviewers for helpful comments on the final draft.  
The software we developed for this project is free and open 
source, available at https://github.com/sparkletop/forces. The soft-
ware may change in the future, but the setup that was used for the 
present study can be replicated using version 0.2, which is available 
at https://github.com/sparkletop/forces/releases/tag/v0.2. 
8. ETHICAL STANDARDS 
All participants were informed about the purpose of the study and  
consented to participate. Data storage and processing complies with 
EU’s General Data Protection Regulations (GDPR). 
9. REFERENCES 
[1] N. Alessandro, J. Tilmanne, A. Moreau, and A. Puleo. 
AirPiano: A Multi-Touch Keyboard with Hovering Control. 
Proceedings of NIME’15, 2015, 255–58.  
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
365
 
 
[2] J. Benedek and T. Miner. Measuring Desirability: New 
Methods for Evaluating Desirability in a Usability Lab Set-
ting. Proceedings of UPA 2002 Conference, 2010. 
[3] F. Berthaut, C. Arslan, and L. Grisoni. Revgest: Augmenting 
Gestural Musical Instruments with Revealed Virtual Objects. 
Proceedings of NIME’17, 2017, 180–85. 
[4] C. Brower. A Cognitive Theory of Musical Meaning. Jour-
nal of Music Theory 44 (2), 2000, 323–379.  
[5] A. Clark and D. J. Chalmers. The Extended Mind. In The Ex-
tended Mind, edited by R. Menary. MIT Press, Cambridge, 
Massachusetts, 2010, 27–42. 
[6] R. W. Gibbs and T. Matlock. Metaphor, Imagination, and 
Simulation. In The Cambridge Handbook of Metaphor and 
Thought, edited by R. W. Gibbs. Cambridge University 
Press, Cambridge, 2008, 161–176. 
[7] M. Grimshaw and T. Garner. Sonic Virtuality – Sound as 
Emergent Perception. Oxford University Press, Oxford and 
New York, 2015. 
[8] J. Han and N. Gold. Lessons Learned in Exploring the Leap 
Motion Sensor for Gesture-Based Instrument Design. Pro-
ceedings of NIME’14, 2014, 371–74. 
[9] M. Johnson. The Body in the Mind: The Bodily Basis of 
Meaning, Imagination, and Reason. The University of Chi-
cago Press, Chicago and London, 1987. 
[10] M. Johnson and S. Larson. “Something in the Way She 
Moves": Metaphors of Musical Motion. Metaphor & Symbol 
18 (2), 2003, 63-84. 
[11] G. Lakoff. Women, Fire and Dangerous Things: What Cate-
gories Reveal about the Mind. The University of Chicago 
Press, Chicago and London, 1987. 
[12] G. Lakoff. The Neural Theory of Metaphor. In Cambridge 
Handbook of Metaphor and Thought, edited by R. Gibbs. 
Cambridge University Press, Cambridge, 2008, 17-38. 
[13] S. Larson. Musical Forces: Motion, Metaphor, and Meaning 
in Music. Indiana University Press, Bloomington, 2012. 
[14] M. Oullet, J. Santiago, Z. Israeli, and S. Gabay. Is the Future 
the Right Time? Experimental Psychology 57 (4), 2010, 308–
314. DOI: 10.1027/1618-3169/a000036 
[15] J. Ratcliffe. Hand and Finger Motion-Controlled Audio Mix-
ing Interface. Proceedings of NIME’14, 2014, 136–39. 
[16] J. Saslaw. Forces, Containers, and Paths: The Role of Body-
Derived Image Schemas in the Conceptualization of Music. 
Journal of Music Theory 40 (2), 1996, 217–243.  
[17] R. Scruton. The Aesthetics of Music. Clarendon Press, Ox-
ford, 1999. 
[18] E. S. Silva., J. Abreu, J. H. de Almeida, V. Teichrieb, and G.r 
L. Ramalho. A Preliminary Evaluation of the Leap Motion 
Sensor as Controller of New Digital Musical Instruments. 
Proceedings of the 14th Brazilian Symposium on Computer 
Music. Brasília: Escola de Música de Brasília, 2013, 59–70. 
[19] SuperCollider. https://supercollider.github.io/, accessed on 
January 21, 2020. 
[20] L. Talmy. Force Dynamics in Language and Thought. The 
Parasession on Causatives and Agentivity at the Twenty-
First Regional Meeting, Chicago, Illinois, Chicago Linguistic 
Society, 1985. 
[21] M. Walther-Hansen. The Force Dynamic Structure of the 
Phonographic Container: How Sound Engineers Conceptual-
ise the ‘Inside’ of the Mix. Journal of Music and Meaning, 
12, 2014. 
[22] M. Walther-Hansen. Balancing Audio: Towards a Cognitive 
Structure of Sound Interaction in Music Production. In Mu-
sic, mind, and embodiment, edited by R. Kronland-Martinet, 
M. Aramaki and S. Ystad. Springer, 2016, 228-242.    
[23] M. Walther-Hansen. New and Old User Interface Metaphors 
in Music Production. Journal of the Art of Record Produc-
tion 11, 2017.   
[24] M. Walther-Hansen. 2020. Making Sense of Recordings. Ox-
ford University Press.  
[25] S. Wilson and J. D’Escriván. Composition with SuperCol-
lider. The SuperCollider Book, edited by S. Wilson, D. 
Cottle, and N. Collins, MIT Press, 2011, 81–104. 
[26] B. Winter and T. Matlock. Primary Metaphors are both Cul-
tural and Embodied. In Metaphor – Embodied Cognition and 
Discourse edited by B. Hampe. Cambridge University Press, 
Cambridge and New York, 2017, 99–116. 
[27] J. C. Wu, Y. Zhang, and M. J. Wright. Towards Robust 
Tracking with an Unreliable Motion Sensor Using Machine 
Learning. Proceedings of NIME’17, 2017, 42–47. 
 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
366