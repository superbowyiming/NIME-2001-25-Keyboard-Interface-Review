A Longitudinal Field Trial with a Hemiplegic Guitarist
Using The Actuated Guitar
Jeppe Veirum Larsen
Department of Architecture,
Design and Media Technology
Aalborg University
Aalborg, Denmark
jvl@create.aau.dk
Hendrik Knoche
Department of Architecture,
Design and Media Technology
Aalborg University
Aalborg, Denmark
hk@create.aau.dk
Dan Overholt
Department of Architecture,
Design and Media Technology
Aalborg University
Copenhagen, Denmark
dano@create.aau.dk
ABSTRACT
Common emotional eﬀects following a stroke include de-
pression, apathy and lack of motivation. We conducted a
longitudinal case study to investigate if enabling a post-
stroke former guitarist re-learn to play guitar would help
increase motivation for self rehabilitation and quality of life
after suﬀering a stroke. The intervention lasted three weeks
during which the participant had a fully functional electri-
cal guitar ﬁtted with a strumming device controlled by a
foot pedal at his free disposal. The device replaced right
strumming of the strings, and the study showed that the
participant, who was highly motivated, played 20 sessions
despite system latency and reduced musical expression. He
incorporated his own literature and equipment into his play-
ing routine and improved greatly as the study progressed.
He was able to play alone and keep a steady rhythm in time
with backing tracks that went as fast as 120bpm. During
the study he was able to lower his error rate to 33% and his
average ﬂutter decreased.
Author Keywords
Motivation, Stroke, Hemiplegia, Re-enabling, Music, Gui-
tar, Actuation, Assistive Technology
CCS Concepts
•Applied computing→Sound and music computing; Per-
forming arts; •Hardware →Sensors and Actuators;
1. INTRODUCTION
Every year 15 million people worldwide suﬀer strokes, of
whom 33% survive with permanent disabilities. Demographic
projections show a likely increase in this trend. Common
emotional eﬀects following a stroke include depression, apa-
thy and lack of motivation [34, 9], which are a major prob-
lem in the later stages of home-based rehabilitation. An
increasing amount of research has been dedicated to the po-
tential role of music and music performance in helping peo-
ple cope with the physical and emotional eﬀects of a stroke.
Performing music exercises the brain, increases quality of
life through a sense of agency, empowerment, and belong-
ing [27], and provides intrinsic motivation [33] to engage in
the activity. The intrinsic motivation is an important factor
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
to help stroke survivors stay motivated in order to encour-
age to self-rehabilitation and combat the life-long physical
and emotional eﬀects of a stroke.
Musicians hit by a stroke ﬁnd themselves suddenly un-
able to play a musical instrument with one side of their
body. While their musical knowledge remains intact, they
can no longer use it to play an instrument. Enabling for-
mer musicians to play their instrument by making their in-
strument accessible would likely improve their motivation.
However, it is unclear how the reduced musical expression in
assistive interfaces for musical expression (aIME) from e.g.,
added latency, the simpliﬁcation of input gestures, custom-
made interfaces, and change in feedback modalities, aﬀects
long-term motivation as aIME research typically focuses on
short-term proof of concept evaluations [6, 30, 35, 20].
This paper investigates how latency, delayed auditory
feedback and reduced expressiveness aﬀects long term mo-
tivation using the Actuated Guitar [15, 16].
2. BACKGROUND
2.1 Musical Beneﬁts in Rehabilitation
People who listen to or perform music use several mental
registers that trigger a coordinated action of multiple mech-
anisms [1], including the motor cortex, cerebellum, sensory
cortex, visual cortex and the audio cortex [17]. In gait train-
ing acute stroke patients using simple Rhythmic Auditory
Stimuli (RAS) (prerecorded music with metronome over-
lay) showed signiﬁcant improvements in gait velocity, stride
length and stride symmetry compared to normal gait train-
ing for stroke victims [31]. In addition to listening or moving
to music, performing music is one of the most challenging
and complex tasks for the brain as it requires precise tim-
ing, hierarchically organised actions, precise pitch interval
control, and rhythm [38]. Studies with stroke patients suf-
fering from a moderate impairment of motor function of the
upper extremities show that playing an instrument for three
weeks results in more improved motor function than with
conventional therapies [28]. This music-supported therapy
builds on repetition and draws on the additional beneﬁts of
the playfulness and emotional impact of active music mak-
ing, which increases the participant’s motivation. Besides
the physical and motivational beneﬁts, performing music
has also positive eﬀects on memory, attention, neglect, ex-
ecutive function, and emotional adjustment [32]. Even in
short interventions music reduces depression, anxiety, and
hostility [32].
2.2 Gestures and Mapping
The separation of the sound source from the control inter-
face gives more possibilities when designing Digital Musi-
cal Interfaces (DMI) than with traditional acoustic instru-
ments whose sound source is an integrated part of the in-
305
terface [23, 22]. However, designing musical instruments -
whether acoustic or DMI - is still a lesson in how to avoid
frustration and boredom. If the instrument is too simple
it might not provide rich musical expression and result in
boredom, but if it is too complex it could cause frustration
and scare away the user before they were able to achieve
any rich musical expression [36][21].
A gesture is a human action used to generate sounds [22]
and a common DMI model [22] splits the instrument into
two parts: a gestural controller and a sound generator. A
gesture when referring to DMI is a human action used to
generate sounds [22]. The gestural controller takes these
gestures as inputs. The gestural interface does not emit
any sound besides what is called primary feedback (visual,
auditive i.e., instrument noise, tactile, kinesthetic). The
gestures are mapped to the sound generator, which can fa-
cilitate outcomes that would otherwise be impossible with
existing acoustic instruments either because of user limita-
tions or because of the instrument itself. The sound gener-
ator outputs the sound of the DMI, also called secondary
feedback. Bongers further expands the description of feed-
back with passive feedback that is produced by the physical
characteristics of the system (i.e., clicking noise of a button)
or active feedback that is produced in response to a certain
gesture [7]. The acquisition of gestures can be accomplished
by six diﬀerent types of interfaces [11]: distance sensing in-
terfaces (DSI), motion tracking interfaces (MTI), tangible
interfaces (TI), biometric interfaces (BI), touch screen in-
terfaces(TSI), and wind controller interfaces (WCI) each or
in combination allowing for certain interactions.
2.3 Musical Expression
Levitin et al. describe gestures, and hence musical expres-
sion, through what they call themusical control spacewhere
the performer can control the temporal stages - the begin-
ning, middle and end of a musical event [18]. During the
three stages the performer can, depending on musical in-
strument or DMI, vary the expressiveness through pitch (se-
lected note, vibrato, slide etc.), loudness (attack, tremolo,
bowing etc.) and timbre (bow or pick angle, bow or pick
position, palm muting, etc.).
2.4 Latency and Synchronisation
Many instruments exhibit an inherent latency between ac-
tuator activation and the occurrence of sound. For exam-
ple, by moderating the velocity a pianist can increase the
latency from pressing a piano key to the audible onset of a
soft note by as as much as 100ms [4].
While musicians detected latencies as low as 7-10ms [12],
people tapping along to a beat had, on average, more ten-
dency to tap before the beat. Thisanticipation bias amounted
to around 50ms for people without musical training and
about 14ms for musically trained people [2]. This bias
does not aﬀect the ability to keep a continuous and steady
beat where variation in inter-tap intervals can be as low as
4ms [26]. Increased delayed auditory feedback from activa-
tion causes disruption and leads to note errors (sequencing
of notes), elapsed time, key stroke velocity, and inter-hand
coordination. It peaks at 200ms whereupon it diminishes
again [12, 24].
In terms of evaluating temporal accuracy Pfordresher [24]
used the coeﬃcient of variation (CV) and the standard de-
viation of inter-onset-intervals (IOIs/mean IOI) computed
for each trial as the primary measure of timing variabil-
ity. The average ﬂutter (diﬀerences between adjacent Inter-
Onset-Intervals) of the hits from a professional percussion-
ist ranged from 10 to 40ms between 2-8% of the associated
tempo [10], suggesting that tempo aﬀects the anticipation
bias. The relative size of the ﬂutter increased with smaller
tempos, which suggests that the inter-onset-intervals of the
consecutive onsets varied substantially. Flutter resulted in
an oﬀset or diﬀerence between the inter stimulus onset in-
terval (ISI), e.g. a metronome beat, and the IOI, e.g. hit
on a drum, resulting in synchronisation errors (SE) [29].
People are more sensitive to auditory advance than audi-
tory delay as they can detect auditory advance asynchronies
between video and sound at around 20 - 75ms and audi-
tory delays from 100 - 188ms [3]. Asynchronies of 50ms or
more between diﬀerent orchestra members are common in
musical performances due to, e.g. ﬂutter, but even the spa-
tial arrangements increase asynchronies, e.g. a distance of
10 meters adds 30ms delay to the sound because of travel
time [25].
2.5 Function allocation and Control Site
A general Human Factors design approach isHuman-Machine
Function Allocation in which the functions are divided be-
tween the human user and the machine [19]. Bailey [5]
deﬁnes several approaches to function allocation where the
leftover allocation in aIME design is interesting. In leftover
allocation as many functions as possible are given to the
user to emphasise the natural movements of the user, and
the leftovers are to be handled by the technology. When de-
signing aIMEs the human body oﬀers several diﬀerent con-
trol sites that can be used for controlling a device. Webster
et al. identify commonly used sites for controlling assistive
devices: hand/ﬁnger, arm, head, forehead, eye, leg, knee,
foot and mouth [37]. The control sites for aIMEs should
have precise rhythmical motion within the latency limits.
In addition, the control site should be suited to prolonged
used.
3. STUDY METHOD AND aIME DESIGN
We planned a three week case study using a mixed meth-
ods approach to allow for an in-depth and long-term inves-
tigation. The methods used were observations, interviews,
and detailed data logging of the participants usage of the
Actuated Guitar. Before the intervention we conducted
a pre-intervention interview to collect general health and
background information about the participant. We used
the World Health Organisation Quality of Life question-
naire (WHOQOL-100) on both the participant and his wife
before and after the intervention to compare the Quality of
Life scores and to see if there are any increase in QOL or
if any crossover QOL eﬀects happened during the interven-
tion. The domains within Physical Health, Psychological,
Level of Independence and Social Relationships were of par-
ticular interest. We determined that it was the Psychologi-
cal Domain where the study might have the biggest impact
as the participant could become more positive as he gained
or re-gained functions and abilities, experienced greater self-
esteem, and saw an increase in thinking, learning, memory,
and concentration because of repeated practice.
The Functional Independence Measure (FIM) question-
naire is a widely used questionnaire for determining a per-
son’s performance of self-care, bowel-bladder control, trans-
fer, locomotion, communication, and cognition to indicate
how independent and well functioning the person is in a
given setting [13]. We used this to get a more thorough
understanding of the participant’s general level of function
and Independence at the start of the intervention.
3.1 The participant
A 64-year-old male former school teacher participated in the
study. At the time of the intervention he was 15 years post-
306
stroke and a right-side hemiplegic with complete paralysis
of his right arm. He was still able to walk using an ankle
bracelet and shoes but had a signiﬁcant limp. He lived at
home with his retired wife who was the primary care taker.
Before the stroke he had been a avid organ, piano, and
guitar player in diﬀerent semi-professional bands since his
teens. After the stroke he was unable to play any instru-
ments but had relearned to some extent how to play melody
on the piano with the left hand instead of the normally used
right hand. The aIME had to take into account his disabili-
ties as well as his remaining abilities. Using the principle of
leftover allocation the full functioning left hand could fret
chords as usual, but the assistive part had to substitute the
typical gestures of the strumming hand.
3.2 aIME Design
The study used the Actuated Guitar [15, 16, 14] there is a
regular oﬀ-the-shelf electrical guitar (Epiphone SG) ﬁtted
with a motorised fader that strums the guitar when a pedal
is pressed. The current implementation of the Actuated
Guitar only allow for the simplest right hand gesture on
a guitar, (strumming of all strings), as it requires lower
precision than, e.g. picking or plucking, but still allows the
player to play most chords. The actuator is placed above
the strings and drive a pick across the strings to strum the
strings when a footpedal is pressed, see Figure 1.
3.2.1 Modiﬁcations and Data Logging
A few important changes were made to the Actuated Guitar
before the the longitudinal ﬁeld trial began.
A new foot pedal was developed as the original 3D-printed
prototype was worn out from previous tests showing that
the current components and design was too fragile for lon-
gitudinal use. To ensure that the pedal could withstand
prolonged use we installed a rugged momentary button in
a hard plastic enclosure, see Figure 1. The plastic enclo-
sure also served as enclosure for additional components for
collecting data from the user during use of the Actuated
Guitar.
For registering button pushes the existing momentary
button was used. We measured how hard the button was
pushed with a force sensor sandwiched between the button
and the casing. We also ﬁtted a distance sensor in front of
the button to measure how high the participant lifted his
foot from the button, if he did so at all. All the sensors
were connected to their own Arduino to avoid increasing
the latency of the guitar strum. An Adafruit Data Logging
Shield with a built in clock and SD-card reader was used to
log the date, sensor and button data at each millisecond for
the highest precision.
The change to the foot pedal did not alter how the guitar
performed and a button press still resulted in a strum of all
strings.
Two foam stoppers were installed at each end of the fader
to shorten the distance the pick had to travel, which lowered
latency, and to reduce noise when the pick hit each end of
the fader.
The new footpedal and the motorised fader with foam
block can be seen on Figure 1.
3.3 System Latency
By using a GoPro camera that recorded 240 frames per
second we found a 45ms system latency between the closing
of the pedal button and the plectrum picking the ﬁrst string.
For more precise alignment the camera recorded an LED
that lit up when the button closed the circuit. The complete
six string strum (from hitting the ﬁrst to leaving the last
string) took 28ms. See Figure 2.
Figure 1: The pedal with the momentary button
and built-in datalogger (left) and the electrical gui-
tar with the motorised pick (right).
Figure 2: The total amount of time it takes from
the button being pressed to the pick moving to the
pick movement stopping.
4. THE INTERVENTION
The intervention lasted three weeks, throughout which the
participant had the guitar at his home to play whenever he
chose. The set-up consisted of the Actuated Guitar, gui-
tar ampliﬁer, guitar tuner, the pedal, a note stand, clear
instructions and a video camera to record all sessions. The
pedal and tuner were attached to a wooden board with Vel-
cro for fastening and easy re-positioning. The equipment
was set up in the living room and was able to remain there
for the entire intervention without being moving or disas-
sembled.
The questions going into the intervention centred on whether
the participant could do the following despite the inherent
system delay and reduced expressiveness:
•play a song without support?
•play along to a slow backing track with bigger antici-
pation bias?
•play along to a fast backing track?
•stay motivated and play during the free session?
During the intervention the participant could play vol-
untarily, while twice a week he played in a researcher-led
mandatory session. During the mandatory sessions he played
the same song of his own choosing at his own tempo, and
then played along to a simple four chord backing track ﬁrst
at 60 beats per minute (bpm) and then at 120bpm. The
researcher, who observed the sessions, noted down any in-
teresting observations. The remaining time was a so-called
free session without any restrictions during which he could
play whatever and whenever he wanted. During the free ses-
sions his wife helped equip the guitar, since the soft guitar
strap prohibited the participant from equipping the guitar
himself, and turned the video camera on and oﬀ. The reg-
ular strap was replaced with a strap with clips at each end,
which made it easier for her to help equip the guitar.
307
Figure 3: The participant playing the guitar during
a free session.
5. RESULTS
During the three-week intervention the participant played a
total of 20 sessions (14 free, 6 mandatory). We counted the
duration of a session from the point at which the participant
was ready to play in the chair with an equipped guitar to
the moment he put the guitar down again,
see Figure 5. On average, a mandatory sessions lasted 14
minutes and the free sessions 31 minutes.
Figure 4: The total time spent per session during
the three weeks of the intervention.
During the ﬁrst few days the participant investigated the
guitar and its potential by playing diﬀerent chords up and
down the fretboard.
From the third day he included an iPad running YouTube
for backing track support for the mandatory children’s song.
Around the same time he started to include his own musical
tools - an old metronome and old books about guitar chords
- and continued to challenge himself and expand his musical
repertoire.
He used his piano playing ability to create supporting
backing tracks, which gave him drum and bass to play along
to. The backing tracks were not random chords but actual
full-length songs he had played along to on the piano be-
fore he had the stroke. He played entirely from memory.
The chords he played included extended, major and minor
chords.
Based on both the logged data and observations the gen-
eral design of the guitar and pedal worked well for a long-
term study, and the delay and reduced musical expression
Figure 5: The percentage of synchronisation errors
exceeding ±50ms in the mandatory sessions.
were not a problem. The momentary button itself got worn
out because of the extended use. This required the par-
ticipant to press harder and harder to strum the guitar.
However it did not alter his motivation to play the guitar.
We visually inspected the audio wave to determine the er-
ror rate during the intervention. The ﬂutter or beat oﬀset
were evaluated by comparing the beat to the actual strum-
ming of the strings.
According to [25] we labelled strums that occurred more
than 50ms before or after the backing track beat as errors.
We obtained the onsets from visual inspection of the audio
wave form. This was done by inspecting the audio wave
form the video recordings of the participant playing along
to the 60bpm and 120bpm backing track and comparing the
peaks. The error threshold was set to a ﬂutter of 50ms [2,
10], see Figure 5.
Figure 5 shows the percentage of synchronisation errors
exceeding the threshold of ±50ms for each mandatory ses-
sion and tempo. We reviewed the video of six strum outliers
exceeding 200ms and excluded these from the data set. The
outliers we removed were caused by small readjustments to
the guitar position (n2), button interaction error (n2), or
lack of concentration during play, e.g. looking at the re-
searcher (n2). The synchronisation errors showed a steady
decline during the intervention from 39 to 22% for 60bpm
and from 51 to 17% for 120bpm, which conforms to the
power law of learning [8].
The participant generally strummed later when playing
to the 120bpm track than when playing to the 60bpm track,
see Figure 6. The ﬁrst session average from the 60bpm ac-
tually shows that he was also late compared to the follow-
ing averages, and with anticipation bias in mind it is clear
that this was not a sign of better performance. On day 17
of the intervention the ﬁnal mandatory session had to be
stopped as the participant struggled to activate the button
to strum the guitar and gave up on ﬁnishing the session. It
was decided that he could stop using the guitar and pick
it up 5 days later when the intervention was scheduled to
end. However, his motivation to play the guitar was so
strong that he kept playing despite the failing button, see
Figure 4. Logged data from the force sensor shows that
308
the force required to activate a strum slowly increased dur-
ing the intervention and during the sixth mandatory session
required more than three times the force.
Figure 6: The synchronisation error averages per
day in milliseconds from the mandatory sessions at
60 and 120BPM.
The QOL questionnaire showed a small QOL improve-
ment in Pain and Discomfort , Positive Feelings, Thinking,
Learning, Memory and Concentration , no change in Sleep
and Rest and Self-Esteem, and a small QOL decrease in
Energy and Fatigue, Bodily Image and Negative Feelings.
The results of the FIM test (108 out of 123) placed the
participant on step 6 as a person having ’modiﬁed inde-
pendence’ on a scale going from 1 (Total Assistance) to 7
(Complete Independence). The score matched the partici-
pant’s inability to use his right arm. He needed assistance
with such tasks as buttering bread, cutting up meat, and
putting on a t-shirt. The cognitive sub-part of the FIM test
focusing on problem solving and memory showed that he
needed supervision or assistance less than 10% of the time
and that he often used an iPad or similar device to solve
cognitive challenges.
6. DISCUSSION
The participant showed strong motivation to play the gui-
tar long-term despite the inherent delay and reduced ex-
pressiveness as he played 14 free sessions over a total of
7.2 hours spread out across the three-week intervention. In
addition to the time spent playing the guitar he also incor-
porated his former backing tracks stored in his old keyboard
setup, which showed an even higher degree of motivation.
We used a Synchronisation Error (SE) threshold of 50ms
which was rather strict, as normal anticipation bias can be
±50ms. The participant learned to incorporate the inher-
ent delay of the system as the SE showed a clear decline
throughout the intervention. Fewer SE during the 120bpm
backing track can partly be explained by the fact that the
relative size of ﬂutter increases with slower tempos [10] and
thereby produces more SE when playing along to the 60bpm
backing track. The SE average also supports the conclu-
sion that he gradually learned the interface and the built-in
delay, see Figure 6. The ﬁgure also reveals that he strug-
gled more with the 120bpm tempo since the averages are
higher (later) overall. According to anticipation bias [2],
they should be around -14 to -50ms early.
In the future the data logger should be able to log the
MIDI tempo data to get more precise and extensive data.
We did not consider the quality of the chords played,
which could potentially tell a lot about how the participant
coordinated the strum and chords. Any mismatch could
aﬀect rhythm and timing if the participant experienced it
as disruptive and might have lost focus, which would af-
fect the results. A visual and auditive comparison of the
recorded video from the beginning and the end of the inter-
vention reveal that his ability to coordinate the strum and
and the fretting of chords improves immensely. Despite the
profound latency of the system and the worn out button.
According to the power law of learning the SE should con-
tinue to decline based on the amount of practice/learning,
but as seen in Figure 6 the SE average increased a lot during
the ﬁfth and last mandatory session. Figure 5 also shows
an increase from 7 to 16% in the 120bpm tempo in SE per-
centage, exceeds the threshold. The increased force needed
to activate the button most likely aﬀected the data and
could explain why the averages in both 60bpm and 120bpm
increased in the last mandatory session.
The QOL questionnaire showed an increase in positive
feelings, which ﬁts well with how the participant used the
guitar during the intervention and indicates that he was
highly motivated and enjoying himself. Thinking, learning,
memory and concentration also increased, which also ﬁts
well with the many sessions and hours played. Bodily Ap-
pearance and Negative Feelings decreased, which can seem
contradictory as he was able to perform a task that he could
not do before. The lower scores might indicate an increased
awareness of his own situation and limitations. The QOL
questionnaire did not have the sensitivity to reveal any con-
clusive improvements or changes in quality of life. In coming
studies a more suitable questionnaire should be used.
While the current design was good for short-term use
longer term trials would need to resort to higher quality
to avoid wear.
7. CONCLUSION
The participant was able to play the guitar without support
and only needed help for equip the guitar as it used a reg-
ular guitar strap. The mandatory session showed that the
participant was able to play along to both the slow and fast
backing track lowering his synchronisation errors with 17%
at 60bpm and 34% at 120bpm. This showed that the partic-
ipant was able to learn and compensate for the system delay
of 45ms from pushing the foot pedal to the pick reaching the
ﬁrst string. Playing 14 free sessions for a combined total of
more than 7 hours is a clear indication of the participant’s
motivation to use the Actuated Guitar despite the latency
and limited expressive possibilities. Other indicators of a
high degree of motivation was how the participant incorpo-
rated his own devices into the study. He used his iPad to
ﬁnd tunes on YouTube to play along too and used his old
piano setup and P.A. with the backing tracks from the time
before his stroke.
8. REFERENCES
[1] Why Is Music Eﬀective in Rehabilitation? In
A. Gaggioli, editor, Advanced Technologies in
Rehabilitation: Empowering Cognitive, Physical,
Social, and Communicative Skills through Virtual
Reality, Robots, Wearable Systems, and
Brain-Computer Interfaces, number v. 145 in Studies
in health technology and informatics. IOS Press,
Amsterdam ; Washington, DC, 2009.
[2] G. Aschersleben. Temporal Control of Movements in
Sensorimotor Synchronization. Brain and Cognition,
48(1):66–79, Feb. 2002.
[3] G. Aschersleben and W. Prinz. Delayed Auditory
Feedback in Synchronization. Journal of Motor
Behavior, 29(1):35–46, Mar. 1997.
[4] A. Askenfelt and E. V. Jansson. From touch to string
309
vibrations. I: Timing in the grand piano action. The
Journal of the Acoustical Society of America ,
88(1):52–63, 1990.
[5] R. Bailey. Human Performance Engineering:
Designing High Quality Professional User Interfaces
for Computer Products, Applications and Systems,
3/e. 1996.
[6] S. Bhat. TouchTone: An electronic musical
instrument for children with hemiplegic cerebral
palsy. In Proc of TEI’10, pages 305–306. ACM, 2010.
[7] B. Bongers. Physical interfaces in the electronic arts.
Trends in gestural control of music, pages 41–70, 2000.
[8] S. K. Card, A. Newell, and T. P. Moran. The
Psychology of Human-Computer Interaction . L.
Erlbaum Associates Inc., 1983.
[9] F. B. Charatan and A. Fisk. Mental and emotional
results of strokes. New York state journal of medicine ,
1978.
[10] S. Dahl. The Playing of an Accent ? Preliminary
Observations from Temporal and Kinematic Analysis
of Percussionists*. Journal of New Music Research ,
29(3):225–233, Sept. 2000.
[11] B. Farrimond, D. Gillard, D. Bott, and D. Lonie.
Engagement with Technology in Special Educational
& Disabled Music Settings. Youth Music Report,
pages 1–40, 2011.
[12] S. A. Finney. Auditory Feedback and Musical
Keyboard Performance. Music Perception: An
Interdisciplinary Journal, 15(2):153–174, Dec. 1997.
[13] R. Keith, C. Granger, B. Hamilton, and F. Sherwin.
The functional independence measure. Adv Clin
Rehabil, 1:6–18, 1987.
[14] J. V. Larsen and H. Knoche. Hear you later alligator:
How delayed auditory feedback aﬀects non-musically
trained people’s strumming. In New Interfaces for
Musical Expression 2017New Interfaces for Musical
Expression, 2017.
[15] J. V. Larsen, D. Overholt, and T. B. Moeslund. The
Actuated Guitar: A platform enabling alternative
interaction methods. In SMC Proceedings of the
Sound and Music Computing Conference , pages
235–238, 2013.
[16] J. V. Larsen, D. Overholt, and T. B. Moeslund. The
Actuated Guitar: Implementation and User Test on
Children with Hemiplegia. In Proc. NIME’14, 2014.
[17] D. J. Levitin. This Is Your Brain on Music:
Understanding a Human Obsession . Atlantic Books
Ltd, 2011.
[18] D. J. Levitin, S. McAdams, and R. L. Adams. Control
parameters for musical instruments: A foundation for
new mappings of gesture to sound. Organised Sound,
7(02), Aug. 2002.
[19] F. Liu, M. Zuo, and P. Zhang. Human-Machine
Function Allocation In Information Systems: A
Comprehensive Approach. In PACIS, page 117, 2011.
[20] F. Lyons, B. Bridges, and B. McCloskey. Accessibility
and dimensionality: Enhanced real time creative
independence for digital musicians with quadriplegic
cerebral palsy. Proceedings NIME 2015 Scientiﬁc
Program, 1:1–4, 2015.
[21] T. Magnusson. Designing constraints: Composing and
performing with digital musical systems. Computer
Music Journal, 34(4):62–73, 2010.
[22] E. R. Miranda and M. M. Wanderley. New Digital
Musical Instruments: Control and Interaction beyond
the Keyboard. Number v. 21 in The computer music
and digital audio series. A-R Editions, Middleton,
Wis, 2006. OCLC: ocm62533819.
[23] R. Moog. The musician: Alive and well in the world
of electronics. In The Biology of Music Making:
Proceedings of the 1984 Denver Conference. St Louis,
MMB Music, Inc , pages 214–220, 1988.
[24] P. Pfordresher and C. Palmer. Eﬀects of delayed
auditory feedback on timing of music performance.
Psychological Research, 66(1):71–79, Feb. 2002.
[25] R. A. Rasch. Synchronization in performed ensemble
music. Acta Acustica united with Acustica,
43(2):121–131, 1979.
[26] D. Rubine and P. McAvinney. Programmable
Finger-Tracking Instrument Controllers. Computer
Music Journal, 14(1):26, 1990.
[27] E. Ruud. Music and the Quality of Life. Norsk
Tidsskrift for Musikkterapi , 6(2):86–97, July 1997.
[28] S. Schneider, P. W. Sch ¨onle, E. Altenm¨uller, and
T. F. M¨unte. Using musical instruments to improve
motor skill recovery following a stroke. Journal of
Neurology, 254(10):1339–1346, Oct. 2007.
[29] K. Takano and Y. Miyake. Two types dynamics in
negative asynchrony of synchronization tapping. In
SICE-ANNUAL CONFERENCE-, volume 2, page
1792. SICE; 1999, 2004.
[30] C. Tam, H. Schwellnus, C. Eaton, Y. Hamdani,
A. Lamont, and T. Chau. Movement-to-music
computer technology: A developmental play
experience for children with severe physical
disabilities. Occupational Therapy International,
14(2):99–112, June 2007.
[31] M. Thaut, G. McIntosh, R. Rice, R. Miller,
J. Rathbun, and J. Brault. Rhythmic auditory
stimulation in gait training for Parkinson’s disease
patients. Movement disorders, 11(2):193–200, 1996.
[32] M. H. Thaut. Neurologic Music Therapy in Cognitive
Rehabilitation. Music Perception, 27(4):281–285, Apr.
2010.
[33] K. W. Thomas and B. A. Velthouse. Cognitive
elements of empowerment: An “interpretive” model of
intrinsic task motivation. Academy of management
review, 15(4):666–681, 1990.
[34] S. C. Thompson, A. Sobolew-Shubin, M. A. Graham,
and A. S. Janigian. Psychosocial adjustment following
a stroke. Social Science & Medicine , 28(3):239–247,
1989.
[35] S. Vickers, H. Istance, and M. Smalley. EyeGuitar:
Making rhythm based music video games accessible
using only eye movements. In Proceedings of the 7th
International Conference on Advances in Computer
Entertainment Technology, pages 36–39. ACM, 2010.
[36] M. M. Wanderley and N. Orio. Evaluation of input
devices for musical expression: Borrowing tools from
hci. Computer Music Journal , 26(3):62–76, 2002.
[37] J. G. Webster. Electronic Devices for Rehabilitation.
John Wiley & Sons Incorporated, 1985.
[38] R. J. Zatorre, J. L. Chen, and V. B. Penhune. When
the brain plays music: Auditory–motor interactions in
music perception and production. Nature Reviews
Neuroscience, 8(7):547–558, July 2007.
310