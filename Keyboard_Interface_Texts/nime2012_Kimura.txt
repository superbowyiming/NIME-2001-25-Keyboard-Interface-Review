Extracting Human Expression For InteractiveComposition with the Augmented ViolinMari  KimuraThe Juilliard School60 Lincoln Center Plaza, New York,NY 10023 USAmarikimura@me.com
Nicolas RasamimananaPhonotonic84 rue du Faubourg St Martin,75010 Paris, FRANCEIRCAM1, place Igor-Stravinsky,75004 Paris, FRANCEnicolas@phonotonic.net
Frédéric BevilacquaBruno ZamborlinNorbert SchnellEmmanuel FlétyIRCAM1, place Igor-Stravinsky,75004 Paris, FRANCEFrederic.Bevilacqua@ircam.fr
ABSTRACTAs a 2010 Artist in Residence in Musical Research at IRCAM,Mari Kimura used the Augmented Violin to develop newcompositional approaches, and new ways of creating interactiveperformances [1]. She contributed her empirical and historicalknowledge of violin bowing technique, working with the RealTime Musical Interactions Team at IRCAM.  Thanks to thisresidency, her ongoing long-distance collaboration with theteam since 2007 dramatically accelerated, and led to solvingseveral compositional and calibration issues of the GestureFollower (GF) [2]. Kimura was also the first artist to developprojects between the two teams at IRCAM, using OMAX(Musical Representation Team) with GF.  In the past year, theperformance with Augmented Violin has been expanded inlarger scale interactive audio/visual projects as well.  In thispaper, we report on the various techniques developed for theAugmented Violin and compositions by Kimura using them,offering specific examples and scores.KeywordsAugmented Violin, Gesture Follower, Interactive Performance1. INTRODUCTIONThe Augmented Violin project brings together the!"#"$%&'"() of motion capture systems and the study ofbowed string performance. The outcome of this project isessentially an "augmented bow", which can be used to controldigital media.  It is not an extra contraption one needs to installon the acoustic violin or bow, nor a mechanical device toreplace the existing instrument. The Augmented Violin honorsthe violin as is, and simply extracts expression from what isalready happening in performance, which seems like the mostpractical approach that makes sense for musicians.To start our research, we first laid out specific goals relatedto the potential compositional schemes pursuing our work withIRCAM’s Augmented Violin (customized) and using a newseries of software tools, such as the gesture recognition systembased on the MuBu-GF platform [3] as well as other specificdevelopments.
2. BACKGROUNDAt NIME 2006 conference, Kimura learned about IRCAM’sAugmented Violin, and since has been working on creatingseveral compositions and presenting more than dozen premieresand performances incorporating the Augmented Violin into hercompositions. Improving on the original design of theAugmented Violin, which required wearing a wrist band tohouse the battery portion connecting to the sensor attached tothe bow with an ‘umbilical’ cord, Kimura commissioned acustom-fit glove created by New York fashion designer MarkSalinas. This glove, worn on the right hand, houses both thesensor and battery portions of the Mini-MO, a wireless 3Daccelerometer and 3-axis gyroscope motion sensor developedby Emmanuel Fléty [17]. The Mini-MO communicates with thecomputer via an Ethernet receiver, thus freeing the violinist’sbow arm completely from any wiring. The thin, flexible latexfabric of the glove houses the small device un-intrusively forperformance: a practical solution for violinists (see Figure 1).
Figure 1. The Augmented Violin Glove, with the latestMini-MO (Modular Musical Objects) created by IRCAM.In 2007, during her residency at Harvestworks in NYC,Kimura created her first work for Augmented Violin entitledVITTESSIMO [4] in collaboration with Frédéric Bevilacqua;she also described the creative process of this work [5].  For ageneral review of related works on augmented and hyper-instruments based on strings instruments, we refer the reader toPoepel, Overholt [15] and Bevilacqua et al [16].Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and thatcopies bear this notice and the full citation on the first page. To copyotherwise, to republish, to post on servers or to redistribute to lists,requires prior specific permission and/or a fee.NIME’12, May 21-23, 2012, University of Michigan, Ann Arbor.Copyright remains with the author(s).
3. RESEARCH3.1 ‘Cloning’ with MuBu-GFCLONE BARCAROLLE (2009) by Kimura [4] uses the‘cloning’ function with GF.  The “cloning” is made possibleusing the gesture follower (using the gf and mubu externals inMax). When a musical phrase is played for the first time, thepatch records both the audio and the sensor parameters. Theaudio is stored in a buffer, while also analyzed to obtain theaudio energy profiles.  These audio energy profiles are thencombined to the sensors parameters of Mini-MO to form amultimodal set of data that is memorized by the gesturefollower. All parameter profiles are normalized (between 0 and1) and form a complementary set of descriptors of the phrase:the accelerometer/gyroscope give very precise information onthe bowing attacks and releases while the audio energy givesmore global information on the phrase morphology.When the same phrase is played the second time, the systemis set to “follow mode”. In this case, the multimodal parameters(audio energy and sensors values), measured in real time, arecompared to the stored data. This allows for the continuousestimation of playing speed. The original buffer can thus bereplayed using the current playing speed. For this, a phasedvocoder is used to preserve the original speed (superVP). Thereplay of the audio buffer is thus the “clone”, preciselysynchronized to the current playing. This adaptive behaviorprovides the musician with a range of expressivity differentfrom the standard use of “record/play” of samples with a fixedspeed. (See Figure 2.  Excerpt from “Clone Barcarolle” (2009))
Figure 2.  Excerpt from “Clone Barcarolle” (2009).
3.2 Soft / Loud EndingKimura’s pieces are based on three main bowing gestureprocessing. They were designed upon the concept of movementqualities [6], where the goal is not to achieve a particulargesture or action but rather a way of performing it, i.e. aquality. We believe such an approach to be promising as it canprovide for a higher level of abstraction in interaction design,while still keeping a sense of mediation [7].Ending a performance can be done in many differentmanners. We decided to try detecting musical expressionsincluding two different musical endings, shown in thisexample: 1) soft ending, 2) loud ending.  Using the results ofthe analysis, we developed a simple “ending detector”,analyzing the combination of physical and musical gestures:right hand movement together with sound and musical contextfor the violin. (see Figure 3).
Figure 3.  Soft (1) and Loud (2) ending tracking.We here wanted to focus on soft on-the-string endings,versus loud off-the-string endings. The Soft/Loud Endingdetection was designed to grab such ending qualities ofKimura’s performance. This detector is combining gesture andaudio cues. It simply relies on bow orientation (tilt) when soundis fading away (audio energy). Based on this, two differentconfigurations are registered, soft ending and loud ending.Through a k-Nearest-Neighbor algorithm operating on bow tiltand audio energy, we are able to distinguish in real time theperformed ending quality.3.3 “Stillness” detectorWith a realization that the cornerstone of a ‘bowed’ instrumentis the ability to sustain sound, unlike ‘plucked’ instruments, wewanted to simply detect if and when the bow has achieved a“steady” state while holding a note.  This “stillness” detectorbecame one of the most important interactive methods inKimura’s compositions.The stillness detector actually tracks two kinds of bowingmovements: smooth sustained movements and jerky fastmovements. As such, it can be related to the musical flowcreated by the artist through the alternation of short and long,energetic and smooth strokes.This gesture descriptor is based on the acceleration in thebowing direction. It enables one (1) to detect bow sustainedmovements and (2) to register the time this movement wasmaintained. ‘Stillness Detection’ can be used to recognize thesustained characteristics of a bowing movement, i.e. one of itsmovement qualities. Registered time can be used todifferentiate short and long strokes, e.g. by setting thresholdvalues, or to design time evolving interactions directlydepending on the parameter value.To compute the stillness gesture descriptor, we first deriveacceleration in the bowing direction to get the movementsmoothness, aka ‘jerk’. This value accounts for the level ofmovement smoothness. From this value we are able todistinguish between two states. ‘Still’ corresponds to low jerkvalues, i.e. smoothest movements, as opposed to ‘Non-Still’,with higher jerk values. This detection can be stabilized usingan additional threshold for minimum time in a state. These twothreshold values actually depend on each violinist, eachmusician having his/her own movement characteristics, and canbe set during practice.This “stillness” detector turned out also as a useful substitutefor the superVP.ring~ external in Max.  A ring buffer is rathercumbersome to control in real time performance, since itconstantly records a certain amount of the “past” then “rollsoff”.  Using the “stillness” detector, it is possible to simulatethe ring buffer, which is used in Kimura’s composition CANONELASTIQUE (2010). Using two superVP.scrub~ externalscontrolled by “stillness” detection, it was possible to constantlychange the 2nd voice (the “past”) by applying “stillness” controlso as to follow the 1st voice (the performer).  In CANONELASTIQUE, one can create an “elastic” past (see Figure 4).
Figure 4. “Canon Élastique” creating ‘elastic past’.3.4 Agogic player, “flow-follower”Another application of “stillness” detection is “agogic player”,or what Kimura calls a “flow-follower”, an alternative to a“score follower”.  Performers in real life don’t have to follownote by note, beat by beat, in order to play together in sync.Rather, they follow each other’s musical “flow” together, oragree on musical “common sense” of the particular musicalscenario, a piece of music or context. By using “stillness”detection, it is possible to demonstrate how a quantized pianoperformance (either MIDI or audio) can co-perform, or followthe violinist’s musical flow without using a score follower.Even when the violinist is preparing a stroke without playing,the piano part can naturally and musically be slowed downbefore the violin’s entrance (see Figure 5).In this short excerpt of a Bossa Nova tune by Tom Jobim,which requires lucid tempo and rubato, the violinist startsplaying as the piano plays the 8th note scale upward.  Theviolinist’ right arm movement at this point is still preparing toplay the first note, and it is ‘steady’.  By applying the“stillness” detection here, it is possible to naturally slow downthe piano part slightly, as any sensible human pianist would inreal life.In the second measure, the “stillness” detector also applies“agogic” or a small ‘breath’ between the beats.   As a result, thecomputer piano naturally plays along, following the flow of themusic created by the bowing, making human and machineinteraction mutual and omni-directional: not one triggering andthe other obeying.
Figure 5. “Flow Following” between human and computer.3.5 Combining OMAX with GestureFollowerWhile working on concatenating musical material using GF,we found interesting to join this system with theimprovisational system called OMAX, developed by theIRCAM’s Musical Representation Team [8, 9, 10, 11]. OMAX,which runs on MaxMSP, creates a “co-improvisation” thatlearns the player’s improvisation performance and plays back
the concatenated performance in real time. OMAX was mainlydesigned to include a “computer operator” who controls itsbehavior such as ‘fading out’ to end its improvisation.  UsingGF interacting with OMAX, it is possible to create anautonomous improvisation without the computer operatorperforming OMAX. Working with the distinct behavior ofOMAX, and with the aforementioned various gesturedetections including “ending” or “stillness” detector, it is nowpossible to terminate OMAX’s improvisation from pre-definedconcatenated gestures, such as pizzicato or sustained fadingtones. Kimura’s compositions using OMAX and GF, to date,include VIOMAX (2010) and Duet x2 for violin and cello(2011) using two Mini-MO-s.4. CURRENT AND ONGOING4.1 Recent multi-media projectsThe Augmented Violin can be used for audio/visual projects aswell as collaborations.  Kimura’s work PHANTOM (2009) is acollaborative project with New York-based artist Toni Dove inher multimedia work, “Lucid Possessions”.  In PHANTOM, theAugmented Violin controls interactive video and a robot with3D projections.More recent audiovisual works with the Augmented Violininclude “VOYAGE APOLLONIAN” for interactive audio/video(visuals created by graphic artist Ken Perlin), which wasrecently featured in Scientific American [12]. The AugmentedViolin was used to control the Quicktime movie using“stillness” and other concatenated gesture detections.4.2 EigenspaceKimura’s most recent project is entitled EIGENSPACE. Forviolinists, bowing motion is kind of an ‘arc’, which involvesarm motions that include ‘inertia’ movements before and afterthe strokes.  In EIGENSPACE (2011), this ‘inertia’ movementis incorporated to interact with the audio and video (see Figure6).We came up with a concept to simulate this motion with aspring, using “eigenvalues”. This last gesture processingintroduces a physical behavior between bow gesture data andsound (or video) parameters. Similarly to works by Henry [13],interaction here is grounded on a damped mass spring system.In our case, however, the physical system is controlled by thebowing energy computed from bow acceleration. The musiciancan therefore imprint different movements to the physicalsystem, which reacts according to its settings. Changing itsparameters, like weight, mass or stiffness factor, alters thesystem reaction depending on the created eigenmodes.  Theseeigenmodes can be related to different movement qualities asthey exhibit various behaviors for a same mass displacement:jerky, smooth, or periodic.  Playing on these modes enables tocreate an organic relation between the artist and the computer.In EIGENSPACE, the graphics created by the team ofJapanese movie director Tomoyuki Kato interact with theeigenvalues and the ‘spring’ motion during the ‘retake’ of thebow.   Extracting the ‘retake’ motion of the bow, the ‘spring’ orthe inertia mechanism, can be calibrated to the desired‘stiffness’, ‘reaction time’ and ‘damping’ according to thecomposer’s musical needs and the violinist’s expressive bowingmotion.
Figure 6.  “Eigenspace” premiere at Roulette in Brooklyn,NY, Oct. 9th, 2011.5. CONCLUSIONThe Augmented Violin is one of the most unintrusive sensordevices available for violinists to provide bowing and motiondata to a computer media system such as MaxMSP. Its mainappeal to violinists is that it honors and enhances the existinginstrument without any wirings and contraptions, which shouldbe more user-friendly to average string players.In this article, we have presented various compositionaltechniques that build on the unique capabilities of theAugmented Violin, illustrated through Kimura’s compositions.These include the ability to “clone” oneself, while applyingvarious transformations to the “clone” controlled by the playervia the Augmented Violin; recognizing different phrases in realtime; analyzing “before” and “after” gestures; recognizing“soft” and “loud” endings; detecting “stillness” and followingmusical flow. We have shown how to combine the gesturefollower with other systems, like the improvisational systemOMAX (also developed at IRCAM), and into multi-mediacollaborations projects between Kimura and others. TheAugmented Violin has also inspired other composers to createinteractive compositions that had been previously impossible.We will need to further develop user-friendly interfaces, thusaffording many string players to try out this elegant musicalsystem of the future.6. ACKNOWLEDGMENTSSpecial thanks to IRCAM’s Real Time Musical InteractionTeam, and IRCAM’s Musical Representation Team, GérardAssayag and Benjamin Lévy. This work was made possiblethrough the Composer-in-Residence in Musical Researchprogram at IRCAM.  Thanks to Hervé Brönnimann for his helpin the preparation of this manuscript.7. REFERENCES[1] F. Bevilacqua, F. Guédy, N. Schnell, E. Fléty, and N.Leroy.  “Wireless sensor interface and gesture-follower formusic pedagogy”. In Proceedings of the 2007 Conferenceon New Interfaces for Musical Expression (NIME07), NewYork, NY, USA, 2007.[2] F. Bevilacqua, B. Zamborlin, A. Sypniewski, N. Schnell,F. Guédy, and N. Rasamimanana. “Continuous realtimegesture following and recognition”.  In EmbodiedCommunication and Human-Computer Interactin, volume5934 of Lecture Notes in Computer Science, pages 73–84.Springer Berlin / Heidelberg, 2010.
[3] N. Schnell, A. Röbel, D. Schwarz, G. Peeters, and R.Borghesi. “MuBu & Friends - Assembling Tools forContent Based Real-Time Interactive Audio Processing inMax/MSP”. In  Proceedings of the 2009 InternationalComputer Music Conference (ICMC), Montreal, Canada,2009.[4] M. Kimura.  The World Below G and Beyond, solo CD.Mutable Music label, 2010.[5] M. Kimura.  “Making of Vitessimo for Augmented Violin:Compositional Process and Performance”. In Proceedingsof the 2008 Conference on New Interfaces for MusicalExpression (NIME08), Genova, Italy, 2008.[6] S. Fdili Alaoui, B. Caramiaux, and M. Serrano. “Fromdance to touch: movement qualities for interactiondesign”. CHI, ACM (2011), 1465–1470.[7] M. Leman. Embodied music cognition and mediationtechnology. The MIT Press, Cambridge, Massassuchetts,2007.[8] G. Assayag, G. Bloch and S. Dubnov. “Introducing VideoFeatures and Spectral Descriptors into the OmaxImprovisation System”. Demo, International ComputerMusic Conference, Belfast, Ireland, August, 2008.[9] Assayag, G. , Bloch, G.  “Navigating the Oracle: aHeuristic Approach”. In Proceedings of the 2007International Computer Music Conference (ICMC),Copenhagen, Denmark, 2007.[10] A. Cont, S. Dubnov, and G. Assayag. “Anticipatory Modelof Musical Style Imitation using Collaborativeand Competitive Reinforcement Learning”.  InAnticipatory Behavior in Adaptive Learning Systems,(Martin Butz and Olivier Sigaud and GianlucaBaldassarre, Berlin), 2007.[11] G. Assayag, G. Bloch, M. Chemillier, A. Cont, and S.Dubnov. “Omax Brothers : a Dynamic Topology ofAgents for Improvization Learning”. In Workshop onAudio and Music Computing for Multimedia, ACMMultimedia 2006, Santa Barbara, 2006.[12] L. Greenemeier. “String Theory: Violinist Taps ArtificialIntelligence to Interact with Her Unique Sound”. ScientificAmerican, May 31, 2011.[13] C. Henry. “Physical Modeling for Pure Data (pmpd) andreal time interation with an audio synthesis”. InProceedings of the Sound and Music ComputingConference, SMC, 2004.[14] N. Rasamimanana, F. Bevilacqua, N. Schnell et al.“Modular Musical Objects Towards Embodied Control OfDigital Music”. TEI, (2011), 9-12.[15] C. Poepel, D. Overholt. “Recent developments in violin-related digital musical instruments: where are we andwhere are we going?” In Proceedings of the InternationalConference on New Interfaces for Musical Expression,Paris, France, pp. 390–395, 2006.[16] F. Bevilacqua, F. Baschet, and S. Lemouton. Theaugmented string quartet: experiments and gesturefollowing, Journal of New Music Research, 2012.[17] E. Fléty, and C. Maestracci. “Latency improvement insensor wireless transmission using IEEE 802.15.4.” InProceedings of the International Conference on NewInterfaces for Musical Expression, pages 409–412, Oslo,Norway, 2011.