Mechatronic Expression:  Reconsidering Expressivity in Music for Robotic Instruments    Steven Kemper Mason Gross School of the Arts Rutgers University  New Brunswick, NJ, USA  skemper@mgsa.rutgers.edu   
 Scott Barton Worcester Polytechnic Institute Worcester, MA, USA  sdbarton@wpi.edu  ABSTRACT Robotic instrument designers tend to focus on the number of sound control parameters and their resolution when trying to develop expressivity in their instruments. These parameters afford greater sonic nuance related to elements of music that are traditionally associated with expressive human performances including articulation, timbre, dynamics, and phrasing. Equating the capacity for sonic nuance and musical expression stems from the “transitive” perspective that musical expression is an act of emotional communication from performer to listener. However, this perspective is problematic in the case of robotic instruments since we do not typically consider machines to be capable of expressing emotion. Contemporary theories of musical expression focus on an “intransitive” perspective, where musical meaning is generated as an embodied experience. Understanding expressivity from this perspective allows listeners to interpret performances by robotic instruments as possessing their own expressive meaning, even though the performer is a machine. It also enables musicians working with robotic instruments to develop their own unique vocabulary of expressive gestures unique to mechanical instruments. This paper explores these issues of musical expression, introducing the concept of mechatronic expression as a compositional and design strategy that highlights the musical and performative capabilities unique to robotic instruments.  Author Keywords Musical robotics, expression, music for robotic instruments  CCS Concepts Applied computing~Sound and music computing; 1. INTRODUCTION Designers of robotic instruments1 place a great deal of emphasis on the capability of their instruments to be “expressive” [14, 23, 30, 34]. While robotic instruments are typically adept at performing complex sequences of pitches and rhythms with technical precision, the sonic nuance traditionally associated with expressive human performance, including articulation, timbre, dynamics, and phrasing, is more difficult to model, requiring complex hardware and software design. Much of the current research on robotic instruments highlights capabilities for sonic nuance as a function of the number of sound control parameters and their resolution [e.g. 33]. This research 
                                                             1 This paper employs the term “robotic instruments” for both mechatronic instruments, which possess autonomous performance capabilities as well as truly robotic instruments, 
suggests that refining these parameters will lead to a more expressive instrument.  Equating greater control over sonic nuance with “expression” resonates with mainstream contemporary understandings of musical expression. From this perspective, with its roots in Romantic-era aesthetics, a performer is able to transmit the meaning of a piece of music, often thought of as emotion, to the listener through subtle variations of articulation, timbre, dynamics, and phrasing. This perspective is referred to as “transitive” in that meaning or emotion is considered to be transmitted from the performer to the listener [25, 32]. Scruton summarizes this position and its prevalence in contemporary discourse, “…despite all the skepticism that has been heaped on Romantic aesthetics, the popular view remains essentially that of Rousseau and Diderot: music evokes emotion because it expresses emotion. Music is the middle term in an act of emotional communication…” [32].  The transitive concept of musical expression is problematic in the case of robotic instruments. How can a machine be expressive if it cannot feel that which can be expressed? Concerns over expressivity in mechanical music date back to the musical automata of the eighteenth and nineteenth centuries. In 1752, Quantz described Vaucanson’s mechanical flute player as a technically proficient musician but claimed that it was devoid of human expression: “With skill a musical machine [i.e. Vaucanson’s flute-player] could be constructed that would play certain pieces with a quickness and exactitude so remarkable that no human being could equal it either with his fingers or with his tongue. Indeed it would excite astonishment, but it would never move you” [29, emphasis added]. In this quote, Quantz acknowledges the technical precision of Vaucanson’s automaton, but considers the instrument to be incapable of stirring an emotional response in the listener.  In 1818 a critic from the London times described a performance by Johann Maelzel’s Automaton Trumpeter as technically precise, but lacking in expression, “Nothing can exceed the accuracy and neatness of the execution, or the steadiness of the tone: in the rapidity with which the same note may be repeated in succession, and in some passages of a similar nature, it surpasses the powers of any living trumpeter; it fails only in expression, and in the swell of the note, a defect which is common to all music produced by mechanism” [4]. Again, we find mechanical performance set apart from human performance, with the former less expressive than the latter.  Contemporary robotic instrument designers continue to be inspired by the concerns over expressivity in musical automata in order to increase the parametric capabilities of their instruments. For example, Maes et al. state, “…sounds could be programmed to go on or off at fairly precise timings, but nuances, dynamics, and timbral 
which include feedback from the environment. Colloquially, both types of instruments are often classified as “robotic.” 
NIME Proceedings Template for LaTeX
Ben Trovato⇤
Institute for Clarity in
Documentation
1932 Wallamaloo Lane
Wallamaloo, New Zealand
trovato@corporation.com
G.K.M. Tobin†
Institute for Clarity in
Documentation
P .O. Box 1212
Dublin, Ohio 43017-6221
webmaster@marysville-
ohio.com
Lars Thørväld‡
The Thørväld Group
1 Thørväld Circle
Hekla, Iceland
larst@afﬁliation.org
Lawrence P . Leipuner
Brookhaven Laboratories
Brookhaven National Lab
P .O. Box 5000
lleipuner@researchlabs.org
Sean Fogarty
NASA Ames Research Center
Moffett Field
California 94035
fogartys@amesres.org
Anon Nymous
Redacted
8600 Datapoint Drive
San Antonio, Texas 78229
cpalmer@prl.com
ABSTRACT
This paper provides a sample of a LATEX document for the
NIME conference series. It conforms, somewhat loosely, to
the formatting guidelines for ACM SIG Proceedings. It is an
alternate style which produces atighter-lookingpaper and
was designed in response to concerns expressed, by authors,
over page-budgets. It complements the documentAuthor’s
(Alternate) Guide to Preparing ACM SIG Proceedings Us-
ing LATEX2✏ and BibTEX. This source ﬁle has been written
with the intention of being compiled under LATEX2✏ and
BibTeX.
To make best use of this sample document, run it through
LATEX and BibTeX, and compare this source code with your
compiled PDF ﬁle. A compiled PDF version is available to
help you with the ‘look and feel.’The paper submit-
ted to the NIME conference must be stored in an
A4-sized PDF ﬁle, so North Americans should take
care not to inadvertently generate letterpaper-sized
PDF ﬁles.This paper template should prevent that from
happening if thepdflatexprogram is used to generate the
PDF ﬁle.
The abstract should preferably be between 100 and 200
words.
Author Keywords
NIME, proceedings, LATEX, template
CCS Concepts
•Applied computing! Sound and music comput-
ing; Performing arts;•Information systems! Music
retrieval;
⇤Dr. Trovato insisted his name be ﬁrst.†The secretary disavows any knowledge of this author’s ac-
tions.‡This author is the one who did all the really hard work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
Please read the comments in the nime-template.tex
ﬁle to see how to create the CCS Concept Classiﬁ-
cations!
1. INTRODUCTION
The proceedingsare the records of a conference. ACM seeks
to give these conference by-products a uniform, high-quality
appearance. To do this, ACM has some rigid requirements
for the format of the proceedings documents: there is a
speciﬁed format (balanced double columns), a speciﬁed set
of fonts (Arial or Helvetica and Times Roman) in certain
speciﬁed sizes (for instance, 9 point for body copy).
The good news is, with only a handful of manual set-
tings,1 the LATEX document class ﬁle handles all of this for
you.
The remainder of this document is concerned with show-
ing, in the context of an “actual” document, the LATEXc o m -
mands speciﬁcally available for denoting the structure of a
proceedings paper, rather than with giving rigorous descrip-
tions or explanations of such commands.
2. THE BODYOF THE PAPER
Typically, the body of a paper is organized into a hierar-
chical structure, with numbered or unnumbered headings
for sections, subsections, sub-subsections, and even smaller
sections. The command\sectionthat precedes this para-
graph is part of such a hierarchy.2 LATEX handles the num-
bering and placement of these headings for you, when you
use the appropriate heading commands around the titles of
the headings. If you want a sub-subsection or smaller part
to be unnumbered in your output, simply append an aster-
isk to the command name. Examples of both numbered and
unnumbered headings will appear throughout the balance
of this sample document.
Because the entire article is contained in thedocument
environment, you can indicate the start of a new paragraph
with a blank line in your input ﬁle; that is why this sentence
forms a separate paragraph.
1Two of these, the \numberofauthors and \alignau-
thor commands, you have already used; another,\bal-
ancecolumns, will be used in your very last run of LATEX
to ensure balanced column heights on the last page.2This is the second footnote. It starts a series of three
footnotes that add nothing informational, but just give an
idea of how footnotes work and look. It is a wordy one, just
so you see how a longish one plays out.
84
possibilities—and thus the expressive musical potential of these instruments—were extremely limited” [20]. While the focus of these critiques centers on specific musical parameters such as lack of dynamic control, the underlying issue is that mechanical performance is less expressive than human performance.   More recent scholarly understandings of musical expression focus on an “intransitive” perspective where the listener embodies musical actions, both visual and sonic [25]. By focusing on the listener’s capacity to generate meaning through the embodiment of musical actions, rather than the transmission of emotion from performer to listener, robotic instruments discover a territory within which to become expressive.   This paper will discuss the distinctions between expressivity and expressive capabilities, as well as musical expression as an embodied experience. Within the context of music for solo robotic instruments as well as collaborative performances and human-robotic improvisation, we introduce the concept of mechatronic expression to differentiate expressive meaning in performances that include robotic instruments from music performed by humans. 2. EXPRESSIVITY VS. EXPRESSIVE CAPABILITY IN ROBOTIC INSTRUMENTS  “Expressivity” is a primary goal in the field of musical robotics. Designers of robotic instruments extensively describe their instruments’ expressive capabilities in terms of the number of controllable sonic parameters as well as the resolution of these parameters. Murphy sums up this sentiment, stating, “…expressivity, in this context, refers to the ability of a mechatronic musical system to affect a wide range of musical parameters…” [23]. Murphy et. al equate parameters of sound control, or “degrees of freedom,” with expressivity, “To achieve musically interesting expressivity from mechatronic instruments, many degrees of freedom are needed” [24]. Many other designers of robotic instruments similarly equate musical expression and sonic nuance, including [14, 15, 19, 26, 30, 33, 34].    The focus on sonic nuance as a means towards greater expressivity parallels the development of digital musical instruments (DMIs), where the discussion often centers on the idea that increased parametric control and resolution leads to a more expressive instrument [1, 21]. This reasoning stems from the idea that performers encode expressive intentions using expressive-related cues, which are articulated via musical parameters such as tempo, volume, articulation, timbre, etc. [28].     Gurevich and Treviño critique this notion, stating, “this kind of argument conflates an ambiguously defined expressive content with the means by which it is expressed” [12]. Dobrian clarifies the relationship between expressivity and expressive capabilities with relation to DMIs, “These musicians [DMI performers] feel that the expression comes from the performer, and the instrument enables—and ideally facilitates and amplifies—that human expression. Although we may speak of an ‘expressive instrument’ for the sake of brevity, it is important to recognize that we usually mean ‘an instrument that affords expression,’ that is, ‘an instrument that enables the player to be expressive” [8]. While both robotic instruments and DMIs possess parameters of sound control that produce sonic nuance, DMIs are designed to “facilitate and amplify” the expressivity of human performance.   Robotic instruments perform music autonomously without human performers exercising expressive control. Under the transitive view of expression, autonomous robotic instruments assume the role of conveying emotional meaning in music. Because machines cannot feel that which can be expressed and the connections to those humans who designed, built, programmed, and composed for those machines is often distant, robotic performances are challenged to realize the same kind of expressivity interpreted in human performances. 
3. RECONSIDERING MUSICAL EXPRESSION IN ROBOTIC INSTRUMENTS  3.1 Musical Expression as Embodied Experience  As described in the introduction, the Romantic-era notion that the performer transmits emotion through music continues to be the prevailing way musical expression is understood. However, contemporary studies of musical expressivity focus on an “intransitive” perspective that is rooted in phenomenology, including embodiment theory, structuralism and communication theory [25, 35]. Rather than considering musical expression as transmitting meaning from performer to listener, the actions of musical performance are embodied by the listener. Paddison argues that all theories of expression dating back to Aristotle are in fact mimetic, and specifically refers to Adorno’s notion of ‘mimetic impulse’ as “[carrying] with it the idea of an embodied, biological and physiological impulse” [25].   Recent studies of musical gesture have similarly described musical expression as an embodied experience [11, 13, 18]. Such theories represent a logical extension of the mimetic/embodied understanding of musical expression that Paddison defines. Cadoz and Wanderley describe a bidirectional, gestural channel of communication, where it is impossible to dissociate action from perception [6]. Performative gestures can convey emotional meaning, for example analysis of someone’s gait can determine if they are happy or sad [7]. In addition to studies of gesture and embodiment, the area of affective computing represents an interesting avenue towards greater understanding of the expressive capabilities of machines. Picard argues that computers can express emotions, although they themselves do not possess emotions as long as they have channels over which to communicate, such as a voice or images on a screen [27].   The understanding of gesture and embodiment as the pathway for intransitive expressive communication suggests that listeners create meaning by embodying both visual-sonic and purely sonic gestures. In this view, listeners create musical meaning based on their interpretation and embodiment of sensory stimuli. Shifting the focus from performance to perception implies that meaning conveyed through musical performance emerges in the listener based on their experience of a performance rather than being transmitted through an act of one-way communication by the performer. This perspective problematizes the notion that human performers are inherently more expressive than robotic instruments. By understanding musical expression in its intransitive context, resulting from embodied experience, robotic music is liberated from the necessity to mimic human performers in order to be considered expressive. Performances by humans and robots may be different; however, each is capable of conveying expressive meaning. 3.2 Robotic Musicianship Weinberg and Driscoll’s concept of robotic musicianship presents a framework to consider the expressive meaning of performances with robotic instruments, including not only the capability for sonic nuance, but the visual aspects of performance as well as our ability to imbue meaning by anthropomorphizing these instruments [36]. Breton and Weinberg describe the creative goals of musical robots as distinct from humans, ranging, “from utilizing compositional and improvisational algorithms that humans cannot process in timely manner…to exploring mechanical sound production capabilities that humans do not possess (from speed to timbre control), robotic musicians bear the promise of creating music that humans could never create by themselves and inspire humans to explore new and creative musical experiences, invent new genres, expand virtuosity, and bring musical expression and creativity to uncharted domains” [5].  Robotic instruments are capable of such musical gestures as hyper-virtuosic speed, complex rhythms, humanly impossible articulations, 
85
non-idiomatic gestures such as rapid trills on the lowest and highest note of the instrument, and algorithmic control [17]. They also possess their own idiosyncrasies and limitations that vary by instrument, such as micro-variations in timing caused by physical forces including friction and gravity [16]. When these idiosyncrasies are explored in robotic music, they can be evocative for a listener. This relationship is highlighted as robotic instruments diverge from mimicking human performers and explore their own vocabulary and limitations. For example, audiences may perceive the “struggle” of a robotic striking arm performing a hyper-virtuosic rhythmic passage. Its rapid movement is translated to the rest of the instrument, causing it to shake, the materials to strain, and mechanical noises to increase. Discussing his robotic percussion instrument, MahaDeviBot, Kapur describes how such imperfections can “humanize” a robotic instrument. He writes, “MahaDeviBot seems to have a personality—albeit one that changes each time it was reassembled—in its imperfections” [16].   The visual component of robotic instruments provides another important avenue for conveying expression [36]. Several robotic instrument designers have sought to increase the expressivity of their instruments by designing them to look humanoid, for example the robotic rock bands Compressorhead, Captured by Robots, and Z-Machines, as well as [34]. Humans tend to anthropomorphize even non-humanoid robots [10], therefore a listener may imbue a non-humanoid robotic instrument with human-like qualities. For example, the tangents that change pitch on the Expressive Machines Musical Instruments (EMMI)’s Poly-tangent Automatic multi-Monochord (PAM) are often referred to as fingers. Other visually expressive elements can be added to robotic instruments, such as the inclusion of LEDs on the EMMI’s AMI and CARI robots [31].  4.  MECHATRONIC EXPRESSION IN MUSIC FOR ROBOTIC INSTRUMENTS  4.1 Mechanical Struggles in Mecxpression Study 1 Mecxpression study 1 (2017) for robotic percussion and live processing by Steven Kemper represents the first piece in a series that specifically explores mechatronic expression. This piece features the solenoid-driven robotic percussion instrument, Configurable Automatic Drumming Instrument (CADI), originally designed by EMMI, and revised by Kemper. Contact microphones are attached to several of the solenoids, which directly amplify the mechanical sounds of the striking arms. Condenser microphones are used to amplify and process the sounds of the percussion instruments.   Mecxpression study 1 highlights CADI’s mechanical gestures, including rapid attacks, repetitive patterns, mechanical noise, and a “broken machine” aesthetic. One of the recurring ideas in this piece is the exploration of performance failure in robotic instruments. At several moments throughout the piece, the velocity values are purposefully set high enough to cause striking arms to move, but too low to enable them to hit the instruments. This is a common “error” in performance of solenoid-driven robotic instruments, especially robotic percussion, due to solenoid actuators’ nonlinear dynamic response [22]. Small misalignments between striking arms and percussion instruments can also cause this problem.  As the piece develops, striking velocities increase in a way that allows the arms to hit their intended targets. At the moments where the velocities are too low to strike the instrument, the arm appears to be wiggling in space, causing anxiety for the audience as to whether the machine is functioning properly. In this case the robotic instrument can be understood as expressing a sense of “struggle,” creating tension while the audience “roots” for the robotic instrument to succeed in striking its target, and release when the percussion instrument is finally struck. 
4.2 Expressivity via Human-Robot Collaboration One way of enhancing the expressivity of a musical robot is to allow it to interact with a human performer in the sound-making process. Scott Barton has experimented with this idea with the creation of Cyther, a human-playable robotic zither. A human addresses the instrument from the top of the strings while from the bottom, strings are excited and dampened by electromechanical means. In one formulation, the machine is responsible for pitch and rhythm while the human modulates the strings in order to shape timbre. In another, the human and the machine play strings together. The result is one where human and machine are both responsible for a part of the sonic product, further, each requires the other to create the emergent whole. Volition and physical capabilities coalesce in the shared medium of the instrument. The experience, and thus the instrument that enables it, is inherently collaborative [2].  Collaborative instruments such as Cyther allow kinds of musical expression that are not possible through either human-played acoustic instruments or autonomous musical robots. The dexterity and degrees of freedom of our movement coupled with the sensitivity of our auditory and haptic systems allow us to shape sound in a way that is extremely difficult to realize through mechanical means. At the same time, the temporal precision, speed and physical idiosyncrasies of machines can produce musical statements that humans cannot, thus revealing mechatronic expressivity. Putting the two together in collaborative instruments allows these unique characteristics to interact to produce an emergent expressivity that is only possible through human-machine symbiosis: a cyborg expressivity. These possibilities have exciting creative potential and elevate us above the contemptuous debate of whether or not machines are replacing humans. 4.3 Human-Robot Improvisation An improviser is both performer and audience (particularly in free improvisation). One must listen to the other in order to make choices about what to say next. This scenario is particularly interesting when the other is a robot (a machine that can sense, interpret and respond accordingly). In the last five years or so, Scott Barton has devoted considerable time to the development and exploration of computational systems that allow human-robot improvisation [3]. These experiences (as a system designer, co-performer and audience member) have shown the ability of machines to convey and inspire expressivity. From the intransitive perspective, we understand co-location and collaboration as a facilitator for expressivity. The motions and sounds of a robotic percussion ensemble compel a human guitarist to entrain to the resultant rhythms. The process of synchronization is both a perceptual and a cognitive one, where periodicity, accent, and displacement are felt as much as they are known. As the gestures of the robot are embodied in the human collaborator, the machine’s expressive potential is revealed. This subsequently inspires the human’s own expressivity, which is perhaps down a path previously unexplored. The character of this experience, as is the case with collaborative robotic instruments, is one where human and machine expressivity meet and converge via embodiment’s connective and revelatory powers. 5. CONCLUSION AND FUTURE DIRECTIONS Considering expressivity from an intransitive, embodied perspective avoids associations between musical performance and the transmission of emotional meaning from performer to listener. This liberates robotic instruments from the necessity to attempt to reproduce the types of sonic nuance traditionally associated with human performances and provides a pathway to understand music made with robotic instruments as expressive on their own. This paper has focused primarily on the concept of mechatronic expression from aesthetic and 
86
creative perspectives. In future work, the authors hope to evaluate the listener’s experience of different approaches to expressivity in robotic music, for example using the structured interview approach of Fyans et al. [9]. Additionally, we hope to develop methodology to assess expressivity as a design rationale in the area of robotic instruments and explore Zappi and McPherson’s concept of dimensionality and appropriation in music created for robotic instruments [37].  6. REFERENCES [1] D. Arfib, J-M. Couturier, and L. Kessous. Expressiveness and Digital Musical Instrument Design. Journal of New Music Research, 34, 1 (Jan. 2005) 125–136. [2] S. Barton et al. Cyther: a human-playable, self-tuning robotic zither. In Proceedings of the 2017 New Interfaces for Musical Expression conference, Copenhagen, Denmark, 2017, 319-324. [3] S. Barton. The Human, the Mechanical, and the Spaces in between: Explorations in Human-Robotic Musical Improvisation. In Musical Metacreation: Papers from the 2013 AIIDE Workshop (WS-13-22), Northeastern University, Boston, Massachusetts, 9-13. [4] A. Bonus. The Metronomic Performance Practice: A History of Rhythm, Metronomes, and the Mechanization of Musicality. Ph.D. Thesis, Case Western Reserve University, Cleveland, OH, 2010.  [5] M. Bretan and G. Weinberg. A Survey of Robotic Musicianship. Communications of the ACM 59, no. 5 (2016), 100–109. [6] C. Cadoz and M. Wanderley. Gesture – Music. In Trends in Gestural Control of Music, by Marcelo Wanderley and M. Battier (Paris: IRCAM, 2000). [7] A. Camurri et al. Multimodal Analysis of Expressive Gesture in Music and Dance Performances. In Camurri and Volpe, G., eds. Gesture-Based Communication in Human-Computer Interaction. New York, Springer-Verlag, 2004, 20-39.   [8] C. Dobrian and D. Koppelman. The ‘E’ in NIME: musical expression with new computer interfaces. In Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France, 2006, 277- 282. [9] A. Cavan Fyans et al. Examining the Spectator Experience. In Proceedings of the 2010 New Interfaces for Musical Expression conference, Sydney, Australia, 2010, 451-454.  [10] S. Fussell et al. How People Anthropomorphize Robots. In HRI’08, Amsterdam, Netherlands, 2008, 145-152.  [11] R. Godøy and M. Leman. Musical Gestures: Sound, Movement, and Meaning. Routledge, 2010. [12] M. Gurevich and J. Treviño. Expression and Its Discontents: Toward an Ecology of Musical Creation. In Proceedings of the 7th International Conference on New Interfaces for Musical Expression (NIME ’07), 106–111. [13] R. Hatten. Interpreting Musical Gestures, Topics, and Tropes: Mozart, Beethoven, Schubert. Indiana University Press, 2004. [14] E. Hayashi. Automated Piano: Techniques for Accurate Expression of Piano Playing. In Musical Robots and Interactive Multimodal Systems. Springer, Berlin, Heidelberg, 2011, 143–163. [15] A. Kapur et al. Integrating Hyperinstruments, Musical Robots & Machine Musicianship for North Indian Classical Music. In Proceedings of the 7th International Conference on New Interfaces for Musical Expression (NIME ’07), 238–241. [16] A. Kapur et al. Collaborative Composition for Musical Robots. Journal of Science and Technology of the Arts 1, no. 1 (2009), 49. [17] S. Kemper. Composing for Musical Robots: Aesthetics of electromechanical music. Emille: The Journal of the Korean Electro-Acoustic Music Society 12 (2014), 25-31. 
[18] M. Leman et al. Sharing Musical Expression Through Embodied Listening: A Case Study Based on Chinese Guqin Music. Music Perception: An Interdisciplinary Journal 26, 3 (2009), 263–278. [19] A. Lim, T. Ogata, and H. Okuno. Towards expressive musical robots: a cross-modal framework for emotional gesture, voice and music. EURASIP Journal on Audio, Speech, and Music Processing 2012, 1, 3. [20] L. Maes, G-W. Raes, and T. Rogers. The Man and Machine Robot Orchestra at Logos. Computer Music Journal, 35, no. 4, 28–48 (2011), 28-29. [21] E. Miranda et al. Artificial Evolution of Expressive Performance of Music: An Imitative Multi-Agent Systems Approach. Computer Music Journal 34, 1 (2012), 80–96. [22] J. Murphy, A. Kapur, and D.Carnegie. Better Drumming Through Calibration: Techniques for Pre-Performance Robotic Percussion Optimization. In Proceedings of the 2012 conference on New interfaces for musical expression. [23] J. Murphy. Expressive Musical Robots : Building, Evaluating, and Interfacing with an Ensemble of Mechatronic Instruments. Ph.D. Thesis, Victoria University of Wellington, 2014, 1. [24] J. Murphy, et al. Expressive Robotic Guitars: Developments in Musical Robotics for Chordophones. Computer Music Journal 39, 1 (2015), 59–73. [25] M. Paddison. Mimesis and the Aesthetics of Musical Expression. Music Analysis 29, no. 1–3 (2010), 126–48. [26] H. Park et al. A Study about Sound Quality for Violin Playing Robot. Procedia Computer Science, 56, (2015), 496-501. [27] R. Picard. Affective Computing, Cambridge: MIT Press, (2000), 3. [28] C. Poepel. On Interface Expressivity: A Player-based Study. In Proceedings of the 2005 Conference on New Interfaces for Musical Expression (NIME ’05), 228–231. [29] A. Richards. Automatic Genius: Mozart and the Mechanical Sublime. Music & Letters 80, 3 366–89 (1999) p.383. [30] G-W Raes. Expression Control in Automated Musical Instruments. http://logosfoundation.org/g_texts/expression-control.html, accessed January 24, 2018. [31] T. Rogers, S. Kemper, and S. Barton. MARIE: Monochord-Aerophone Robotic Instrument Ensemble. In Proceedings of the 15th International Conference on New Interfaces for Musical Expression (NIME) (Louisiana State University, Baton Rouge, LA, 2015). [32] R. Scruton. Expression: II: The Nature of Musical Expression. Grove Music Online. Oxford Music Online. Oxford University Press, accessed January 24, 2018.  [33] K. Shibuya. Violin Playing Robot and Kansei. In Musical Robots and Interactive Multimodal Systems. Springer, Berlin, Heidelberg, 2011, 179–193.  [34] J. Solis and A. Takanishi. Wind Instrument Playing Humanoid Robots. In Musical Robots and Interactive Multimodal Systems. Springer, Berlin, Heidelberg (2011), 195–213. [35] R. Stecker. Davies on the Musical Expression of Emotion. The British Journal of Aesthetics 39, 3 (July, 1999), 273–81.  [36] G. Weinberg and S. Driscoll. Toward Robotic Musicianship. Computer Music Journal, 30, no. 4 (2006), 28–45. [37] V. Zappi and A. McPherson. Dimensionality and Appropriation in Digital Musical Instrument Design. In Proceedings of the 2014 New Interfaces for Musical Expression conference, London, UK, 2014, 455-460. 7. Video Examples Mecxpression study 1: https://youtu.be/FPvlIppwjj8 Human-robot improvisation (Cyther):  https://youtu.be/n7y2tibpF0w   
87