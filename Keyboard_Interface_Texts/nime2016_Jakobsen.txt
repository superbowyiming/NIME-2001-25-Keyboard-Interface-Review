Hitmachine: Collective Musical Expressivity for Novices  Kasper Buhl Jakobsen Aarhus University Aabogade 34, DK-8200 Aarhus N, Denmark kasperbj@cs.au.dk  
 Jakob Winge Aarhus University Aabogade 34, DK-8200 Aarhus N, Denmark jakobwinge@gmail.com  
 Marianne Graves Petersen Aarhus University Aabogade 34, DK-8200 Aarhus N, Denmark mgraves@cs.au.dk  ABSTRACT This paper presents a novel platform for expressive music making called Hitmachine. Hitmachine lets you build and play your own musical instruments from Legos and sensors and is aimed towards empowering everyone to engage in rich music making despite prior musical experience. The paper presents findings from a 4-day workshop where more that 150 children from ages 3-13 built and played their own musical instruments. The children used different sensors for playing and performed with their instruments on stage. The findings show how age influenced the children’s musical understanding and expressivity, and gives insight into important aspects to consider when designing for expressive music for novices.  Author Keywords Tangible Interaction; Interactive Music; Workshop; Prototyping; Lego; Children; Novice Music Making  ACM Classification H.5.2. User Interfaces; H.5.5 Sound and Music Computing 1. INTRODUCTION When playing in a band the best result is achieved when everyone listens to each other and collaborates. However, it is difficult to be part of such collaboration without prior musical experience and extensive practice on a musical instrument. When sitting down with the piano for the first it is almost impossible to play great sounding music, because not only do you have to know how it works but you also have to practice proper coordination. To some, the task of mastering an instrument can be too overwhelming and unmanageable, thus many people never realize that music might also be for them. In schools music lessons often use traditional instruments, leading to repeating situations of the same children playing the instruments every time while the rest are merely playing egg shakers or triangles etc. New interfaces for musical expression bridging the physical and digital world are challenging the notion of what constitutes a musical instrument and open up for unlimited possibilities of designing how to play music.  
 Figure 1. Examples of instruments. Left: The instrument is played by pointing a color sensor towards the colored bricks. Right: The instrument is played by moving ones hands in front of two distance sensors.  In this paper we present Hitmachine, a platform for tangible music making for novices. Hitmachine was contextualized in a 4-day  
 
 Figure 2. Children are playing on stage. Two pairs with each their instrument are playing each their lead melody to shared background music.  workshop at an elementary school, where more than 150 children built and played their own musical instruments. The instruments were wireless and built from Lego and Lego Mindstorms sensors (See Figure 1), and could afterwards be played on stage to a background beat (See Figure 2). The children ranged in age from 3-13 and we elaborate how age influenced their musical understanding.  2. RELATED WORK ON TANGIBLE MUSIC INTERFACES Designing tangible user interfaces for allowing musical expression is part of the core research in music technology. Existing projects have a variety of different foci and operate on multiple different scales ranging from wearable or handheld  artifacts to tabletops and even large-scale room installations and differ in whether they are aiming at single or multiuser engagement. In the following overview on existing work we distinguish between what we call rhythmic music involving notes and rhythms and abstract music constituted of more soundscape like sounds.  Tabletops are a popular way of facilitating tangible music making, some of the first being Audiopad [16] and Jam-O-Drum [2]. The most famous is ReacTable [8] that allows musicians to experiment with sound, change its structure and control its parameters using physical pucks. ReacTable creates rhythmic music from the arrangement and manipulation of the pucks. Similar projects are RadarTable [18] and Spela Wip [20]. A less real-time rhythmic music tabletop is NotePut [14] where physical blocks shaped as notes are placed on the table, creating a score that defines a rhythm and melody. This score is then played back by the sound of an instrument.  Some examples of wearable interfaces are MuMYO [15] and Smart Hand [12]. MuMYO is an armband that reacts on movement, and even though it is easy to control how it makes sound, it is difficult to control the order of the notes. Smart Hand is a glove where certain areas on the fingertips and palm react to touch from the other hand. It lets the user produce rhythmical music like a traditional musical instrument it does not provide any rhythmical aid.  Other tangible interfaces are handheld like Tenori-On [13] that keeps a steady rhythm and generates a melody from finger presses and Noisa [19] that creates dynamic rhythmical patterns and lets the user manipulate the pitch. Other handheld examples 
241
are Caress [11] that lets the user play percussion on finger pads and Tingle [4] a pin-art toy like device that mainly creates abstract music. Tingle has an 8-bit configuration where it is possible to produce notes.  And a lot of existing music making interfaces does, like traditional musical instruments, take practice to master. Bucket System [3] is an improvisational interface for experts, and like ReacTable it is designed for musicians. However, unlike ReacTable, the Bucket System uses traditional instruments as input device meaning you have to be able to play an instrument in advance to use it. Using traditional instruments as input device are also seen in the keyboard-based LiVo [21] and ChordEase [10]. In LiVo the keys are mapped to Japanese syllables, thereby making keypresses generate artificial singing, yet you still have to be able to maneuver a piano. ChordEase tries to make sophisticated musical expression easier by laying out jazz chords to be played only by using the white keys. The Installation called Interactive Musical Fruit [5] also tries to make playing easier. It is designed for novices and lets them play by manipulating the orientation of ‘fruits’. Though, in terms of playing actual rhythmic music, the notes are chosen randomly, removing all control of the melody. Two examples of large-scale installations for creating abstract music. Resonate [9] lets users collaborate in making a soundscape by playing a net of white strings connected to the ceiling and MotionComposer [1] tracks and interprets dance and movement as sounds.  Finally, Petersen et al. [17] discuss collaborative music making for children where they build their own instruments from Lego bricks supplemented with MakeyMakey and copper-tape to add interactivity to the instrument. However, while construction of the instruments with Lego bricks allowed for rich expressivity, the opportunities for musical expressions were rather limited.  3. DESIGN GOALS The goal of Hitmachine is to empower children to collectively make expressive music without the need for prior musical skills. It should enable them to collaborate in building their own musical instruments as well as express themselves through playing them.   The aforementioned examples of tangible music making interfaces all deal with musical expressivity in some form, but not many are focused on designing for musical expressivity for novices. By using Lego we draw upon experience shared by a lot of children, creating an easy entry into the musical universe. The kind of expressivity we aimed for was for the children to be able to play real quality music making actual melodies and not only striking random notes. We wanted them to play with real quality sounds as used by professional producers, and not only the sound of simple midi-instruments. When playing a traditional instrument like the piano or the guitar, it is difficult to avoid dissonance without having practiced that particular instrument. So one of our main challenges was how to create an experience of playing something that sounds good, despite that this usually takes a lot of practice. We define sounding good as avoiding dissonance and untight rhythmical strokes, while having elements that resemble the hit songs in western mainstream culture. However, we are aware that it ultimately comes down to taste what sounds good, so western popular music is just the chosen musical paradigm.  We wanted Hitmachine to facilitate live music making through exploration and improvisation, having the kids actually play the their instruments instead of having the system play back something after it was done or made.   In order to empower the children to play their instruments live in a way that would create great sounding music, we chose 
to provide a form of tonal and rhythmical aid, but unlike with e.g. Guitar Hero [6] we wanted the children to not only play along, but to create their own original music. We wanted no visual cues distracting the participants from listening, and the choice of melody and rhythm should be more free. In addition to that we focused on making this a collaborative experience centered on the sound and the other participants instead of having a screen as the center of attention.  4. TECHNICAL IMPLEMENTATION Hitmachine is designed to empower children to collectively make expressive music. It is a platform and not only a system because it provides the tools and materials necessary for the children to build their own instruments, instead of using finished designs. 4.1 Physical Setup Hitmachine is designed to be situated in a room including a creator table with 8x2 headphones, where participants can build their own instruments and listen to their sounds in headphones without background music. In the same room is a stage where they can bring their instrument and connect it to speakers for all the others to hear (See Figure 3).  
 Figure 3. Physical setup of creator table and stage. In front of the stage are two stands with iPads.  On stage the children perform with their instruments and play a lead melody to the background beat generated in the Beat Builder. On each side there are speakers playing the background music. The instruments are connected to the speakers by selecting the corresponding color on one of two iPads placed in front of the stage (see Figure 4).  
 Figure 4. Two iPads were used to connect instruments to the stage. The colors correspond to the color labels on the 8 instruments. In this figure the red instrument is connected to the stage.   A maximum of two groups can perform on the stage simultaneously, so only one color can be chosen on each iPad at a time. When an instrument’s color is highlighted on a tablet everything that is played by that instrument now comes from the speakers along with the background music. When the color is deselected again or another color chosen instead, the sound of the instruments goes back into the headphones. In this way participants can easily and quickly move back and forth between creator table and stage and continuously rebuild or make changes to their instruments.  The two spaces are separated with themed lighting. In the creator space there is white light illuminating the table so that the children can clearly see the building parts (See Figure 12). 
242
In the stage space, lights are creating the atmosphere of a concert venue with colored lights moving and flashing to the beat and big loudspeakers as well as an elevated stage with space for an audience in front of it (See Figure 13).   Figure 5 shows an overview of central components in Hitmachine. Lego Mindstorms technology is used for the instruments – 8 pre-programmed Mindstorms units (EV3’s) including sensors, hooked up wirelessly to a backend that interprets sensor input and converts it to musical notes. A server handles the communication between EV3s and the musical framework. In addition to this, Hitmachine also includes the Beat Builder web application, that is a backend into the musical framework that lets us control central elements of the music including the creation of a background beat (will be explained in more detail in section 4.4). The instruments sound like lead synthesizers and can be played as part of a greater whole to the background beat. The possible notes are chosen in a way that ensures they always fit well together and in addition they are also quantized in time to always fit within a predefined rhythmical pattern. Thus you can create great sounding music without prior experience.  
 Figure 5. Central components and architecture of Hitmachine. 4.2 EV3: Lego Mindstorms Unit The Lego Mindstorms EV3 (3rd generation computer modules in the Evolution product line) is running Linux on an SD card and is pre-programmed to receive and interpret the sensor data. Five different sensors provide handles into the music and another sensor can change the timbre of the instrument. The handles control each their separate musical aspect. The five sensors for music making are a pushbutton, a long-range distance sensor (ultrasound), a short-range distance sensor (infrared), a color sensor, and a gyroscope. The sensor for changing the timbre is a servomotor used as a potentiometer and will from here on be referred to it as the sound changing wheel (See Figure 6).  
 Figure 6. Five sensors for playing music and a wheel for changing sound. The EV3 has four ports in each end. On one side these ports are marked as numbers 1, 2, 3, 4 and on the other side as letters A, B, C, D. Sensors for playing should be connected to the numbers and the sound changing wheel should be connected to 
either of the letters. The four ports have distinct functionalities and provide different handles into the musical framework (See Figure 7). The sensor in port 1 determines which note to play and the sensor in port 2 determines when to strike it. The sensor in port 3 is used to add another note, so if sensors are connected in both port 1 and 3 it is possible to play two different notes at the same time, thereby creating harmonics. The sensor in port 4 is used to choose between two different octaves for the notes to be played in. Finally, by turning the sound changing wheel you can choose the timbre of the sound among more than 100 different synthesizers. The EV3 is programmed to be able to handle all combinations of sensor-port pairings, so the actual way of playing is highly customizable. Each EV3 unit has a Wi-Fi dongle enabling wireless transmission to the server, so they can be carried around. 
 Figure 7. Functionality of the ports regarding the music. 4.3 The Musical Framework When playing on stage, the children played their lead melodies to a background beat. Hitmachine provides both a tonal and rhythmical aid, to help the children when performing. The musical framework define the rules for how the sensor input is mapped to a musical output, and ensures that the lead melody will fit to the background music.  4.3.1 Tonal Aid The tonal aid is based on eliminating the possibility to play certain notes. Often this is done, by limiting the playable notes to a pentatonic scale. However, we wanted to enhance the musical expressiveness, and therefore increased the possible playable notes in certain situations. The main priority was to do so without also increasing the difficulty of playing something that harmonized with the background music. Thus, the possible notes were chosen as the pentatonic scale corresponding to the scale of the background music plus the notes of the chord that are currently playing in background music. As an example, take the situation where the background music is in the scale of C major/A minor, with a chord progression of Am, C, G, F. At all time the notes within the pentatonic scale are playable for the lead, which is C, D, E, G, A. In addition to this when the chord G major is played, the note B is added as a possibility, and likewise when the chord F major is played the note F is added. This is because the other notes in G major and F major are already in the pentatonic scale. For G major they are G and D and for F major they are A and C. When the chords A minor and C major are played, no extra notes are added because these chords are only constituted of notes, that are also within the pentatonic scale. For A minor it is A, C, E and for C major it is C, E, G. Figure 8 depicts the situation where the chord F major is played. Notice how the note F is added in addition to the pentatonic scale.  
 Figure 8. When the chord F is played in C major. Grey notes constitute the pentatonic scale, blue are the notes in the chord F and green are the note (F) added to the scale. 
243
4.3.2 Rhythmical Aid    In addition to the tonal aid, Hitmachine also provides a rhythmical correction. The musical framework is built in such a way, that an algorithm takes every input and converts it into an output at the nearest “correct” timing according to the rhythm of the background music. This means that even though a note is stricken too early it will first be played back at the following correct place in time.   The correct timing is defined by what we call the main rhythm. This rhythmical pattern is created by the workshop facilitators through the Beat Builder, which is an interface for controlling the backend of Hitmachine. Hitmachine only uses with common time (4/4), and the main rhythm can be created with the precision of 16th notes. The pattern is created by clicking and thereby coloring the desired 16th notes with the mouse (See Figure 9). The colored notes represent the possible places in time for the children’s lead instruments to play. If all 16 fields are colored, the lead can strike notes on every 16th note, if every 2nd field is colored they can strike notes on every 8th note and so on. Figure 9 depicts a situation where only the third of every sixteen 16th notes are playable for the lead.  
 Figure 9. Screenshot from Beat Builder showing 16 rhythm fields. Green fields are part of the main rhythm and white fields are not.  The pattern does not have to be regular or symmetric, but no matter how it is chosen, it will be looped bar after bar. The main rhythm does not only restrict when notes can be stricken but also when they can be released. Figure 10 shows three examples of different possible lengths of a note that all start at the same time but end at different times either at the end or the beginning of one of the available notes in a certain main rhythm.  
  Figure 10. The blue bars indicate the main rhythm. The red bars indicate three different possible timings for a note to be released.  The rhythmical correction means that the instruments are not necessarily played in real time, but with a tiny delay, corresponding to how much the player is “off” in relation to the main rhythm. To compensate for this delay to some degree the algorithm makes what we call forgiving correction, which means that there is a short time buffer (the correction buffer zone) in the beginning of each possible note in the main rhythm. If a note is stricken within this buffer zone, it is played back immediately, instead of at the next correct place in time (See Figure 11). However, the time buffer is so short that most people will not be able to hear that the note is not tight.  4.4 How the Beat Builder Works The Beat Builder is the backend interface into the musical framework and works as a generative tool to create background 
 
 Figure 11. Forgiving correction. The grey fields are part of the chosen main rhythm and the white are not. If a note is stricken within the timeframe of the crosshatched areas the note will be played immediately instead of queued.  music for the children’s lead instruments. It is designed to be controlled by the workshop facilitators and not the children, and is a simplified Digital Audio Workstation (DAW). The Beat Builder makes it possible to choose the form of the musical track that is the order of verses, bridges, drops, and choruses. This order can be varied freely. The background music is based on a scale and chord progression along with a main rhythm for all the music to follow. The scale and chord progression are chosen from a set of popular chord progressions identified by hooktheory.com [7] through analysis of 6768 popular pop songs. This limits the diversity of progressions, but ensures that the chords resonate with something most people in western culture are familiar with. The creation of the main rhythm is described above, but the main rhythm does not only define when the lead instruments can play, but is also the rhythmical foundation for the background music. However it is also possible to separate the two and e.g. allow the lead to strike any 16th note. This provides more expressivity but is harder to control. The background music consists of several musical layers. These layers are drums, chords, bass, and strings. Each layer is restricted to follow the main rhythm but does not need to use every note. Thereby one rhythm can be created for the drums within the main rhythm while another rhythm is created for the bass. For each layer the timbre of the instruments can be chosen among several different software instruments. For the drums a whole drum kit is chosen and then the rhythm of the kick, snare, hihat, and ride cymbal can be programmed individually. The framework also allows for crashes to be placed at certain places, e.g. right before a chorus. The chords can be played by pianos, guitars, flutes, or a range of polyphonic synthesizers, and each note in each chord can be varied in length. The timbre of the bass can be chosen among several vst-bass-instruments or synthesizers. Finally, strings can be added as a way to provide extra sound and energy to the chorus. Furthermore the Beat Builder also lets you define the overall tempo and makes it possible to apply filters to gradually dull the sound of certain instruments. The comprehensive sound library combined with the all the possible ways of changing the music facilitate the creation of almost infinitely many different background beats, and even make it possible to aim towards specific genres by combining typical temporal and rhythmical traits of e.g. pop, hiphop, EDM, or dubstep.  5. WORKSHOP A 4-day workshop was conducted at an elementary school. During 4 days Hitmachine was tested with 11 different age groups ranging from 3-13 resulting in a total of more than 150 children. Each age group attended the workshop separately.  Each day contained 2-4 sessions and the length of each session varied for each age group ranging from 20 minutes for the youngest and 3 hours for the oldest (An average of 1.5 hours). 
244
 Figure 12. Children are building instruments in pairs at the creator table and testing sounds with headphones.   All workshop sessions were divided into two phases. Firstly, a building/testing phase (See Figure 12) where the children were assigned the task of building instruments in pairs and testing them. Secondly, a concert phase where the groups then took turns performing on the stage with their final instrument to the Hitmachine background music track while the rest acted as a crowd (See Figure 13). Each group was instructed to explain their instruments’ sensor mappings (How they played it).  
 Figure 13. Two girls are playing on stage. Colored, flashing, and moving disco lights creates a more concert like atmosphere. The children were divided into pairs, each getting one of the 8 EV3 units to build on. They were then shown a demonstration of how to connect a sensor and given the task to build musical instruments together. Each EV3 was marked with a color corresponding to the color of one of 8 sets of headphones lying on the table.   The physical setup allowed all of the groups to iterate several times between building at the creator table and performing on stage. 5.1 Data and Analysis During the workshop we took pictures of all instruments, gathered video from the general activities as well as from interviews with the groups of children regarding how they experienced the activity. In addition, we took field notes.  To analyze this, we developed a qualitative data analysis tool that allows for web based sorting and tagging and linking of pictures and videos. There already are a variety of existing programs, but the reason we developed our own was to be able to link pictures to videos. This way we could tag all pictures of instruments and link them to corresponding videos where they were actually used. Tags could then be searched making it easy to find a specific picture with video among the over 500 pictures and 12 hours of video material from the workshop. 6. EXPERIENCES FROM THE WORKSHOP Overall the workshop was a success in terms of engaging the children in collaborative play with music and empowering novices in participating alongside their more musically experienced classmates. All children were intensively engaged in both building instruments and performing with them on stage. Not only did we experience the children playing but also laughing, dancing and singing a lot. Several children told us 
that the workshop was fun and that they had never tried anything like it. In the interviews with the teachers, they told us that they had never seen some of the more shy children be so engaged also including children without prior musical experience. One boy said “I’ve never played music but I can play this in a way” and another boy said “This is the best music I’ve ever made”. These are examples from more than 100 quotes alike.  6.1 The Influence of Age on Musical Comprehension We identified some interesting differences between the various age groups regarding how they played. There was a clear tendency for the sophistication of play to increase with age. Some of the younger children were sometimes happily engaged in performing with their instruments on stage while interacting with the sensors in a way they did not react to. E.g. treating a distance sensor like a color sensor or activating a sensor that changed notes without striking these notes at all. The older children seemed to be more capable of identifying their own role and impact. Some of them just spammed the sensors with input, but for several children we saw that they slowly figured out how to follow the rhythm when they played. The older children were able to collaborate on making music, and explored this by switching roles on stage to sometimes be the one in charge of choosing notes and other times of hitting them. Through the interviews we discovered that some children could not convincingly differ between higher or lower pitches. Again we saw a correlation between the number of children in a group that could differentiate between notes and their age, the oldest being most able. We did not experience any gender specific differences. 6.2 Abstract Musical Aspects The Hitmachine is designed to let the children manipulate certain musical aspects through the sensors, while others are left to the Beat Builder. The children could change notes of the lead with port 1, strike them with port 2, add another note with port 3, and change the octave with port 4. Furthermore they could change the timbre of the sound with the sound changing wheel. However, they could not e.g. change the dynamics (differences in volume), bend notes, or deviate from the predefined scale and rhythm. In the case of Hitmachine we saw, that none of the children understood the possibility of playing more than one note at a time on the same instrument by using both port 1 and 3. At the same time no one was able to properly use port 4 for changing octaves. We saw sensors in port 4 be activated as if they were the ones for striking notes, which indicates that the concept of different octaves might have been to abstract or non-transparent in the design. However, some might have been able to hear the difference without us knowing and their reason for not using it could e.g. be that it was not interesting enough. Several of the younger children also turned the sound changing wheel continuously as a part of their play. 
  Figure 14. One boy is choosing notes by pointing a color sensor while the other continuously changes the sound by turning the wheel. This resulted in a chaotic musical output, with the timbre of the lead synthesizer changing every second (See Figure 14). It 
245
illustrates, that some musical aspects are easier to comprehend that others. As a lot of the sounds were radically different, it was easier for the children to identify a change in timbre than a change in pitch. It engaged them despite how it sounded because they were able to identify that their actions made a difference. However, the older children told each other to stop turning the wheel. E.g. a girl said to her partner “Don’t touch the wheel, I just found our sound”. This indicates that they thought of the sound as something to be found and then maintained while playing. We emphasize that the differences in age were large, ranging from 3-13, and that the musical understanding seemingly increased gradually with age.    When designing for tangible music making it is the designer’s task to adequately choose which aspects should be accessible through the handles and how. More handles into the music means more aspects to control. This creates more freedom, but at the same time increases the difficulty. So an important design task is to balance this control with the expressivity, to best foster musical engagement. To do this designers of interactive musical interfaces have to not only be experts in hearing the differences in musical aspects themselves but also in identifying which aspects are comprehendible for others.  7. CONCLUSION   This paper presents a platform for tangible music making for novices called Hitmachine, along with findings from a 4-day workshop where it was situated in an elementary school setting. Hitmachine provides tonal aid and rhythmical correction to empower people without prior musical experience to create great sounding music. The platform is focused on the musical paradigm of modern western popular music, and contains a musical framework that aligns the output to fit with well-known traits of this type of music. Over the course of the 4 days, more than 150 children were engaged in building their own musical instruments from Legos and Lego Mindstorms sensors as well as performing with them on stage. We found that age had a significant impact in regard to how sophisticated the children were playing.  8. ADITIONAL AUTHORS Jeppe Stougaard, Aarhus University, Aabogade 34, DK-8200 Aarhus N, Denmark, jstougaard@gmail.com  Jens Emil Grønbæk, Aarhus University, Aabogade 34, DK-8200 Aarhus N, Denmark, jensemil@cs.au.dk Majken Kirkegaard Rasmussen, Aarhus University, Aabogade 34, DK-8200 Aarhus N, Denmark, mkirkegaard@cs.au.dk  9. REFERENCES [1] Bergsland, A. and Wechsler, R. 2015. Composing Interactive Dance Pieces for the MotionComposer, a device for Persons with Disabilities. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA. [2] Blaine, T., Perkis, T., 2000. The Jam-O-Drum Interactive Music System: A Study in Interaction Design, in: Proceedings of the 3rd Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques, DIS ’00. ACM, New York, NY, USA, pp. 165–173. [3] Dahlstedt, P., Nilsson, P. A., Robair, G. 2015. The Bucket System - a computer mediated signaling system for group improvisation. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA. [4] Duindam, R., Scharrz, D., Leeuw, H. 2015. Tingle: A Digital Music Controller Re-Capturing the Acoustic Instrument Experience. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA. 
[5] Erkut, C., Serafin, S., Fehr, J., Figueira, H.M.R.F., Hansen, T.B., Kirwan, N.J., Zakarian, M.R., 2014. Design and Evaluation of Interactive Musical Fruit, in: Proceedings of the 2014 Conference on Interaction Design and Children, IDC ’14. ACM, New York, NY, USA, pp. 197–200. [6] GuitarHero. http://www.guitarhero.com/. [Accessed: 01-Feb-2016]. [7] Hooktheory. http://www.hooktheory.com [Accessed: 01-Feb-2016] [8] Jordà, S., Kaltenbrunner, M., Geiger, G., Alonso, M., 2006. The reacTable: A Tangible Tabletop Musical Instrument and Collaborative Workbench, in: ACM SIGGRAPH 2006 Sketches, SIGGRAPH ’06. ACM, New York, NY, USA.  [9] Knichel, B., Reckter H., Kiefer P., 2015. resonate - a social musical installation which integrates tangible multiuser interaction. NIME’15, May 31-June 3, 2015, Louisiana State Univ., Baton Rouge, LA.  [10] Korda, C. 2015. ChordEase: A MIDI remapper for intuitive performance of non-modal music. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA.  [11] Momeni Ali, 2015. Caress: An Enactive Electro- acoustic Percussive Instrument for Caressing Sound. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA.  [12] Myllykosky, M., Tuuri, K., Viirret, E., Louhivuori, J. 2015. Prototyping hand-based wearable music education technology. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA.  [13] Nishibori, Y., Iwai, T., 2005. TENORI-ON, in: ACM SIGGRAPH 2005 Emerging Technologies, SIGGRAPH ’05. ACM, New York, NY, USA.  [14] Noteput http://www.jonasheuer.de/index.php/noteput/ [Accessed: 01-Feb-2016].  [15] Nymoen, K., Haugen, M. R., Jensenius, A. R. 2015. MuMYO - Evaluating and Exploring the MYO Armband for Musical Interaction. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA.  [16] Patten, J., Recht, B., Ishii, H., 2006. Interaction Techniques for Musical Performance with Tabletop Tangible Interfaces, in: Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology, ACE ’06. ACM, New York, NY, USA.  [17] Petersen, M.G., Rasmussen, M.K., Jakobsen, K.B., 2015. Framing Open-ended and Constructive Play with Emerging Interactive Materials, in: Proceedings of the 14th International Conference on Interaction Design and Children, IDC ’15. ACM, New York, NY, USA, pp. 150–159.  [18] Radartable http://cavi.au.dk/research-areas/radartable/. [Accessed: 01-Feb-2016]. [19] Tahiroğlu, K., Svedström, T., Wikström, V., 2015. NOISA: A Novel Intelligent System Facilitating Smart Interaction, in: Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’15. ACM, New York, NY, USA, pp. 279–282. [20] Spela Wip http://play.moveplaycreate.com/post/34583919219/inte ractive-music-table-spela-wip. [Accessed: 01-Feb-2016]. [21] Yamamoto, K., and Igarashi, T. 2015. LiVo: Sing a Song with a Vowel Keyboard. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA.
246