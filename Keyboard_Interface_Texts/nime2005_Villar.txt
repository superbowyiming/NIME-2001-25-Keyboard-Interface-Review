Pin & Play & Perform
A rearrangeable interface for musical composition and performance
Nicolas Villar, Adam T . Lindsay , Hans Gellersen
Computing Department
Infolab21, Lancaster University
United Kingdom
{villar, atl, hwg }@comp.lancs.ac.uk
ABSTRACT
We present the Pin&Play&Perform system: an interface in
the form of a tablet on which a number of physical controls
can be added, removed and arranged on the ﬂy. These con-
trols can easily be mapped to existing music sofware using
the MIDI protocol. The interface provides a mechanism for
direct manipulation of application parameters and events
through a set of familiar controls, while also encouraging a
high degree of customisation through the ability to arrange,
rearrange and annotate the spatial layout of the interface
components on the surface of the tablet.
The paper describes how we have realized this concept us-
ing the Pin&Play technology. As an application example, we
describe our experiences in using our interface in conjunc-
tion with Propellerheads’ Reason, a popular piece of music
synthesis software.
Keywords
tangible interface, rearrangeable interface, midi controllers
1. INTRODUCTION
Music software can oﬀer most the functionality of a whole
studio in a single computer for a fraction of the cost. But
much of the advantage is lost when the physical interface dis-
appears and is replaced by a graphical replica. The mouse
and keyboard are generic input devices, and although eﬀec-
tive for word processing and graphic design oriented tasks,
they are not ideally suited for musical editing and compo-
sition. We propose a type of interface that leverages the
beneﬁts of current software, the standardized MIDI proto-
col and novel technology to realize a system that aims to
improve usability and experience of digital music manipula-
tion.
We have designed and built an interface that allows the
free placement, arrangement and ‘on the ﬂy’ rearrangement
of a set of physical controls on the surface of a tablet which
is connected to a desktop computer. These physical controls
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
Nime’05 May 26-28, 2005, V ancouver, BC, Canada
Copyright remains with the author(s).
can be mapped onto a set of graphical user interface elements
on an existing music application with MIDI input capabili-
ties. Physical manipulations and screen-based ones can be
intermixed without any constraint and without confusing
the application because our interface appears as a standard
MIDI controller device to the application, which does not
need to be modiﬁed in any way. The physical interface el-
ements can be actively rearranged while the application is
running.
Conceptually our system is based on a framework for tan-
gible interaction which consists of a set of building blocks in
the shape of interactive objects that can be easily and nat-
urally manipulated. It also provides a simple way in which
these building blocks can be associated with graphical con-
trols on the interface of an application.
This paper provides a brief overview of previous work in
this area, and discusses how our system builds upon and
addresses some of the shortcomings of existing systems. We
then describe our realized system, Pin&Play&Perform, and
the technology used to implement it. Finally, to demon-
strate the concept in practice, we describe how the interface
was used in conjunction with an existing piece of commercial
music software.
2. BACKGROUND AND MOTIV A TION
2.1 Tangible user interfaces
The possibilities tangible user interfaces (TUIs) were ﬁrst
demonstrated in the work of Fitzmaurice, Ishii, Ullmer and
Buxton in [2] and [3]. The theory suggests that new in-
teraction modalities are to be found by exploiting the rich
aﬀordances provided by our physical environment, includ-
ing our architectural surroundings and the physical objects
within it.
Many compelling examples of TUIs can be found in the
domain of music control, creation and performance as in
the Audiopad [6], ReacTable [4], Block Jam [5] and the
Squeezables [10]. This is not surprising, since the alterna-
tive interaction modalities can often be more useful, intuitive
or enjoyable to a musician than the traditional keyboard
and mouse, which are better suited to word processing and
graphic design tasks.
2.2 The case for ﬂexibility
The experimental musician and innovator John Bowers
has used the termperformance ecologyto describe the arena
for activity created by a musician in his immidiate surround-
ings. In his work regarding the ethnographically-informed
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
188
Figure 1: Example layout with ﬁve channel busses
design of improvising machines [1], he discusses the impor-
tance of the spacial arrangment of control devices and in-
struments - not only in allowing the musician to be more
eﬀective in his performance, but also in communicating his
intentions to co-performers and audience. The ability to ef-
fectively organize the layout of these ecologies is a recurring
theme throughout his work.
In the theoretical paper ‘Towards a Musician’s Cockpit’
Vertegaal et al. [8] make the case for the importance of
customization in musical systems. Instruments should be
physically ﬂexible to adapt to the changing needs of a per-
former, but this functionality should also be ‘freezable’ to
allow the system to be properly internalized and learnt.
The theory has been realized in the form of the SensOrg
system [9], which consists of an arrangement of physical
modular devices and music software that provide a high-
degree of customization and adaptation to allow a suﬃcient
level of usability in an electronic musical system. These in-
tentions are particularly evident in one of the components of
the system - the Flexipad - that allows a number of buttons
and faders to be positioned and oriented on a metal pad
to comfortably ﬁt the position and size of the performer’s
hand.
We believe that this work makes a strong case for taking
adaptability as a central concern in the design of a musical
interface, and one that is not clearly reﬂected in much of
the current work. With this in mind, we have used our
own platform for tangible interaction, called Pin&Play, to
implement a type of highly ﬂexible tangible interface for
interacting with existing music software.
3. REARRANGING THE MUSICAL INTER-
FACE
With Pin&Play&Perform we set out to develop an inter-
face which allows the easy addition, removal, arrangement
and on the ﬂy rearrangment of physical controls on a tablet.
The physical forms of these controls are familiar to users of
musical equipment, and how they are operated should be
obvious.
The idea is that these controls can be freely arranged on a
surface of the tablet according to their intended purpose and
the personal ergonomic preferences of the user. A scratch
mix may require only volume sliders, arranged in a horizonal
array to emulate a mixing desk (e.g., as seen in Fig. 1). Fine-
tuning one or two tracks could call for a diﬀerent conﬁgura-
tion, emulating a single channel bus, with additional rotary
knobs controlling eﬀects parameters placed above the chan-
Figure 2: Example layout with two “channels” and
a cross-fader
nel volume slider (Fig. 2). A user may choose to cross fade
between two tracks with a horizontally-placed slider, a con-
ﬁguration that is mechanically impractical in commercial,
non-DJ mixing desks, but easily supported by our system.
Flexibility is the most distinguishing characteristic of our
interface. New controls can be added, removed and freely
laid out on a surface according to the functional and er-
gonomic needs of the user. Once in place they are ﬁrmly
attached, allowing the interface arrangement to remain sta-
tic when required, which is an important requirement of an
interface if its functionality is to be properly internalized
and learnt. The set of available controls are familiar in-
put devices with an easily recognizable operative aﬀordance
(pressing, turning and sliding). The user does not need to
learn how to operate the controls, only remember (or spec-
ify) what a particular control does.
4. PIN& PLA Y TECHNOLOGY
The current Pin&Play technology used to realize our sys-
tem has been developed from a concept originally published
in [7], and is the result of ongoing work into developing this
technology as a usable platform and accompanying develop-
ment toolkit. The system is characterized by an augmented
surface, which provides data connectivity and physical sup-
port to tangible interactive artifacts that can be added to
or removed from the surface. When an artifact is added,
it becomes connected and acquires a digital representation.
When it is removed, it retains its state and form. An ar-
tifact may provide other interaction capabilities that allow
it to be manipulated while it is on the surface (input), or
express some information (output).
A Pin&Play surface consists of two conductive fabric sheets
separated by a layer of isolating rubberized foam. One of
the conductive layers is connected to the signal line of a
network interface, which carries both power and data sig-
nal. The second conductive layer is connected to the the
return to ground. This allows the surface to act as a sort of
two-dimensional network and power line.
Pin&Play interactive objects (nodes) can be attached to
the surface by means of a specially designed coaxial pin con-
nectors (Figure 3). These connector can pierce the several
layers that make up the surface, providing physical attach-
ment as well as a power and network connection to the node.
Each node can provide its own interface by means of me-
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
189
Figure 3: Pin&Play surface and node design.
chanical controls or sensors, while other nodes that contain
actuators such as lights or simple displays can be used to
express basic output.
The software programming interface of the system allows
the detection and unique identiﬁcation of each of these nodes
as they are attached and removed from the surface. When-
ever a node is added to the surface, an equivalent virtual ob-
ject is created. Whenever one of the nodes is manipulated,
an event is issued by the virtual object with information
about its new state. This provides a straightforward, event-
based programming interface for application developer, with
events such as “Slider node with ID 1234 has been added”,
“Button node with ID 5678 has been removed” or “Dial node
with ID 1010 has been rotated to 50%” being issued. These
events can then be linked to actions or further processing by
the application. The nodes also have the ability to signal on
request, which results on a small bright light built onto the
physical node being turned on or oﬀ.
5. PIN&PLA Y&PERFORM
The Pin&Play&Perform is intended to be used as a com-
plementary form of control to an application, alongside its
traditional mouse-controlled GUI environment. In our pro-
totypes we have used a wooden tablet augmented with a
Pin&Play surface of about 30cm x 30cm. This size allows
the tablet to be easily moved around, handled or placed on
a desk surface (Figure 4). The tablet on which the controls
are arranged can be of considerably larger or smaller size,
depending on the requirements. It may even be vertically-
placed (e.g. wall mounted) as the controls stay securely
attached thanks to the pin connectors. The surface is con-
nected to the PC via a USB link.
The interface kit also contains a ‘control bag’ a with num-
ber of button, dial and slider controls. The controls can be
freely arranged and oriented on the surface of the tablet. It
is also possible to make annotations about the controls on
the surface by using a normal whiteboard marker.
5.1 Functionality
Pin&Play&Perform is developed on top of the Pin&Play
platform. It consists of an additional software layer that
translates node events into MIDI control commands. The
program masquerades as a generic MIDI controller, which
can be recognized as a valid MIDI input device by the Win-
dows platform. Whenever a new node is attached to the
Figure 4: Pin&Play&Perform in operation.
tablet, a virtual MIDI object is created. If the node con-
tains a button, then the MIDI object is created as a Note.
Pressing and releasing the button will result in NoteOn and
NoteOﬀ events being issued. In the case of the node con-
taining a slider or dial control, the MIDI object takes the
form of a MIDI controller with a variable value of 0 to 127.
This assignment of controller numbers to controls should
be transparent to the user, but if required, a small applica-
tion window can be called up on the PC to display the cur-
rent status of the surface: how many and which controllers
are present, and what their MIDI mapping is.
Exactly how the linking of the MIDI controllers to the
input of a music application is carried out is very much
determined by the capabilities of the application itself. In
this respect, we found the mechanism used by Reason to be
particularly easy to use and well suited as a demonstrator
of our concept.
5.2 Reason integration
Reason, by Propellerhead software, is a mature audio soft-
ware suite that emulates a variety of diﬀerent real-world
audio devices. These devices may be connected with one
another in a variety of diﬀerent ways, and controlled with
a “realistic graphical interface” that mimics the function of
the physical devices. Hundreds of parameters may be con-
trolled on-screen with the GUI.
The program has a very simple way of linking a MIDI
controller with an on-screen control: selecting ”Edit MIDI
Remote Mapping” from the ”Options” menu causes all con-
trols to which a MIDI controller can be assigned to be high-
lighted with green arrow. Clicking on any of these controls
causes the window in Figure 5 to appear. To create the link,
a user can at this time manipulate any control on the sur-
face (e.g. turning a dial by a few degrees) which will then
be understood by Reason as the controller to be used for
this particular control. Selecting ”OK” completes the link-
ing process and the control can be used straight away. The
physical control can repositioned or reoriented and the link
will be maintained. To break the link, the user must delete
the MIDI mapping.
The physical controls were in large part chosen because
of their similarity to the on-screen controls. However, it
is not necessary that a physical slider be used to control
an on-screen slider-like control. A user, according to their
personal preference, may want to use a physical rotary dial
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
190
Figure 5: Assigning a MIDI controller to an on-
screen control of the Reason GUI.
to carry out the same action. In this case, the mapping
remains the same: the minimum and maximum of position
of the dial (clockwise and counter-clockwise) are equivalent
to those on the slider (left and right). If a button is linked
to an on-screen dial or slider the mapping is not so obvious,
but is still allowed and works. In this case pressing the
button once will cause the control to go to its maximum
state and pressing it again will take it to its minimum state.
If the reverse happens – linking of a dial or slider to an on-
screen button – the behavior is that whenever the control
is rotated or translated, regardless of direction, the state of
the on-screen button will be toggled.
6. DISCUSSION
We set out to design and build an interface to improve
the usability and experience of a musician working with
existing music software. In particular, we have given the
user the ability to deﬁne their physical performance and
composition environment, and use this as a complementary
channel of control alongside the traditional graphical inter-
face of the application. As it does not seek to replace cur-
rent ways of working with music software, but rather ex-
ist alongside it to provide the tangible beneﬁts, we imagine
that Pin&Play&Perform could be easily incorporated into
the existing working environment of an electronic musician.
It will be interesting to see how the interface is used in prac-
tice – the ﬂexiblity will almost certainly allow it to be ap-
propriated and used in ways we have not thought of. We
intend to carry out a series of exploratory studies, where a
Pin&Play&Perform kit is supplied to a number of people
who make regular use of the types of applications it sup-
ports. So far, the informal comments we have received are
encouraging, and future experience and feedback gathering
will inform us futher about the possibilities of our interface.
7. SUMMARY
We have developed a new type of tangible, rearrangeable
interface to be used alongside the graphical environment of
existing music software applications. An overview of pre-
vious work in the area of ﬂexible physical interfaces was
provided in order to highlight its distinguishing character-
istics and potential. Through a combination of existing ap-
proaches at improving the usability of musical applications,
as well as the application of tangible interface technology, we
have realized a working prototype of the interface. Finally,
the feasibility of such a system has been proven by using it
in conjunction with a popular piece of music software.
8. ACKNOWLEDGMENTS
The work described in this paper was carried out as part
of the Equator IRC, funded by the UK’s Engineering and
Physical Science Research Council.
9. REFERENCES
[1] J. Bowers. Improvising machines: Ethnographically
informed design for improvides electro-acoustic music.
http://www.ariada.uea.ac.uk/ariadatexts/ariada4/index4.html.
[2] G. Fitzmaurice, H. Ishii, and W. Buxton. Bricks:
Laying the Foundations for Graspable User Interfaces.
In Proceedings of CHI, pages 442–449. ACM Press,
1995.
[3] H. Ishii and B. Ullmer. Tangible Bits: Towards
seamless interfaces between people, bits and atoms. In
Proceedings of CHI: Human factors in computing
systems, pages 234–241, 1997.
[4] M. Kaltenbrunner, G. Geiger, and S. Jorda. Dynamic
patches for live musical performance. InProceedings of
the 4th Conference on New Interfaces for Musical
Expression (NIME 04), 2004.
[5] H. Newton-Dunn, H. Nakano, and J. Gibson. Block
jam: A tangible interface for interactive music. In
Proceedings of the 3rd Conference on New Interfaces
for Musical Expression (NIME 03), 2003.
[6] J. Patten, B. Recht, and H. Ishii. Audiopad: A
tag-based interface for musical performance. In
Proceedings of Conference on New Interface for
Musical Expression (NIME ’02), 2002.
[7] K. van Laerhoven, A. Schmidt, and H. Gellersen.
Pin&Play: Networking objects through pins. In
G. Boriello and L. Holmquist, editors,Proceedings of
Ubicomp 2002, volume 2498, pages 219–229, Sept.
2002.
[8] R. Vertegaal and M. Kieslinger. Towards a musician’s
cockpit: Transducers, feedback and musical function.
InProceedings of ICMC ’96. ICMA, 1996.
[9] R. Vertegaal and T. Ungvary. The sensorg: A musical
cyberinstrument with cognitive ergonomical touch. In
Proceedings of IEEE SMC’98 Interntational
Conference on Systems, Man and Cybernetics.IEEE,
1998.
[10] G. Weinberg and S. Gan. The squeezables: Toward an
expressive and interdependent multi-player musical
instrument. Computer Music Journal, 25(2):37–45,
July 2001.
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
191