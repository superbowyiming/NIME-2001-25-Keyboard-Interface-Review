Physical Modelling Concepts for a Collection of Multisensory Virtual Musical Instruments  James Leonard  ICA Laboratory Grenoble Institute of Technology  France leonard@imag.com Claude Cadoz  ACROE & ICA Grenoble Institude of Technology France cadoz@imag.fr    ABSTRACT This paper discusses how haptic devices and physical modelling can be employed to design and simulate multisensory virtual musical instruments, providing the musician with joint audio, visual and haptic feedback. After briefly reviewing some of the main use-cases of haptics in Computer Music, we present GENESIS-RT, a software and hardware platform dedicated to the design and real-time haptic playing of virtual musical instruments using mass-interaction physical modelling. We discuss our approach and report on advancements in modelling various instrument categories, including physical models of percussion, plucked and bowed instruments. Finally, we comment on the constraints, challenges and new possibilities opened by modelling haptic virtual instruments with our platform, and discuss common points and differences in regards to classical Digital Musical Instruments.   Author Keywords Physical Models, Virtual Musical Instruments, Force-Feedback Systems, Multisensory Interaction.  ACM Classification H.5.5 [Information Interfaces and Presentation] Sound and Music Computing– Modelling, H.5.5 [Information Interfaces and Presentation] Sound and Music Computing– Systems, H.5.2 [Information Interfaces and Presentation] User Interfaces– H a p t i c  I/O.   1. INTRODUCTION Over the course of the last two decades, Digital Musical Instruments have established themselves as both a dominant research topic in Computer Music, and as widespread tools for musicians and composers to expressively manipulate sound synthesis processes and audio content, as demonstrated in numerous events such as NIME. Countless new gestural interfaces allow for novel ways of controlling sound, breaking the constraints of mechanical musical instruments. However, in their very essence these systems strongly differ from traditional instruments, leading some authors to question if they can, in fact, be called “instruments” [1].   Indeed, a traditional instrument is a mechanical object that transforms mechanical energy into acoustical energy: the musician exerts physical effort on the mechanical object, and experiences global feedback (haptic, aural, visual) from the instrument, including the haptic perception of the mechanical sound vibrations. In contrast, the classical DMI architecture [2] is separated into a gestural controller destined to capture various features of a user’s gesture and a sound synthesis process that produces sounds whose parameters are controlled by the captured gestural data according to mapping strategies. So how can we confer to these digital instruments the feel and tangibility of real instruments? A possible answer, and a growing 
topic in the computer music community, is to employ physically-based methods to model an instrument and force-feedback devices to interact with its simulation.    In this context, we recently proposed GENESIS-RT [3], a system that allows designing virtual musical instruments using mass-interaction physical modelling, simulating these instruments in real time and interacting with them using force-feedback t r a n s d u c e r s .  Instruments created with GENESIS-RT are multisensory, i.e. the user interacts haptically with a single model that provides the instrument’s complete mechanical, acoustical and visual behaviour. This paper proposes a first look back over a series of models, or rather a series of physical modelling concepts that have emerged through our collective experience designing haptic instruments with GENESIS-RT, covering several emblematic instrumental categories such as struck, plucked and bowed instruments as well as some more adventurous concepts.  First, we will discuss the specifics of a multisensory approach to haptic interaction with virtual musical instruments, compared to a classic multimodal scheme. Section 3 will then briefly present the GENESIS-RT platform, including some key new features and describing the general modelling and simulation workflow. We will then focus on the modelling concepts for designing various types of haptic musical instruments, accompanying a more formal approach with several examples of instruments designed with the platform, and conclude. 2. USING HAPTICS FOR TANGIBILITY OF VIRTUAL MUSIC INSTRUMENTS Haptic systems are increasingly present in new musical interfaces, as systems become more affordable and as more and more studies show the benefits they yield in terms of control and manipulation [4, 5]. Generally speaking, the addition of haptics to a DMI can be aimed at (1) providing various information to the user in the form of a “haptic display” to enhance t h e  c o n t r o l  o f  a  D M I ,  o r  ( 2 )  m a k i n g  a  virtual instrument more tangible by allowing the user to interact gesturally with a haptic representation of some or all of its mechanical features. We are specifically interested in the latter, and will comment on two approaches to representing the virtual instrument: using several local models for the mechanical interface, the sound synthesis and the visualization (multimodal approach) and using a single physical model to represent the whole instrument (multisensory approach).   2.1 Multimodal, local model approach Following the classic DMI architecture, it is possible to augment the gestural controller with haptic feedback in order to enable haptic perception of certain components of the instrument. We can separate two different sub-categories: (a) Programming the mechanical behaviour of the gestural control section using a local haptic model. Information of the interaction between the user and the haptic model is then mapped (possibly arbitrarily) to sound synthesis parameters. Some examples are the Virtual Piano Action [6], or the DIMPLE software [7] in which the user interacts with a rigid dynamics model, of which  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without f e e  p r o v i d e d  t h a t  c o p i e s  a r e  n o t  m a d e  o r  distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. NIME’15, May 31-June 3, 2015, Louisiana State University, Baton Rouge, LA. Copyright remains with the author(s).  150
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
information is then send via OSC to any sound synthesis process. (b) Adding direct haptic feedback of the sound synthesis section to the gestural controller, such as Nichols’ vBow friction driven haptic interface [8] or Marshall’s vibrotactile feedback incorporated into a DMI controller [9].  One can note that although these systems tend to unify the gestural control section and the sound synthesis, the sound is still driven by mapping of sensor data and the physical interaction is still local to a subsection of the instrument.  Technical implementation relies on asynchronous computation loops for haptics and sound, generally using low to mid-priced haptic systems such as the Phantom Omni1 and the Novint Falcon2.  We call this approach multimodal, since it addresses the haptic, aural and sometimes visual modalities with stimuli that are, by principle, physically decoupled from one-another due to the control-based architecture of the mapping stage. 2.2 Multisensory global model approach Another option to implement haptic DMI is to model a virtual instrument a s  a  multisensory physical object, jointly bearing visual, mechanical and acoustical properties. Physically-based models are needed for this purpose.  With such a model, gestural controller and sound synthesis sections are completely bound and interdependent: any haptic interaction with one part of the instrument will affect it as a whole and the player is haptically coupled with the whole single model. We can distinguish: (a) Systems aimed for simple interfacing of the user with a physical simulation by using relatively standard haptic devices. Works such as [10, 11, 12, 13] enable haptic interaction with a sound-producing physical model. However, the computation scheme and hardware technologies are comparable to those of standard DMIs: the haptic devices employed are generally fairly cheap models, limited in terms of reactivity and peak force feedback, and the computation of the interaction is done in soft real time, often relying on event based protocols such as OSC [11] or MIDI [10]. Indeed, while they do allow for direct haptic interfacing with a physical model, these systems do not strive for rigorous and coherent exchange of physical energy between the musician and the virtual instrument. (b) Haptics for instrumental interaction. These systems [14, 15] aim to obtain a physical coupling between musician and simulated instrument as close as possible to a real world physical interaction, with an accurate representation of the exchange of energy between the two (Cadoz refers to this as the ergotic function of a gesture).  To achieve this physical coupling, technical solutions rely on high performance haptic interfaces and synchronous high-speed computational loops. In doing so, such systems aim to capture the feel, playability and expressiveness of acoustic instruments while a l s o opening to the novel possibilities of physical modelling sound synthesis. Our presented work, named GENESIS-RT, fits into this final category, with the dual aim of offering a high-level, musician-friendly physics-based modelling environment in which users can design virtual musical instruments, and allowing multisensory interaction with the designed instruments by                                                                  1 http://geomagic.com/en/products/phantom-omni/overview/ 2 http://www.novint.com/index.php/novintfalcon 
simulating them on a dedicated multisensory simulation platform offering coherent aural, visual and haptic feedback.  3. PREVIOUS WORKS AND THE GENESIS-RT SYSTEM GENESIS-RT constitutes a first platform for the modelling and multisensory real time simulation of virtual musical instruments, using concepts and technologies for physical modelling and real time haptic simulation developed at ACROE-ICA. In this section, we will briefly describe each of them and discuss how they have been adjusted and combined into our new platform. 3.1 The CORDIS-ANIMA formalism CORDIS-ANIMA [16] is a physical modelling and simulation formalism based on mass-interaction networks. It is the basis for all physical models developed at ACROE-ICA. It presents itself as a number of elementary modules of two types: MAT (material points such as masses) and LIA (interactions between material points, among which a large collection of interaction types, both linear and non linear). Models are built by assembling MAS and LIA modules, as shown in Figure 1.    
  Figure 1. CORDIS-ANIMA computational scheme 3.2 The GENESIS software GENESIS [17] is ACROE’s modelling and simulation software for musical creation with CORDIS-ANIMA, allowing modelling vibrating objects (from elementary oscillators to complex musical scenes) and simulating them off-line at 44.1 kHz, in order to capture their acoustic behaviour.  GENESIS implements a one-dimensional version of CORDIS-ANIMA, meaning that all MAT physical modules move along a single scalar axis (x by convention). The modelling interface presents itself as a bench representing the y-z plane, where MAT modules can be placed and connected together with interactions (see Figure 2). Note that y-z coordinates on the bench do not affect the behaviour of a LIA connecting two MAT modules, as all physical calculations are based on their respective scalar positions along x.   
  Figure 2. Representation of a physical model in the GENESIS bench.  Modules can be given physical parameters that dictate their physical behaviour, and initial conditions. A list of the different physical modules in GENESIS is given in Table 1. From here on, modules will be referred to by their three letter acronyms in GENESIS.  When simulated, GENESIS models are displayed on a 3D projection of the bench, as shown in Figure 3, allowing observing displacements of the MAT modules along the perpendicular x-axis. The sound output from a GENESIS model is simply an observation of the position or force along x occurring at one or more points in the model over time. 151
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
 Figure 3. Simulation of a GENESIS model, showing displacement along the x axis. 3.3 The TGR Haptic Device Although GENESIS was designed for off-line simulation, real time instrumental interaction with CORDIS-ANIMA models has been a key concern of ACROE-ICA since their beginnings. In particular, this has led to the design and refinement of the TGR (transducteur gestuel rétroactif) haptic device, focusing on high dynamic response, high peak force feedback (up to 200 N) and a modular slice-based design permitting adaptation to various types of gestures: 1D piano-keys (cf. Figure 4), 3D joystick, bowing system, etc. More details concerning the haptic device and the associated electronics can be found in [18].  In tandem with the haptic device itself, dedicated electronics provide the necessary power for the actuators and conditioning for the sensor signals, and the platform is connected to a real-time digital simulation system allowing interfacing the haptic device with a high rate synchronous simulation of the physical model.  
  Figure 4. The TGR Haptic Device 3.4 GENESIS-RT GENESIS-RT, initiated in 2012, is our first attempt to connect the high performance haptic workstations cited above with the GENESIS high-level modelling tools, allowing using composer-friendly means to design haptic virtual musical instruments which are then compiled into optimized simulation code and can be played in real-time with the TGR. Considerable effort was put towards metrological aspects, such as system calibration and physical correspondences between the user’s real world and the instrument’s virtual world, of which details can also be found in [3]. 3.4.1 First Prototype The initial hardware architecture for the physical simulations was a TORO DSP board, designed by Innovative Integration, on which simulations can be interfaced with the haptic inputs and outputs in a completely deterministic computational loop at 44.1 kHz, with single-sample latency. While the computing power of the DSP is fairly limited, it has been sufficient for the design several models, displaying the variety of possible physical interaction with virtual instruments and the richness and subtlety in the resulting sounds. 
Table 1. List of Physical Modules in GENESIS 3.4.2 New features and extensions Since the first prototype, several modifications and enhancements have been done to GENESIS-RT, namely: • The ability to model and simulate synchronous multi-rate physical models, in which mechanical sections of the instrument and the haptic loop are simulated at 1-10 kHz and the acoustic components are simulated at 44.1 kHz. • Implementation of this multi-rate simulation scheme on a new hardware platform, based on a Hard Real Time Operating System (RTOS). This has increased attainable model complexity by a factor of 50 compared to the TORO architecture. Models of approximately 5000 audio-rate physical modules can now be simulated in real time at 44.1 kHz, with mechanical sections and the haptic loop running at 4.41 kHz, including models containing structural non-linear interactions in the acoustic components.  • Integration into a new release, including all standard GENESIS features along with additional functionalities for multi-rate haptic modelling and simulation. An extensive insight into these new aspects is planned for future publication. However, they are not central for understanding the process of modelling and playing virtual musical instruments, which is the aim of this paper. We will simply remark that the increased computing power allows for larger and more complex models, with no noticeable change in the haptic ‘feel’ of the instruments. 4. MODELLING VIRTUAL INSTRUMENTS WITH GENESIS-RT Creating physical models for multisensory interaction in the GENESIS-RT environment is similar to classic modelling with GENESIS, especially concerning the design of vibrating sections. The TGR device is integrated directly into the CORDIS-ANIMA model as a series of MATTGR modules (one for each allocated 1D haptic key in our case).   In standard GENESIS, virtual musicians and gestures are created within the physical model, for instance by launching a MAS with a given speed towards a vibrating structure using a BUT collision interaction, or using an infinitely heavy MAS moving at constant speed to bow a structure via an LNL (non-linear) interaction. In this case the main concern is the resulting 
Name Parameters Description <MAT> Physical Modules MAS Inertia Punctual mass SOL None Fixed point CEL Inertia, Stiffness, Damping Elementary physical oscillator <LIA> Physical Modules RES Stiffness Elasticity FRO Damping Viscosity REF Stiffness, Damping Visco-Elasticity BUT Stiffness, Damping, Threshold Visco-Elascticity conditioned by a position difference LNL f(position) lookup table f(speed) lookup table General non-linear interaction In/Out Modules SOX None Fixed point. Sends position of connected <MAT> to output SOF None Fixed point. Sends received force to output 
152
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
sound produced by the vibrating structure, which allows for many convenient arrangements concerning the nature and scales of the exciting systems. However, in the case of modelling haptic virtual instruments several additional factors have to be taken into account: (a) Models should (must?) be designed as stable passive physical systems, into which energy can be injected via the user’s gestures in a repeatable and controlled fashion, as is the case in real instruments. Not meeting these requirements can result in undesirable and potentially dangerous instabilities of the haptic device. (b) The mechanical feel of the instrument perceived by the player is at least as important as the sound that results of the interaction between the two. This means taking into account the correspondences and mechanical impedance matching between the real world and the simulation to adapt the dual constraints of position range and force-feedback range according to both the model and the interaction(s) with it. (c) Connecting models to the real world introduces new problems, such as noise on the sensor data. If left untreated, this noise pollutes the interaction with models and the resulting sound, making them unusable. A first batch of approximately 20 virtual instruments has been implemented with this new platform, exploring both the variety of interactions that the user can experience (bowing, picking, pushing, striking, etc.) and the yet unchartered domain of complexity permitted for the simulated physical models of the instruments. Below, we will comment on our experience with modelling and simulating certain classes of musical instruments with the system, accompanying the explanation of several models with sound and video examples3. 4.1 Percussion-based instruments Percussion based instruments are one of the most common categories of musical instruments, and incidentally some of the easiest to model using GENESIS-RT.   4.1.1 Piano-inspired mechanics A simplified model for piano-like action, first shown in [3], has been refined and extended. Each TGR key is connected to a small mechanical apparatus that implements the key frame, a small hammer connected to the key, a backcheck to avoid double bounces of the hammer and (optionally) a damper to mute the vibrating structure once the key is released. The hammer can be connected via a BUT interaction to any kind of vibrating structure. Figure 5 shows a 12 key version in which each hammer is connected to a tuned vibrating structure.  
 Figure 5. Model based on piano-inspired mechanics exciting tuned acoustic structures                                                                  3 Link to video/audio content: https://youtu.be/mBKuhlsfsMs 
The vibrating structures are composed of two MAS-REF chaplets connected to a SOL on one end and left open at the other. The hammer strikes the first chaplet, which then excites the second one via a viscous FRO interaction, resulting in a progressive envelope. First and second chaplets are tuned slightly apart, producing a natural beating in the sound.  4.1.2 Percussion of drum-like instruments Percussion can also be used as a means to directly strike a structure, as if using a hammer or a drum-stick. In this case the main difficulty in obtaining a clean hit is the natural bounce of the hammer/drumstick, which is somewhat tricky to obtain when using the piano-key morphology of the haptic device. This can be solved by modelling weakly attached vibrating structures, such as cymbals on stands or suspended structures, in which case the structure can slowly sway back and forth when struck, avoiding involuntary damping or bounces that can occur if the contact between the TGR key and the structure is sustained.  
 Figure 6. Direct percussion of a suspended plate model, weakly attached in the two top corners.  For direct contact with a vibrating structure such as the above and for plucked and bowed instruments, we employ small propagation lines (MAS-REF) between the MATTGR modules and the vibrating structure, essentially to reduce the transmission of sensor noise while maintaining rigid physical coupling. 4.2 Plucked instruments Plucking interactions can be modelled simply in GENESIS by using non-linear elastic interactions based on a position lookup table with the following profile:   
  Figure 7. Non-linear force/distance profile for a plucking interaction During the first phase, the pick pushes up/down from its initial position (Figure 8a). Then when a critical force is reached the pick “goes through the string”, which then vibrates freely (Figure 8b). The interaction is symmetrical in order to obtain an identical behaviour when picking up or down. 
Contact'Phase' Slipping'Phase'
153
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
We have designed a string model for two handed manipulation that can be plucked using a haptic key with the right hand and also fretted (pinning the string down onto a fret board) at different lengths with several other haptic keys, as shown in Figure 8c. The fretting interactions are composed of BUT interactions between the TGR key and the string, allowing the player to push the string down in several points, and additional BUT interactions between the string and SOL modules placed below it, representing the frets.  The reason for the stair-like placement of the frets (Figure 8c) is simple: in most real stringed instruments fretting gestures are orthogonal to the excitation gesture. However, we are in a 1D space so the excitation of the string (plucking) and the fretting occur along the same axis. If the frets are placed at the same height, pinning the string down against one and plucking it results in rattling against the following frets, causing “fret buzz”. We have placed the frets carefully so the string always vibrates freely with minimal buzzing: just enough for it to sound real and lively, but not enough to make the instrument unplayable. A consequence, however, is that the higher the frets along the neck, the more pressure must be put on the string to pin it down cleanly. Re-implementing this model in a 2D space where plucking and fretting operate more independently would allow for the frets to be evenly levelled.  
  
 
 Figure 8. Plucked string (a) during the interaction with the pick (b) vibrating openly and (c) pinned down on the third fret We have also experimented models using picking mechanisms on different types of vibrating structures, such as coupled strings or large membranes. 4.3 Bowed instruments Bowing interactions are one of the more sensitive cases for modelling and haptic interaction, since (a) the excitation gesture is continuous (sustained interaction between the bow and the string) making it particularly sensitive to any noise issues and (b) the force/velocity conditions in which self-sustained oscillations are obtained can be equally sensitive. A detailed view of self-sustained excitation systems in GENESIS can be found in [19].  In acoustic studies, a bowing interaction is often approached as a classic non-linear force/velocity profile. The pressure of 
the bow on a string modulates this profile in a way that a specific operating point for the strings oscillations may be obtained by several speed/pressure combinations. Florens [14] and others [15] have implemented force-feedback simulations of this scenario.  In the present case, we are primarily interested in obtaining convincing bowing interactions with GENESIS-RT, which is one-dimensional. Consequently, we chose a 1D mechanism that can be controlled by a single TGR key. It is only sensitive to velocity and has no pressure control; therefore oscillating conditions can only be obtained within a specific velocity range. Nevertheless it reveals to be rather realistic in terms of sound and touch and with a little practice one can quite easily trigger the oscillations and maintain them, even when changing the direction of the bow/TGR key.  The non-linear force/velocity profile has also been slightly adjusted (Figure 9) to avoid unwanted excitation of the string by the haptic device’s sensor noise: at speeds very close to zero (in the range of electronic sensor noise) the interaction is null. As soon as the user displaces the bow, he enters the interacting zone where he either dampens the string or excites it depending on velocity. 
  Figure 9. Non-linear force/velocity profile for a bowing interaction We have implemented this bowing interaction on a string-like instrument, shown in Figure 10. It is in fact a beam, made of main central connections and parallel connections every two MAS on either side. The parameters of the parallel REF connections on each side are slightly inhomogeneous, adding to the richness of the sound. The central connections are non-linear springs that have been adjusted so that their stiffness is modified according to the distance between each MAS: when the string is stretched, it increases continuously in pitch.   
 Figure 10. View of the bowed beam during real time simulation.  One TGR key is used to bow the string with the interaction described above, and another key is attached to one end of the string, allowing the user to stretch the string by moving the bridge up and down. 
Damping'Phase'No'force' Excitation'Phase'
154
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
Given some time to practice with the instrument, one can learn to bow the string back and forth while maintaining the oscillations and simultaneously alter the pitch. The informal results are encouraging: completely novice users often experience difficulty in finding the velocity “sweet spot” for the string’s oscillations, however once they have found it and experimented with it, taking time to embody the instrument through the simultaneous sound, visual and haptic feedback, the skill stays with them, developing towards finer control of the bowing sound qualities. As in many multisensory manipulations, cutting back or removing one sensory feedback renders the task much more difficult, and nearly impossible when muting either the haptic feedback or sound.  The model could of course be extended to enhance sound related aspects. Among other things, we are considering adding a sounding board in future versions. 5. PERSPECTIVES: A LIBRARY OF REAL TIME INSTRUMENTS Through the process of crafting various mechanical and acoustical components, we are starting to see the emergence of a library of real-time instrumental components: several instrumental components, in particular mechanical excitation systems adapted to the haptic device, have stabilized into fully re-usable components designed for different musical gestures: piano-based striking system, picks, bowing, fretting or damping, etc. These subsections can then put to use in all sorts of contexts, with all kinds of vibrating structures: plucking or bowing a drum-like membrane, using piano-like mechanics to excite various points a single complex structure, and so forth.  In a way, the functional separation of DMIs exists here, with all the modularity and flexibility that it implies: we build mechanical objects that we can gesturally interact with (the gestural control section) and we connect them to one or several vibrating objects (sound synthesis). However in our scenario the junction between the two is explicitly of a physical nature, not informational. Doing so, we obtain a virtual instrument that preserves complete gesture-to-sound physical coupling, but still benefits from the flexibility of physical modelling sound synthesis (for instance building completely non-existent but physically plausible instruments) and of complete freedom in the design of novel and sophisticated mechanical interfaces for the instrument. Concerning this final point, we feel that we have only scratched the surface, and are eager to pursue our modelling activity. 6. CONCLUSIONS This paper has reported our advancements in modelling a  variety of multisensory virtual musical instruments using the GENESIS-RT platform. Recent developments have greatly increased available computing power, opening possibilities for designing complex and rich instruments, containing both mechanical excitation systems and various types of vibrating structures. While modelling these instruments proves similar in some ways to traditional physical modelling with GENESIS, it presents a number of new challenges and specificities due to the coupling between the real world and the simulated one through the haptic device. In addressing the challenges of this “haptics in the loop” situation, we are now seeing the emergence of several stabilised instrumental components, building blocks that can be used and adapted in various modelling cases. Finally, although our models are specific to CORDIS-ANIMA, we believe their underlying concepts to be sufficiently general to apply to a variety of situations and techniques. 
7. REFERENCES [1] R. Pedrosa, & K. MacLean. Perceptually Informed Roles for Haptic Feedback in Expressive Music Controllers. In Haptic and Audio Interaction Design, 2008, 21-29. [2] M.M. Wanderley, & P. Depalle. Gestural control of sound synthesis. Proceedings of the IEEE, 92(4), 2004, 632-644. [3] J. Leonard, N. Castagné, C. Cadoz & J.L. Florens. Interactive Physical Design and Haptic Playing of Virtual Musical Instruments. Int. Computer Music Conference (ICMC’13), Perth, Australia. (2013) [4] E. Berdahl, G. Niemeyer and J.O. Smith. Using Haptics to Assist Performers in Making Gestures to a Musical Instrument. Proceedings of NIME’09. 2009, 177-182. [5] A. Luciani, J.L. Florens, D. Couroussé, J. Castet. Ergotic Sounds: A New Way to Improve Playability, Believability and Presence of Virtual Musical Instruments. Journal of New Musical Research, Vol 38. 2009, 303-323.  [6] B. Gillespie. The virtual piano action: Design and implementation. Int.Computer Music Conference. 1994. [7] S. Sinclair, M.M. Wanderley. A Run Time Programmable Simulator to Enable Multi-Modal Interaction with Rigid Body Systems. Interacting with Computers, Vol 21. pp: 54--63. (2009) [8] C. Nichols. The vBow: development of a virtual violin bow haptic human-computer interface. In Proceedings of NIME’02. 2002, 1-4. [9] M.T. Marshall and M.M. Wanderley. Vibrotactile feedback in digital musical instruments. In Proceedings of NIME’06. 2006, 226-229.  [10] S. Rimell, D.M. Howard, A.M. Tyrrell, R. Kirk & A. Hunt. Cymatic: restoring the physical manifestation of digital sound using haptic interfaces to control a new computer based musical instrument. Int. Computer Music Conference. 2002. [11] C. Erkut, A. Jylhä, M. Karjalainen, E.M. Altinsoy. Audio-tactile interaction at the nodes of a block-based physical sound synthesis model. Haptic and Audio Interaction Design. 2008, 25-26. [12] E. Berdahl, A. Kontogeorgakopoulos, and D. Overholt. HSP v2: Haptic Signal Processing with Extensions for Physical Modeling. Proceedings of HAID 2010, Copenhagen, Denmark, Sept. 16-17, 2010, pp. 61--62. [13] E. Berdahl, A. Kontogeorgakopoulos. The FireFader Design: Simple, open-source, and reconfigurable haptics for musicians. Proceedings of the 9th Sound and Music Computing Conference. 2012, 90-98. [14] J.L. Florens. Real time Bowed String Synthesis with Force Feedback Gesture. Invited pa-per. 585, Mus. 06, vol. 88, Acta Acustica. (2002) [15] S. Sinclair, M.M Wanderley, V. Hayward and G. Scavone. “Noise-Free Haptic Interaction with a Bowed String acoustic Model”. World Haptics Conference. (2011) [16] C. Cadoz, A. Luciani, J.L. Florens. CORDIS-ANIMA: A Modeling and Simulation System for Sound and Image Synthesis: The General Formalism. Computer Music Journal, Vol. 17, No. 1. pp: 19--29. (1993) [17] N. Castagné, C. Cadoz, A. Allaoui, O. Tache. G3: GENESIS Software Environment Update. Int. Computer Music Conference (ICMC). 2009, 407-410. [18] J.L. Florens, A. Luciani, N. Castagne, C. Cadoz. ERGOS: a Multi-degrees of Freedom and Versatile Force Feedback Panoply. Eurohpatics. 2004, 356-360. [19] F. Poyer, C. Cadoz. Sound Synthesis and Musical Composition by Physical Modelling of Self-Sustained Oscillating Structures. Proceedings of the 4th Sound and Music Computing Conference. 2007, 14-21. 155
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 