GuitarAMI and GuiaRT: two independent yet
complementary Augmented Nylon Guitar projects
Eduardo A. L. Meneses
IDMIL, CIRMMT
McGill University
Montreal, QC, Canada
eduardo.meneses@mail.mcgill.ca
Sérgio Freire
School of Music
Universidade Federal de
Minas Gerais (UFMG)
Belo Horizonte, MG, Brazil
sfreire@musica.ufmg.br
Marcelo M. Wanderley
IDMIL, CIRMMT
McGill University
Montreal, QC, Canada
marcelo.wanderley@mcgill.ca
ABSTRACT
This paper describes two augmented nylon-string guitar
projects developed in diﬀerent institutions. GuitarAMI
uses sensors to modify the classical guitars constraints while
GuiaRT uses digital signal processing to create virtual gui-
tarists that interact with the performer in real-time. Af-
ter a bibliographic review of Augmented Musical Instru-
ments (AMIs) based on guitars, we present the details of the
two projects and compare them using an adapted dimen-
sional space representation. Highlighting the complemen-
tarity and cross-inﬂuences between the projects, we propose
avenues for future collaborative work.
Author Keywords
Augmented Musical Instruments, Guitar, Nylon Guitar,
Hexaphonic Pickup
CCS Concepts
•Applied computing→Performing arts; Sound and
music computing;
1. INTRODUCTION
Augmented Musical Instruments (or AMIs) are created
by embedding sensors and actuators on traditional instru-
ments, allowing the performer to control and modify the
instrument’s sonic output [1].
However, due to physical diﬀerences, it is necessary to es-
tablish diﬀerent approaches to augment electric or acoustic
(including the classical nylon-string) guitars. Throughout
the second half of the twentieth century, the classical guitar
gradually began to use electronic sound manipulation pro-
cesses, in situations predominantly linked to electroacoustic
music. On the other hand, the electric guitar appropriated
such manipulations as an intrinsic part of its performance
practice. This instrument is one of most frequently used
for augmentation, in part because of its popularity, but
mainly because of the ease in adding further electronic com-
ponents to it—since its inception, such elements belong to
the essence of an electric guitar.
If we can think of electric guitar pedals as devices of
timbre manipulation, we can conclude that the combina-
tion formed by the electric guitar, pedal boards and eﬀect
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
processors, along with the ampliﬁcation system, presents a
conﬁguration comparable with most AMIs. Electric guitar
pedals, however, are commonly seen as a regular part of the
instrument and not an extension or enhancement of its char-
acteristics. We can justify this understanding by consider-
ing that the eﬀects used in electric guitars usually have one-
dimensional control possibilities, like the Wah-Wah pedal,
or only oﬀer the option to activate or deactivate some previ-
ously conﬁgured process [2]. Therefore when we talk about
AMIs, we usually refer to the use of sensors and actuators,
as well as synthesis and sound manipulation processes using
software and programming languages speciﬁcally designed
for the task.
Hexaphonic pickups can also be considered as sensors
used in augmentation. Built in both magnetic and piezo-
electric formats, they are an integral part of some commer-
cial guitar models and systems 1. However, they are also
found in AMIs that demand access to raw audio captured
from each string and use algorithms for gestural acquisition
[3]. Moreover, we can observe an increased interest in using
actuators attached to acoustic guitar soundboards (active
control), and these cases will be addressed similarly.
We organized this paper as follows: the next section re-
views AMIs based on guitars. After that, we present the
projects GuitarAMI and GuiaRT, based on nylon (or clas-
sical) guitars and independently developed by the authors.
The following section compares both projects using a di-
mension space representation [4]. Finally, we present the
next steps to be taken jointly by the two projects.
2. GUITAR-BASED AUGMENTED MUSI-
CAL INTRUMENTS
2.1 Electric guitars
We can ﬁnd several AMI projects built with electric gui-
tars using direct or indirect gestural acquisition [3]: the
Situated Trio, the Smeck Guitar, the Mobile Wireless Aug-
mented Guitar , the Multimodal Guitar , the Augmentalist,
the Feedback Resonance Guitar, the RANGE Guitar , the
UniCoMP, the Talking Guitar, Graham’s Guitar, and the
Fusion Guitar.
Wessel and colleagues describes the Situated Trio [5], an
improvised trio performance where one of the performers
plays a Gibson electric guitar with a hexaphonic piezoelec-
tric pickup. The authors explore both the analog audio
outputs and the symbolic (MIDI) data acquired from the
hexaphonic pickup in Max/MSP to control eﬀects, including
non-linear distortion, spatialization, overdub/looping pro-
cesses, among others.
Puckette explored, with the Smeck guitar processing, the
1Such as Line 6 Variax guitars, Godin Synth Access, or
Roland GK-3/GR-55 modules.
222
new possibilities oﬀered by the individualization of guitar
string signals, using a Steinberger/Gibson electric guitar
and a hexaphonic piezoelectric pickup [6].
Bouillot and colleagues’ Mobile Wireless Augmented Gui-
tar consists of an electric guitar, a Wii Remote attached to
the guitar’s head, an embedded microprocessor to perform
a phase vocoder synthesis process, and a portable computer
for processing the audio signal [7].
Reboursi` ere and colleagues built the Multimodal Guitar
using an electric guitar, a hexaphonic pickup, and three
force-sensitive resistors (FSRs) installed on the back of the
instrument. The played notes converted into MIDI signals
and the data generated by the FSRs are sent to Max/MSP
or to Pure Data to control preprogrammed sound manipu-
lation algorithms [8].
Newton and Marshall’s Augmentalist is not an AMI per
se, but a platform for augmenting electric guitars using sen-
sors (Phidgets2) and microcontrollers [9]. The authors pro-
posed to analyze how performers used the resources made
available by the Augmentalist to create customized AMIs
for each performance.
Edgar Berdahl’s Feedback Resonance Guitar uses dual
embedded electromagnets to modify the electric guitar by
exciting each string individually (active control), and a
smartphone to generate control data for the used algo-
rithms. Signals used to excite the strings include the gui-
tar’s sound (feedback loop), pre-recorded audio signals or
other instrument’s sounds acquired in real-time [10]. The
goal is to create new timbres and textures, as well as use
resonance eﬀects as compositional support.
MacConnell and colleagues presented theRANGE Guitar
as an AMI built with an electric guitar and variable (mem-
brane) potentiometers, non-invasively installed on the in-
strument [11]. The embedded processor 3 receives the mem-
brane potentiometers data to control a guitar eﬀect patch
programmed in Pure Data.
H¨odl and Fitzpatrick’s UniCoMP is a platform that al-
lows the performers to send control data to a computer,
using the sensors and the touch screen of mobile phones.
This data is transmitted to a laptop and mapped within
Max/MSP [12].
Donovan and McPherson presented the Talking Guitar
[2]. The camera acquires the electric guitar position by
tracking the movement of a ball illuminated by LED at-
tached to the guitar’s head.
Graham and Bridges used an electric guitar with multi-
channel pickup, along with a Kinect sensor, in the design
of a gesture-driven system for spatial music performance,
implemented in Pure Data [13].
The Fusion Guitar is an electric guitar embedded with
an iPhone/iPod dock and a built-in speaker system. The
iOS app can apply eﬀects, emulate ampliﬁers, record or loop
guitar sounds, and serve as a tutor system for learning to
play the instrument [14].
2.2 Acoustic guitars
There are substantially fewer acoustic (steel or nylon-
string) guitar AMI projects, including: the SmartGuitar,
the L¨ahdeoja’s Augmented Guitar, the Angulo’s Guitar, the
Vo-96, the ToneWoodAmp, the Tonik Pulse, the Yahama
TransAcoustic Guitar, and the Acpad.
In the SmartGuitar, Mamou-Mani and colleagues used
sensors, actuators, and microcontrollers to explore the body
of an acoustic guitar as a loudspeaker, projecting a mix of
the acoustic and synthesized—or digitally manipulated—
2Http://www.phidgets.com/.
3The Beaglebone (http://beagleboard.org/).
sounds [15]. The SmartGuitar is part of a group of projects
called SmartInstruments, which also includes cello and
trombone. The HyVibe company released a crowdfunding
campaign to realize the SmartGuitar as a commercial prod-
uct.
The L¨ahdeoja’s Augmented Guitar uses a hexa-
phonic pickup system4 and audio exciters—electrodynamic
actuators—produced by Tectonics Elements to achieve an
outcome similar to the SmartGuitar. The pickup sends
audio to a computer running Max/MSP for processing—
granular synthesis, Modal feedback, and timbre modiﬁca-
tion using ﬂute samples—and sent to the audio exciters at-
tached to the guitar soundboard [16].
In [17], the authors set up a classical guitar with piezoelec-
tric sensors for each string, and the audio signals are used for
real-time estimation of fundamental frequencies and ampli-
tudes. The data is fed to a graphical interface programmed
in Processing.
The Vo-96 is a commercial acoustic synthesizer devel-
oped by Paul Vo. The Vo-96 uses a magnetic transducer
that converts the steel strings’ energy into electric signals
that are processed by the built-in analog and digital signal
processing hardware [18].
Three recent commercial products use actuators to excite
the body of the instrument, interacting with the natural
sound of the acoustic guitar and modifying sonic character-
istics to simulate traditional eﬀects such as reverb, echo or
delay. They are the ToneWoodAmp5, the Tonik Pulse6 and
the Yamaha TransAcoustic guitar7.
The Acpad is a wireless MIDI device that is designed to be
attached to an acoustic guitar soundboard [19]. The device
allows performers to interact with the instrument using a
technique knows as ﬁngerstyle to generate MIDI data.
Table 1 presents a summary of this section. It presents
the AMI names, their augmentation goals, acquisition meth-
ods, and sensors. For the acquisition methods we use the
gestural acquisition deﬁnition presented in [3]: 1) We have
direct acquisition by using one or various sensors to convert
the performer’s gestures in control data, and 2) We have
indirect acquisition when gestures are extracted from the
sound produced by the instrument.
3. GUITARAMI
Acoustic musical instruments, although very versatile, have
intrinsic sonic limitations due to their construction charac-
teristics. For the classical nylon strings guitar, these con-
straints include short sustain and the lack of sound inten-
sity control after the attack. GuitarAMI uses sensors that
generate data from gestures to control algorithms that over-
come some of these limitations [20].
From 2014, three GuitarAMI prototypes were built,
which had been tested in performances and music educa-
tion activities [21]. The second GuitarAMI prototype was
also used in performance for the B.E.A.T. 8 trio (drums,
trumpet and guitar)9.
The ﬁrst and second prototypes had four elements: a
sensor module, the processing unit, audio interface, and
computer. Both used a cable connection between the sen-
4Ubertar (http://www.ubertar.com/hexaphonic/).
5http://www.tonewoodamp.com/
6http://toniksound.com/
7https://usa.yamaha.com/products/musical_
instruments/guitars_basses/ac_guitars/ta_series/
index.html
8An acronym for Brazilian Electronic Aleatorium Trio.
9This performance can be seen at https://youtu.be/
dUd-1i0h104.
223
Table 1: Summary of the classical / electrical AMI descriptions.
Name [reference] Augmentation Goal Acquisition method Sensors
Electric Guitars
Situated Trio [5] Performance (Audio and MIDI) Indirect Hexaphonic pickup
Smeck Guitar [6] Audio descriptors acquisition Indirect Hexaphonic pickup
Mobile Wireless Aug. Guitar [7] Phase Vocoder control Direct Wii Remote
Multimodal Guitar [8] Synthesis control Direct and indirect Hexaphonic pickup and FSRs
Augmentalist [9] AMI construction platform Direct Various sensors (Phidgets)
Feedback Resonance Guitar [10] Active/Feedback control Direct and indirect Touchscreen and accelerometer
RANGE Guitar [11] Generate control data Direct Membrane potentiometers
UniCoMP [12] Real-time audio manipulation Direct Touchscreen and accelerometer
Talking Guitar [2] Control a speciﬁc audio process Direct LED and webcam
Graham’s Aug. Guitar [13] Spatial performance Direct and indirect multichannel pickup and Kinect
Fusion Guitar [14] Embed trad. eﬀects Direct Touchscreen and accelerometer
Nylon and Steel-string Guitars
SmartGuitar [15] Real-time sound manipulation Direct Footswitches
L¨ahdeoja’s Aug. Guitar [16] Real-time sound manipulation Indirect Hexaphonic pickup
Angulo’s Aug. Guitar [17] Generate control data Indirect Hexaphonic pickup
Vo-96 [18] Embed trad. eﬀects and active control Indirect Hexaphonic pickup
Acpad [19] Generate control data Direct FSRs
sor module and the processing unit. The module used the
ADXL345 accelerometer and the HC-SR04 ultrasonic sen-
sor, while the processing unit had ﬁve footswitches and an
Arduino Leonardo as the microcontroller. Wireless commu-
nication was implemented in the third prototype, using two
RF transmitter-receivers (NRF24L01+). The audio pro-
cessing was divided between a multi-eﬀects processor and
the algorithms executed by the computer. While the multi-
eﬀects processor was responsible for standard sound ma-
nipulations, such as distortion, delay, reverb and wah-wah,
the computer running Pure Data was able to receive gestu-
ral data from GuitarAMI processing unit to control preset
algorithms—as well as new algorithms made by performers
or composers.
The current prototype (under construction—Figure 1)
has embedded a new microprocessor into the GuitarAMI
processing unit, using the Prynth10 framework, while main-
taining the footswitches and LCD. The sensor module com-
municates through Wi-Fi using OSC protocol, which al-
lows GuitarAMI to interact with other devices and Digital
Musical Instruments (DMIs) connected to the same net-
work. We also replaced two sensors in the sensor mod-
ule: the LSM9DS0 (Magnetic, Angular Rate and Gravity
sensor—MARG) replaced the ADXL345 accelerometer, and
the HC-SR04 ultrasonic sensor was replaced by the newer
HC-SR04+, for better hardware compatibility.
Figure 1: Current GuitarAMI Prototype. (a) Processing
unit with embedded microprocessor and visual feedback;
(b) GuitarAMI Wi-Fi sensor module and guitar body.
10Https://prynth.github.io/.
3.1 Interaction strategies
There are two GuitarAMI patches currently in use. The
ﬁrst patch ( Experimentos 0.3.pd) presents spectral freeze,
FM modulation, and looping capabilities; The second patch
(Time Machine ) uses time stretching procedures described
in [22], particularly theDirect FFT/IFFT approach, already
explored in tutorials and community patches (as can be
found in Pure Data 11 and Max/MSP12 forums).
The Time Machine is based on the phase vocoder time
bender patch [23]. We have performed some modiﬁcations
to allow the use of circular tables, that act as circular
buﬀers, although storing audio in the frequency domain.
The buﬀers store the last played minute. The Time Ma-
chine patch contains two operation modes: the Freeze mode
and the Delorean mode. The Freeze mode, also available in
the Experimentos patch, acts as a regular spectral freeze:
the audio signal is analyzed and resynthesized from the
blocks acquired immediately after the user activates the
function, i.e., pushes the freeze button. The synthesized
sound is windowed and constantly triggered, creating a con-
tinuous sound. The Delorean mode acts as time stretching
by using two subpatches: the buﬀer and the prepare buﬀer.
The buﬀer patch stores audio data in the frequency domain
using two tables. The prepare buﬀer patch is responsible for
reading the arrays and reproducing it with the tabread4∼
object.
During the performance with GuitarAMI, the performer
can access the buﬀer containing what was performed during
the last minute using two diﬀerent methods. The ﬁrst op-
tion is by using one of the footswitches to activate a sound
resynthesis process—and control its duration—to hold the
current sound output indeﬁnitely. The second option is by
performing gestures in front of the ultrasonic sensor to start
time stretching operations in the buﬀer, using the most re-
cent recorded information as the starting point. By moving
the guitar (i.e., using the accelerometer/MARG), the per-
former can control the buﬀer reading speed.
4. GUIART
The GuiaRT was started in 2011 at the Center for Stud-
ies on the Musical Gesture and Expression ( CEGeME), at
the School of Music of the Federal University of Minas
Gerais (UFMG), with the mounting of LR Baggs hexa-
11Http://forum.pdpatchrepo.info/.
12Https://cycling74.com/forums/page/1.
224
phonic piezo pickups on a nylon Spanish Alhambra gui-
tar. Its primary purpose was—and remains—the develop-
ment of a DSP-based acoustic-digital interface to be used in
interactive musical contexts by guitarists, whose technical
and creative abilities would be expanded using real-time
extraction and manipulation of low and midlevel descrip-
tors. As the available commercial packages dedicated to this
task—the combination of hexaphonic pickups and low-level
extraction—were expensive and closed systems, the deci-
sion was to program almost from scratch, feeding the raw
6-channel audio into the Max/MSP environment. Given the
sound typologies aﬀorded by the nylon-strings guitar 13, it
seemed quite straightforward to focus on onset detection as
the project’s starting point. The ﬁrst challenge was posed
by the recognition of a strong mechanical coupling between
the strings, mediated by the bridge and the top plate. This
coupling can produce higher energy levels than the energy
produced by soft strokes on a resting string. Thus, the ﬁrst
task was to seek to minimize these inﬂuences, using ﬁlters,
and non-linear variable thresholds applied to six diﬀerent
5-string sub-mixes. After that, the algorithms were able to
detect onsets within a 10 ms margin error and to estimate
amplitudes and durations. Quantitative analyses of diﬀer-
ent right-hand techniques have been accomplished with this
setup, like tremolos, block chords, layer control and strum-
ming.
In 2014, the project acquired a new guitar—a Yamaha
CG182—and RMC pickups for both instruments. At that
time, the connectors on the lower side of the guitars were
also replaced with more robust XLR 7-pin connectors (see
Figure 2). Further eﬀorts were dedicated to the extrac-
tion of additional descriptors, such as fundamental frequen-
cies, spectral centroids, slurs, harmonics, pizzicatos and vi-
bratos/bendings. A similar approach can be found in [24].
As each extraction procedure has a diﬀerent latency (count-
ing from the detected onset), the global list describing an
event is generated at its oﬀset. The deﬁnition of several
midlevel descriptors—extracted in real-time from short ex-
cerpts selected by the performer—came next [25]. Visual
and aural feedback is oﬀered to the guitarist.
Figure 2: (a) Guitar and (b) 7-pin connector used in
GuiaRT.
Although GuiaRT was well equipped with sensing and
processing procedures—according to Rowe’s 1993 terminol-
ogy [26], it still lacked a response module. Experiments with
the direct audio captured in real-time were not successful:
13It is relevant to note that extended techniques—like per-
cussion on diﬀerent parts of the instrument, playing on the
nut side of a stopped string or changing the size of the
sound hole—are not well captured by these pickups, which
are ﬁxed as saddles on the bridge.
this was due not only to notable diﬀerences in timbre be-
tween the acoustic and captured sounds, but also to the in-
accurate segmentation of slurred sounds, and mostly to the
coupling among the strings. Therefore the decision was to
use a specialized audio library—Ilya Eﬁmov Nylon Guitar,
which was able to deliver the diﬀerent typologies extracted
by GuiaRT, besides presenting a sound quality quite sim-
ilar to those of the guitars in use. The use of this library
allowed a symbolic approach to the response module re-
garding pitch classes, rhythmic values, articulations, among
others. GuiaRT’s main sound/musical identity is that of a
guitarist improvising in duo or trio with him/herself. Short
motifs or phrases may act as initial stimuli. Additionally,
diﬀerent signal processing routines may also be applied to
individual string sounds, or even to a global sound (cap-
tured by a contact microphone). Among these procedures,
the prolongation of notes, ring modulation, and audio pro-
cessing using FM synthesis parameters [27] are worthy of
mention.
GuiaRT has also been used in artistic and pedagogical
contexts. In 2015, it was used in the multimedia concert
Kandinsky Sonoro, by the group Klang, which used three
paintings of the Russian artist as starting points for collec-
tive compositions.
4.1 Interaction strategies
During a performance with GuiaRT, the performer may se-
lect, using a switch pedal, speciﬁc excerpts for analysis and
variation. Up to now, two ”virtual” guitarists can perform
the variations, making use of the audio library mentioned
above. Each excerpt is stored as a list of events with their
respective low-level descriptors. Just after the release of
the pedal, the excerpt is analyzed and classiﬁed as one out
of the seven crudely pre-deﬁned textures: melody, arpeg-
gio, sustained melody, counterpoint, melody with chords,
percussive chords, block/strummed chords. Three midlevel
descriptors are used for this classiﬁcation: chord propor-
tion, most prominent chord size, and superimposition index
[25]14.
So far, GuiaRT has eight variation procedures to be ap-
plied to an excerpt. They may be roughly divided into two
categories, one preserving rhythm and the other preserv-
ing pitch content. The inversion, shuﬄing and mirroring of
strings are in the ﬁrst category, as well as the imposition of a
4-element pitch-class set. There are two retrograding proce-
dures; both applied independently to each string: one uses
the onset times for reversing the time structure (and shifts
the corresponding durations), the other transforms oﬀsets
in onsets—and vice-versa. Another rhythm variation is the
shuﬄing of IOIs (inter-onset-intervals) and durations per-
formed on each string. The variations may be cascaded at
performer’s choice.
The choice of variations is made by a weighted draw. The
performer must deﬁne a range of weights (between 0 and
100) for each variation type, for all basic textures. This
draw is repeated after the playback of a variation. The
user may also shorten the set of available variations, or
even alternate between pre-deﬁned types. Any variation
must be triggered in real-time. The considerable amount
of symbolic data extracted by GuiaRT makes possible the
deﬁnition of many diﬀerent triggering strategies, ranging
from immediate to texture-dependent triggers. A complete
14Two projects that explore the Disklavier are worthy of
mention here, due to their conceptual similarity to the
GuiaRT setup. Both Risset’s Duet for One Pianist [28]
and George Lewis’s Interactive Trio rely on the idea of a
virtual performer, whose musical material is derived from
the part played by a real performer.
225
performance plan using GuiaRT may include—besides the
setting of parameters for low-level extraction, the choice
of diﬀerent variation and triggering strategies—also further
modules for signal processing and spatialization 15.
5. CONTEXTUALIZED COMPARISON OF
THE TWO PROJECTS
We can make some observations from the reviews in section
2: 1) There is an apparent division between AMIs that use
loudspeakers and those which have body-coupled actuators
for sound output; 2) AMI input signals can be of a diverse
nature, including data generated by gestures, audio signals
or even data extracted from the audio captures; 3) Sensors
and computers can or cannot be embedded according to
the AMI design. With these ideas in the background, it is
possible to objectively compare the two projects using the
dimension space16 representation [4].
We will remove and adapt some axes to ﬁt our comparison
needs: The Inter-actors axis is redundant since both AMIs
have the classical guitar in their construction and expect
one user per system. The Role of Sound axis also can be
excluded since both AMIs share the same category: artis-
tic/expressive. We will also use two additional axes: the
Audio Output axis will be employed to represent the num-
ber of outputs, varying continuously from “one” to “many”;
and the Gesture Acquisition axis will be used to represent
the acquisition method with discrete values: direct, indi-
rect, or hybrid [3].
The Musical Control axis has its origins in [29], where
diﬀerent levels of control allowed by a sensor are discussed:
1) The control over a musical process corresponds to an
abstract mapping that controls any desired process (macro-
scopic level); 2) The note level corresponds to discrete con-
trol of synthesis parameters triggered by an attack (orla
level); 3) The timbral level corresponds to continuous con-
trol of synthesis parameters simultaneously (microscopic
level). In [4] these three levels are discussed as discrete
features; nevertheless, in the case of AMIs, the options may
be more complex. Certainly, the performer must be able to
control simultaneously the three levels while playing on the
acoustic instrument (what demands, by itself, a high level
of expertise); on the other hand, the augmented procedures
may include one or more levels of control. For the present
comparison, we have split this axis in two, one related to the
continuous timbral control, which is highly correlated with
the Degrees of Freedom axis, and the other to the control
of musical processes. The two projects present symmetrical
values for these axes: GuitarAMI has a reﬁned control over
timbral modiﬁcations, while GuiaRT oﬀers a larger control
over musical processes.
Both AMIs use the loudspeaker as main audio output, al-
though presenting a signiﬁcant diﬀerence between the rout-
ing capabilities. While the GuitarAMI only delivers one
channel (mono output), the GuiaRT counts with a sepa-
rate output and panning options for each—real or virtual—
string.
Regarding the Feedback Modalitiesaxis, the performer on
GuiaRT does not need visual feedback to play along with
the selected variations, although some functions may be dis-
played in the computer screen. The GuitarAMI provides vi-
sual feedback in both Pure Data patches and GuitarAMI’s
processing unit LCD.
15Some video excerpts may be seen at http://www.musica.
ufmg.br/sfreire/wordpress/
16The original dimension spaces axes are: Required Exper-
tise, Musical Control, Feedback Modalities, Degrees of Free-
dom, Inter-actors, Distribution in Space, and Role of Sound.
Both instruments are very similar in spatial distribution:
the performance occurs in a single location, and the per-
former interacts with a single AMI built around the nylon-
string guitar.
In the Gestural Acquisition axis, we can perceive a con-
ceptual diﬀerence between the two AMIs. Since GuiaRT
uses the signals produced by the strings as the main data
acquisition method, it has no impact on the performer’s
traditional gestures. Still, the performer needs to use at
least one footswitch to control several procedure param-
eters. GuitarAMI uses data from the embedded sensors
(IMU and ultrasonic) to process the audio signal, but no
control is extracted from the audio. We can say that, in
this aspect, the systems are complementary and they can
incorporate each others control data acquisition process.
Figure 3 depicts the dimensional space plots for each
project.
Augmented musical instruments share a common ques-
tion: How should the richness and the expertise of perfor-
mances on acoustic instruments with the sonic/expressive
possibilities allowed by new technologies be integrated? Dis-
tinct projects propose diﬀerent responses for these chal-
lenges, and most of them tend to blur the borders of
the roles traditionally assigned to performers, composers,
builders/programmers, and audience. Both GuitarAMI
and GuiaRT demand from performers not only instrumen-
tal expertise but also commitment to creative and techno-
logical issues. In our opinion, the emergence of these new
and alternative ways of making and performing music are
equally important as the new sounds obtained from the aug-
mentation processes.
Required Expertise
Control overMusical Processes
FeedbackModalities
AudioOutput
Degrees of Freedom(Timbral Control)
Distributionin Space
GesturalAcquisition
(a) GuitarAMI
Required Expertise
Control overMusical Processes
FeedbackModalities
AudioOutput
Degrees of Freedom(Timbral Control)
Distributionin Space
GesturalAcquisition
(b) GuiaRT
Figure 3: Dimension Space Plots.
6. CONCLUSIONS AND FUTURE WORK
By analyzing the dimension space plots, one can readily
perceive that the GuitarAMI restrictions are met by the
GuiaRT and vice versa. With this complementarity in
mind, it is expected that a collaboration between projects
can expand the distinct possibilities of the AMIs and re-
duce their restrictions. Besides the use of augmented ny-
lon guitars, another critical link between GuitarAMI and
GuiaRT is the fact that they are developed in laboratories
that have been performing international research collabo-
rations for about a decade. Hence, it is natural to expect
mutual inﬂuences between them. As discussed in the last
section, GuiaRT’s central core is highly dependent on spe-
ciﬁc hardware—a modiﬁed guitar, an audio interface and
a computer running Max-MSP. Despite that, some of the
algorithms in use may be easily converted to alternative
programming languages. On the other side, GuitarAMI is
226
more portable, since the processing unit and sensors may
function with several setups.
These characteristics can be used to deﬁne the ﬁrst steps
of a productive collaboration: on GuiaRT’s side, the data
generated by continuous sensors will help to reﬁne sound
processing algorithms; for GuitarAMI, the use of symbolic
data may also help to reﬁne some processes of selecting seg-
ments for audio manipulation. Both projects will proﬁt
from cross-examination and adaptation of the sound pro-
cessing routines in use, including spatialization. Moreover:
shared creative processes—in co-located or remote forms—
will undoubtedly reveal new paths not only to this starting
cooperation but also as a resource for other nylon-string
based AMI projects.
7. ACKNOWLEDGMENTS
We like to thank all the funding agencies and research cen-
ters that support GuitarAMI and GuiaRT projects. The
GuitarAMI project is supported by IDMIL 17 and CIR-
MMT18, McGill University, and by an NSERC Discov-
ery Grant to the third author. The GuiaRT project is
supported by the Brazilian funding agencies CNPq 19 and
FAPEMIG20.
8. REFERENCES
[1] E. Miranda and M. Wanderley. New Digital Musical
Instruments: Control and Interaction Beyond the
Keyboard. Computer Music and Digital Audio Series.
A-R Editions, Inc., 2006.
[2] L. Donovan and A. McPherson. The talking guitar:
Headstock tracking and mapping strategies. In Proc.
NIME Conf., pages 351–354, United Kingdom, 2014.
[3] M. Wanderley and P. Depalle. Gestural control of
sound synthesis. Proc. IEEE, 92(4):632–644, 2004.
[4] D. Birnbaum, R. Fiebrink, J. Malloch, and
M. Wanderley. Towards a dimension space for musical
devices. In Proc. NIME Conf., pages 192–195,
Canada, 2005.
[5] D. Wessel, M. Wright, and J. Schott. Situated trio:
An interactive live performance for a hexaphonic
guitarist and two computer musicians with expressive
controllers. In Proc. NIME Conf., Ireland, 2002.
[6] M. Puckette. Patch for guitar. In Proc. of PD
Convention, 2007.
[7] N. Bouillot, M. Wozniewski, Z. Settel, and
J. Cooperstock. A mobile wireless augmented guitar.
In Proc. NIME Conf., pages 189–192, Italy, 2008.
[8] L. Reboursi` ere, C. Frisson, O. L¨ahdeoja, J. Mills,
C. Picard-Limpens, and T. Todoroﬀ. Multimodal
guitar: A toolbox for augmented guitar performances.
In Proc. NIME Conf., pages 415–418, Australia, 2010.
[9] D. Newton and M. Marshall. Examining how
musicians create augmented musical instruments. In
Proc. NIME Conf., pages 155–160, Norway, 2011.
[10] D. Overholt, E. Berdahl, and R. Hamilton.
Advancements in actuated musical instruments.
Organised Sound, 16(2):154–165, 2011.
[11] D. MacConnell, S. Trail, G. Tzanetakis, P. Driessen,
and W. Page. Reconﬁgurable autonomous novel
guitar eﬀects (RANGE). In Proc. 10th Int. SMC
Conf., pages 674–677, Sweden, 2013.
17Input Devices and Music Interaction Laboratory.
18Centre for Interdisc. Res. in Music Media and Technology.
19Conselho Nacional de Desenv. Cient´ ıﬁco e Tecnol´ ogico.
20Fund. de Amparo ` a Pesquisa do Estado de Minas Gerais.
[12] O. H ¨odl and G. Fitzpatrick. Exploring the design
space of hand-controlled guitar eﬀects for live music.
In Proc. 39th ICMC, Australia, 2013.
[13] R. Graham and B. Bridges. Gesture and embodied
metaphor in spatial music performance systems
design. In Proc. NIME Conf., United Kingdom, 2014.
[14] Fusion guitars website – unleash the electric guitar.
Https://fusionguitars.com/, 2018. [Online;
accessed 31-March-2018].
[15] IRCAM(AI) - Acoustique Instrumentale -
Smartinstruments.
Http://instrum.ircam.fr/smartinstruments/,
2017. [Online; accessed 20-December-2017].
[16] O. L ¨ahdeoja. An augmented guitar with active
acoustics. In Proc. 12th Int. SMC Conf. , Ireland,
2015.
[17] I. Angulo, S. Giraldo, and R. Ramirez. Hexaphonic
guitar transcription and visualisation. In Proc. 2nd
Int. TENOR Conf. , United Kingdom, 2016.
[18] The vo-96 system.
https://www.kickstarter.com/projects/38513516/
the-vo-96-acoustic-synthesizer , 2016. [Online;
accessed 04-April-2018].
[19] Acpad website – the electronic orchestra for your
guitar. Https://www.acpad.com/, 2016. [Online;
accessed 09-November-2016].
[20] E. Meneses. Guitarami: desenvolvimento,
implementa¸ c˜ ao e performance de um instrumento
musical aumentado que explora possibilidades de
modiﬁca¸ c˜ ao de caracter´ ısticas intr´ ınsecas do viol˜ ao.
Master’s thesis, UNICAMP, 2016.
[21] E. Meneses, J. Fornari, and M. Wanderley. A study
with hyperinstruments in free musical improvisation.
In Proc. of 11th SIMCAM Simp. , Brazil, 2015.
[22] D. Arﬁb, F. Keiler, U. Z ¨olzer, V. Verfaille, and
J. Bonada. Time-frequency processing. In DAFX:
Digital Audio Eﬀects, chapter 7, pages 219–278. John
Wiley & Sons, 2011.
[23] M. Puckette. The Theory and Technique of Electronic
Music, volume 11. World Scientiﬁc Pub Co Inc, 2006.
[24] E. Egozy. Deriving musical control features from a
real-time timbre analysis of the clarinet. Master’s
thesis, Massachusetts Institute of Technology, 1995.
[25] S. Freire and P. Cambraia. Analysis of musical
textures played on the guitar by means of real-time
extraction of mid-level descriptors. In Proc. 12th Int.
SMC Conf., pages 509–514, Ireland, 2015.
[26] R. Rowe. Interactive Music Systems. MIT Press, 1993.
[27] S. Freire. Real-time audio processing by means of FM
synthesis parameters: Fundamentals and preliminary
compositional applications. In Proc. 41st ICMC,
pages 86–89, USA, 2015.
[28] J. Risset and S. Van Duyne. Real-time performance
interaction with a computer-controlled acoustic piano.
Computer Music Journal , 20(1):62–75, 1996.
[29] W. Schloss. Recent advances in the coupling of the
language Max with the Mathews/Boie Radio drum.
In Proc. 16th ICMC, pages 398–400, Scotland, 1990.
227