About TIME: Textile Interfaces for Musical Expression
Sophie Skach
Intelligent Instruments Lab
Iceland University of the Arts
sophieskach@lhi.is
Victor Shepardson
Intelligent Instruments Lab
University of Iceland
lvictorshepardson@hi.is
Thor Magnusson
Intelligent Instruments Lab
University of Iceland
lthormagnusson@hi.is
ABSTRACT
Research in new musical interfaces includes exploring some-
times unconventional materials, drawing from a wide range
of sources, but typically affiliated with the computing indus-
try. These interfaces are made out of plastic, metal, glass
and rubber, built with sensors that seek precision and er-
gonomic control. However, there are other more unconven-
tional interfaces, such as flexible and soft instruments that
allow for different types of interaction. One of the materi-
als that has not been explored extensively in this context,
are textiles, in particular e-textiles. Here, we survey the
NIME archive and provide an overview of the work on non-
rigid interfaces. Further, this paper presents a new textile
based musical interface and evaluates its potential as a mal-
leable, expressive instrument through a case study with 6
musicians. The findings of the qualitative analysis conclude
a set of guidelines for the development of future e-textile
interfaces.
Author Keywords
textile interfaces; soft materials; soft instruments; e-textiles;
human-material interaction; textile synthesizers
CCS Concepts
•Hardware → Analysis and design of emerging devices and
systems; •Human-centered computing → Sound-based in-
put / output; •Applied computing→ Sound and music com-
puting; Performing arts; •Information systems→ Music re-
trieval;
1. INTRODUCTION
Designing musical instruments has established a set of ma-
terials we are used to interacting with. Traditionally, they
are made of primarily rigid materials such as wood, metal,
or glass. Interfaces for digital music making often feature
hard plastics, rubbers and other synthetics, which often pro-
vide little malleability or softness. More recently, makers,
musicians, and researchers have experimented with less con-
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’24, 4–6 September, Utrecht, The Netherlands.
ventional materials, seeking to expand the possibilities of
sonic interaction. Wearable, small scale sensors as well as
flexible, soft circuit technologies have allowed designers to
turn a large variety of materials into musical interfaces [6,
51]. An example of a widely used wearable interface are the
MiMu Gloves1, enabling control of sounds through gestural
movement [34]. Similar designs in less commercial settings
have also been explored by others [46, 23]. Paradoxically,
this sort of wearable might be seen as eschewing contact
entirely as in the case of distance-sensing instruments like
the theremin or computer-vision based systems [31, 26, 17,
20].
Here, we embrace a material-driven approach to develop-
ing musical interfaces. Although there is a long tradition of
marrying interactive textiles and computational audio [42,
51, 45], we review specifically how the NIME community
has explored this field and look at ways of interaction be-
tween humans and materials.
In this paper, we ask, what role soft, malleable materials
have played in the 24 years of NIME history, what materials
fulfil these properties, and how they are embedded in musi-
cal interfaces. Further, if instruments can be soft, how does
that affect our interaction with them? What can we learn
from the works in this field so far? The paper addresses
these material questions by closely examining one material
that has only marginally been exploited in this context and
offers not only flexibility in its tactile sensation, but in its
form factors and application areas: Textiles.
They come in different structures, materials, and char-
acteristics. We can stretch, squeeze, stroke them, wear
them and wrap them around us. For more than a decade,
they have been an increasingly popular material in HCI and
have been proven suitable as interfaces for musical interac-
tion [51, 49, 18, 54, 46]. As such, we abbreviate textile
interfaces made for musical expression as TIME. As with
other non-rigid materials, they are infrequently represented
within NIME, as pointed out by Boem et al. [4].
In pursuit of addressing this gap and establishing tex-
tiles as a material for soft, musical interfaces, we offer the
following contributions:
• First, we survey all NIME proceedings to date using
a set of search keywords for soft material interface
designs.
• Second, we present the design and qualitative evalua-
tion of two new textile interfaces (hand crocheted and
machine knit) for musical expression and aim to coin
them as TIMEs.
• Finally, the paper concludes with a set of design con-
siderations for developing future TIMEs.
1https://www.mimugloves.com
2. TEXTILES IN NIME
The first part of this paper gives an overview of the use
of interfaces made of soft materials within the history of
NIME. While there is a multitude of ’TIMEs’ outside of
the NIME proceedings (and in recent years increasingly in
general HCI conferences, art and design venues), we focus
our search criteria around this conference’s proceedings to
understand its community’s approach to the use of textile
materials better.
All proceedings from 2001 to 2023 were scanned using a
set of keywords appearing in paper titles indicating poten-
tial design of e-textile interfaces. In total, 7 keywords were
selected and yielded a collection of 39 papers used in this
survey:
1. textile – 5 papers – [11, 10, 61, 47, 60]
2. fabric – 4 papers – [27, 8, 48, 43]
3. soft – 1 paper – [63] (although in total 26 results
yielded, all but one software related works)
4. non-rigid – 2 papers – [4, 53] (of which 1 is a survey
paper rather than presentation of a design)
5. flexible – 5 papers – [9, 3, 33, 1, 22]
6. wearable – 19 papers – [59, 58, 62, 30, 7, 24, 44, 56,
16, 23, 13, 38, 50, 39, 29, 21, 36, 12, 28]
7. malleable – 3 papers – [25, 15, 19]
The papers included in this survey relate to presented
interface designs, and do not include e.g. the words flexible
or soft in relation to software applications. Further, in the
rare occasion of two or more keywords co-occurring in a
paper title, the first one that appeared in the title was used
to add to the allocation above.
2.1 Surveying the NIME Proceedings
The first two years of the NIME proceedings show no men-
tion of any of the listed search parameters or key words.
The first time one of the above keywords was used was in
2003 (‘wearable’) [44], followed by ‘fabric’ in 2006 [27], with
one paper each. To emphasise again, we include only use of
keywords to describe material properties rather than ‘flex-
ible’ use of something [57, 14]. It was not until 2007 that
musical interfaces began to be referred to more commonly as
soft, wearable, flexible, and so on. Most papers acknowledg-
ing these properties were published in the 2010 proceedings
(7 in total: 3 ‘wearable’, 2 ‘flexible’, 1 ‘malleable’ and 1
‘fabric’).
The first use of the term ‘textile’ in NIME was in 2017,
then with two papers [11, 61]. Since then, ‘textile’ has been
used in titles only three more times, once in 2019 [10], and
twice in 2020 [47, 60]. While in other domains, such as
robotics, ‘fabric’ is more commonly used than ‘textile’, in
NIME, it appears equally few times, including one paper
using both terms [61].
The term ‘non-rigid’ yielded only two papers, both in
2020 mentioning it in their title [4, 53]. While this was
expected, the results of papers describing soft interfaces also
using ‘soft’ in their title was surprising. On a first glance,
26 papers are filtered, but only one of them refers to the
materiality of an interface [63], and not to ‘software’. On the
other hand, the term ‘wearable’ is by far the most popularly
used amongst the selected keywords of this survey. Most
‘wearable’ papers, however, introduce a piece of electronic
hardware that is modified to be worn on the body, but has
nothing ‘soft’ or ‘textile’ about it.
After examining how the keywords were used in the NIME
context, we looked at other terms often associated and used
together with soft, textile or fabric interfaces. The keywords
that were observed to co-occur with the ones defining the
search criteria include the following: ‘touch’ [48, 27, 13, 7],
‘multitouch’ [43, 11], ‘multimodal’ and ‘deformable’ [61],
‘physicality’ [44, 16], and lastly ‘haptic’ [59, 30]. Specifying
the words further, more explicitly indicating the use of soft,
textile materials gathered these: ‘stretch’ in [8], ‘knitting’
and ‘knitted’ in [60], ‘mesh’ [47], ‘fiber’ [15].
Other, additional keyword categorisations amongst the
surveyed 39 papers emerged, too. For example, titles ref-
erencing the type of embodied interaction associated with
interface, such as ‘breathe’ [24, 30], ‘gesture’ [58, 48] or
‘hand’ [36]. Also mentioning the body part involved in the
interaction appeared with the use of ‘finger’ [12] and ‘foot’
[28]. Also the design object itself was often referred to, like
‘glove’ [23, 29], ‘sponge’ [33], or ‘shoes’ [39].
2.2 What actual textile NIMES are there?
Examining the interfaces of the surveyed papers further
shows that only a handful of them are actuallymade of tex-
tiles. Among them, there are some that use textiles without
attending to or discussing their material role and character-
istics. Most interfaces featuring textile elements within the
proceedings archive therefore don’t contribute to the design
of fabric or textile interface designs, but rather evaluate ex-
isting textile sensors [27], or the development of appropriate
software of piezoresistive fabrics as a touch interface [48].
Other works included in our keyword search suggest other,
non-textile materials with similar properties. This ranges
from foam [25], to velostat [53], to a sponge [33]. While
these materials can all be labelled as soft, the survey teaches
us that also not all flexible interfaces are soft, as we see in
the saw instrument by [9]. Nor are the the majority of
wearable interfaces and controllers, for example wearable
wooden shoes [39].
In the category of wearable musical interfaces of the NIME
archive, textile materials often serve as a casing for electron-
ics [59, 62, 30], or accessories like straps to mount wearable
sensors to the body [7, 24]. In these examples, the use of
fabrics is not discussed, nor is the material described in the
design process. An example of fairly known wearable mu-
sical interfaces featuring fabric without being talked about
explicitly are gloves. They always use fabric as a base layer
at least, which is rarely pointed out or discussed for the soft,
flexible, yet robust properties coming with these designs [23,
21, 36], or only marginally when a series of different designs
is presented [29]. In general, gloves are a popular interface
for gestural musical control, even more so in the universe
outside the NIME proceedings, while for example shoes or
foot-mounted controllers are represented with only two pa-
pers of the proceedings [28, 39].
So, what are the actual textile interfaces for musical ex-
pression, or as we call them here, TIMEs? Starting with
the first paper using textile in their title, a pressure sensor
matrix was designed [11], leading to many adaptations in
the years followed. This was, however not the first sensor
matrix introduced to NIME using commercially available
conductive fabric [43]. In the same year of the word ‘tex-
tile’ in a paper title, a knitted keyboard was introduced
[61], also followed by future iterations [60]. Designing tex-
tile interfaces for musical control began before the term was
prominently used. For example, a wearable felt pouch was
described in 2010 [3], and a stuffed knitted ball in 2013 [19].
Even earlier than that, a stretchy fabric sheet was presented
in 2007 [8], and fabric sensor prototypes were introduced in
2008 [15], also made of commercially available conductive
fabric. More recently, designs using optical fibre based wo-
ven fabrics in 2019 [10], and a large scale silver mesh fabric
in 2020 [47] were included. Since then, no papers in the
proceedings have featured textile based interfaces anymore.
(Until this year, 2024.)
2.3 Summary
Why are there so few interface designs that exploit the tex-
tile properties in the history of NIME so far? While we have
seen an increase in popularity and interest of wearable, flexi-
ble interfaces over time, material exploration seems still lim-
ited. This may be due to a lack of malleable, soft electronic
components widely available on the market that we need to
build these interfaces. It may also be due to easy access to
suitable textile materials to this community. Many practi-
tioners and researchers have a background in engineering,
computer science, or composition. While there has been a
more interdisciplinary and collaborative approach in recent
years, involving material experts, such as textile designers,
is still not very common. Lastly, the non-presence of textile
interfaces may suggest that textiles are, generally speaking,
no desirable material in this community. With so far de-
veloped wearable and flexible controllers there is already a
lot of potential for embodied sound design through postural
movement, or marked gestures demonstrated in the history
of NIME, even without textiles.
Nevertheless, textiles being such an integral element of
our everyday material interactions, we have a unique re-
lationship to them, culturally and functionally [45]. They
encourage material communication in addition to embod-
ied interactions. Moreover, they can be used as a bridging
element to open audio technologies to a more diverse au-
dience, as has been shown [52]. Numerous works outside
NIME proceedings also show that textiles have long found
their way into musical expression [51, 46, 18, 55, 54], not
least thanks to developments of ‘textile friendly’ electron-
ics like the Lilypad [6], educational initiatives like Fabri-
cademy2, or the tutorials and open sourced work of Irene
Posch [40, 41] and Kobakant [46]. We therefore believe it is
worth exploring textiles further in the context of NIME.
3. TEXTILES FM
We address the above discussed gap of textile instruments in
NIME by developing two e-textile interface designs and test
them in a first, exemplary study. The two interfaces we pro-
duced are: a crocheted and a knitted one, both particularly
elastic and soft textile structures consisting of interlocking
loops. They were connected to a soft circuit and turned
into FM synthesizers using the Bela platform [35]. This
section describes the design process of this soft, interactive
system, including its integration to an electrical circuit, and
the sound design for these interfaces.
3.1 Design and Fabrication
3.1.1 Tools and Materials
Both interfaces are made from locally sourced Icelandic sheep’s
wool, making them sustainable and affordable designs. While
the crocheted loops are handmade and require a single cro-
chet hook, the mat is produced on a computerised knitting
machine (a Kniterate3 commonly available at Fab Labs and
2https://textile-academy.org/
3https://www.kniterate.com/
Figure 1: The ‘Crocheted Loops’ interface connected to the
textile circuit as presented to participants
Figure 2: The ‘Knitted Mat’ interface with the positive-
negative Jacquard pattern, the lighter one including the
piezoresistive yarn.
Textile Centers and a domestic version of the far more ex-
pensive industrial counterparts). Therefore, the tools and
materials needed to replicate these designs are accessible to
a large community. To turn these textile structures into
interactive musical interfaces, conductive yarn made from
stainless steel and synthetic fiber is used alongside a woollen
base yarn. It creates a piezoresistive surface and can be con-
nected to the circuit.
3.1.2 Crocheted Loops
This yellow and blue design features 7 crocheted samples in
the form of loops and strips of different widths, thicknesses
and lengths, sewn onto a jersey knit fabric, see Figure 1.
Each crocheted sample presents one sensor that is later con-
nected to different sound modifying effects. These sensors
are placed and attached to the base fabric and signify semi-
predetermined gestures such as stretching, pulling, squeez-
ing, slipping in, out and through them, and pressing them
(demonstrated in Fig.5. Where on the sensor the connec-
tions to the circuit are placed is flexible and can be easily
unplugged and changed.
Figure 3: Close up of the textile circuit featuring textile
cables with safety pin ends connected via resistors to the
Bela Mini.
3.1.3 Knitted Mat
This is a double-bedded knitted fabric produced in a tech-
nique called a ’positive-negative’ Jacquard. This interface
is a flat piece of fabric (or mat) designed so that the en-
tire surface becomes touch sensitive without distinguishable
sensors. Here, the sensing area is determined by where the
connectors are placed, with two corresponding cables per
output. For example, they can be connected at opposite
corners of the fabric like in Figure 2, or close to each other,
on the same edge or different edges, or clipped to the centre
of the fabric.
3.2 Textile, Soft Circuit
For both designs, the same circuit, the same amount of
sound outputs, and the same sensing type (piezoresistive
sensing) was used for further mapping. To connect to the
circuit board, textile based, soft cables were produced adapt-
ing the principle of [41, 40]. The ends of the cables consist
of safety pins and metal clips, that make it easy to plug and
unplug interfaces, or change a sensor’s mapping. The wire
itself is made from highly conductive yarn, insulated with a
braided paracord. A close-up of this is shown in Figure 3.
This technique allows for an entirely soft, bendable circuit
without compromising on the textile properties of the inter-
face. The only hard, rigid component is the computer itself,
to which the textile cables as well as the sound output are
connected, a Bela Mini4, also seen in Fig.3.
3.3 Sound Design and Mapping
To explore a variety of different mappings from textile state
to sound, four PureData patches were designed by differ-
ent researchers. A repository of them is available online 5.
Each patch uses a different strategy for mapping the six raw
signals from the piezoresistive textiles.
Some patches use FM synthesis as a primary sound source,
while others use field recordings of sewing, kitting and felt-
ing machines made during development of the interfaces at
the Icelandic Textile Center. In some cases each input con-
trols a different sound, so that the textile objects behave
more like a collection of instruments, while in other cases
each controls one parameter of a monophonic synth, so that
the textiles acted as one entangled controller. Finally, some
patches make sound continuously, while others filter the in-
put signals so that sound is only produced as the textiles
4https://learn.bela.io/products/bela-boards/bela-mini/
5https://github.com/Intelligent-Instruments-Lab/about-
time-patches
are manipulated. We aimed for variety in the macro-scale
behavior [2] of the interface so as to be able to triangulate
insights about the role of the textile materials, distinct from
that of the sound mapping.
4. USER STUDY
The two interfaces were put to test in a qualitative user
study. We recruited musicians who had experience with
digital interfaces and MIDI controllers and invited them
to explore the interfaces in a jam session, which forms the
base for observational studies using ethnographic methods;
as well as for a ”think-aloud” approach [37] where partici-
pants comment on their actions and explorations while in-
teracting with the interface. All sessions were given consent
to be recorded and were later reflected upon together with
the musician in a semi-structured interview for qualitative
assessment.
4.1 Participants
Six electronic musicians were recruited for this study. All
of them are practising musicians, and at the time of the
study locally resident, however from different cultural back-
grounds, representing different nationalities. Their educa-
tional background is in music, too, ranging from electronic
music studies to composition.
None of them has a background in textiles or related
fields, and were not familiar or had any prior experience
with electronic textiles and textile sensing. One participant
has previously performed with the MiMu Gloves, and three
other participants have encountered other custom made in-
teractive musical gloves before. However, they have not
considered these gloves to be e-textile artefacts.
4.2 Experimental Setup
4.2.1 Data Collection
The study sessions were video- and audio-recorded using a
static camera and a mobile phone. To preserve the privacy
of the participants, only their hands were captured. No sen-
sor data from the e-textile interfaces themselves were cap-
tured, nor were the sound experiments. The audio output
from the computer was perceived via headphones. (This
way we hoped the participants would feel less pressure to
deliver a polished or advanced performance of some sort.)
4.2.2 Procedure
At the start of the session, information of participants’ back-
ground and prior encounters with e-textiles was gathered.
Then, the crocheted interface was introduced (Fig.1) and
participants were given a few minutes to freely explore it
with one of the sound patches. This served the purpose
of initial familiarisation with the interface’s material prop-
erties, and interaction paradigm. Participants were then
asked to continue to play with the interface while contin-
uously commenting on their actions and decisions. This
evaluation method of verbalizing thoughts is known as the
“thinking aloud” protocol [37]. In this phase, the differ-
ent patches were introduced, presenting the different sound
mappings.
At the end of this exploratory jam session, the partic-
ipants were prompted to explore the same patches with
an additional, third interface: a ‘classical’ MIDI controller
(Korg NanoKontrol) with sliders and knobs as seen in Fig.6.
Figure 4: A participant exploring the knitted mat in the
“think aloud” phase
Figure 5: Two participants interacting with some of the sen-
sors of the crocheted loops interface
4.2.3 Interview
The last part of the study consisted of a semi-structured
interview, suggesting to reflect on the experience with the
e-textile interfaces, as well as comparing this experience to
the interaction with the MIDI controller. All participants
were asked the same set of open ended questions, covering
their interactional and aesthetic preferences amongst the
two e-textile interfaces, their thoughts on the sound design
and mapping to the textiles, their tactile perception and
sensation of the crocheted and knitted interfaces, as well
as general thoughts on the jam session as a whole. Finally,
they were given the opportunity to add any additional com-
ments. The interviews lasted between 20 and 35 minutes per
participant, and were audio recorded only.
5. STUDY FINDINGS
We divide the findings of the study into the received, voiced
feedback by participants (results drawn from audio data),
and observational findings based on annotated videos (re-
sults drawn from nonverbal data). Participant citations in
this section are anonymised and referred to as P1-P6.
5.1 Participant Feedback
5.1.1 Think-Aloud Commentary
Here, participants talked about the relation between their
gestures and other postural movement and the sound out-
put of the interface. Some expressed expectations in regards
to the gesture-sound mapping, such as performing a large
scale gesture or applying increased force on the fabric would
map to equally “extreme” or “drastic” changes in the sound
modification. Participants also took these mappings fur-
ther and suggested additional representations that would
Figure 6: Four participants interacting with the MIDI con-
troller.
appear natural or direct to them. For example, pressing
the fabric down with the entire palm instead of individual
fingers causes the sound to slow down. Pulling a loop would
commonly be linked to increasing volume or amplitude, and
stretching in various ways to stretching the sound, too. Au-
ditory distortion was imagined through creating material
tension, like holding the fabric in both hands, pulling it
apart as a whole and in different directions.
Many comments included analogies to other musical in-
struments, interfaces or devices, too. Scratching the knit-
ted mat surface, experiencing the sensation of the raw wool
the interface is made of, reminded of scratching a record
(P1), or relating the gestural “elegance and finesse” to play-
ing a harp (P3). Moreover, the crocheted loops as well as
the knitted mat were imbued with character while being
explored. Sometimes, the knitted mat “went crazy” when
bulking and folding it up, or the crocheted loops were “hid-
ing” the sound until they liked the participant’s performed
touch. In general, the participants’ language suggested an
anthropomorphism of the textiles as a soft, collaborative
instrument.
5.1.2 Interview Results
Reflecting on the jam sessions after completing the study
in a semi-structured interview gave participants the oppor-
tunity to elaborate and add to their thoughts expressed in
the “think-aloud” method.
Describing their experience with these interfaces yielded
a different vocabulary and metaphors compared to rigid
controllers, appearing coherently throughout the interviews.
In addition to being talked about as emotional interfaces,
fabrics were said to have different moods and characteris-
tics commonly assigned to female attributes (P2). Both,
sounds and fabrics were ‘scratched’ (P1), ‘wrapped’ (P3),
‘scrunched’ (P1-3), ‘stretched’ (P1-4, P6), ‘compressed’ (P1-
3), ‘sculpted’ (P6), and ‘folded’ (P1, P3). Also perceptions
of touch and sound effects were described as ‘open’ rather
than switched on, and ‘gentle’ or ‘rough’ rather than quiet
or loud. Similarly, the sound interaction was perceived as
‘weaving in and out of the performance’ (P3, P5).
In regards to the tactile sensation, the relived experiences
with the textiles prompted participants to draw emotional
connections to the material. While they described the MIDI
controller as generally feeling cold, touching the woollen
fabrics were described as emotional, warm (P2, P3, P4),
rich in cultural association (P2), and even ‘genuine’ (P5).
When asked about a preference between the two e-textile
designs, most preferred the knitted mat. This was explained
by its association to improvisation friendly performance at-
tributes (P1-3, P6), and feeling more ‘jazzy’ (P1). For most
participants, it presented a ‘blank page’, encouraging more
adventurous interactions and overt gestural movement.
Talking about potential applications of interactive textile
surfaces, participants were creative with their suggestions.
Expanding on possibilities of the knitted mat, future de-
signs like large blankets and towels were envisioned (P1-3,
P6), as were multi-functional pillows or smart covers for
classical instruments (P4). Also wearable objects were of
interest, imagining a performances with the entire body in-
teracting with the textile material (P4, P6). Performances
with e-textile interfaces were associated to noise machines
(P4, P5), as well as material and object focused sound art
installations (P1,P6).
5.2 Observational Results
In addition to the feedback voiced by the participants, the
recorded videos were analysed using ethnographic methods.
They were imported to the transcription software Elan [5]
(commonly used in Conversation Analysis and Linguistics).
The videos were annotated for their gestural information,
focusing on differences between the textile interfaces and
the MIDI controller. We hand coded different gesture types
(for example, whether one or both hands were used, whether
only two or more fingers were in contact with the controls),
which helped to identify patterns of interaction when using
the different interfaces.
5.2.1 Gestural Interaction
The observations of the video recordings gave nonverbal
clues in addition to the verbalised thoughts about the qual-
ity of the human-material connection while playing with
the crocheted and knitted interface. In addition to paying
attention to how fingers and hands were used, we also ex-
amined the rest of the arms and the upper body in relation
to interacting with the interfaces. Ranging from using one
finger alone to leaning onto the fabric with both arms and
forward leaning torsal posture, participants challenged the
designs to see “how much they can get out of it” (P1-3),
using more marked gestures and larger scale movements.
In Figure 6, different participants can be seen interacting
with the MIDI controller. In comparison, Figures 4 and 5
show gestures while playing with the two textile interfaces.
Here, the interface as a whole can be deformed, compressed,
scrunched up, folded or stretched. The pressure applied to
the material determines the sound output, so using one or
two hands, or even just one finger, would change what the
textile sensor captures. It was also possible for different
sensing surfaces to be in contact with each other, which
could be achieved by bulking the entire interface up, as seen
in Fig.4
5.2.2 Comparison with the MIDI controller
The interactions with the MIDI controller yielded a dif-
ferent set of gestures. Here, a smaller part of the human
hands is needed to achieve an effect. While the fabrics are
sometimes grabbed with both hands to be compressed, in-
teractions with the MIDI controller sometimes involve one
finger only, yielding smaller scale movements, limited to fin-
gers and forearm, rather than more marked postural shifts.
Most participants used only one or two fingers during the
entire interaction, only one participant (P5) used 3-4 fin-
gers at the same time. Yet, the gestures performed with
multiple fingers in use is simultaneous and parallel pushing
of sliders. An example of this can be seen in Fig.6.
Common gestures performed with the sliders and knobs,
given their form factor and affordances resulting therefrom,
are pinching and turning with two fingers, and pushing and
pulling with one finger (rather than, for example, stretching
and squeezing).
The videos also reveal that the overall posture of partic-
ipants if often very slumped over the controller, with their
face very close to the interface, almost as if leaning forwards
and into it. This may be enhanced due to the session being
performed while seated.
Also when asked about the comparison of working with
the two interfaces, participants noticed that despite having
more precise control with the MIDI, it is “not fun to watch”
(P1). Furthermore, the aspect of control, as was noted
by participants P1-4, is not necessarily prioritised when it
comes to musical improvisation and live performances.
6. DISCUSSION
The findings of this preliminary user study, together with
the findings of the survey of the NIME proceedings how e-
textiles can contribute to and potentially form a new path
for the NIME community.
Given the relative rarity of e-textiles in these proceed-
ings, while they become ever more prominent in other HCI
disciplines (not least through an interdisciplinary network
including sound artists), it is worth pointing out some of the
demonstrated perks of using e-textiles for musical interfaces.
Textiles expand ergodynamic possibility spaces [32] in the
sense that they contribute to the possible operations when
playing them, as well as the possible use cases or functions.
Textiles as malleable, soft, sometimes fuzzy surfaces can
encourage new ways of thinking, creating different concep-
tual models, building on different sets of affordances and
constrains than other interface materials. Especially in com-
parison with other controllers, the material affordances of
textiles have shown to facilitate an extended set of interac-
tional gestures. This can lead to new paradigms in interac-
tion design.
6.1 Impossible MIDI Gestures
Commented on by participants, and observed in video anno-
tations are the differences in gestural movement when play-
ing with the textiles. While the MIDI controller affords lim-
ited movement of fingers and forearms, the textiles seemed
to “unlock” an expanded, new set of gestures, largely some
that would be impossible to perform with a rigid interface,
as it appears larger muscular groups are in use (for exam-
ple when stretching a fabric). Arguably, all instruments
and musical interfaces embody the constraints of their use
by design, the experienced and observed differences in in-
teracting with the interfaces suggest that the textile designs
impose a wider range of possible bodily movements during
interaction. Gestural movements that may feel natural and
intuitive in music performance become restricted, if not im-
possible on traditional MIDI controllers. Together with the
suggestions of some participants to involve the entire body,
the set of possible musically interactional movements can
go beyond gestures.
The interactions with the e-textiles resulted not only in a
much wider range of gestures, but also in a shift in language,
an established emotional connection to the material, and
interestingly also a tendency to anthropomorphise textiles
seemingly easily. Assigning personality traits like “moody”
(P2), or describing the fabric to react and gesture itself
(P6) are just two examples of how participants talked of
the interfaces.
6.2 Design Considerations for TIMEs
The participant feedback and conducted observations of the
test sessions leaves us with a set of design considerations
for future textile interface designs for musical expression
(TIMEs). We can summarise them in the following bullet
points:
• Exploit the affordances of different sizes: the major-
ity of participants expressed the idea or desire to scale
the textile interface up, e.g. as a blanket or a fabric
to wrap the whole body. Working with textiles easily
allows to create larger (as well as smaller) interfaces,
which can further facilitate a wider range of interac-
tion gestures.
• Natural Mapping: Energy input should correspond to
energy output, gestures directly aligned with sound
effect (e.g.stretch) appeared desirable and more intu-
itive to participants. This can be incorporated by the
sound design, but also by the position and design of
the textile sensors, as well as the use of different textile
materials.
• (Textile) Quality Over Quantity: the mantra gathered
from participants seemed to be: one sensor with more
nuanced sound effects rather than many sensors that
may not work as anticipated. One piece of fabric could
have multiple effects mapped to different gestures or
manipulation intensities.
These suggestions are by no means finalised guidelines,
as they derive from a small sample set of users and two
interface designs. Nevertheless, they give us an indication
of what musicians may look for when encountering textile
instruments.
7. CONCLUSION
There is a lot more to be explored about the textile in-
terfaces than possible here. We aim to carry this research
further and better understand the material impact on ges-
tures, affective interaction, sound design, and last but not
least, the linguistic metaphors emerging from the engage-
ment with expressive e-textile interfaces. As this analog
interface to a digital world, textiles also fit this year’s con-
ference theme of “tactility in a hybrid world”.
A final lesson learned from this preliminary user study
included the briefing of the participants. None of them had
experience with interacting with e-textiles, and some ex-
pressed that while encountering such “untouched” interface,
it would have been useful to get an induction and learn
about gestures and materials in a similar way that classical
instruments are learned; opening a question around virtu-
osity of textile instruments that is yet to be investigated.
If nothing else, fabrics as instruments should prompt a
playful, fun interaction. Hereby, the absence of high pre-
cision control was a low cost to carry for the benefit of a
conversational, anthropomorphised material collaboration.
In that context, it appeared as such anthropomorphism of
devices happens more easily if we encounter a material like
textiles, that are warm and more familiar. Also the of-
ten mentioned emotional material connection and associ-
ated textile products (blanket, towel, garment) is a design
aspect that other MIDI interfaces don’t provide and that
can be beneficial for sound art performances.
8. ACKNOWLEDGMENTS
We thank all participants and colleagues for their helpful
feedback, especially Giacomo Lepri and Nicola Privato for
their help with sounds mappings and Bela. This research is
supported by the European Research Council as part of the
Intelligent Instruments project (INTENT), under the Eu-
ropean Union’s Horizon 2020 research and innovation pro-
gramme (Grant agreement No. 101001848).
9. ETHICAL STANDARDS
To ensure the welfare and data protection of all involved
participants, the study was approved by the ethics commit-
tee of the University of the Arts Iceland and the University
of Iceland. Participants were handed information sheets and
signed consent forms.
10. REFERENCES
[1] S. Alexander-Adams and M. Gurevich. A Flexible
Platform for Tangible Graphic Scores. In Proceedings
of the International Conference on New Interfaces for
Musical Expression (NIME), Baton Rouge, LA, 6
2015.
[2] J. Armitage, T. Magnusson, and A. McPherson. A
Scale-Based Ontology of Musical Instrument Design.
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 339–349, 5
2023.
[3] K. Beilharz, A. Vande Moere, B. Stiel, C. Calo,
M. Tomitsch, and A. Lombard. Expressive Wearable
Sonification and Visualisation: Design and Evaluation
of a Flexible Display. In Proceedings of the 2010
Conference on New Interfaces for Musical Expression
(NIME 2010), Sydney, Australia, 2010.
[4] A. Boem, G. M. Troiano, G. Lepri, V. Zappi, and
Q. Mary. Non-Rigid Musical Interfaces:
}Exploring Practices, Takes, and Future Perspective.
In Proceedings of the International Conference on
New Interfaces for Musical Expression (NIME-20),
Birmingham, UK, 2020.
[5] H. Brugman and A. Russel. Annotating
Multi-media/Multi-modal Resources with ELAN. In
M. T. Lino, M. F. Xavier, F. Ferreira, R. Costa, and
R. Silva, editors, Proceedings of the Fourth
International Conference on Language Resources and
Evaluation (LREC’04), Lisbon, Portugal, 5 2004.
European Language Resources Association (ELRA).
[6] L. Buechley, M. Eisenberg, J. Catchen, and
A. Crockett. The LilyPad Arduino: Using
Computational Textiles to Investigate Engagement,
Aesthetics, and Diversity in Computer Science
Education. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ’08,
page 423–432, New York, NY, USA, 2008. Association
for Computing Machinery.
[7] D. Cavdir. Touch, Listen, (Re)Act: Co-designing
Vibrotactile Wearable Instruments for Deaf and Hard
of Hearing. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), 2022.
[8] A. Chang and H. Ishii. Zstretch: A stretchy fabric
music controller. In Proceedings of the 7th
International Conference on New Interfaces for
Musical Expression, NIME ’07, pages 46–49, 2007.
[9] L. Dahl, N. Whetsell, and J. V. Stoecker. The
WaveSaw: A Flexible Instrument for Direct Timbral
Manipulation. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), New York, USA, 2007.
[10] J. U. Davis. IllumiWear: A Fiber-Optic eTextile for
MultiMedia Interactions. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Porto Alegre, Brazil, 6
2019.
[11] M. Donneaud and P. Strohmeier. Designing a
Multi-Touch eTextile for Music Performances. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Aalborg,
Denmkar, 5 2017.
[12] G. Dublon and J. Paradiso. FingerSynth: Wearable
Transducers for Exploring the Environment with
Sound. In Proceedings of the International Conference
on New Interfaces for Musical Expression (NIME),
London, UK, 6 2014.
[13] S. Fels, R. Pritchard, and A. Lenters. ForTouch: A
Wearable Digital Ventriloquized Actor. In Proceedings
of the International Conference on New Interfaces for
Musical Expression (NIME), Pittsburgh, PA, USA, 6
2009.
[14] E. Franco, N. J. L. Griffith, and M. Fernstr ¨om. Issues
for Designing a flexible expressive audiovisual system
for real-time performance & composition. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), 2004.
[15] A. Freed. Application of new Fiber and Malleable
Materials for Agile Development of Augmented
Instruments and Controllers. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME08), Genova, Italy, 2008.
[16] A. B. Godbehere and N. J. Ward. Wearable Interfaces
for Cyberphysical Musical Expression. In Proceedings
of the International Conference on New Interfaces for
Musical Expression (NIME), Genova, Italy, 6 2008.
[17] M. Graf and M. Barthet. Mixed Reality Musical
Interface: Exploring Ergonomics and Adaptive Hand
Pose Recognition for Gestural Control. In Proceedings
of the International Conference on New Interfaces for
Musical Expression, The University of Auckland, New
Zealand, 6 2022.
[18] L. Grant. Felt Sensors, 2012.
[19] M. Grierson and C. Kiefer. NoiseBear: A Wireless
Malleable Instrument Designed In Participation with
Disabled Children. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Daejeon, Korea, 5 2013.
[20] J. Han and N. Gold. Lessons Learned in Exploring
the Leap Motion(TM) Sensor for Gesture-based
Instrument Design. In Proceedings of the
International Conference on New Interfaces for
Musical Expression, pages 371–374, London, United
Kingdom, 6 2014. Goldsmiths, University of London.
[21] Y. Han, J. Na, and K. Lee. FutureGrab: A wearable
synthesizer using vowel formants. In Proceedings of
the International Conference on New Interfaces for
Musical Expression (NIME), Michigan, USA, 5 2012.
[22] J. Harding, R. Graham, and E. Park. CTRL: A
Flexible, Precision Interface for Analog Synthesis. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME),
Blacksburg, Virginia, USA, 6 2018.
[23] K. Hayafuchi and K. Suzuki. MusicGlove: A
Wearable Musical Controller for Massive Media
Library. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Genova, Italy, 2008.
[24] L. F. Jones, H. Shen, J. Brown, and J. Boyd. A
Wearable Technology for Wind Musicians: Does It
Matter How you Breathe? In Proceedings of the
International Conference on New Interfaces for
Musical Expression, Mexico City, Mexico, 6 2023.
[25] C. Kiefer. A Malleable Interface for Sonic
Exploration. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Sydney, Australia, 6 2010.
[26] C. Kiefer, N. Collins, and G. Fitzpatrick. Phalanger :
Controlling Music Software With Hand Movement
Using A Computer Vision and Machine Learning
Approach. In Proceedings of the International
Conference on New Interfaces for Musical Expression,
pages 246–249, Pittsburgh, PA, United States, 2009.
[27] R. Koehly, D. Curtil, and M. M. Wanderley. Paper
FSRs and Latex/Fabric Traction Sensors: Methods
for the Development of Home-Made Touch Sensors. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Paris,
France, 6 2006.
[28] K. Konovalovs, J. Zovnercuka, A. Adjorlu, and
D. Overholt. A Wearable Foot-mounted /
Instrument-mounted Effect Controller: Design and
Evaluation. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Aalborg, Denmark, 5 2017.
[29] C.-H. Lai and K. Tahiro˘ glu. A Design Approach to
Engage with Audience with Wearable Musical
Instruments: Sound Gloves. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Michigan, USA, 5 2012.
[30] Y. Li, Z. Piao, and G. Xia. A Wearable Haptic
Interface for Breath Guidance in Vocal Training. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), 2021.
[31] M. J. Lyons, M. Haehnel, and N. Tetsutani.
Designing, Playing, and Performing with a
Vision-based Mouth Interface. In Proceedings of the
International Conference on New Interfaces for
Musical Expression, pages 116–121, Montreal,
Canada, 2003.
[32] T. Magnusson. Ergodynamics and a semiotics of
instrumental composition. TEMPO, 73(287):41–51, 1
2018.
[33] M. Marier. The Sponge. A Flexible Interface. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Sydney,
Australia, 6 2010.
[34] T. Mitchell, S. Madgwick, and I. Heap. Musical
Interaction with Hand Posture and Orientation: A
Toolbox of Gestural Control Mechanisms. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, Ann Arbor,
Michigan, 2012. University of Michigan.
[35] G. Moro, A. Bin, R. H. Jack, C. Heinrichs, A. P.
McPherson, and others. Making high-performance
embedded instruments with Bela and Pure Data.
2016.
[36] M. Myllykoski, K. Tuuri, E. Viirret, and
J. Louhivuori. Prototyping hand-based wearable
music education technology. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Baton Rouge, LA, USA,
2015.
[37] J. Nielsen. Usability Engineering. Academic Press,
Boston, Boston, MA, USA, 1993.
[38] J. Nugroho and K. Beilharz. Understanding and
Evaluating User Centred Design in Wearable
Expressions. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Sydney, Australia, 6 2010.
[39] S. Papetti, M. Civolani, and F. Fontana.
Rhythm’n’Shoes: a wearable foot tapping interface
with audio-tactile feedback. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Oslo, Norway, 2011.
[40] I. Posch. Crafting Tools for Textile Electronic
Making. In Proceedings of the 2017 CHI Conference
Extended Abstracts on Human Factors in Computing
Systems, CHI EA ’17, pages 409–412, New York, NY,
USA, 2017. Association for Computing Machinery.
[41] I. Posch and G. Fitzpatrick. Integrating Textile
Materials with Electronic Making: Creating New
Tools and Practices. In Proceedings of the Twelfth
International Conference on Tangible, Embedded, and
Embodied Interaction, TEI ’18, pages 158–165, New
York, NY, USA, 2018. Association for Computing
Machinery.
[42] E. R. Post, M. Orth, P. R. Russo, and
N. Gershenfeld. E-broidery: Design and fabrication of
textile-based computing. IBM Systems Journal,
39(3.4):840–860, 2000.
[43] J.-S. Roh, Y. Mann, A. Freed, and D. Wessel. Robust
and Reliable Fabric, Piezoresistive Multitouch
}Sensing Surfaces for Musical Controllers. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Oslo,
Norway, 6 2011.
[44] J. Ryan and C. Salter. TGarden: Wearable
Instruments and Augmented Physicality. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME), Montreal,
Canada, 2003.
[45] S. E. Ryan. Garments of paradise : wearable discourse
in the digital age. MIT Press, 2014.
[46] M. Satomi and H. Perner-Wilson. Kobakant DIY
Wearable Technology Documentation.
https://www.kobakant.at/DIY/?cat=24, 2007.
[47] M. Schebella, G. Fischbacher, and M. Mosher. Silver:
A Wire Mesh Textile Interface for the Interactive
Sound Installation Idiosynkrasia. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Birmingham, UK, 2020.
[48] A. Schmeder and A. Freed. Support Vector Machine
Learning for Gesture Signal Estimation with a
Piezo-Resistive Fabric Touch Surface. In Proceedings
of the International Conference on New Interfaces for
Musical Expression (NIME), pages 15–18, Sydney,
Australia, 6 2010.
[49] S. Skach, A. Stolfi, A. Xamb´ o, R. Stewart, L. Turchet,
and M. Barthet. Embodied interactions with e-textiles
and the internet of sounds for performing arts. In TEI
2018 - Proceedings of the 12th International
Conference on Tangible, Embedded, and Embodied
Interaction, volume 2018-Janua, pages 80–87, 2018.
[50] A. Stahl and P. Clemens. Auditory Masquing:
Wearable Sound Systems for Diegetic Character
Voices. In Proceedings of the International Conference
on New Interfaces for Musical Expression (NIME),
Sydney, Australia, 6 2010.
[51] R. Stewart. Cords and chords: Exploring the role of
e-textiles in computational audio. Frontiers in ICT,
6(MAR):1–12, 2019.
[52] R. Stewart, S. Skach, and A. Bin. Making Grooves
with Needles: Using e-textiles to Encourage Gender
Diversity in Embedded Audio Systems Design. In
Proceedings of the 2018 Designing Interactive Systems
Conference, DIS ’18, pages 163–172, New York, NY,
USA, 2018. Association for Computing Machinery.
[53] K. Tahiro, M. Kastemaa, and O. Koli. Al-terity:
Non-Rigid Musical Instrument with Artificial
Intelligence Applied to Real-Time Audio Synthesis. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME),
Birmingham, UK, 7 2020.
[54] S. Topley. Giant Noisy Pompoms, 10 2016.
[55] S. Topley. Playground - giant crochet ball musical
instruments, 10 2018.
[56] G. Torre and M. Fernstr ¨om. Celeritas: Wearable
Wireless System. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), New York, USA, 6 2007.
[57] K. Transpose. Mobile Clavier: New Music Keyboard
}for Flexible Key Transpose. In Proceedings of the
2007 Conference on New Interfaces for Musical
Expression (NIME07), New York, 2007.
[58] K. D. Tsoukalas and J. R. Kubalak. L2OrkMote:
Reimagining a Low-Cost Wearable Controller for a
Live Gesture-Centric Music Performance. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME),
Blacksburg, Virginia, USA, 6 2018.
[59] L. Turchet and M. Barthet. Demo of interactions
between a performer playing a Smart Mandolin and
audience members using Musical Haptic Wearables.
In Proceedings of the International Conference on
New Interfaces for Musical Expression (NIME),
Blacksburg, Virginia, USA, 6 2008.
[60] I. Wicaksono and J. A. Paradiso. KnittedKeyboard:
Digital Knitting of
}Electronic Textile Musical Controllers. In
Proceedings of the International Conference on New
Interfaces for Musical Expression (NIME),
Birmingham, UK, 7 2020.
[61] I. Wicaksono, J. A. Paradiso, and R. Environments.
FabricKeyboard: Multimodal Textile Sensate Media
as an Expressive and Deformable Musical Interface.
In Proceedings of the International Conference on
New Interfaces for Musical Expression (NIME),
Aalborg, Denmkar, 5 2017.
[62] D. Wilcox. robotcowboy: 10 Years of Wearable
Computer Rock. In Proceedings of the International
Conference on New Interfaces for Musical Expression
(NIME), Blacksburg, Virginia, USA, 6 2018.
[63] F. Yoshimura and K. Jo. A ”voice” instrument based
on vocal tract models by using soft material for a 3D
printer and an electrolarynx. In Proceedings of the
International Conference on New Interfaces for
Musical Expression (NIME), Porto Alegre, Brazil, 6
2019.