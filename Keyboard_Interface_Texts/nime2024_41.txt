MusiCane: an Accessible Digital Instrument inspired by
the white cane
Emmanouil
Dimogerontakis
Aalborg University
Copenhagen
manos-dimos@hotmail.com
Dan Overholt
AAU CREATE
Aalborg University
Copenhagen
dano@create.aau.dk
Stefania Serafin
AAU CREATE
Aalborg University
Copenhagen
sts@create.aau.dk
ABSTRACT
We introduce the design and implementation of MusiCane,
a musical device with the aim of creating new accessible
avenues for music-making to promote mutual engagement
across diverse social groups. MusiCane offers the possibil-
ity for active participatory music making, thereby broaden-
ing electronic music interactions and aesthetics. It seeks to
bridge different communities through playful and engaging
means, particularly including individuals who face barriers
in traditional music-making practices. This Accessible Dig-
ital Musical Instrument (ADMI) is conceptualized based on
insights derived from discussions and meetings with blind
individuals and therapists. Its primary objective is to ex-
plore the creative potential of the white cane as an inter-
active medium. The design process involved incorporating
feedback and perspectives from these stakeholders, to en-
sure the device’s relevance and effectiveness in addressing
the unique needs and experiences of users with visual im-
pairments. To realize these objectives, a musical interactive
installation has been produced, marking the first iteration
of a prototype for a multi-user experience. The project
not only contributes to the inclusive design of musical in-
struments, but also strives to create an environment where
individuals from various backgrounds can come together,
fostering collaboration, creativity, and engagement in the
realm of music.
Author Keywords
ADMI, Musical interaction, Mutual engagement
CCS Concepts
•Applied computing → Sound and music computing;•Human-
centered computing → Interaction devices;
1. BACKGROUND
1.1 Motivation
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’24, 4–6 September, Utrecht, The Netherlands.
This project is a continuation of the research of MultiSen-
sory lab (ME-lab) of Aalborg University that the author
was part of during his internship. The starting points of
the project were to investigate recent advancements of mu-
sic technology in the field of ADMIs, and insights from a
visit with a small team of researchers who work in ME-lab
in the Blindecenter Bredegaard located in Fredensborg.
The visit to the center for the blind involved an introduc-
tory discussion on the challenges faced by visually impaired
individuals with multiple disabilities, discussing therapeu-
tic strategies and individualized goals. The researchers met
the residents, shared their work on inclusivity practices, and
observed daily life and creative activities within the center.
This provided valuable insights into therapeutic practices,
addressing individual needs and disabilities through creative
multimodal therapies. Creating a daily routine to stimulate
senses and enhance the quality of life for people with multi-
ple disabilities is complex, and varies among individuals.
The visit highlighted well-being inequalities, particularly
the social isolation faced by the community, a challenge ob-
served in various communities of individuals with different
disabilities[15].
Based on this initial research, the design goals of the
project were determined:
• Inclusion of visually impaired people to electronic mu-
sical practices with a collaborative instrument.
• Social inclusion of sighted and non-sighted novices
through active music making.
• Bridge the communities of normal sighted people and
visually impaired people.
• Help the non-visually impaired people to be more em-
pathetic towards visually impaired people.
• De-medicalization of creative therapeutic practices.
1.2 Design notes
There is growing interest within the NIME community in
creating ADMIs, promoting the inclusion of individuals with
impairments in musical activities [5]. The concept of using
music technology for inclusion can be traced back to the
creation of the Rhodes electric piano, and with the advance-
ment of technology, it is now possible to customize ADMIs
to meet the specific needs and abilities of individual users
or groups. In fact, some ADMIs even have the capability to
adjust the interaction for each user in real-time[5].
Designing such instruments could make the music making
experience more accessible to impaired people by eliminat-
ing physical and intellectual barriers that people are ex-
periencing with traditional musical instruments, although
different social and technical barriers are being created.
Physical barriers are easier to identify and therefore eas-
ier to tackle, analyze, and create interactions and designs.
The intellectual barriers have a tendency to withdraw them-
selves from the observer, and consequently the design pro-
cess needs a more dialogical and iterative approach[8] [20].
In this project, we present the first iteration of a pro-
totype collaborative ADMI between visually impaired peo-
ple and sighted people, taking into consideration that each
design element needs to include the disabled people’s feed-
back without having the approach of ’fixing the problems
of impaired people’ through the prism of non-disabled re-
searchers. It is important to have an active dialog between
users, therapists and designers to explore different affor-
dances of different communities, and overcome the fear of
using and testing new electronic musical devices[20] [9].
2. RELATED WORK / STATE OF THE ART
Assistive Music Technology (AMT) is an established field
with significant contribution from the music therapy com-
munity, although within Computer Music literature much of
the work is considered relatively unexplored[5][10]. Recent
literature and discussions about designing musical interac-
tions for disabled people question the usage and the impact
in their users. This leads to more broad questions in human
centered design about the design savior complex[2].
In [20] they present a middle ground in the dipole of
“tokenistic” and “participation-ism” forms of the designer’s
role. The work suggests using a dialogic design approach
when it comes to create musical interactions, overcoming
the problems of misinterpreting the true needs of the user’s
because of the designers’ idea of what is like to be. And
also, the dictating approach of designing without outer in-
fluences. Simplifying this idea, the designer should be part
of the designing cycle and not in the top of the pyramid
when a design decision must be taken.
Another vital discussion in the literature [5], [7], points
out that it is crucial to form a distinction between which
theoretical model (Social model or Medical model of Dis-
ability) is more inclusive when developing a design for mu-
sical practices. The medical model suggests that a disabling
barrier lies on the user, and therefore puts the user in a po-
sition that there is something wrong within him/her, while
the social model suggests that dictating affordances of musi-
cal interfaces and exclusive attitudes are the disabling fac-
tors, therefore invention of new musical devices that will
serve the affordances of each specific community or user
can be conducted. Within the same conceptual realm, Frid
proposed that the terms assistive and adaptive fall into the
medical model, because they put the stakeholders in the po-
sition that they need/seek for help and that the designers
adapt existing technologies for specific users/disabilities.
Recent ADMIs focus on specific abilities of particular
groups, using a wide range of different technologies and
touchless or tangible interactions[8], like eye sensors1, touch
sensors[1], breath sensors, etc. It is promising that there are
commercial products released in the form of ADMIs. One of
them is the eye harp, where an eye sensor detects the move-
ment of the eyes and maps it to various audio parameters
via computer software. Another product is the skoog, which
consists of a cube where the user triggers audio events from
the 5 surfaces through embedded touch sensitive sensors.
The cube is connected to an app, and the user has access
to different abstractions for musical training and entertain-
ment. Based on the design, it is possible to have one or
more users with one or multiple cubes.
1https://eyeharp.org/
An additional ADMI with a design focused for Special Ed-
ucational Needs (SEN) schools is the LoopBlocks[8]. This
apparatus is a DIY tangible wooden step sequencer with
photoresistors as the sensors for interaction. By blocking
the light with a rounded token, the user unmutes the as-
signed step of the sequence to trigger musical events. It
consists of 4 different rows, with each of them representing
an audio cue, and allows interaction between multiple users.
In current studies, there is a shift towards multimodal
designs like the installation Sound Forestthat is presented
in [6]. The installation consists of laser-emitting interactive
strings and vibro-tactile platforms and speakers. The in-
teraction was placed in a room of the Swedish Museum of
Performing Arts in Stockholm, and it was tested with indi-
viduals from different abilities and ages. Musical interaction
came from the excitation of virtual strings, by triggering au-
dio events/samples in a pre-made composition, a technique
called adaptive music which is being used widely in video
games. The users received multimodal feedback in the form
of vibrations, sounds and visuals. The haptic feedback was
a whole body experience, with the vibration being produced
in the floor by 2 tactile loudspeakers.
3. ETHICAL STATEMENT
While MusiCane’s final target group includes a socially vul-
nerable minority as one of the stakeholder groups, testing
with blind users was omitted due to the limits of our scaled-
down prototype. We endeavor to include the full spectrum
of needs and backgrounds, providing inclusive access to our
design in the near future - when we have a full-scale pro-
totype. In this way, all individuals will be able to access
and benefit from the technology, and be able to take part in
our research. As blind user’s needs are not evaluated in this
paper, we acknowledge this as a limitation of our research
outcomes, instead of a limitation of the people or boundary
on the people who might like to use it.
4. DESIGN
When creating an DMI, one significant consideration is the
causal relationship between action and sonic outcome. For
ADMIs, this relationship needs to occur automatically, es-
pecially when users don’t have any experience performing
electronic instruments. To reduce the cognitive load of in-
teracting with an ADMI and create a causal relationship
experience, some design conditions need to fulfill:
• Direct mapping between input and output.
• Multimodal feedback.
• Comprehensive and known affordances.
In these studies [3] [10] the work presents a set of prin-
ciples and considerations for the development of musical
instruments for people with disabilities. Extending these
principles, Frid made a collection of properties for design-
ing and evaluating an ADMI. These 9 properties are Ex-
pressiveness, Playability, Longevity, Customizability, Plea-
sure, Sonic quality, Robustness, Multimodality and Causal-
ity [7]. Other aspects of the design can be considered as
well, such as the educational possibilities, to be plug-and-
play, to adopt the musical preferences of the users, to inspire
derivative designs, and to be low cost.
4.1 Initial Idea
Our design approach centers around exploring music inter-
actions with the white cane, recognizing its unfamiliarity
to the majority despite being indispensable for the visually
impaired. Different types of canes serve distinct purposes,
but primarily navigation and obstacle avoidance. The white
cane’s unique affordances are often misunderstood, com-
pounded by its limited perception as a functional assistive
device due to medicalization.
To enhance interaction with the MusiCane, designated
spaces with distinctive tiles or steps are incorporated, creat-
ing an auditory landscape for users. Each tile offers unique
sensory cues, allowing visually impaired individuals to nav-
igate with auditory/tactile feedback. This design aims to
creatively use the white cane, familiarize sighted individuals
with it, and bridge the gap between blind and sighted com-
munities. The shared experience promotes mutual under-
standing and empathy, fostering open conversations about
diverse perspectives.
4.2 Current Iteration
The authors employed an iterative design process to assess
usability and address challenges. A key obstacle was the
technological limitation of achieving a wide detection ranges
for tiles. To overcome this, the prototype was scaled down,
featuring a custom cane with haptic and auditory feedback,
a tactile surface, and an iOS musical application. Initial us-
ability testing focused on individuals without visual impair-
ments to evaluate basic functionality before accommodating
visually impaired users. This approach ensures a controlled
assessment and valuable insights for further refinement.
4.3 Tangible Interaction
The musical interaction involves a custom cane, a tactile
surface, and an iOS application. The cane, equipped with
sensor technologies, generates unique sonic events based on
its placement on the tactile surface. The surface, designed
for tactility, creates a dynamic environment responsive to
the cane’s movement.
The iOS application enables dyadic interaction in this
iteration of the project, allowing a second user to actively
participate. Using the iPad’s touchscreen, this second user
(normally sighted) manipulates parameters, influencing the
auditory landscape created by the cane’s movements.
4.4 Sound Synthesis
For sound synthesis, a straightforward subtractive synthe-
sizer was implemented, allowing users to control various pa-
rameters. The initial user selects a path, and the second
user activates MIDI notes corresponding to each step on the
chosen route. The synthesizer, serving as the foundational
tool, enables dynamic adjustments to filters, oscillators, and
envelopes, adding musicality to the navigation process and
introducing users to basic sound synthesis.
In practice, the chosen path determines the sequence of
MIDI notes triggered by the custom cane on the tangible
surface. These notes are interpreted by the synthesizer, cre-
ating a real-time auditory representation. The second user
can then creatively modify the sonic characteristics through
the iOS app.
4.5 Feedback design
Multimodal feedback is important for the design of DMIs
in general [17]. Acoustic instruments include a variety of
multisensory instant feedback (haptic/visual/sonic). Po-
tentially, multisensory feedback enhances the musical inter-
action and the sensation of controlling the sonic outcome.
Recent studies suggest that vibrations play a significant role
in music perception and improve the music experience [16].
The audio output consists of a single channel, which plays
synthesized notes triggered by user actions. Haptic feedback
is provided through a vibro-tactile sensation in the cane
handle, activated when users make incorrect steps on the
surface. The surface, designed for tactile feedback, responds
to both cane contact and direct touch. This streamlined
system offers an auditory and haptic experience, guiding
users through navigation with a combination of synthesized
sounds and responsive vibrations.
4.6 Mutual Engagement
Building upon the insights from [4] [18], design features are
incorporated to foster mutual engagement in music creation.
Our approach was guided by the following design principles:
• Mutual awareness of action.
• Shared and consistent representations.
• Mutual modifiability.
Recognizing the potential cognitive overload associated
with implementing all proposed design features, we strate-
gically divided tasks and interactions within the system [4].
For the first principle, mutual awareness of action, we
achieved this by configuring the system to trigger audio
events. Subsequently, the other user gains the capability
to sculpt these events, allowing for a dynamic and collabo-
rative musical experience.
To address the second principle, shared and consistent
representations, specific parameters of the shared experi-
ence were mapped to each user, remaining unchanged over
time. This approach ensures that users can intuitively com-
prehend their individual contributions to the sonic outcome
throughout the interaction.
Lastly, for mutual modifiability, each user was assigned
parameters of the sound synthesis to enable the modifica-
tion of their shared sonic output. This division of responsi-
bilities facilitates a collaborative and dynamic environment
where both users actively contribute to shaping the musical
experience.
4.7 Hardware and Software Design
The ADMI, initially envisioned as a standalone wireless mu-
sical instrument with Bluetooth, utilizes the RFID reader
for tile/step detection and as a controller for external mu-
sical equipment. It supports various protocols for control-
ling software applications (OSC, MIDI), digital synthesiz-
ers/effects, and synthesizers
Converting RFID tags to MIDI allows exploration of dif-
ferent mappings, unlocking creative possibilities for musical
interactions. To receive audio and vibro-tactile output, a
Bluetooth-enabled microprocessor development board was
integrated into the cane, enhancing flexibility.
In software design, Pure Data (PD) and MobMuPlat, a
mobile music platform, were employed for prototype user
interfaces and audio engines.
5. IMPLEMENTATION
In this iteration, a bespoke cane was crafted using a laser
cutter, featuring embedded microprocessors to transmit data
from the ground— an acrylic surface—to the iPad. Simul-
taneously, it receives the audio and vibro-tactile feedback
generated by the interaction of users with the iPad appli-
cation and the cane. The elements of this interaction are
visually represented in the accompanying Figure 1.
The first user engages with the iPad application by se-
lecting a specific path (see number 1 in the Figure 1), while
the second user tries to trace and activate the desired path
in the acrylic surface (see number 2 in the Figure 1) by
triggering MIDI notes using the cane (see number 3 and 4
in the Figure 1). Through the activation of these notes,
the first user gains the ability to interact with the music
application, modifying parameters of the sound synthesis.
Also, the second user controls a parameter by adjusting the
timing that triggers the next note.
Figure 1: The first iteration of the prototype.
5.1 Hardware Implementation
5.1.1 Tip of the Cane
The central interactive element of the ADMI is the RFID
reader module placed in the tip of the cane (as it is illus-
trated in Figure 1 number 3), and it is capable of detecting
the RFID tags where are located in the acrylic surface. The
module is designed around the MFCR-522 chipset. In the
module, a PCB antenna is integrated which generates a high
frequency electromagnetic field to detect tags. The module
utilizes SPI protocol for data transmission to a microcon-
troller. The RFID module at the tip of the cane is connected
to an ESP32 Thing Plus microcontroller. This board was
selected because of its wireless connectivity features based
on the ESP32-WROOM, a generic Wi-Fi, Bluetooth, and
BLE (Bluetooth Low Energy) MCU module.
5.1.2 Wireless Audio and Vibro-tactile feedback
To establish a solid and wireless playback sound produced
by the iPad application, the ESP32-Audio-kit development
board was chosen. This audio board allows receiving and
transmitting high fidelity Bluetooth audio. We used an
ESP32-A1S board with an internal AC101 codec and a de-
velopment board that includes a headphone jack for audio
output alongside pin-headers for left/right speakers. This
hardware was configured as a Bluetooth receiver for both
haptic and sonic feedback by adapting Phil Schatmzmann’s
basic-a2dp-audiokit.ino. In the Left Speaker output of the
board a speaker was connected to provide the audio output
and in the Right Speaker output a tactile audio transducer
was connected to provide the haptic feedback.
5.2 Software Implementation
5.2.1 UID (Unique IDentifier) to MIDI
To create a wireless communication and convert the UIDs
to MIDI information, a custom code in the Arduino IDE
was created, which was flashed in the ESP32 Thing Plus
board. Three main libraries were used to create that code,
the MIDI library, the BLE library and the MFRC522 li-
brary.
After this, the RFID reader was enabled to scan and de-
tect the UID tags. Each UID is converted into MIDI note
on information. In more detail, code was created to trans-
late each UID to a different MIDI note, and inside these
statements MIDI messages for note-on and note off are es-
tablished with an assigned MIDI note.
5.2.2 MobMuPlat
MobMuPlat [12] is a platform for running Pure Data (Pd)
patches on mobile devices. It handles Pd patches, user
interfaces, networking, mobile hardware interactions, and
communication with external devices like MIDI and HID.
The GUI is customizable, and MobMuPlat supports iOS,
Android, OSX, and cross-platform (Java Swing) editors for
development and distribution.
For the creation of the GUI, a 4x4 grid with on-off switches
was created in the upper side of the screen. The grid repli-
cates the tiles/steps in the acrylic surface and by activating
them, the specific note is enabled and will be reproduced
when it will be triggered. At the bottom, four faders, 3
additional buttons and one knob were placed to control the
parameters of the subtractive synthesizer.
Figure 2: The GUI of the musical application. In the up-
per part the Grid is placed for choosing the path. And in
the bottom part there are the parameters of the subtractive
synthesis. 1-4 are the ADSR parameters, 5-7 are the wave
form selection and 8 is the filter cut-off.
Inside the Pure Data patch, the information from the
custom GUI is being received in the form of OSC messages.
The states of each particular step in the grid are stored in
a buffer and then are unpacked to match a specific MIDI
note. Then by inserting the MIDI notes into the patch are
being separated using an if statement. The logic of the if
statement is that if a specified midi note and a specified
point of the grid are activated, then this note is triggered.
In the same spirit, the vibro-tactile feedback is produced,
again if statements are used to filter the MIDI information
and trigger the oscillator that is mapped to the haptic feed-
back. The statement was build around the idea that when a
midi note is trigger from the RFID reader, but the assigned
point on the grid is deactivated, then the fixed time envelope
of the oscillator is triggered. One oscillator is a sine wave
using the object [osc ∼]with a fixed frequency of 220Hz,
which is around the peak frequency of the tactile sensitiv-
ity [?]. The audio engine consists of a custom subtractive
synthesizer. Then main elements are three oscillators with
different waveforms, sine, triangle, and square. The pitch
of the oscillators is controlled by the MIDI notes that have
been passed from the if statement and are converted to fre-
quencies. Then these oscillators are interconnected with a
filter, followed by an amplification stage. In this stage, an
ADSR envelope is connected and also an LFO (Low Fre-
quency Oscillator) was added to the final stage in order to
create an amplitude modulation effect.
5.3 Mappings
One of the reasons for the creation of new instruments is to
allow real-time control of new sound-worlds, and the control
of existing timbres through alternative interfaces to enable
individuals in the spontaneous creation of music [14]. In
computer music, mapping is often used in relation to algo-
rithmic composition, where a parameter with a particular
set of values is scaled or transformed so that it can be used
to control another parameter.
The decision was to use explicit mapping strategies, pre-
senting the advantage of keeping the designer in control of
the implementation of each of the instrument’s component
parts, and therefore providing an understanding of the ef-
fectiveness of mapping choices in each context. Also, well-
defined mappings are one of the most important aspects
when it comes to design an ADMI [8][3] to reduce the cog-
nitive load of the user. A last key component for choosing
explicit mapping strategies is the design principles for mu-
tual engagement in music making that were presented is
the in the Section DESIGN, followed by the literature [4].
We believe that for achieving shared and consistent rep-
resentations and mutual awareness of action, that explicit
mappings were the best choice.
5.3.1 Explicit mapping strategies
The available literature generally considers a good choice of
mapping of performer actions to sound synthesis parame-
ters as a few-to-many relationship. Considering two general
sets of parameters, three intuitive strategies relating the pa-
rameters of one set to the other can be devised as [11]: one-
to-one, where one synthesis parameter is driven by one per-
formance parameter, one-to-many, where one performance
parameter may influence several synthesis parameters at the
same time, and many-to-one, where one synthesis parame-
ter is driven by two or more performance parameters.
Concerning explicit mappings between two sets of pa-
rameters, many different abstractions of the performance
parameters have been proposed, from perceptual parame-
ters to focusing on continuous parameters to changes rep-
resented by gestures produced by the user [19].
5.3.2 Two and Three-layer mapping
For MusiCane’s explicit mapping strategy, a two and three-
layer model is being adopted. The first layer is interface-
specific, converting GUI information into chosen parameters
for sound synthesis. Two parameters are then mapped into
a second layer for specific synthesis controls. This model
allows flexibility in changing synthesis engines by address-
ing the second layer. We primarily use two-layer mappings
for gesture-to-sonic output linkage in the subtractive syn-
thesis. However, the grid for path selection and the RFID
activation of MIDI notes involve three-layer mappings.
Attack Blue fader 1 in the GUI
Decay Blue fader 2 in the GUI
Sustain Blue fader 3 in the GUI
Release Blue fader 4 in the GUI
Sine wave Button 5 in the GUI
Triangle wave Button 6 in the GUI
Square wave Button 7 in the GUI
Filter cut-off Knob 8 in the GUI
MIDI note state (mute/unmute) GUI Grid
Table 1: The two-layers mappings. All the parameters of the
subtractive synthesizer mapped to the GUI elements (see
also Figure 2).
The selection of the mappings between the users was
made based upon the idea of what each of the users are
interacting with, the number of the parameters of the syn-
thesis and the mutual engagement. The user with the cane
was controlling the :
• Trigger of the event
• Pitch
• Vibrato Rate
And the user with the iPad was controlling the :
• Midi Note state
• ADSR
• Different wave forms
• Filter cut-off
The third layer mappings hold the essence of our interac-
tion — influencing the path formation, the cane interaction,
and the tracing of the path. We’ve deliberately kept these
features modular. The GUI grid and acrylic surface serve
as the key elements, designed to carry MIDI information.
This format flexibility allows integration with various sound
engines without requiring adjustments. We feel that these
mappings, for example the time difference mapped to vi-
brato rate, transforms the cane’s identity from a navigation
tool into a source of musical expression.
6. EV ALUATION
For the experiment, 22 people in 11 pairs (aged from 23
to 52, mean age: 28, with 10 male, 9 female and 3 other),
were recruited on a voluntary basis. The participants were
presented with the goal of the experiment and a written
consent was obtained prior to each participation. The ex-
periment was conducted at Aalborg University (campus of
Copenhagen), in the Augmented Performance Lab.
The experiment involved each pair of participants in three
phases. In the first phase, participants received an introduc-
tion to the ADMI system, its components, rules, and tasks.
They explored the system to familiarize themselves.
In the second phase, participants chose initial roles; the
user with a cane wore a blindfold for realism. They alter-
nated roles, with the iPad user providing navigation instruc-
tions and adjusting synthesis parameters. The tasks were
completed before they switched roles.
MIDI activation (Pitch) acrylic surface
Vibrato Rate (LFO time) Time difference of activation
between current and previous note
Table 2: The three-layers mappings. All the parameters of the subtractive synthesizer that mapped to the Grid and the cane.
The final phase involved a questionnaire with 22 ques-
tions, divided into two parts. The first part, based on the
System Usability Scale (in order to ascertain the interaction
intuitiveness), included 12 questions with a 7-point Likert
scale. The second part had 10 questions with a 5-point Lik-
ert scale, evaluating mutual engagement and social impact.
Participants provided oral feedback. The experiment lasted
40-50 minutes per pair.
Figure 3: Two participants try the interaction.
6.1 Data Analysis
The overall score was 72%, rated as GOOD in the SUS scor-
ing system. Regarding the statement, “I would like to use
it again” 90.9% of the users had a positive answer. For the
statement “I found the overall experience pleasant” 86.4%
of the participants stated a positive answer, while 2 of them
(9.1%) had a neutral response. In the statement “the inter-
action was too complex for me” 77.3% of the participants
responded negatively, while 2 had a neutral response. In the
statement “I perceived both the interactions with the stick
and the app unresponsive and without coherence”, 81.8%
of participants stated a negative response, with 2 neutral
responses. Lastly, towards the statement “I perceived both
the interactions with the stick and the app unresponsive and
without coherence”, and “the interaction was too complex
for me”, 81.8% responded negatively about the incoherence
of the system with 2 individuals neutral and 77.3% were
negative towards over-complexity.
For the mutual engagement and the social impact part
of the questionnaire, the quality of the responses was able
to give various insights. Overall, the scores indicate a well-
defined purpose for the system, evident not just from ex-
plicit descriptions but more importantly from seamless par-
ticipant interaction. The project’s objectives were effec-
tively conveyed in the trials, as was demonstrated through
engaged participant experiences. Notably, 21 out of 22 par-
ticipants responded positively to trying it with a visually
impaired person, with only 1 responding neutrally. Addi-
tionally, most participants did not feel discomfort or over-
whelmed, with 6 responding positively, 1 neutrally, and 14
negatively to discomfort.
The participants seemed to be highly engaged with their
partners through the interaction. Regarding the statement
“I enjoyed playing together with my partner”, 20 out of 22
individuals replied positively and 2 replied neutrally. To-
wards “I didn’t feel like interacting with my partner” 18
people replied negatively and 4 neutrally. And an interest-
ing aspect is that 13 people felt more creative after collab-
orating with their partner, 5 had a neutral position and 4
negative.
In summary, all participants successfully completed tasks
using both interaction mediums. Despite some challenges,
participants actively engaged, investing time to become more
familiar. The feedback from questionnaires, discussions,
and observations bolstered our confidence to further develop
the system.
7. DISCUSSION
MusiCane’s overall construction was robust enough to be
tested as prototype, including responsive hardware and soft-
ware, and reliable Bluetooth communication. Participants
didn’t report latency issues in haptic feedback or audio.
LiPo batteries were effective, though regular charging was
needed for the high-power consumption of the speaker at
loud volumes. The musical app’s GUI showed stable re-
sponses, with minor bugs related to GUI scaling. Audio
synthesis performed well, with no reported sonic or control
issues during testing. Areas for improvement include en-
hancing UID to MIDI conversion, by incorporating MIDI
off information when the card isn’t detected. Exploring
methods to store MIDI information directly as UID data
on passive tags and adding MIDI velocity through distance
detection or a second sensor could also be valuable.
User feedback, collected through questionnaires and in-
terviews, was positive. The goals of promoting mutual
engagement, introducing novices to electronic music, and
exploring social impacts were successfully met. Users en-
joyed the collaborative and unconventional nature of the
interaction, rediscovering affordances by co-creating musi-
cal output. Technical aspects were recognized for improve-
ment, also suggesting refinements in testing methodologies
and questionnaires. The haptic feedback, even in its simple
form, was found helpful for navigation, making sense of the
interaction. Insights gained from user experiences provide a
strong foundation for advancing and refining the MusiCane
platform.
Completing the initial idea requires a careful, step-by-
step design approach to address emerging issues, particu-
larly for individuals with different abilities facing potential
stigma. Addressing ontological uncertainties, or “unknown
unknowns,” involves creating low-functionality prototypes,
focusing on key design features in an iterative process[13].
Future steps include exploring long-range RFID technology,
testing Ultra-High Frequencies (UHF), and experimenting
with conductive and flexible filament for 3D printed tactile
layers on touch screens. Including more sensors like a gyro-
scope in the MusicCane is also planned in the future. These
are very important to be completed before the next stage
of evaluation, in order to build a full-scale MusiCane and a
larger interactive (floor-based) zone, which can be then be
evaluated by blind users as main stakeholders.
Software improvements involve creating more abstractions
for sound synthesis, musical games, and multi-purpose in-
teractions. Participant feedback suggested developing a heatmap-
like system for vibrations, offering amplitude-sensitive hap-
tic feedback based on tile distance from the desired path.
To pursue these ideas, a dialogic design strategy involving
people and therapists experienced in blindness is essential.
8. CONCLUSIONS
This work aimed to develop a first prototype of an ADMI
inspired by conversations with blind people and focusing
on collaborative music making practices between blind and
sighted people. The MusiCane system consists of a proto-
type augmented cane, a surface and a musical app, where
users mutually interact in pairs to co-create music with a
subtractive synthesizer built with Pure Data. MobMuPlat
was used to design an app running Pd, allowing users to
create and find musical paths in the surface. The main goal
of each pair of individuals was to create and find a path by
activating MIDI notes, and then playing with parameters
of the synthesis to create electronic music together.
An experiment was contacted to assess the usability of
this creation, the mutual engagement and the potential so-
cial impact on the users. The evaluation contacted with 22
sighted participants in pairs of 11. The work contributes
to a relatively new interdisciplinary research field, covering
the areas of inclusion practices and NIME. It is hoped that
the research done during this study can work as a founda-
tion for future investigations in these fields, creating new
engaging musical experiences resulting in new perspectives
on musical expression, and new frameworks for musical col-
laboration between visually impaired and sighted people.
9. REFERENCES
[1] Skoogmusic | easy to play musical instruments for
ipad, iphone skoog 2.0.
[2] C. Bennett and D. Rosner. The promise of empathy:
Design, disability, and knowing the ”other”. pages
1–13, 04 2019.
[3] A. Blatherwick, L. Woodbury, and T. Davis. Design
considerations for instruments for users with complex
needs in sen settings. In Proceedings of the
International Conference on New Interfaces for
Musical Expression, pages 216–221, 2017.
[4] N. Bryan-Kinns. Mutual engagement in social music
making. In A. Camurri and C. Costa, editors,
Intelligent Technologies for Interactive Entertainment,
pages 260–266, Berlin, Heidelberg, 2012. Springer
Berlin Heidelberg.
[5] E. Frid. Accessible digital musical instruments—A
review of musical interfaces in inclusive music
practice. Multimodal Technologies and Interaction,
3(3), 2019.
[6] E. Frid, H. Lindetorp, K. F. Hansen, L. Elblaus, and
R. Bresin. Sound Forest: Evaluation of an Accessible
Multisensory Music Installation. In Proceedings of the
2019 CHI Conference on Human Factors in
Computing Systems, pages 1–12. ACM, 2019.
[7] E. Frid, C. Panariello, and C. N´ u˜ nez-Pacheco.
Customizing and Evaluating Accessible Multisensory
Music Experiences with Pre-Verbal Children—A Case
Study on the Perception of Musical Haptics Using
Participatory Design with Proxies. Multimodal
Technologies and Interaction, 6(7):55, 2022.
[8] A. F ¨orster and M. Komesker. LoopBlocks: Design
and Preliminary Evaluation of an Accessible Tangible
Musical Step Sequencer. PubPub, 2021.
[9] A. F ¨orster and S. Lepa. Digital Musical Instruments
in Special Educational Needs Schools: Requirements
from the Music Teachers’ Perspective and the Status
Quo in Germany. ACM Transactions on Accessible
Computing, page 3616015, 2023.
[10] K. Graham-Knight and G. Tzanetakis. Adaptive
Music Technology: History and Future Perspectives.
2015.
[11] A. Hunt and M. M. Wanderley. Mapping performer
parameters to synthesis engines. Organised Sound,
7:97–108, 2002.
[12] D. Iglesia. The Mobility is the Message: the
Development and Uses of MobMuPlat.
[13] M. B. Jensen, C. W. Elverum, and M. Steinert.
Eliciting unknown unknowns with prototypes:
Introducing prototrials and prototrial-driven cultures.
Design Studies, 49:1–31, 2017.
[14] R. Kirk, M. Abbotson, R. Abbotson, A. Hunt, and
A. Cleaton. Computer music in the service of music
therapy: the midigrid and midicreator systems.
Medical Engineering Physics, 16:253–258, 1994.
[15] U. M. MacGlone, J. Vamvakaris, G. B. Wilson, and
R. A. R. MacDonald. Understanding the wellbeing
effects of a community music program for people with
disabilities: A mixed methods, person-centered study.
Frontiers in Psychology, 11, 2020.
[16] S. Merchel and M. E. Altınsoy. Auditory-tactile music
perception. The Journal of the Acoustical Society of
America, 133:3256, 2013.
[17] E. R. Miranda and M. M. Wanderley. New digital
musical instruments: Control and interaction beyond
the keyboard (computer music and digital audio
series). 2006.
[18] K. Stensæth. “musical co-creation”? exploring
health-promoting potentials on the use of musical and
interactive tangibles for families with children with
disabilities. International journal of qualitative studies
on health and well-being, 8:20704, 08 2013.
[19] M. Wanderley and P. Depalle. Gestural control of
sound synthesis. Proceedings of the IEEE,
92(4):632–644, 2004.
[20] E. Zayas-Garin and A. McPherson. Dialogic Design of
Accessible Digital Musical Instruments: Investigating
Performer Experience. PubPub, 2022.