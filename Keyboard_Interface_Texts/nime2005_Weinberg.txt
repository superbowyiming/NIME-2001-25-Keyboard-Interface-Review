iltur – Connecting Novices and Experts Through 
Collaborative Improvisation  
Gil Weinberg 
Georgia Tech 
840 McMillan St. 
Atlanta GA 30332 
(1) 404-894-8939 
gil.weinberg@coa.gatech.edu 
Scott Driscoll 
Georgia Tech 
840 McMillan St. 
Atlanta GA 30332 
(1) 404-894-8939 
gtg137p@mail.gatech.edu
  
ABSTRACT 
The iltur system features a novel method of interaction between 
expert and novice musicians through a set of musical controllers 
called Beatbugs. Beatbug players can record live musical input 
from MIDI and acoustic instruments and respond by transforming 
the recorded material in real-time, creating motif-and-variation 
call-and-response routines on the fly. A central computer system 
analyzes MIDI and audio played by expert players and allows 
novice Beatbug players to personalize the analyzed material using 
a variety of transformation algorithms. This paper presents the 
motivation for developing the iltur system, followed by a brief 
survey of pervious and related work that guided the definition of 
the project’s goals. We then present the hardware and software 
approaches that were taken to address these goals, as well as a 
couple of compositions that were written for the system. The 
paper ends with a discussion based on observations of players 
using the iltur system and a number of suggestions for future 
work.  
Keywords 
Collaboration, improvisation, gestrual handheld controllers, 
novices, mapping 
1.  MOTIVIATION 
Recent developments in music technology led to a cultural and 
social transformation in the manner in which we make, perform, 
and consume music. Music today is more accessible than ever 
thanks to innovations in recording, compression, and distribution. 
New developments for the home studio allow more people, 
novices as well as professionals, access to high quality and 
affordable equipment to create and distribute their music directly 
to their audiences. These promising developments, however, are 
undermined by some byproduct social effects. It has been shown 
that although music toady is more accessible and ubiquitous than 
ever, most of the music that we listen to is consumed in an 
incidental, unengaged and/or utilitarian manner [1]. Furthermore, 
the home studio proliferation undermines one of the most valuable 
traits of music – its collaborative and social nature – by promoting 
private and isolated musical practice where the value of live group 
interaction is marginalized. The iltur system attempts to address 
these social effects by allowing novices to become actively 
engaged in rich, thoughtful, and meaningful musical activities as 
they transform and personalize experts’ musical improvisation. 
The transformation algorithms embedded in the system aim to 
provide both simple low-level musical control (such as direct 
manipulation of pitch, timbre, and rhythmic values) and more 
elaborate control of high-level musical percepts that have been 
shown to be perceived by novices, such as rhythmic stability and 
contour similarity [2, 3]. We believe that by altering and 
personalizing elaborate musical phrases of their choice, novices 
could be part of a meaningful and expressive musical experience 
that does not necessarily require prior theoretical knowledge or 
advanced performance skills. Our system also attempts to use 
digital communication to create novel interdependent group 
playing interactions, in contrast with some of the home-studio 
technologies that undermine music’s inherent collaborative social 
nature. Thanks to these communication lines, and unlike 
traditional improvisation in a group, Beatbug players can gain 
direct control over their peers’ music. The outcome of the system 
can, therefore, be seen as a crossbred hybrid musical product, 
created by the combination of experts’ educated virtuosic 
musicianship and novices’ unmediated expressive musicality. 
 
Figure 1. The Beatbugs 
2. RELATED WORK 
Previous work with the Beatbug controller [4] in the framework of 
the Toy Symphony project [5] showed that even novices who 
demonstrated high levels of musicality, expression, and a unique 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
Nime’05, May 26-28, , 2005, Vancouver, BC, Canada. 
Copyright remains with the authors. 
 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
17
personal voice, found it conceptually and technically difficult to 
create and develop their own musical ideas from scratch. It was 
also clear that novices were imitating instructors before they could 
express their own unique musical voice. Based on these 
observations and in order to help learners and novices to quickly 
connect to expressive and meaningful musical experiences, we 
decided to expand the Beatbugs’ functionality to allow for easy 
capture of musical phrases and intuitive gestrual personalization 
through transformation.     
The approach we used for algorithmic transformation is informed 
by the work of researches such as Lewis [6], Rowe [7], Pachet [8], 
Johnson-Laird [9], and Pressing [10] who developed a variety of 
theoretical and practical approaches for interactive improvisation. 
In these works, the computer utilizes transformative, generative 
and/or sequenced methods and can serve as a playing companion 
or as an extended musical instrument. In iltur, on the other hand, 
unlike generative computerized improvisation systems that are 
based on artificial intelligence or machine analysis and synthesis, 
the central computer is programmed to provide a supporting 
infrastructure for the interdependent connections among human 
players. Expert and novice musicians are the ones who are 
responsible for the musical decisions as they learn to listen to each 
other and shape their peers’ music by creatively manipulating of 
the system’s algorithms. Such multi-user algorithmic 
collaborations for the creation of artistic artifacts have been 
explored in other media as well, such as digital graphics and video 
[11]. In music, composers such as John Cage, The League of 
Automatic Music composers and the Hub, and a variety of 
Internet musicians explored similar interdependent interactions 
[12]. These experiments, however, usually require advanced 
musical understanding by players and audiences and often lead to 
inaccessible “high art” musical products. More recent 
collaborative musical installations for novices on the other hand 
(see a recent survey in [13]), tend to simplify the musical 
experience and are not geared to interdependently connect 
between novices and professionals. In iltur we attempt to address 
both novices and experts by facilitating novel interdependent 
collaborative group connections among players. 
3. GOALS AND CHALLANGES 
Based on these motivations and informed by the related work, we 
identify two main challenges for our software and hardware 
design. The first challenge focuses on the individual player. Here, 
the goal is to define and encode the appropriate musical 
parameters that would allow novices to control intuitive musical 
percepts that they can relate to, and to create pleasing musical 
results without compromising their ability to make mistakes and 
improve over time. The second challenge focuses on the 
collaboration between novices and experts. Here the goal is to 
create a coherent multi-player interaction where players can 
clearly follow and identify their contribution to the collaborative 
process, while still allowing for an immersive and interdependent 
experience to emerge.  
4. SYSTEM HARDWARE 
The original Beatbugs were designed to provide players with 
discrete control over triggering rhythmic events and continuous 
control over high-level musical transformations [4]. These 
requirements guided the sensors selection: a piezoelectric sensor 
for detecting discreet hits and two bend sensor antennae for 
continuous manipulation of musical patterns. Similarly to the 
original Beatbug, the iltur Beatbug uses a PIC 16f876 processor to 
read analog data from the antennae, control the LEDs, and 
encode/decode MIDI data. An internal speaker and a set of LEDs 
are used to help convey the interaction to players and audiences; 
The white LEDs flicker in synchrony with the phrase’s notes and 
the colored LEDs indicate the position of the antennae – the lower 
the left and right antennae are bent, the brighter the green and 
orange LEDs become, respectively. Several improvements were 
made for the iltur hardware in comparison to the original Beatbug 
system in an effort to make it more robust and portable. Since the 
original Beatbugs’ bend sensor antennae were vulnerable targets 
for twisting, kinking and tear, a new sensing mechanism was 
implemented in the iltur Beatbugs by Roberto Aimi [14], which 
utilizes Hall effect sensors and magnets mounted under the 
antennae. This electromagnetic sensing method proved to be more 
robust, although it provides only 3 bit bending resolution, in 
comparison to the 7 bit resolution of the older resistive bend 
sensors.    
 
Figure 2. The iltur System Schematics 
Other hardware improvements addressed the system size and 
portability. The Toy Symphony Beatbugs were connected to a 18-
unit rack that included a desktop computer, a monitor, a mixer, 
audio and MIDI interfaces, MIDI drum trigger, multi channel 
amplifier and propriety routing box [4]. The iltur system, on the 
other hand, uses a laptop computer, a software-based mixer, and 
the piezoelectric sensors are connected directly to the audio inputs 
(by way of the patch box), which eliminate the need for a MIDI 
drum controller module. The system, therefore, is housed in a 
small 6-unit rack, which consists mostly of standard, off-the-shelf 
equipment including a Mark of the Unicorn 828 audio interface, 
Emagic AMT MIDI interfaces, and a Lectrosonics PA-8 8-
channel amplifier that supports up to 8 internal Beatbugs speakers. 
The only non-standard device in the rack is the custom patch box, 
which provides power to the bugs and converts each bug’s 10-pin 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
18
Neutrik Minicon connector to power, MIDI in and out, and audio 
in and out. An Apple G4 PowerBook, running Cylcling74’s 
Max/MSP and Propellerheads’ Reason, is responsible for the 
detection, transformation, and synthesis algorithms. 
5. INTERACTION AND MAPPINGS 
The control functionality in iltur is consistent in both MIDI and 
audio applications. To control the three main interaction functions 
– recording, triggering, and transformation – players use different 
combinations of hitting and bending gestures. Recording is 
conducted by simultaneously bending both antennae while hitting 
the bottom part of the Beatbug. This action proved to be effective 
since the sensitive piezoelectric sensor at the top of the Beatbug 
can easily sense hits at different locations on the Beatbug’s 
surface (see Figure 3.) Hit velocities during triggering are 
generally mapped to different locations in the recorded buffer. To 
stop a phrase while it is playing, a strong hit on the top of the 
Beatbug is used. The continuous transformation algorithms for the 
MIDI and audio applications are described separately below.  
 
Figure 3.  A player taps the Beatbug while bending the 
antennae to start recording.  
5.1 MIDI Transformations 
A number of MIDI improvisation algorithms were developed to 
allow novices access to musical aspects that they can relate to and 
easily control. Some of the transformation algorithms are direct 
and simple such as linear pitch shifting of MIDI events or the 
addition of short notes in between recorded notes in correlation to 
antennae bending. Other algorithms are more sophisticated in 
their effort to simulate high-level musical aspects such as contour 
directionality and rhythmic density. Players can also trigger 
different permutations of the recorded MIDI phrases such as 
inversion and retrograde, by hitting the Beatbug while bending 
only one antenna, left and right respectively. When the system 
enters recording mode, a Max External titled mysequence (written 
by Alex Powell in an effort to enhance efficiency and time 
accuracy) starts listening to incoming note events. The external 
object receives MIDI messages that corresponds to the times, 
velocities, and pitches of every note. Hitting the Beatbug at any 
time while recording stops the recording process and starts the 
triggering/playback mode. In this mode mode, a timer loop in 
Max is repeatedly sending a playat <time> messages to the 
external object. When the Beatbug is hit, the hit velocity is 
measured and scaled to the length of the recorded phase. Soft hits 
(0-64 is MIDI velocity) starts the phrase at time 0 while harder 
velocities (65-126 in MIDI velocity) are lineally mapped to 
different time positions in the phrase. Very strong hits stops the 
phrase (hits can range higher than the standard 7 bit MIDI 
resolution since we capture audio from the piezoelectrric sensor.) 
Before a note is chosen to be played, calculations are made to 
determine whether its timing and/or pitch should be altered based 
on input from the antennae. The left antenna controls the rhythmic 
transformation. Different algorithms were written for this antenna 
and they are used by different Beatbugs at different parts of the 
compositions. One of these algorithms is designed to control how 
similar the rhythm will be in comparison to the original phrase. 
Stochastic operations are use to control how many notes will be 
chosen for modification. The lower the antenna is bent; the more 
notes are chosen to be modified. In order to create surprising 
variations of the original rhythm, a stochastic decision is made to 
either remove a chosen rhythmic value or add a new note that 
would subdivide the given rhythmic value into two. When a new 
note is created, its velocity equals that of the note from which it 
stemmed and its pitch value is chosen based on the contour 
algorithm, controlled by the right antenna. For this antenna a 
number of stochastic algorithms for pitch and melodic contour 
transformation were developed and used in different Beatbugs. 
One of these algorithms is designed to control contour 
directionality, a musical concept that has been shown to have 
perceptual significance for novices as well as experts [15]. In one 
case, it has been shown that novices’ ability to retain melodic 
contour of a semi-known melody is much better than retaining 
specific pitches [16]. Trehub at al. showed that contour can be 
perceived by infants as young as one year old, strengthening the 
assumption that this percept is well ingrained in human cognition 
[3]. These studies suggest that by providing an intuitive access for 
transformation of melodic contour we can allow novices to 
meaningfully improvise by creating variations with different 
levels of similarity to the original phrase. The right antenna, 
therefore, is mapped to control the contour of the melodies played 
by MIDI instruments. The lower the antenna is held; the more are 
the changes in the original contour direction that are taking place.  
Contour reshaping is accomplished in the same Max External 
used for rhythmic transformation. A currentTransposition value is 
initially set to 0 and is recorded throughout the resampling 
process.  Based on a stochastic process, the function decides how 
often to change the direction of the contour and whether to change 
the pitches themselves.  The more the antenna is held down, the 
more often a rising melody will turn lower, keeping the same 
interval relationship between its pitches. The algorithm can 
always return to the unchanged melody or to any setting of the 
antenna despite its reliance on stochastic techniques because of its 
reseeding policy to the random number generator. All operations 
that change the sequence are performed on a copy of the data, and 
an operation will not take place on that copy unless one or more 
antennae is held down. For other sections of the iltur 
compositions, more direct pitch and rhythmic transformations are 
used such as linear pitch shifting and the enhancement of 
rhythmic density using simple value multiplication. 
5.2 Audio Transformations 
In order to provide novices with intuitive real-time transformation 
of musical audio we first focused on capturing note onset times 
and detecting pitch from a particular acoustic instrument chosen 
for the system - the trumpet. Novices can then use the Beatbug to 
rearrange and manipulate the recorded notes to their liking. To 
capture note onset times we used the Max/MSP external bonk~ 
(written by Miller Puckette), with extensive filtering and trumpet-
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
19
specific parameter tuning to maximize accurate detection. The 
attack noise from “tonguing” a note instead of slurring was 
especially helpful in this regard. In controlled settings we reached 
a detection level of about 90 percent of staccato notes and about 
50 percent of legato notes. In legato sections, therefore, we 
captured short series of notes instead of individual pitches. In 
playback, much care had to be taken to avoid clicks and pops by 
cross-fading audio, but at the same time minimizing delay so that 
the Beatbug would feel responsive. Several different mapping 
methods were investigated to allow Beatbug players to control 
and transform the segmented audio recordings.  
 
 
Figure 4. Audio transformation schematics. Pitched audio is 
detected and segmented by the computer. The Beatbug sends 
control information to trigger and transform the audio. 
 
One of the effective mapping combinations included bending one 
antenna to control pitch shifting, bending the other antenna to 
determine playback buffer start location, and striking the piezo 
sensor to trigger playback using hit velocity to determine volume. 
We also experimented with real-time combinations of pitch 
shifting and delay lines to harmonize melodies, a method that was 
effective mostly with long and slow notes. Another mapping 
strategy utilized hit strength to control playback start time, while 
the antennae were used to control delay lines and filter cutoff 
frequency, providing some control over timbre. We also 
experimented with higher-level segmentation of phrases, where 
the recorded audio was divided into three segments that were 
determined by the largest silent gaps between notes. This mapping 
scheme allowed players to start playback at a variety of musically 
sensible times while shaping the phrases via pitch shifting. The 
pitch-shifting algorithm in all these mappings was restricted to 
discrete half note step (up to a fifth in both directions) and did not 
change playback times (by compressing or stretching the audio 
using the MSP External pitch~ by Tristan Jehan). In another 
mapping scheme players could select notes for playback based on 
pitch, playing higher pitches as they bend the antenna further.  
This scheme allowed players to reorder the recorded notes to form 
their own new melodic contours. However, an accurate choice of 
specific notes using a continuous controller such as a bendy 
antenna was a challenging task for most players. The effectiveness 
of each of the mapping strategies was dependent on the type of 
input musical material (staccato, legato, slow, fast, muted, and 
unmated playing, all led to different results.) The most effective 
mapping for the Jazz composition “iltur 2” (See section 6.2) was 
controlling pitch shifting and buffer start time with the antennae, 
and mapping hit strength to control volume. 
 
6. COMPOSITIONS 
Two Jazz compositions were written for the system. The first – 
iltur 1 – features the MIDI application and the second – iltur 2 – 
focuses on audio transformation and manipulation, while 
improving on the original MIDI application.   
6.1 iltur 1  
In “iltur 1” two Beatbug players control two different algorithms 
for rhythmic ornamentation and transposition of musical material 
played by a MIDI piano. A rhythm section – a drummer and a 
double bass player – helps in establishing the piece’s Jazz feel. 
The composition begins with a melody played by the piano and 
repeated in transformation by the Beatbugs players who control its 
expressive envelope using hit velocity and antenna bending. The 
Beatbugs play two different mallet instrument sounds (using 
Propellerheads’ Reason samples), which helps differentiating their 
sounds from the piano sound, and from each other. The piece then 
moves to an improvisation section where Beatbug players record 
short segments of a piano solo on the fly. Players can then trigger 
different sections and permutations of the phrase and manipulate 
it by bending the antennae as described in Section 5.1. These call-
and-response routines are based on a repeated harmonic structure.   
 
Figure 5. iltur 1 in  concert  -  International Computer Music 
Conference, Miami 2004 
The next section of the piece is played in a freer manner.  The 
only instructions for the players are to listen to each other, to pick 
up musical ideas that they like, and to transform these phrases 
back and forth. In this section the drummer and the bass player are 
also encouraged to participate by adding variations on the original 
and transformed material1. The piece then returns to the theme 
section, featuring the same Beatbug variation as at the beginning 
of the piece. 
                                                                    
1 A movie clip of iltur1 free improvisation section can be found at 
http://undertow.arch.gatech.edu/homepages/gweinberg/IlturNwe
amoFree.mov 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
20
6.2 iltur 2 
In iltur 2, one of the Beatbugs improvises and transforms audio 
recordings of muted trumpet playing while the other Beatbug 
interacts with the MIDI piano. Here too, a double bass player and 
a drummer provide the rhythm section. The piece, written in 11/8, 
begins with the melody section played by the muted trumpet, 
while one of the Beatbug player capture specific melody notes 
using a delay line and extend them by manipulating pitch and 
timbre. In the improvisation section, the audio-based Beatbug 
player can create his own melodic contour with the captured and 
analyzed audio material. Other transformations include 
segmentation and reshuffling of the trumpet solo and effect 
manipulation such as pitch shift and delay (See section 5. 2 for 
details). The other iltur 2 Beatbug interacts with a MIDI piano, 
utilizing a similarity algorithm based on contour directionality and 
rhythmic density (See section 5.1). In the Jazz standard tradition, 
the piece starts with a melody section, goes into a number of 
improvisation cycles, by the MIDI piano, the Trumpet, and the 
Beatbugs, and ends with a repetition of the melody2.  
 
 
Figure 6.   iltur 2 studio recording 
7. DISCUSSION 
iltur 1 and iltur 2 were recorded and videotaped in a number of 
studio sessions as well as public concerts such as at the Northwest 
Electro-Acoustic Music Organization Festival in San Diego 2004, 
the International Computer Music Conference in Miami 2004, and 
the Listening Machines Concert at the Eyedrum Atlanta 2005. 
About ten different players, novices and experts, played the 
Beatbgus in these performances, which led to a number of 
findings regarding the two main goals for the project: our effort to 
create rich musical experiences for individual novices through 
gestural manipulation of intuitive musical percepts and our 
attempt to form a unique collaborative musical experience for 
novices and experts, which is interdependent and coherent.   
7.1 Intuitive and Expressive Musical 
Experiences for Novices 
A variety of transformation algorithms were developed in an 
effort to provide expressive and institutive musical control for 
                                                                    
2 A movie clip of a studio recording of iltur 2 can be found at 
http://undertow.arch.gatech.edu/homepages/gweinberg/iltur2.m
ov 
novices. Some of these algorithms utilized direct mapping 
between continuous bending gestures and fundamental musical 
aspects such as pitch and rhythm. Other algorithms used more 
sophisticated stochastic operations in an effort to allow players to 
control aspects such as melodic similarity or rhythmic density. 
Based on observations and discussions with players, we believe 
that the simple direct mappings were generally more effective 
than the sophisticated stochastic algorithms since they provided 
more predictable control for players. A few users, however, 
preferred to interact with the stochastic operations, stating that 
these were surprising and encouraged them to concentrate on the 
“dialog” between their actions and the musical output. The 
effectiveness of the experience was also closely related to the 
musical and harmonic context. The first section of iltur 1, for 
example, includes a repeated harmonic structure where the 
Beatbug players record improvising over a full chord progression 
cycle before playing the variation back over the same harmony. 
Here, players could simply bend the antennae sporadically to 
ornament chosen parts of the phrase, while the default harmonic 
structure was kept intact. In the second section of iltur 2, on the 
other hand, players record a phrase that is based on a particular 
harmonic progression, and play it back over a different harmonic 
context. Here, players had to put more effort in manipulating the 
melody so that it fits the accompaniment, a task that was further 
complicated by the irregular 11/8 time signature of the piece. In 
these sections players were able to improve their playing skills 
through practice, and some even developed a certain level of 
“virtuosity” which produced better and more harmonically 
appropriate musical outcome. In another atonal section of iltur 1, 
a freer and more open experience emerged for both experts and 
novices. Players who preferred this mode of interaction stated that 
it posed less boundaries and allowed more creativity and 
expression. Some players, however, mentioned that the free 
section was confusing, and that at times it was difficult to predict 
and control the output of the Beatbugs. In addition to software 
mappings, another key factor in providing an expressive and 
intuitive experience for novices was the design of the gestural 
interaction with the Beatbug. The simple bending and hitting 
gestures proved to be intuitive and easy for most players to 
experiment with. Gestural combinations such as hitting the 
Beatbug while bending the antennae for recording, on the other 
hand, required more coordination and larger hands (see Figure 6). 
These combinations made the interaction more physically 
demanding in compression to the simple interactions with the Toy 
Symphony Beatbugs. 
7.2 Coherent Interdependent Group 
Interaction 
Our effort to create a coherent multi-player experience that allows 
novices and experts to clearly follow and identify their 
contribution to the collaborative process was generally successful. 
Contributing factors were the design of simple sequential 
interactions based on turn taking as well as to the implementation 
of visual and localized-audio feedback in the Beatbugs. Of the 
two modes of visual feedback, the flashing of white LEDs proved 
to be more useful to the players than the continuous color change 
LEDs. We believe that the correlation between the sound and 
flashing light made the discrete feedback more meaningful than 
the continuous subtle color changes. The flashing also conveyed 
the more musically critical and potentially acoustically ambiguous 
information regarding who was playing and when. Also helpful 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
21
was the design of simple sequential interactions where Beatbug 
players took turns in communicating with only one player at a 
time. Both novice and expert players found it easy to follow the 
interaction and comprehend their contribution to the music. 
Conveying the interaction to the audiences, on the other, was a 
more difficult challenge. For example, in our early 
experimentations the Beatbugs used the same timbre as the 
instrument they were manipulating (piano and trumpet.) The 
audience, however, found it difficulty to identify where the sound 
came from and who was playing at any given moment. We 
addressed this problem by differentiating the Beatbugs’ timbre 
from the instrumental timbre, by choosing a different MIDI 
program for the piano Beatbug and implementing filters and other 
audio effects on the trumpet Beatbug. Another challenge that had 
to be addressed was that iltur had to be performed coherently both 
in small studios and in large concert halls. To address this 
challenge we tried to duplicate the output system to the house PA 
while still playing the sounds through the internal speakers for 
personal and intimate feedback. However, this amplification 
scheme made it difficult for audiences to identify the spatial 
source of the Beatbugs. By mapping the Beatbugs’s sound to 
separate monitors on stage instead of the house PA we were able 
to better convey the interaction to audience, spatially.  
 
 
Figure 7. Interdependent group collaboration in iltur.  
Performed at the New West Electro-Acoustic Music Festival, 
San Diego 2004 
8. FUTURE WORK 
Several directions for future work, in hardware and software, are 
currently being explored. Inspired by Aimi’s and Young’s new 
Beatbug design [14], we are considering the addition of 
accelerometers to sense players’ arm gestures and the addition of 
wireless communication to allow freer and less cumbersome 
operation. We are also working on an improved approach for 
multiplayer interdependent application that would allow Beatbug 
players to send phrases to each other, utilizing multi-level 
transformation algorithms to develop and personalize phrases in 
sequential steps.  For this application we plan to make use of the 
system’s ability to support up to 8 Beatbugs, capacity that was not 
utilized fully in the applications that are described here. 
 
9. ACKNOWLEDGMENTS 
We would like to thank to Georgia Tech College of Architecture 
and the Music Department for their support for this project. We 
are also thankful for the MIT Toy Symphony team, and in 
particular to Tod Machover and Roberto Aimi, for their 
contribution to the development of the Beatbug controller.  
 
10. REFERENCES 
 
[1] DeNora, T. Music in Everyday Life Cambridge U.K.: 
Cambridge Univ. Press 2000. 
[2] Desain, P. (de)composable theory of rhythm perception. 
Music Perception, 9, 4 (1992), 439-454. 
[3] Trehub, S.E., Bull, D., Thorpe, L.A.. Infants' perception of 
melodies: The role of melodic contour, Child Development 
Vol. 55, (1984), 821-830. 
[4] Weinberg, G., Aimi, R., and Jennings, K. The Beatbug 
Network – A Rhythmic System for Interdependent Group 
Collaboration. Proceedings of the Conference on New 
Instruments for Musical Expression  (Dublin, Ireland, May 
24-26, 2002). 
[5] Machover, T. Shaping Music Minds. BT Technology 
Journal, 22,4 (2004) pp. 171-179. 
[6] Lewis. G, Too Many Notes: Computers, Complexity and 
Culture in Voyager. Leonardo Music Journal 10 (2000) 33-
39. 
[7] Rowe, R. Machine musicianship. Cambridge, Mass. London: 
MIT press, 2001. 
[8] Pachet, F. The continuator: Musical interaction with style. In 
Proceedings International Computer Music Conference, 
(Goteborg, Sweden 2001) 
[9] Johnson-Laird, P.N. Jazz improvisation: A theory at the 
computational level.  In: Representing musical structure, eds. 
P. Howell, R. West, & I. Cross. (pp. 291-326). London: 
Academic Press. 1991. 
[10] Pressing, J. (ed) Compositions for Improvisers: An 
Australian Perspective. Melbourne: La Trobe University 
Press. 1994. 
[11] Sims Karl 2003. The retrospective web site - 
http://www.biota.org/ksims/ 
[12] Weinberg, G., "The Aesthetics, History, and Future 
Challenges of Interconnected Music Networks". In 
Proceedings of the International Computer Music 
Conference, (Goteborg, Sweden 2001). 
[13] Blaine, T., Fels, S. Contexts of Collaborative Musical 
Experiences. In Proceedings of on New Interfaces for 
Musical Expression (NIME03) McGill University Montréal 
Canada. 2003. 
[14] Aimi, R, and Young D. “A New Beatbug: Revisions, 
Simplifications, And New Directions.” In Proceedings of the  
International Computer Music Conference, (Miami 2004). 
[15] Schmuckler, M. A. Testing Models of Melodic Contour 
Similarity.  Music Perception Vol. 16 No. 3 (1999) 295-326.  
[16] Sloboda, J. The Musical Mind. Oxford: Clarendon Press. 
1985. 
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
22