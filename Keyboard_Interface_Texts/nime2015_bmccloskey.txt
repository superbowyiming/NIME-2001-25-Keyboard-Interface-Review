Accessibility and dimensionality: enhanced real time 
creative independence for digital musicians with 
quadriplegic cerebral palsy 
 
 
Brendan McCloskey 
Ulster University 
School of Creative Arts and 
Technologies 
L/Derry BT48 7JL 
brendanmccloskey@gmx.com 
 
Brian Bridges 
Ulster University 
School of Creative Arts and 
Technologies 
L/Derry BT48 7JL 
bd.bridges@ulster.ac.uk 
 
Frank Lyons 
Ulster University 
School of Creative Arts and 
Technologies 
L/Derry BT48 7JL 
fr.lyons@ulster.ac.uk 
 
 
ABSTRACT 
Inclusive music activities for people with physical disabilities 
commonly emphasise facilitated processes [1], based both on 
constrained gestural capabilities, and on the simplicity of the 
available interf aces. Inclusive music processes employ 
consumer controllers, computer access tools and/or specialized 
digital musical instruments (DMIs). The first category reveals a 
design ethos identified by the authors as artefact multiplication 
– many sliders, buttons, dials and menu layers; the latter types 
offer ergonomic accessibility through artefact magnification. 
 
 We present a prototype DMI that eschews artefact 
multiplication in pursuit of enhanced real time cr eative 
independence. We reconceptualise the universal click-drag 
interaction model via a single sensor type, which affords both 
binary and continuous performance control. Accessibility is 
optimized via a familiar interaction model and through 
customized ergonomics, but it is the mapping strategy that 
emphasizes transparency and sophistication in the hierarchical 
correspondences between the available gesture dimensions and 
expressive musical cues. Through a participatory and 
progressive methodology we identify an ostensibly simple 
targeting gesture rich in dynamic and reliable features: (1) 
contact location; (2) contact duration; (3) momentary force; (4) 
continuous force, and; (5) dyad orientation. These features are 
mapped onto dynamic musical cues, most notably via new 
mappings for vibrato and arpeggio execution. 
 
Keywords 
Accessibility; bespoke design; cerebral palsy; customized 
mappings; dimensionality; expressivity; feature extraction. 
 
ACM Classification 
H.5.2 [User Interfaces] User-centred design, H.5.5 [Sound and 
Music Computing] Methodologies and Techniques, K.4.2 
[Social Issues] Assistive technologie s for persons with 
disabilities. 
 
1. INTRODUCTION 
Consumer DMI controllers , with numerous differentiated 
control artefacts in close proximity, are not optimally accessible 
to people with impaired motor skills. Assistive computer 
interfaces for people with a physical disability simplify this 
interaction model through large tactile switches, grips or 
overlays, for enhanced touch accessibility; specialised inclusive 
DMIs employ similarly assistive features, but too often require 
menu navigation. With scant few exceptions ,1 the tools 
available to performers with a physical disability are non -
optimal for independent real-time creativity [3] [9]. Inclusive 
music processes employing such interfaces commonly require 
setup and maintenance by a non-disabled facilitator [4] [ 15] 
[21], and creative control of musical cues is achieved through 
sequential and offline processes. 
 
 In pursuit of enhanced real-time and accessible performance 
independence, we employed a participatory methodology that 
quantified the gestural capabilities of a small group of digital 
musicians with quadriplegic cerebral palsy. We then formulated 
a multi-dimensional and transparent mapping strategy based on 
interaction models familiar to them . The participants were 
drawn from Drake Music Project N. Ireland's Wired ensemble,2 
and all three are wheelchair users with quadriplegic cerebral 
palsy, an impairment characterized by upper-limb gesture 
execution noise. They are eminently familiar with the common 
X-Y grid of control artefacts – the performers target and strike 
a single pad or button, or target and move a fader. During 
composition or performance processes, this impoverished mode 
of creative interaction affords discretised control over single 
sound events or single-destination continuous parameters. Our 
research suggests that, while the available performance control 
gesture is constrained, it reveals a number of dynamic and 
repeatable features. The following studies highlight gestural 
control on the Z axis, or targeting contact force and duration; as 
the methodology matured an additional gesture feature – dyad 
orientation – was demonstrated by the group.  
                                                                 
1 The Skoog (http://www.skoogmusic.com/) employs large 
foam rubber buttons on five sides of a cube; force data is 
mapped onto physical instrument models. The Soundbeam 
(http://www.soundbeam.co.uk/) employs remote gesture -
sensing and MIDI sound sources. Both rely on menu-driven 
flexibility.  
 2   http://www.drakemusicni.com/ 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME’15, May 31-June 3, 2015, Louisiana State University, Baton 
Rouge, LA. Copyright remains with the author(s). 
24
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
2. METHODOLOGY 
Adherence to a participatory methodology can enhance the 
transparency of a multi-dimensional performance-control map 
[12], while familiar models can enhance the learnability of 
novel interactions [17]. The iterative capability analyses 
adhered to the Inclusive D esign ethos (after [ 5] [13]): (1) 
emphasise the participants‟ comfort, safety and dignity; (2) 
assess optimal capability, not maximal ability; (3) use a variety 
of person-centred, non-stigmatising formats and methods. The 
three case studies below represent (1) artistic, (2) game-based, 
and (3) indicative methods, allowing formal assessment of the 
range of gesture dimensions available for mapping. Each study 
employed a customised sensor pad consisting of a silic one 
rubber disc (10mm x 50mm) embedded with a DIY force 
sensor and light -emitting diode (LED), housed beneath a 
flexible Perspex overlay. This design reflects the form factor of 
an accessible switch (figure 1), while also extending a switch‟s 
strictly binary functionality. Contact force is mapped onto LED 
brightness, thereby optimising visual, tactile and passive haptic 
feedback. The control layer of the system employed the 
Arduino3 prototyping platform, and Max4 for re-mapping and 
sound synthesis.  
 
 
      
Figure 1. Customised sensor pad (top), with examples of 
accessible switches (bottom). 
 
 
2.1 Case study data 
The first study offered linear control of the pitch and amplitude 
of a simple FM synthesis engine, mapped between 100 ~ 1200 
Hz and -60 ~ 0 dBFS respectively. The performance control 
boundaries were described within the group as “loud, soft, high 
or low”, and the participants demonstrated intentional control 
of repeatedly strong or soft targeting gestures. Across all three 
studies the participants were free to demonstrate their own 
stated intention, or to respond to groupe d exercises, for 
example “those were all high pitched and loud, would you now 
like to try low pitched and soft?”. This first study subjectively 
demonstrated the degree of coarse control amongst the 
participants in the domain of discrete momentary force. 
 
 The second study employed a game -based activity, to 
determine the degree of control over continued force. The 
performers used contact force and duration to control a virtual 
paddle-and-ball game; the ball was assigned virtual physical 
properties (mass, velocity and restitution) using the native Max 
object jit.phys.body. Hard strikes cause the paddle to rise  
                                                                 
3 http://www.arduino.cc/ 4 http://www.cycling74.com/ 
rapidly and the ball to bounce repeatedly; soft presses cause the 
paddle to rise slowly, with reduced movement in the ball. 
Holding the paddle steady causes the ball to come to rest on it.5 
During delivery of this study the explicit control boundaries 
(hard or soft strikes) were soon abandoned by the participants 
in favour of the more engaging implicit boundaries (balancing 
the ball).   
 
 Participant A realised six out of eight soft bounces, before 
demonstrating precise control over sustained force in the 
„balancing‟ control gesture.6 Participant B‟s interaction with the 
environment proved to be equally intentional. He executed 
numerous strikes using maximum pressure, intend ing the 
paddle to rise rapidly, and he was equally responsive to the 
balancing task.7 Participant C immediately demonstrated his 
control of the balancing task.8 The competitive element of this 
task was introduced by the participants themselves, who then 
challenged each other to hold the ball steady on the paddle.  
 
 The third study employed a dynamic table in Max to record 
the participants‟ responses to coarse forces and durations , 
across grouped iterations. The averages for the participants‟ 
individual responses illustrate their ability to differentiate 
between the descriptors “short or long, soft or strong”.  
 
 
Table 1. Grouped averaged responses from study no. 3. 
 
 
 
 
 As an example of the source of these averages, Participant 
A's first cycle consisted of 12 long and soft strikes: he 
discretely varied force between values of 4 and 58, or c. 25% of 
the total 0 -255 range. Long durations were greater than 1 
second 75% of the time, with the remainders around 500ms. In 
the second cycle of 12 strikes, strong force was consistently 
between 60 and 206 (54% dynamism), with short durations 
regularly less than 800ms; in the third cycle, soft force is 
between 3 and 72 (22% dynamism), and long durations were 
greater than 1 second 50% of the time. The remaining 
participants generated comparable responses, with Participant C 
demonstrating less dynamic control during soft strikes. Soft 
strikes, amongst the group, were less than 25% of the available 
range, and hard strikes were between 30% and 50% of the total 
range. Short durations were consistently less than half a second, 
with longer presses over 1.5 seconds. The data from the 
author‟s execution of 21 cycles (soft, short), presented here not 
as an exemplary benchmark but  indicative of the 
responsiveness of the system itself, reveals comparable figures. 
                                                                 
5 Paddle-ball demo. 6 Bounce-balance demo 1. 7 Bounce-balance demo 2. 8 Bounce-balance demo 3. 
25
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
3. MAPPING GESTURES FEATURES 
ONTO DYNAMIC MUSICAL CUES 
After Rovan et al [14] and Hunt et al [7, 8], we employed a 
tripartite mapping strategy, common within DMI design (for 
example [11] [16] [18]). In the guitar model underpinning our 
prototype (figure 2) the coarse and local pitch of a Karplus-
Strong string engine reside in the X-Y domain (discrete contact 
location); instantaneous amplitude, onset shape and timbre are 
governed via the Z axis (momentary contact force).9  
 
    
Figure 2. The prototype performance-control model. 
 
3.1 The affordances of the interface 
In their desig n of a prototype accessible DMI the authors 
eschewed control artefact multiplication in favour of artefact 
magnification and duplication. The instrument‟s interface 
(figure 4) presents a 3x4 grid of dynamic pads (plus latching 
switches on the right, for static musical functions). Its design is 
influenced in equal measure by the participants' capabilities, 
familiar DMI models, and ergonomic accessibility.  
 
 
Figure 3. The prototype interface, and aperture detail. 
 
 The pads sit be neath an overlay of recessed apertures of 
10mm depth for enhanced targeting accuracy, and the face of 
the controller is inclined to optimise comfort. The force data 
from each sensor is mapped as follows: 0 ~ 1023 at the 
hardware input pin → 400 ~ 800 actual range → 0 ~ 255 for 
serial transmission to Max. The lower limit in actual range 
(400) is imposed by the weight of the silicone pad on the force 
sensor; the upper limit of 800 represents the  averaged 
maximum force applied by the participants as a group.  
                                                                 
9 Amplitude is commonly cross-coupled with other sound 
parameters [6] [20] [22]. 
3.2 Details of the final mapping 
Beyond the dimensions of contact location and momentary 
force, a third layer of the prototype‟s mapping strategy exposes 
more nuanced musical cues : a contact duration threshold 
exposes dynamic vibrato, and; dyad orientations generate 
diatonic arpeggios.10 
 
  Dynamic control of the pitch/timbre, amplitude/shape, and 
duration/speed of musical sound – prosodic cues vital to 
expressive communication [10] [ 19] – requires nuanced fine 
motor control. In the absence of such capability we exploit an 
available gesture feature: contact duration. If contact duration 
exceeds 800ms the vibrato algorithm initialises: increased force 
produces a deep vibrato, and decreasing force reduces the 
effect. This ultra-low level dimension is neither assessed nor 
demonstrated herein, but intentional control of vibrato onset is 
clearly demonstrated.11 The mapping is retained in anticipation 
of future opportunities for longitudinal learning and 
exploration. A second expressive cue commonly inaccessible to 
physically disabled performers is the articulation of dynamic 
melodic contours [2]. One of the performers, presented with a 
grid of targets during early prototyping, addressed two sensors 
using the index finger of each hand. The group ultimately 
agreed that this was a valuable and accessible modality.  
 
 The pitches generated by the sensor array are identified in 
Max by their index: the targets are numbered in order from the 
bottom left corner (1), to the top right corner (12). Numerous 
dyad orientations between rows are then possible, and the 
authors limit this functionality to neighbouring rows only 
(figure 5), as two of the participants prefer to execute this 
gesture unimanually. 
 
        
Figure 4. The prototype’s default pitch collection, and 
examples of dyad orientations between neighbouring rows. 
 
The nature of this modality  strongly suggests an arpeggio 
performance-control source. If contact events overlap the 
arpeggio algorithm determines the odd or even quality of the 
pitch indices, and then populates the interval with a chain of 
diatonic thirds. The authors identified a final control dimension 
herein: because variable momentary force is demonstrated, then 
the force of the second of a pair of triggers might also be 
accessible. Arpeggios of an immutable tempo are artistically 
one-dimensional, and the aforementioned gesture feature is 
therefore mapped onto articulation speed. The participants 
clearly demonstrate control over the execution of arpeggios,12 
but arpeggio speed control is currently not assessed. Again, the 
authors retain this mapping in anticipation of future 
opportunities for collaborative design and development.  
                                                                 
10 Author‟s demo of the entire mapping. 11 Participants‟ control of vibrato onset. 12 Participants‟ execution of arpeggios. 
26
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 
4. CONCLUSIONS 
The goal of this project has been to offer digital musicians with 
a physical disability an instrumental interface for enhanced 
real-time creative independence. Independent control over 
gross/local pitch, amplitude, vibrato onset, and melodic contour 
is clearly evidenced herein. The project participants were 
excited by the opportunity to control a dynamic instrumental 
model based on an ostensibly simple targeting gesture; they felt 
a degree of ownership for this prototype, designed as it was for 
their particular capabilities. In respect of the methodology, the 
authors intend to de velop the game -based case study  by 
adhering to a more formalised method of data gathering and 
analysis – for example, a larger user-group; preordained task 
cycle-numbers; reference to clinical data. 
 
 The aperture overlay needs revising: it should be removable, 
with more and smaller sensor targets , based on low-level 
idiosyncrasies of the gesture: o ne participant has a double-
jointed fingertip, which causes the interphalangeal joint to 
connect with the rim of the aperture. The other two participants 
prefer unimanual execution of dyads, necessitating reduced 
spacing of targets, but they find the recessed apertures helpful. 
We deem the mapping strategy, however, to be accessible and 
sophisticated, particularly in the area of vibrato and arpeggio 
control. Intentional control of contact location, momentary 
force, duration, sustained force and dyad orientation, amongst a 
small group of digital musicians with quadriplegic cerebral 
palsy, is demonstrated through this research, as is the suitability 
of a bespoke DMI mapping strategy for enhanced real-time 
creative independence, for those commonly excluded from such 
activities. 
 
5. REFERENCES 
[1]   T. Anderson, C. Smith. Composability: widening 
participation in music making for people with 
disabilities via music software and controller solutions. 
In: Proceedings of the 2nd International conference on 
assistive technologies, April 11-12, 1996, New York: 
   ACM, pp. 110-116. 
[2]  A. Aziz, B. Hayden, C. Warren, S. Follmer. The flote: 
an instrument for people with limited mobility. In: 
Proceedings of the 10th ACM conference on computers 
and accessibility, Halifax, Nova Scotia 2008, p. 295. 
[3] B. Cappelen, A-P. Andersson. Musicking tangibles for 
empowerment. In: Proceedings of the 13th International 
conference on computers helping people with special 
needs, ICCHP 2012. LNCS, p. 255. 
[4] G. Doherty, T. Anderson, M. Wilson, G. Faconti. A 
control centred approach to designing interaction with 
novel devices. In: Proceedings of HCI International 
Conference on Universal Access in human computer 
interaction, New Orleans 2001. Lawrence Erlbaum 
Associates, p. 287. 
[5] J. Goodman, P. Langdon, J. Clarkson. Formats for user 
data in inclusive design. In: C. Stephanidis (ed.) 
Universal access in HCI part 1, LNCS 4554. Springer-
Verlag 2007, pp. 117-126. 
[6] D. Hall. Musical acoustics, (2nd ed.) California: 
Brooks-Cole 1991. 
[7] A. Hunt, M. Wanderley, R. Kirk. Towards a model for 
instrumental mapping in expert musical interaction. In: 
Proceedings of the International computer music 
conference, ICMA 2000, pp. 209-212. 
[8] A. Hunt, R. Kirk, M. Neighbour. Multiple media 
interfaces for music therapy. IEEE Multimedia, 11(3) 
July-September 2004, pp. 50-58. 
[9]  F. Hwang, S. Keates, P. Langdon, J. Clarkson. Mouse 
movements of motion-impaired users: a sub-movement 
analysis. In: Proceedings of the 6th international 
conference on computers and accessibility. New York: 
ACM 2004, p. 102. 
 [10] P. Juslin. Communicating emotion in music 
performance: a review and theoretical framework. In: 
Music and emotion: theory and research, New York: 
OUP 2001, p. 317. 
[11] L. Kessous, D. Arfib. Bimanuality in alternate musical 
instruments. In: Proceedings of the International 
conference on new interfaces for musical expression, 
NIME 2003, p. 141. 
[12] D. Meckin, N. Bryan-Kinns. MoosikMasheens: music, 
motion and narrative with young people who have 
complex needs. NIME 2014 workshop on accessibility. 
[13] K. Price, A. Sears. Performance-based functional 
assessment: an algorithm for measuring physical 
capabilities. In: Proceedings of the 10th ACM 
Conference on Computers and Accessibility, New 
York: ACM 2008, pp. 217-224. 
[14] J. Rovan, M. Wanderley, S. Dubnov, P. Depalle. 
Instrumental gesture mapping strategies as expressivity 
determinants in computer music performance. In: 
Proceedings of the International Kansei technology of 
emotion workshop [online], Paris: IRCAM 1997. 
[15] T. Swingler. The invisible keyboard in the air: an 
overview of the educational, therapeutic and creative 
applications of the EMS Soundbeam. In: Proceedings 
of the 2nd European conference on disability, virtual 
reality and associated technology, Sweden 1998, 
University of Reading: ECDVRAT, p. 255. 
[16] T. Ungvary, R. Vertegaal. Cognition and physicality in 
musical cyberinstruments. In: M. Wanderley and M. 
Battier (eds.) Trends in Gestural Control of Music, 
Paris: IRCAM 2000, pp. 371-386. 
[17] D. van Nort, M. Wanderley, P. Depalle. Mapping 
control structures for sound synthesis: functional and 
topological perspectives. In: Computer Music Journal 
38(3), pp. 6-22, MIT Press 2014. 
[18] M. Wanderley, J. Viollet, F. Isart, X. Rodet. On the 
choice of transducer technologies for specific musical 
functions. In: Proceedings of the International 
computer music conference, Berlin 2000. 
[19] A. Wennerstrom. The music of everyday speech: 
prosody and discourse analysis, New York: OUP 2001, 
p.vii. 
[20] D. Wessel, M. Wright. Problems and prospects for 
intimate musical control of computers. In: Computer 
music journal 26(3) Fall 2002 Cambridge, MA: MIT 
Press, pp. 11-22. 
[21] C. Williams. Unintentional intrusive participation in 
multimedia interactive environments. In: Proceedings 
of the 7th International conference on disability, virtual 
reality and associated technology, Portugal 2008. 
University of Reading: ICDVRAT, p.205. 
[22] L. Wyse, D. Nguyen. Instrumentalising synthesis 
models. In: Proceedings of the International conference 
on new interfaces for musical expression, NIME 2010, 
pp. 140-143.
 
27
Proceedings of the International Conference on New Interfaces for Musical Expression, Baton Rouge, LA, USA, May 31-June 3, 2015 