Unsounding Objects: Audio Feature Extraction for the
Control of Sound Synthesis
Ian Hattwick
Center for Interdisciplinary
Research in Music Media and
Technology
Input Devices and Music
Interaction Lab
McGill University
ian.hattwick@mail.mcgill.ca
Preston Beebe
Center for Interdisciplinary
Research in Music Media and
Technology
Digital Composition Studio
McGill University
preston.beebe@mail.mcgill.ca
Zachary Hale
Center for Interdisciplinary
Research in Music Media and
Technology
Percussion Performance
McGill University
zachary.hale@mail.mcgill.ca
Marcelo Wanderley
Center for Interdisciplinary
Research in Music Media and
Technology
Input Devices and Music
Interaction Lab
McGill University
marcelo.wanderley@mcgill.ca
Phillippe Leroux
Center for Interdisciplinary
Research in Music Media and
Technology
Digital Composition Studio
McGill University
phillippe.leroux@mcgill.ca
Fabrice Marandola
Center for Interdisciplinary
Research in Music Media and
Technology
Percussion Performance
McGill University
fabrice.marandola@mcgill.ca
ABSTRACT
This paper presents results from the development of a digi-
tal musical instrument which uses audio feature extraction
for the control of sound synthesis. Our implementation
utilizes multi-band audio analysis to generate control sig-
nals. This technique is well-suited to instruments for which
the gestural interface is intentionally weakly deﬁned. We
present a percussion instrument utilizing this technique in
which the timbral characteristics of found objects are the
primary source of audio for analysis.
Keywords
digital musical instrument, audio feature extraction, per-
cussion, interdisciplinary
1. INTRODUCTION
Unsounding Objects is a project involving the creation of
digital musical instruments[?] which use audio feature ex-
traction for the control of sound synthesis. The instrument
presented here, the SpectraSurface, is a percussion instru-
ment which allows the use of found objects as input devices.
Percussionists are accustomed to intimate control of timbre
using a wide variety of performance techniques, and the
SpectraSurfaceleverages this expert technique in order to
allow for the intuitive control of a digital percussion instru-
ment.
Most existing digital percussion controllers create control
signals consisting of discrete events which are based upon
the velocity of a hand or stick striking a membrane.1 These
1This includes commercial instruments such as the Roland
V-Drum as well as instruments within the NIME commu-
nity.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’14,June 30 – July 03, 2014, Goldsmiths, University of London, UK.
Copyright remains with the author(s).
instruments are meant to mimic traditional drum set and
hand percussion performance practices and are well-suited
to triggering audio samples; however, they are not well-
suited to capturing percussionists nuanced control of tim-
bre, do not allow for the incorporation of found objects into
performance, and are not compatible with most extended
performance techniques. The Spectrasurface was designed
speciﬁcally to address these issues.
Unsounding Objectsis an interdisciplinary research project
whose team consists of a digital musical instrument de-
signer, a composer, and a performer. An important part
of this project has been the joint development of new musi-
cal instruments, performance practices, and compositional
approaches, and especially how these three research areas
inform each other. While one of the driving goals of our
research is the application of scientiﬁc approaches to audio
feature extraction and mapping strategies to the expansion
of performance practice and composition for percussion in-
struments, we are aware that in order for the development
of a new musical instrument to result in a real contribution
to the musical community it must incorporate the contribu-
tion of the performers who play it and the composers who
write for it. The technical details of the instrument design
which is described in this paper was heavily inﬂuenced by
the collaboration of the research team in regular workshops
and our experience using the instrument in concert.
2. RELATED WORK
Audio feature extraction is commonly used within the Music
Information Retrieval community for the automatic classi-
ﬁcation of music recordings [?]. It also has a long history
of use for the generation of data structures for composition
and music performance [ ?] [ ?]. Many diﬀerent audio fea-
tures are able to be extracted, such as brightness, spectral
centroid, harmonic ﬂux, noisiness, etc. For the purposes of
this paper, we consider an audio feature to be a numeri-
cal representation of some perceptual aspect of the spectral
content of an audio signal.
There are two main approaches taken to the use of au-
dio features for control of synthesis. In the ﬁrst approach
audio signals are analyzed for speciﬁc characteristics such
Proceedings of the International Conference on New Interfaces for Musical Expression
597
Figure 1: The SpectraSurface showing the four sur-
faces as used in the compositionUnsounding Objects
#1.
as spectral centroid or harmonicity, which are then mapped
directly to parameters of sound synthesis [?]. In the second
approach, machine learning techniques are used to classify
and correlate spectral information to pre-analyzed spectral
templates [?, ?]. Machine learning techniques are well-
suited to analyzing large numbers of frequency components,
such as those provided by FFTs; however, they also tend to
be slower to respond to transients, and we found them to be
less eﬀective with complex spectra such as those which oc-
cur at note onsets. A typical use would be the classiﬁcation
of performance techniques as seen in Tindale et al [?].
2.1 Audio Feature Extraction in DMI Design
Audio feature extraction has been applied to digital mu-
sical instrument design in many ways. Jehan proposed a
timbre model which uses pitch, loudness, and brightness
to control sound synthesis; this model was then embedded
into the design of the Hyperviolin [?]. Shiraishi presented
a timbre tracking model in which an input timbre space is
constructed which is a representation of the actual range of
timbres which an acoustic instrument can create. In per-
formance, an input sample frame is analyzed and then its
location within this timbre space is determined. The input
timbre space is then projected onto a target timbre space
which controls sound synthesis [ ?]. O’Modhrain and Essl’s
Pebblebox uses onset detection, amplitude measurement,
and spectral content to allow the manipulation of pebbles
to control granular synthesis [?]. Their approach is note-
worthy for the correlation of selected audio features and
synthesis method. The impulses generated by large quanti-
ties of pebbles striking each other, for example, bear a di-
rect relationship to the triggering of clouds of audio events
in granular synthesis. Further related works are Settel and
Lippe’s implementations of FFT analysis and re-synthesis
[?] and Puckette’s use of spectral envelopes for parameter
mapping [?].
Two related instruments have also been recently devel-
oped at IRCAM. Lorenzo Pagliei’sGeecos use machine learn-
ing techniques to identify speciﬁc performance techniques as
well as using perceptual audio features to directly control
synthesis parameters.2 Bruno Zamberlin’s MoGees3 utilize
piezo sensors to detect audio signals which are then pro-
cessed using a mobile phone application which appears to
2www.youtube.com/watch?v=6Si2Y9Sm4AE, accessed Jan-
uary 29, 2013.
3www.brunozamborlin.com/mogees/, accessed February 4,
2014.
be based on his earlier work implementing gesture following
techniques using Hidden Markov Models [?].
2.2 Relation to Direct and Indirect Gesture
Acquisition
Direct acquisition of performer gesture through sensor data
is the most common approach to digital musical instrument
design [ ?]. Indirect gesture acquisition uses audio anal-
ysis to extract information about a performer’s gestures,
which is then used to control sound synthesis. Both of these
methods depend upon the explicit deﬁnition of correct per-
formance gestures, and are relatively intolerant of gestures
which do not ﬁt this deﬁnition. While you can strum the
strings of a Yamaha Disklavier directly, you cannot use this
gesture for the control of sound synthesis.
Audio feature extraction has the beneﬁt that any sound
created on the instrument will be analyzed, regardless of
performer gesture, as long as an emphasis on predeﬁned
models isn’t built into its algorithm. In practice, the algo-
rithms, mappings, and synthesis techniques will be chosen
based on the instrument’s expected use, thus limiting the
expressive potential of arbitrary gestures. However, there
is still the potential for extended techniques in the sense of
gestures outside the expected norm.
3. DESIGN GOALS FOR THE SPECTRA-
SURFACE
In creating the SpectraSurface our primary goal was the
creation of a digital percussion instrument whose interface
can be any found object, played with any performance tech-
nique. We took the approach of using surfaces upon which
arbitrary objects can be placed. A piezo contact micro-
phone is attached to this surface. When a performer in-
teracts with objects placed upon the surface the sound pro-
duced is captured by the microphone and sent to a computer
for analysis. The implementation described here consists of
four surfaces thus equipped, mounted into a suitcase. A
dedicated 4-channel preamp was created whose output is
sent directly to the line inputs on a multi-channel audio in-
terface. A ﬁxed-gain preamp (powered by a DC adaptor)
and ADC were found to be preferable in order to assure
consistent audio input levels.
Diﬀerent surfaces including metal pans, metal sheets, foam
board, cardboard, corrugated plastic sheets, solid plastic
sheets, and wooden boards were tested for frequency re-
sponse. While we had initial concerns regarding high-frequ-
ency damping in plastic materials and uneven frequency
responses overall, in practice we found that any suﬃciently
rigid and hard surface performed adequately. The surfaces
used in the implementation pictured are a metal pan, plastic
turntable lid, and plywood panels.
Audio analysis and sound synthesis takes place in Max/
MSP. The Zsa.Descriptors library for Max/MSP was used
to calculate Bark coeﬃcients for one of the analyses [?];
the remainder of the analyses and sound syntheses were
programmed by the authors.
3.1 Performance Techniques
Two compositions for the SpectraSurface have been written,
focusing on two diﬀerent performance techniques. The ﬁrst
composition utilizes diﬀerent sized metal bowls which are
placed on the four surfaces. For the beginning of the com-
position marbles are thrown into the bowls in such a way
that they spin around the bowls’ diameter. The spinning of
the marbles generates a self-sustaining process which drives
the sound synthesis. Later in the composition quantities of
rice are placed in the bowl, either gradually or suddenly,
Proceedings of the International Conference on New Interfaces for Musical Expression
598
which the performer then manipulates with his hands.
In the second composition metal pots and cymbals are
placed on the surfaces. A light pair of wooden sticks are
used to strike these objects as well as the surfaces them-
selves.
4. SOFTWARE IMPLEMENTATION
Two analysis algorithms were implemented, each with their
own approach both to analysis and also to mapping to sound
synthesis. These algorithms are described below while de-
tails regarding sound synthesis will be provided in future
publications.
4.1 Fuzzy
Fuzzy was designed to have a fast transient response and
analyzes only four wide frequency bands. The center fre-
quency of each band is set to 200, 500, 1000, and 2000
Hz. The ﬁrst derivative of the amplitude of each frequency
band is taken and run through an averaging ﬁlter and then
an envelope follower with a fast attack and medium decay.
The four output signals are then made available to map-
ping to synthesis parameters. An additional set of signals
are created by generating four bands of spectral tilt through
comparing the amplitude of neighbouring bands as well as
the amplitudes of the ﬁrst and fourth band.
These analysis signals are mapped to synthesis using a
dual-stage preset system. The ﬁrst stage consists of mak-
ing direct mappings between synthesis parameters and con-
trol signals. The second stage consists of interpolating be-
tween multiple mapping presets in a two-dimensional space.
The standard Max/MSP object Nodes was used to imple-
ment this second stage. This system allows for dramatic
changes in timbre as the frequency content of the input
signal changes, as well as allowing for the positioning of
mapping presets within a speciﬁed timbral range.
4.2 Cat’s Eye
For Cat’s Eye, the incoming audio was processed with an
initial high-shelf ﬁlter giving a wide 16db boost at 500hz.
A hipass ﬁlter at 25hz was also applied. The audio signal
was then analyzed using the zsa.energy and zsa.bark ex-
ternals in order to get a list of 25 Bark coeﬃcients. The
coeﬃcients are then analyzed to ﬁnd the local maximum;
the other coeﬃcients are divided by this maximum to gen-
erate a list of relative amplitudes. The square root of each
value of this list is taken in order to compress the overall
amplitude range. The value of each item in the list is then
fed through an envelope follower whose attack and decay
times are exposed for mapping. The resulting list of values
contains the relative amplitude of 25 consecutive frequency
bands.
5. OBSERV ATIONS
In this section we will share our observations regarding the
use of audio feature extraction for control of sound synthe-
sis as well as integration with direct sensing of performer
gesture.
5.1 Characteristics of Audio Feature Extrac-
tion
The audio feature extraction algorithms described above
share certain characteristics with implications for the con-
trol of sound synthesis.
5.1.1 Spectral Tilt
The two analysis methods described above produce values
which represent spectral tilt, or relative amplitudes between
spectral bands. Spectral tilt is theoretically amplitude inde-
pendent – however, the fact that frequency spectrum of an
instrumental sound is highly amplitude and time dependent
causes spectral tilt to generally follow amplitude changes.
For an instrument which is capable of generating sustained
timbres spectral tilt can be a relatively constant value. One
of the challenges of analyzing percussion instruments, how-
ever, is that their amplitude and therefore spectra changes
quickly over time. The spectra produced is also complex,
inharmonic, and highly variable – especially during tran-
sients. As the values from any two spectral bands change,
their relative amplitudes can change to an even greater de-
gree. This makes it diﬃcult to derive discrete, static, or
slow moving control signals – all of the values for spectral
tilt tend to look like amplitude envelopes. A lowpass ﬁlter
on each ﬁlter bands’ amplitude can help to mitigate sudden
jumps but will have an impact on responsivity.
It is necessary therefore to ﬁnd a way to react quickly
to these changes while minimizing the jitter resulting from
these natural ﬂuctuations in spectral content. Care can be
taken during control signal processing and mapping to make
a clear diﬀerence between the results of actual amplitude
envelope following and the spectral information, especially
since it is natural to map the amplitude envelope from the
control signal to the amplitude of the synthesized audio.
Another approach is to use envelope followers with diﬀer-
ent attack and decay speeds for amplitude envelope versus
spectral tilt values.
5.1.2 Frequency Band Selection
The selection of frequency bands plays a large part on the
responsivity of an instrument based on multiband audio fea-
ture extraction. Two approaches were described above, the
ﬁrst of which uses four unequally spaced frequency bands
which were determined heuristically while the second used
25 frequency bands which were based on Bark coeﬃcients.
Since Bark coeﬃcients are based on human perceptual char-
acteristics utilizing them for audio feature extraction has
the beneﬁt of making an intuitive correlation between the
perception of a sound and the control signals generated by
the sound. However, there is no reason why the frequency
content of a sound will map in an optimal way to the fre-
quency bands generated by Bark coeﬃcients.
In our ﬁrst approach we found that as few as four fre-
quency bands provide a reasonable amount of variation be-
tween the amplitude of consecutive frequency bands. To fa-
cilitate frequency band selection for diﬀerent sound sources
it may help to implement a learning stage in which an adap-
tive algorithm is used to determine frequency bands with
relatively equal amplitude responses.
5.1.3 Secondary Sound Interference
On the hardware side, our use of a surface equipped with
contact mics to pick up the audio from objects placed on top
of the surface has several shortcomings. The primary one
is that the surface acts to attenuate the audio coming from
the object as it moves towards the microphone, requiring
signiﬁcant preampliﬁcation which increases the noise ﬂoor.
This also causes sympathetic vibrations induced in the sur-
face to be relatively prominent in relation to the sound from
the objects. In addition, the audio resulting from the cou-
pling of objects to the surface they are placed upon is often
very diﬀerent from the sound perceived by the performer
through the air. This can cause the performer’s perception
of the control signal to be diﬀerent from the actual control
signal.
One constant of audio feature extraction is the inability
to distinguish intentional and unintentional sound input.
Proceedings of the International Conference on New Interfaces for Musical Expression
599
The use of direct sensing to supplement audio feature ex-
traction may help in, for example, wind instruments where
the audio input to the algorithm is muted when the mouth
is taken away from the mouthpiece. However, it may be
that sympathetic resonances and secondary sounds can be
seen as either tolerable or even desirable. As on an acous-
tic instrument expert performance and programming may
mitigate the problem of background sound to a suﬃcient
extent. Certainly this also opens the possibility of perfor-
mance techniques which may not have been foreseen by the
instrument designer.
5.2 Integration with Direct Sensing
Finally, as noted above, the supplementation of audio fea-
ture extraction with direct sensing of performer gesture may
create a best-of-both-worlds situation, in which the intu-
itive control of timbre provided by audio feature extrac-
tion is augmented with the ability for control of discrete
events provided by direct sensing. One problem with this
approach is the relative diﬃculty of implementing audio fea-
ture extraction on the microcontroller platforms which are
frequently used in direct sensing. One characteristic of the
algorithms described above which may make implementa-
tion easier is the fact that the audio and analysis resolution
may be relatively low-quality. For example, if a large part
of the spectral content of a sound is below 1k there is no
need for typical audio sampling frequencies. In addition,
relatively small FFT sizes may be used as accurate repro-
duction of the source signal is not required.
6. FUTURE WORK
The work described above represents the ﬁrst implemen-
tation of Unsounding Objects. Implementations utilizing
breath controllers as well as vibrating strings for audio input
have been created in order to explore the generalizability of
the algorithms described above.
A constant concern has been CPU load of the analysis al-
gorithms, which can be signiﬁcant even on modern PCs. As
described above we plan on implementing audio feature ex-
traction using low-ﬁdelity ADC conversion. We also plan on
creating implementations directly on microcontrollers, lead-
ing to possibilities for the use of large arrays of audio signals
as well as the use of direct gesture acquisition combined with
audio feature extraction within the same instrument.
7. CONCLUSIONS
The research presented in this paper demonstrates an ap-
proach utilizing multi-band audio feature extraction for the
control of sound synthesis in a digital percussion instru-
ment. While the control signals produced by audio feature
extraction are more diﬃcult to work with than those pro-
duced by direct sensing of performance gesture, audio fea-
ture extraction is more tolerant of extended performance
techniques and may not require the performer to learn tightly
deﬁned performance gestures. These qualities make it an ef-
fective alternative and complement to direct sensing for the
control of sound synthesis.
8. ACKNOWLEDGMENTS
This research was supported by a Student Research Grant
from the Center for Interdisciplinary Research in Music Me-
dia and Technology, and also by the CIRMMT Director’s
Interdisciplinary Excellence Prize.
9. REFERENCES
[1] F. Bevilacqua, B. Zamborlin, A. Sypniewski,
N. Schnell, F. Guedy, and N. Rasamimanana.
Continuous Realtime Gesture Following and
Recognition. In S. Kopp and I. Wachsmuth, editors,
Gesture in Embodied Communication and
Human-Computer Interaction, pages 73–84.
Springer-Verlag, Heidelberg, 2010.
[2] T. Jehan. Perceptual Synthesis Engine : An
Audio-Driven Timbre Generator Perceptual Synthesis
Engine : An Audio-Driven Timbre Generator. PhD
thesis, Massachusetts Institute of Technology, 2001.
[3] J.-m. Jot and O. Warusfel. Spat ˜ : A Spatial
Processor for Musicians and Sound Engineers. In
Proceedings of the International Conference on
Acoustic and Musical Research, 1995.
[4] M. Malt and E. Jourdan. Zsa.Descriptors : a library
for real-time descriptors analysis. In Proceedings of
the Sound and Music Computing Conference, 2009.
[5] E. R. Miranda and M. M. Wanderley. New Digital
Musical Instruments: Control And Interaction Beyond
the Keyboard. A-R Editions, Middleton, WI, 2006.
[6] S. O’Modhrain and G. Essl. PebbleBox and
CrumbleBag : Tactile Interfaces for Granular
Synthesis. In Proceedings of the 2004 conference on
New interfaces for musical expression, 2004.
[7] M. Puckette. Low-dimensional parameter mapping
using spectral envelopes. In Proceedings of the
International Computer Music Conference, 2004.
[8] S. Rossignol, X. Rodet, J. Soumagne, J. Collette, and
P. Depalle. Automatic characterisation of musical
signals: feature extraction and temporal
segmentation.Journal of New Music Research,
28(4):281–295, 1999.
[9] Z. Settel and C. Lippe. Real-Time Musical
Applications using FFT-based Resynthesis. In
Proceedings of International Computer Music
Conference, 1994.
[10] S. Shiraishi. A Real-Time Timbre Tracking Model
Based on Similarity. PhD thesis, Royal Conservatory,
The Hauge, 2006.
[11] A. R. Tindale, A. Kapur, W. A. Schloss,
G. Tzanetakis, A. Kapur, W. A. Schloss, and
G. Tzatenakis. Indirect Acquisition of Percussion
Gestures Using Timbre Recognition. pages 10–12,
2005.
[12] C. Traube, P. Depalle, and M. Wanderley. Indirect
Acquisition of Instrumental Gesture Based on Signal,
Physical and Perceptual Information. InProceedings
of Conference on New Interfaces for Musical
Expression, pages 42–47, 2003.
[13] D. L. Wessel. Timbre Space as a Musical Control
Structure. Computer Music Journal, 3(2):45–52, 1979.
Proceedings of the International Conference on New Interfaces for Musical Expression
600