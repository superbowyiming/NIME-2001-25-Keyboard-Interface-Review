Permission to make digital or hard copies of all or part of this work 
for personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on 
the first page. To copy otherwise, or republish, to post on servers or 
to redistribute to lists, requires prior specific permission and/or a 
fee. 
?glyph87LIME2010, 15–18th June, 2010, Sydney, Australia 
Copyright 2010, Copyright remains with the author(s).
 
P[a]ra[pra]xis: towards genuine realtime ‘audiopoetry’ 
 
Josh Mei-Ling Dubrau 
School of English, Media and Performing Arts 
University of NSW 
Australia 
joshdubrau@gmail.com 
 
Mark Havryliv 
Sonic Arts Research Network 
University of Wollongong 
Australia 
mhavryliv@gmail.com 
   
ABSTRACT 
P[a]ra[pra]xis is an ongoing collaborative project incorporating 
a two-piece software package which explores human relations 
to language through dynamic sound and text production. 
Incorporating an exploration of the potential functions and 
limitations of the ‘sign’ and the intrusions of the Unconscious 
into the linguistic utterance via parapraxes, or ‘Freudian slips’, 
our software utilises realtime subject response to automatically-
generated changes in a narrative of their own writing to create 
music. This paper considers the relative paucity of truly 
interactive realtime text and audio works and provides an 
account of current and future potential for the simultaneous 
production of realtime poetry and electronic music through the 
P[a]ra[pra]xis software. It also provides the basis for a 
demonstration session in which we hope to show users how the  
program works, discuss possibilities for different applications 
of the software, and collect data for future collaborative work.   
Keywords 
language sonification, new media poetry, realtime, Lacan, 
semiotics, collaborative environment, psychoanalysis, Freud  
1. I/glyph1197TRODUCTIO/glyph1197 
Interdisciplinarity, and interdisciplinary collaboration in 
particular, continue to gain ground rapidly in the Performing 
Arts. Writing and music are, perhaps, at the forefront of such 
developments with both disciplines reaching outside the 
traditional parameters of their crafts to scientific and 
technological processes as a way of furthering expression. 
Writer Catherine Fargher’s 2006/07 interactive performance 
piece/installation Biohome: The Chromosome Knitting Project    
involved, as one element, the looped video replay of the 
knitting of wool mixed with DNA fibre harvested from salmon 
testes. The live interactive sound, by composer Terumi 
Narushima, utilised recordings made during a SymbioticA wet 
Biology workshop which were then manipulated in Pd to create 
computerised patterns based on a range of knitting stitches [1]. 
This level of interactivity is highly complex and technologically 
demanding, but does not provide actual or realtime interaction 
between text and sound in the work.  
 
 
 
 
 
 
 
 
Other writing practices such as Mezangelle’s eponymous quasi-
computer language rely on the visual aspects of coding practice 
without implementing code within the text itself. Sound is often 
used as an adjunct to the text, as in the audio of the 
data][h!][bleeding texts where she uses distorted voice sounds 
which vaguely reiterate the titles of the pages [2]. Brian Kim 
Stefans’ ‘Alpha Betty’s Chronicles’ is a multicoloured static 
web-page work which was an ‘early experiment with randomly 
generated formatting HTML (with a small program written in 
C++)’ [3].  
It would seem that on a global scale practitioners of multi-
modal performance/theory might be coming to the conclusion 
that text itself is becoming increasingly obsolete, or at least that 
its value is considerably altered. Bill Seaman suggests that 
‘[t]he punning potentials of the text as code provide a hidden 
plane of operative potential that we as a communicative world 
are just beginning to come to understand and employ. At this 
point in time, text should be observed as one media-element 
within a network of other forms of media-elements and 
processes. The evocative life of words becomes palpable in the 
quixotic neighborhood of generative virtual environments’ [4]. 
This is a complex statement to unpack, suggesting some tension 
between the roles of the word as text/sign and the word as 
semantic unit. In the case of multi-modal works, it certainly 
suggests that the ‘evocative life’ of words does not necessarily 
need to be coupled with what one might normally associate 
with their ‘meaning’ in a poetic sense.  
In general, we have found there to be a) a relatively small 
number of artists using text for audio generation and b) an even 
smaller number of artists using text for audio generation where 
some consideration and manipulation of the semantic qualities 
of the text forms a necessary part of the work’s performance. 
Many collaborative audio-text works performed in less 
academically engaged spheres use the musician as an 
interpretive interface between the text and its meaning: for 
example, a spoken word performance accompanied by 
improvised audio often relies on the musical performer 
interpreting the emotional qualities engendered by the speaker’s 
voice and the text’s meaning to enhance the performance. What 
we wanted, however, was a mechanism that would let us use 
words as an interface for the creation of audio.  
2. BACKGROU/glyph1197D 
Text-to-sound converters are not uncommon. Realtime music 
software like Pd, Csound and SuperCollider can receive 
discrete keyboard events when a key is typed. Other software 
maps text (as ASCII characters) either to MIDI note numbers or 
to an MP3 file, invariably based on transmogrifications of 
alphabet positioning to pitch, texture or rhythm. More advanced 
converters create meta-descriptors (which may be based on a 
readability index, or some other lingual parser) which are then 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
467
used to control musical parameters. Please see [5] for an 
extensive listing and discussion of software. 
Words can be treated (and encapsulated) as objects, with 
properties and relationships to other words that can be 
evaluated and used in realtime. Thus, it becomes possible to 
sonify text as whole words, using well-defined relationships 
between different words, rather than sonifying text as characters 
or keyboard events alone. But how might one situate semantic 
content to generate audio that reflects a meaningful relationship 
with text, without a human agent acting as interpreter? 
The P[a]ra[pra]xis Project was the result of a desire to develop 
new ways of performing in, and thinking about, the 
collaborative spoken word/text/music paradigm. One of the first 
things that came to mind in considering the ramifications   was 
the following pronunciation of Swiss linguist Ferdinand 
deSaussure, that ‘[t]he linguistic signal, being auditory in 
nature, has a temporal aspect, and hence certain temporal 
characteristics: (a) it occupies a certain temporal space, and (b) 
this space is measured in just one dimension:  it is a line…  
Unlike visual signals which can exploit more than one 
dimension simultaneously, auditory signals have available to 
them only the linearity of time. The elements of such signals are 
presented one after another: they form a chain. This feature 
appears immediately when they are represented in writing, and 
a spatial line of graphic signs is substituted for a succession of 
sounds  in time’ [6]. Despite this rather simple reduction of the 
auditory and musical domain, we can conclude that there are 
strong parallels in the constraints shared between the delivery 
and reception of music, and the delivery and reception of 
language, whether in graphic or aural form. Audio-textual 
collaborative work then faces this constraint on two fronts, the 
musical and the linguistic, making it a challenge to 
meaningfully combine the two forms for a truly multi-modal 
experience.  
3. THE P[A]RA[PRA]XIS SOFTWARE 
SUITE 
3.1 Initial Development 
The initial development and operating parameters of the 
P[a]ra[pra]xis software suite have been described in a previous 
NIME paper [7]. Since then, the authors have added more 
complex grammatical constraints and a refined Boolean logic 
for the implementation of substitution rules. 
The OSC interface to the realtime poetry software is now bi-
directional and the realtime audio environment is able to 
command additional lingual constraints; this opens the door for 
interaction with live musical performers and poetic 
substitutions based on an analysis of performance and audio 
interactions. 
3.2 Current Work 
Compiling a large dictionary database is destined to be an 
ongoing process. A long-term goal is that when playing 
P[a]ra[pra]xis in real-time, users will be forced to respond to 
lingual substitutions determined by a dynamic, but 
grammatically oriented rule-set, predicated on the use of a 
comprehensive dictionary. A player writing a poem or story will 
be subjected to a continually changing narrative, and will thus 
involuntarily form new chains of signification, by either 
engaging or refusing to engage with the material presented. 
Also, the P[a]ra[pra]xis Collection Editor can be shaped to suit 
individual text projects. 
For example, the following is one of many possible realisations 
of the first few lines of Mark Antony’s oration following 
Caesar’s murder in Shakespeare’s tragedy ‘Julius Caesar’. 
 
Fri[volous ]ends,  Ro[bed ]man[darin]s, Count[less infant]rymen; 
Le[ar]n[é]d [wo]me[n] your ears! 
I come to buy his cares, not to pra[ct]ise hymns. 
The veil that men don [sa]liv[at]es after the[ ]m[asses]; 
The good is softly inter[fe]red with [in] their b[l]o[odli]nes... 
Solemnly let it b[l]e[ed] without a scar! 
 
 
4. DEMO/glyph1197STRATIO/glyph1197 
Our demonstration gives people the chance to play with the 
software in a number of ways. One scenario is the creation of a 
custom dictionary and experimentation with poetic substitutions  
– or, bring a friend! Another scenario may involve the 
adaptation of an existing sonification mapping to suit an 
adapted rule-set; the enthusiastic experimenter is invited to 
write their own synthesis algorithms. 
For those who are attending a NIME for the first time, 
particularly in an inter-disciplinary capacity, this demonstration 
could provide an artistically broad introduction to semantically-
oriented language sonification. We encourage people to bring a 
favourite or original piece of writing to play with. 
5. REFERE/glyph1197CES 
[1] C. Fargher. “Biohome: The Chromosome Knitting 
Project” 
http://www.biohomeproject.net/docs/BioHome_final_Pe
rf_draft_2.pdf 
[2] M. Breeze. “the data][h!][bleeding texts” 
http://netwurkerz.de/mez/datableed/complete/ 
[3] B.K. Stefans. “Alpha Betty’s Chronicles” 
http://www.arras.net/?p=173 
[4] B. Seaman “Recombinant Poetics: Media-Element Field 
Explorations” 
http://projects.visualstudies.duke.edu/billseaman/textsFi
eldExplorations.php 
[5] A. Judge 
http://www.laetusinpraesens.org/docs00s/convert.php 
(draft,  2007) 
[6] F. de Saussure General Course in Linguistics. 
Duckworth, London, 1983, 69-70.  
[7] J. Dubrau and M. Havryliv “P[a]ra[pra]xis: Poetry in 
Motion”. In Proceedings of the 2008 ?glyph87Lew Interfaces for 
Musical Expression Conference, 2008. 
 
 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
468