Mobility, Space and Sound Activate Expressive Musical
Experience in Augmented Reality
Yichen Wang
The Australian National
University
Canberra, Australia
yichen.wang@anu.edu.au
Mingze Xi
Data61, CSIRO
Canberra, Australia
mingze.xi@csiro.au
Matt Adcock
Data61, CSIRO
Canberra, Australia
matt.adcock@csiro.au
Charles Martin
The Australian National
University
Canberra, Australia
charles.martin@anu.edu.au
ABSTRACT
We present a study of a freehand musical system to in-
vestigate musicians’ experiences related to performance in
augmented reality (AR). Head-mounted mixed reality com-
puters present opportunities for natural gestural control in
three dimensions, particularly when using hand-tracking in
a creative interface. Existing musical interfaces with head-
mounted displays use dedicated input devices that are not
designed specifically for musical gestures and may not sup-
port appropriate interactions. We are yet to see widespread
adoption of head-mounted AR musical instruments. We
conducted an empirical study to evaluate musicians’ ( N =
20) experience of performing with a freehand musical in-
terface. The results suggest that the design of freehand
musical interaction in the AR space is highly learnable and
explorable, and that such systems can leverage unique as-
pects of mobility, space and sound to deliver an engaging
and playful musical experience. The mobile musical ex-
perience with a spatial interface design allowed performers
to be more bodily engaged and facilitated gestural explo-
ration for musical creativity. This work contributes to a
more developed understanding of potentials and challenges
in AR-based interface design for musical creativity.
CCS Concepts
•Human-centered computing → Mixed / augmented reality;
•Applied computing → Sound and music computing;
1. INTRODUCTION
Research into new musical interfaces has often been driven
by new types of computers that enable novel musical ges-
tures and experiences [16]. For instance, mobile computers
with touchscreens have been widely explored [37, 20] with
a proliferation of musical applications. In contrast, head-
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’23, 31 May–2 June, 2023, Mexico City, Mexico.
Figure 1: A musician applying freehand gestures to perform
with an augmented reality musical instrument. Our study
aims to understand musical experiences that are possible
with this type of interface.
mounted computers such as Microsoft’s HoloLens or Meta’s
Quest have yet to see a musical “killer app” and the dimen-
sions of an authentic musical system for these types of com-
puters are still relatively unknown. Existing mixed reality
musical instruments use input devices that are not designed
for musical gestures and may not support appropriate in-
teractions [2]. However, current headset-based AR and VR
computers support freehand interaction without controllers,
which could support a more natural user experience [11] and
expressive sonic practices [5]. Therefore, it is worth explor-
ing whether a freehand 3D interaction design could provide
more flexibility to musicians and enhance their musical ex-
pression in AR.
In this paper, we study musicians’ experiences using an
AR musical instrument with freehand interaction. We pre-
sented musicians with a simple musical interface represent-
ing a cylindrical keyboard on a Microsoft HoloLens 2. Sounds
in this interface could be triggered by touching one or more
cubic keys. Study participants evaluated the system through
musical tasks and a short improvisation. The study was de-
signed to consider both HCI-style evaluation [25, 8, 41] and
artistic qualities [33].
Our findings show three key axes where freehand AR mu-
sical interaction supports a playful and engaging musical ex-
perience: mobility, space, and sound. A reflexive thematic
analysis on the interview data revealed that musicians’ ob-
tained a mobile experience and the 3D arrangement of the
interface allowed flexible and explorative musical gestures.
The survey responses suggested that freehand interaction
was learnable and explorable. These findings suggest that
AR spatial design naturally activates musicians’ bodies and
hands, and the layering of digital and real spaces enables
unique bodily interactions. When using such systems, per-
formers may be inspired or limited by their previous musical
experiences; however, musical expression can be frustrated
by inaccurate hand-tracking. Our results contribute a more
developed understanding of how musicians experience free-
hand AR musical expression and provide insights into the
design of expressive 3D musical interfaces.
2. RELATED WORK
2.1 Mixed Reality Musical Instrument
Mixed reality NIMEs have been developed for musical per-
formance or sonic installations. These interfaces can be cat-
egorised as either AR or VR musical instruments [39] for
difference musical experience.
Recent VR musical instruments (VRMIs) mostly were de-
signed with hand-held controllers and headsets to enable
a fully-immersed and embodied musical experience [1, 31,
12], such as ChromaChord [7] and Carillon [13]. These dif-
fer from earlier works that often used screen-based displays
with a physical keyboard or track mouse discussed in the
field overview [34]. The need for complexity, freedom and
virtuosity to support expert gestures in VRMIs have often
been discussed (e.g., [2, 13]), even though controllers can
demonstrate a level of musical control including selection,
manipulation and navigation.
In AR, most musical expressions are screen-based, us-
ing mobile phones and tablets [39], or experience-focused
through users’ engagement with virtual visuals without spe-
cific interfaces [38, 3, 32, 9]. Rather than using controllers
to mediate interactions, these works explored designs for
more direct control of musical elements such as freehand
supported AR musical systems [39, 3] for natural gestural
musical experiences. Additionally, a framework for mixed
reality musical instrument design was recently proposed by
Zellerbach et al. [42], who argued a focus on embodiment
and the “magicality” of interactions design for a more ex-
pressive musical experience inspired by VR and AR NIMEs.
In terms of mixed reality musical instrument evaluation,
existing studies tend to focus on technical aspects such as
usability, immersion and audio in a specific VR context [43,
21]. There have not been propsoed methods for assessing
the musical quality of mixed reality instruments specifically
in active usage. In our work, we have adapted methods for
measuring usability [25] and instrument quality [33] with
the aim of understanding musicians’ musical experiences of
using a novel gestural musical interface.
2.2 Musical Gestures and HCI
Musical gestures are studied in different disciplines with
different objectives, such as sound generation [10], music-
making [19] and new musical interface and expression de-
sign [16]. HCI studies of musical gestures focus on musical
interaction design [14, 16], which mediates between the user
and the sound control in computer music systems. Wan-
derley [36] presented a model of digital musical instrument
(DMI) for sound control design in interactive computer sys-
tems including three main components: the design of in-
put gestures, sound production and feedback. This con-
trasts with acoustic instruments which are built upon ma-
terials, vibrations and physical environments. For example,
touch-screen gestures, computer-based mouse navigation,
VR mediated and embodied interaction designs presented
new forms of musical interaction which not only provide
Figure 2: Top: the AR musical interface consists of three
layers of coloured cubes and a cubic controller in the centre;
touching each cube with a finger activates the corresponding
tone. Bottom: an in-app view of the system in use.
musicians new options in music-making [15] but also enable
bodily interaction for fluid and expressive musical experi-
ences [17].
In the context of our work, a review summarised various
3D musical interaction techniques used for musical expres-
sion in the mixed reality [2]. Although existing techniques
(e.g., selection, manipulation and navigation) have shown
to provide rewarding musical control experiences, there are
still issues such as the lack of natural [11] and appropri-
ate musical interactions [26]. Aspects of expressiveness and
virtuosity [5], which encapsulate sophisticated sound con-
trol and supports performers’ musical expression, have al-
ways been key considerations in designing new musical in-
terfaces [23]. This suggests that more studies should be
performed to explore design such as gesture-based [13, 11]
designs that support flexible and expressive musical perfor-
mances.
3. MUSICIAN-CENTRED STUDY DESIGN
Our study aim was to explore the potential of freehand mu-
sical control in head-mounted AR. We were interested in
musicians’ performance experiences and the novel aspects
that an AR system provides for NIME design. Our study
used an AR musical interface developed by Wang and Mar-
tin [39], available on GitHub 1. This system was chosen
due to its focus on free-hand gestures with a head-mounted
AR system. Users can perform natural hand gestures to
play notes and manipulate options in 3D space (see Fig-
ure 2). In our study, this interface was deployed on Mi-
crosoft’s HoloLens 2 (HL2) [22] headset.
3.1 Participants
1https://github.com/YichenWangs/cubing-sound-release
Participants with musical instrument experience were re-
cruited from a university music and computing community.
20 participants, 10 female and 10 male were selected. 18
participants were 18–24 years and 2 participants were 25–
34 years. Two participants were AR/VR researchers, one
had participated in a VR user study. No participants had
significant musical experiences using HL2.
3.2 Procedure
The study involved individual 60-minute sessions at a me-
dia lab appropriate for AR experiments. Participants’ in-
app views and audio were recorded. In each session, the
participant first completed a 10-minute induction and then
performed a series of musical tasks with the AR instru-
ment following instruction.There were four musical tasks
with escalating difficulty to assess the overall usability and
playability of the interace. Following the tasks, participants
filled out a questionnaire of twenty 9-point Likert-scale ques-
tions to evaluate their musical experience using the AR in-
strument. The questions 2 were grouped into four sections
adapted from previous studies [33]: feature controllability
and usability, learnability, explorability and enjoyment. Af-
ter the survey, participants completed a semi-structured in-
terview regarding their overall experience.
4. RESULTS
4.1 Survey
The survey data were coded to numeric values between
1 (strongly disagree) and 9 (strongly agree) following a
non-parametric approach for data analysis [18]. Given the
within-group study nature, a statistical analysis was per-
formed using Pandas and NumPy libraries. Descriptive
statistics were calculated on participants’ responses to each
question and their aggregated responses to each group of
questions.
The aggregated responses to the learnability and explorabil-
ity questions had the same median value 8. Responses to
the enjoyment questions had a median of 6.5, and the me-
dian for feature controllability and usability questions was 6.
For the feature controllability and usability group, questions
related to the precision and control of the hand interaction
(Q2 & Q4) perhaps pulled the aggregated score down, indi-
cating limitations in the current AR system. In enjoyment,
questions related to sound satisfaction and quality (Q17 &
Q18) showed high variation with the lowest response of 1
and highest of 9 (median: 6 & 5; std 2.211 & 2.188). For ex-
plorability, Q13 also had high variation with a median value
6 and std of 2.012. This shows that participants had differ-
ent perceptions on the variety of playing offered by the AR
system. Notably, Q9 had the smallest interquartile range
(1), showing that participants tended agree that this AR
musical interface allowed them to learn new things (median
7).
4.2 Thematic Analysis
Approximately four hours of post-questionnaire interview
recordings were made, each interview was 6–25 minutes in
length. These recordings were transcribed and corrected
using NVivo Transcription [29], and coded using a reflexive
thematic analysis (TA) method [35] in NVivo [28]. This
reflexive approach prompted a prototyping and promotion
step in which initial codes were clarified and organised to
central themes to form our findings.
2https://yichenwangs.github.io/cube-sound-user-study
Overall, the interviews revealed that this freehand AR
music system presented a playful musical experience that
can activate a deep bodily engagement. Specifically, we
identified three key themes which contributed to this playful
experience (see Figure 3) discussed as follow.
4.2.1 Novel Mobile Musical Experience
Participants reported that they had a new musical experi-
ence with this AR musical instrument which they enjoyed
a lot. P10 pointed out how professional this musical in-
strument was in the AR environment which made their ex-
perience fun and interesting. For P1, the mixed sense of
digital and acoustic aspects delivered a unique instrument
playing experience which prompted different gestures they
had never used before. P16 and P4 shared similar insights
saying that this experience brought them “a new view of
how music can work in a different way” and to “discover
new possibilities and new combos within the same zone and
the same instrument”.
It appears that the mobility of the interface contributed
this to this enjoyment. Six participants highlighted the im-
pact of mobility in their comments. P3 and P19 noted
the engagement with their bodies while performing musical
tasks on the musical interface, which presented a closed and
immersive feeling. While P5, 7, 11, 13 specifically pointed
out the interface design and the three-layer arrangement of
sound cubes facilitated their “less restricted” playing expe-
rience (P13). P5 and P7’s comments focused on the inter-
active experience, where they appreciated the flexibility of
changing the size, rotating and moving the interface around.
Figure 3: Three key themes promoted from the reflexive
TA process, characterising a playful freehand AR musical
expression done in Miro software [30].
4.2.2 Musical Gesture in Air and Usability
A rich vocabulary related to musical gesture was identified
when participants discussed their experience on the AR in-
terface. P3 and P11 were impressed that they could ar-
range (multiple) fingers in space to make sound. P2 and
P13 ’s comments further elaborated that they were not lim-
ited to a certain position or posture to play notes. P7 re-
ported a “tactile” feeling, even though“putting your fingers
into the sound it’s like a digital thing” and there was no
actual physical feedback.
Participants connected their gestural experience with the
concepts of applicability and accessibility . Eleven partic-
ipants suggested that the control-to-sound mechanism on
this interface made playing sounds very easy. They also
mentioned this design would suit a variety of people (P10),
particularly beginners who did not have prior musical expe-
rience or may have a physical disability. As pointed out by
P14, it was how the cube shapes were designed for musical
interaction, which allowed to“touch it and makes sound [in]
very simple ways.”
The participants also discovered difficulties about free-
hand gestures’ usability. There were lots of comments re-
lated to the headset’s hand tracking in the study includ-
ing hard to play some chords, false gesture recognition and
confusion about the instrument location, which affected the
user experience.
4.2.3 Instrument and Sound Characteristic
Our participants mostly agreed that the sound characteris-
tic of this AR system was drone-like, having a long sustain,
and soft and relaxing tones. Some further commented that
the electronic sound seemed to suit the context of an AR
NIME. P9 pointed out that musical preference for this in-
strument could be quite personal and we heard a range of
views. P10 reported that they liked the sound produced in
the system due to their personal preference. Some were neu-
tral about the sound ( “I don’t particularly find this sound
amusing or not.” -P6 ) and others didn’t like it ( “I guess
I don’t love the sound.” - P9 ). P3 clarified that the AR
instrument used in this study might suit a specific musical
style: “it doesn’t lend itself to like a super crazy, fast-paced
jazz performances. But you can walk around the room and
just create this awesome ambient pace by like moving around
from place to place.”
Some participants reflected on their experience with ex-
isting musical instruments. P1, 8, 19 noted that tone was
important and that was what makes an instrument unique
while (P3, 5, 10) suggested that each instrument serves a
specific sound. For example, P1 said “Electronic music is
everything overlap together, and you can’t tell the tones for
like a proper acoustic instrument they have their specific
[sound] for different instruments. It’s prefixed [and] defined
because that’s how the instrument [is].” In contrast to the
above suggestion of a consistent material-sound-instrument
relationship, P10 and P20 reported a feeling of creating
new interactions with an old instrument. Relatedly, P16
stated that they imposed their piano-playing pattern when
trying to play with the AR musical system, an example of
using old interactions on a new instrument.
5. DISCUSSION
Both surveys and interview studies revealed that partici-
pants had an overall playful and positive musical experi-
ence. While the surveys supported the learnability and ex-
plorability of the system, the interviews revealed a more
personal kind of musical engagement that participants had
with this instrument. Connections were drawn between the
AR instrument interface and musical reflections from the
participants’ experiences as music makers. Considering our
overall findings we suggest that expressive AR musical expe-
riences can be framed in terms of three high-level concepts:
mobility, space, and sound.
5.1 Mobility in Music-Making
As reported in Section 4.2.1, participants’ pleasant expe-
rience with the freehand AR interface was not solely from
the excitement of a new technology platform. Instead, the
unique way of playing the instrument, including having mu-
sical gesture performed in the air and a mobile flexible in-
terface, allowed participants to use their hands and bodies
in a less restricted manner for music-making.
In the survey, higher ratings in the learnability and ex-
plorability groups suggested that the AR system was easy
to use and provided a variety of musical options. Further-
more, the interviews revealed that the participants enjoyed
being able to arrange fingers in space and in different angles
and postures. This affordance allowed the participants to
perform musical gestures without sophisticated skills and
facilitated new musical creativity.
The discussion implies a connection with natural hand
interaction even though it wasn’t directly mentioned in in-
terviews. This shows that natural hand interaction is im-
portant for AR musical interfaces. Reflecting on the magi-
cality and naturality of interaction discussed in the existing
literature [42], using freehand interaction in the AR envi-
ronment can provide a unique and natural experience that
is different from other types of musical interfaces.
5.2 Spatial Dimension of Musical Interface
While mobility emphasises the musical experience in playing
the AR instrument, the spatial dimension reflects interface
design and the relationship in the control-to-sound model.
In interviews, we found that the “arrangement” and “cube
shapes” of the AR musical interface allowed participants to
“arrange (multiple) fingers in space” to play. Participants
specifically noted that they found the musical control in
the AR musical instrument simple and accessible. One may
argue that this compromises musical virtuosity in favour of
simpler sound control design; however, the balance between
learning difficulty and virtuosity has long been discussed in
NIME design [40, 5, 24], it may be possible to support both.
More importantly, the finding of designing for “musical
gesture in space” does not imply an arbitrary 3D musical
interface floating in space where musicians perform random
gestures. Instead, it is worth considering that 3D interac-
tions in an AR environment can overcome physical limita-
tions in the real environment. This sits in contrast to the
requirement for tight relationship between materials, tactile
interaction and sound in digital musical instruments [44, 27]
and towards more imaginative non-physical action-sound re-
lationships that may be very expressive.
5.3 Sound and NIME design in AR
In both the survey and interviews, there were discussions
about the musical qualities and aesthetics in this freehand
AR musical instrument. Surveys showed participants were
neutral about the sound quality and aesthetics in this sys-
tem. In interviews, multiple participants disclosed their in-
dividual sound preferences and discussed how that affected
their views on using this instrument. Some participants
appreciated the drone-like sound and the three-layer ar-
rangement, while others found the electronic sound not very
pleasing. Some participants also suggested that the system
might only be suitable for a certain type of music such as a
soundscape. Moreover, the concept of new interaction with
an old instrument may suggest a gap between the sound
generated and imposed interaction pattern in current AR
system and vice versa.
This AR musical system had been originally designed
through an autobiographical approach [4], with one primary
performer and designer focussed on their own musical prac-
tice for head-mounted AR musical expression. This first-
person research process may have led to design decisions
that do not suit everyone. Other users may still require ad-
ditional time to understand and use the system for better
familiarity and intimacy [6], as making music through in-
terface and expression goes beyond a simple one-time study
session. The music synthesis aspects of the system can also
be adjusted either by the design, or by the user through
a graphical interface as in most commercial synthesisers.
That said, a NIME’s context and musical characteristics
should not be neglected in its evaluation.
6. CONCLUSION
In this work, we formally studied musicians’ experiences
with a freehand AR NIME. Our findings identified three
high-level concepts: mobility, space and sound, which con-
tributed to an engaging and playful musical experience in
the AR space. The mobile musical experience with the spa-
tial 3D musical interface allowed performers to be more bod-
ily engaged and facilitated gestural exploration for musical
creativity. These findings provide new design inspirations
to create more engaging, imaginative, and expressive AR
musical expression.
7. ETHICAL STANDARDS
This study was approved by The Australian National Uni-
versity’s research ethics office (Protocol 2022/413). All par-
ticipants provided informed consent and their participation
were voluntary as informed in the study protocol. There
are no observed conflicts of interest in this study.
8. REFERENCES
[1] R. Altosaar, A. Tindale, and J. Doyle. Physically
colliding with music: Full-body interactions with an
audio-only virtual reality interface. In Proceedings of
the Thirteenth International Conference on Tangible,
Embedded, and Embodied Interaction, pages 553–557,
2019.
[2] F. Berthaut. 3D interaction techniques for musical
expression. Journal of New Music Research ,
49(1):60–72, 2020.
[3] S. Bilbow. Evaluating polaris˜ - an audiovisual
augmented reality experience built on open-source
hardware and software. In Proceedings of the
International Conference on New Interfaces for
Musical Expression, The University of Auckland, New
Zealand, June 2022.
doi:10.21428/92fbeb44.8abb9ce6.
[4] A. Desjardins, O. Tomico, A. Lucero, M. E.
Cecchinato, and C. Neustaedter. Introduction to the
special issue on first-person methods in HCI, 2021.
[5] C. Dobrian and D. Koppelman. The e in nime:
Musical expression with new computer interfaces. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 277–282,
Paris, France, 2006. doi:10.5281/zenodo.1176893.
[6] S. Fels. Designing for intimacy: Creating new
interfaces for musical expression. Proceedings of the
IEEE, 92(4):672–685, 2004.
[7] J. Fillwalk. Chromachord: A virtual musical
instrument. In 2015 IEEE Symposium on 3D User
Interfaces (3DUI), pages 201–202. IEEE, 2015.
[8] M. Ghamsari, A. Pras, and M. M. Wanderley.
Combining musical tasks and improvisation in
evaluating novel digital musical instruments. In
Proceedings of the International Symposium on
Computer Music Multidisciplinary Research, pages
506–515, 2013.
[9] S. Glickman, B. Lee, F. Y. Hsiao, and S. Das. Music
everywhere — augmented reality piano improvisation
learning system. In Proceedings of the International
Conference on New Interfaces for Musical Expression ,
pages 511–512, Copenhagen, Denmark, 2017. Aalborg
University Copenhagen.
doi:10.5281/zenodo.1176350.
[10] R. I. Godøy and M. Leman. Musical gestures: Sound,
movement, and meaning . Routledge, 2010.
[11] J. Grubert. Mixed reality interaction techniques.
arXiv preprint arXiv:2103.05984, 2021.
[12] R. Hamilton. Coretet: A 21st century virtual interface
for musical expression. In The 14th International
Symposium on Computer Music Multidisciplinary
Research, pages 1010–1015, 2019.
[13] R. Hamilton and C. Platz. Gesture-based
collaborative virtual reality performance in carillon.
In Proceedings of the 2016 international computer
music conference, pages 337–340, 2016.
[14] S. Holland, A. P. McPherson, W. E. Mackay, M. M.
Wanderley, M. D. Gurevich, T. W. Mudd,
S. O’Modhrain, K. L. Wilkie, J. W. Malloch,
J. Garcia, et al. Music and HCI. In Proceedings of the
2016 CHI Conference Extended Abstracts on Human
Factors in Computing Systems , pages 3339–3346,
2016.
[15] S. Holland, T. Mudd, K. Wilkie-McKenna,
A. McPherson, and M. M. Wanderley. New Directions
in Music and Human-Computer Interaction . Springer,
2019.
[16] A. R. Jensenius. To gesture or not? An analysis of
terminology in NIME proceedings 2001–2013. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 217–220,
London, United Kingdom, June 2014. Goldsmiths,
University of London. doi:10.5281/zenodo.1178816.
[17] A. R. Jensenius and M. J. Lyons. Trends at
NIME—reflections on editing a nime reader. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 439–443,
Brisbane, Australia, 2016. Queensland
Conservatorium Griffith University.
doi:10.5281/zenodo.1176044.
[18] M. C. Kaptein, C. Nass, and P. Markopoulos.
Powerful and consistent analysis of likert-type rating
scales. In Proceedings of the SIGCHI conference on
human factors in computing systems , pages
2391–2394, 2010.
[19] T. Magnusson. Of epistemic tools: Musical
instruments as cognitive extensions. Organised Sound,
14(2):168–176, 2009.
[20] C. Martin, H. Gardner, B. Swift, and M. Martin.
Intelligent agents and networked buttons improve
free-improvised ensemble music-making on
touch-screens. In Proceedings of the 2016 CHI
Conference on Human Factors in Computing Systems ,
pages 2295–2306, 2016.
[21] L. Men and N. Bryan-Kinns. Supporting sonic
interaction in creative, shared virtual environments.
Sonic Interactions in Virtual Environments , page 249,
2022.
[22] Microsoft. HoloLens 2 Overview (2st gen) . Microsoft,
Feb. 2020. URL: https://docs.microsoft.com/
en-us/hololens/hololens2-options?tabs=device.
[23] F. Morreale and A. McPherson. Design for longevity:
Ongoing use of instruments from nime 2010-14. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 192–197,
Copenhagen, Denmark, 2017. Aalborg University
Copenhagen. doi:10.5281/zenodo.1176218.
[24] F. Morreale, A. P. McPherson, and M. Wanderley.
NIME identity from the performer’s perspective. In
T. M. Luke Dahl, Douglas Bowman, editor,
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 168–173,
Blacksburg, Virginia, USA, June 2018. Virginia Tech.
doi:10.5281/zenodo.1302533.
[25] N. Orio, N. Schnell, and M. M. Wanderley. Input
devices for musical expression: borrowing tools from
HCI. In Proceedings of the 2001 conference on New
interfaces for musical expression, pages 1–4, 2001.
[26] D. Overholt. The musical interface technology design
space. Organised Sound, 14(2):217–226, 2009.
doi:10.1017/S1355771809000326.
[27] J. Pigrem and A. P. McPherson. Do we speak sensor?
cultural constraints of embodied interaction. In T. M.
Luke Dahl, Douglas Bowman, editor, Proceedings of
the International Conference on New Interfaces for
Musical Expression, pages 382–385, Blacksburg,
Virginia, USA, June 2018. Virginia Tech.
doi:10.5281/zenodo.1302633.
[28] QSR International Pty Ltd. NVivo qualitative data
analysis software. Version 12, 2020.
[29] QSR International Pty Ltd. NVivo transcription.
2022. URL: https://www.qsrinternational.com/
nvivo-qualitative-data-analysis-software/
about/nvivo/modules/transcription.
[30] RealtimeBoard Inc. 2022. URL: https://miro.com/.
[31] C. Rhodes. Membrana Neopermeable. In Music
Proceedings of the International Conference on New
Interfaces for Musical Expression, June 2022.
doi:10.21428/92fbeb44.6e17eaf5.
[32] G. Santini. Augmented piano in augmented reality. In
R. Michon and F. Schroeder, editors, Proceedings of
the International Conference on New Interfaces for
Musical Expression, pages 411–415, Birmingham, UK,
July 2020. Birmingham City University.
doi:10.5281/zenodo.4813449.
[33] G.-M. Schmid. Evaluating the experiential quality of
musical instruments. Springer, 2017.
[34] S. Serafin, C. Erkut, J. Kojs, N. C. Nilsson, and
R. Nordahl. Virtual reality musical instruments: State
of the art, design principles, and future directions.
Computer Music Journal , 40(3):22–40, 2016.
[35] G. Terry and N. Hayfield. Essentials of thematic
analysis. American Psychological Association, 2021.
[36] M. M. Wanderley. Gestural control of music. In
International Workshop Human Supervision and
Control in Engineering and Music , pages 632–644.
Citeseer, 2001.
[37] G. Wang. Ocarina: Designing the iphone’s magic
flute. Computer Music Journal , 38(2):8–21, 2014.
[38] Y. Wang, H. Gardner, C. Martin, and M. Adcock.
Augmenting sculpture with immersive sonification. In
2022 IEEE Conference on Virtual Reality and 3D
User Interfaces Abstracts and Workshops (VRW) ,
pages 626–627. IEEE, 2022.
[39] Y. Wang and C. Martin. Cubing sound: Designing a
NIME for head-mounted augmented reality. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, The University of
Auckland, New Zealand, June 2022.
doi:10.21428/92fbeb44.b540aa59.
[40] D. Wessel and M. Wright. Problems and prospects for
intimate musical control of computers. Computer
music journal, 26(3):11–22, 2002.
[41] G. W. Young and D. Murphy. HCI models for digital
musical instruments: Methodologies for rigorous
testing of digital musical instruments. arXiv preprint
arXiv:2010.01328, 2020.
[42] K. C. Zellerbach and C. Roberts. A framework for the
design and analysis of mixed reality musical
instruments. In Proceedings of the International
Conference on New Interfaces for Musical Expression ,
The University of Auckland, New Zealand, June 2022.
doi:10.21428/92fbeb44.b2a44bc9.
[43] J. Zhang and N. Bryan-Kinns. Qiaole: Accessing
traditional chinese musical instruments in VR. In
2022 IEEE Conference on Virtual Reality and 3D
User Interfaces Abstracts and Workshops (VRW) ,
pages 357–362. IEEE, 2022.
[44] J. Zheng, N. Bryan-Kinns, and A. P. McPherson.
Material matters: Exploring materiality in digital
musical instruments design. In Designing Interactive
Systems Conference, pages 976–986, 2022.