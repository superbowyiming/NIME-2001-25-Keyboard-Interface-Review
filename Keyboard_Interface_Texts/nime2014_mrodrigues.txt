Composing for DMIs - Entoa, music for Intonaspacio
Clayton Rosa Mamedes
NICS - CIDDIC, Unicamp
IDMIL - CIRMMT,
McGill University
Campinas, Brazil
Montreal, Canada
claytonmamedes@nics.unicamp.br
Mailis G. Rodrigues
CITAR UCP
IDMIL - CIRMMT,
McGill University
Porto, Portugal
Montreal, Canada
mailisr@gmail.com
Marcelo M. Wanderley
IDMIL - CIRMMT
McGill University
527 Sherbrooke St. West
Montreal, Canada
marcelo.wanderley@mcgill.ca
Jônatas Manzolli
NICS Unicamp
165 Rua da Reitoria
Campinas, Brazil
jonatas@nics.unicamp.br
Denise H. L. Garcia
CIDDIC Unicamp
421 Rua Sérgio B. de Holanda
Campinas, Brazil
d_garcia@iar.unicamp.br
Paulo Ferreira-Lopes
CITAR UCP
Rua Diogo Botelho, 1327
Porto, Portugal
pﬂopes@porto.ucp.pt
ABSTRACT
Digital Musical Instruments (DMIs) have diﬃculties estab-
lishing themselves after their creation. A huge number of
DMIs is presented every year and few of them actually
remain in use. Several causes could explain this reality,
among them the lack of a proper instrumental technique,
inadequacy of the traditional musical notation and the non-
existence of a repertoire dedicated to the instrument.
In this paper we present the case study of Entoa, the ﬁrst
written music for Intonaspacio, a DMI we designed in our
research project. We propose a process of composition that
considers the design of the instrument as a required step
in order to start deﬁning an instrumental technique. We
present an overview of the instrument and strategies for
mapping data from sensors to sound processing, in order to
accomplish an expressive performance.
Keywords
DMI, Music Composition, Gestural Acquisition, Mapping
Introduction
New musical instruments normally emerge from a necessity
of new sounds, and new musical paradigms. Over the last
few years we have been witnesses of a ﬂourishing area of new
musical interfaces, evidenced by the continuous presentation
of new musical instruments every year. However, only few
of them have an historical continuity, i.e., a few number of
these digital musical instruments (DMIs) are still in use.
Nevertheless, this is not a common subject in conferences
as NIME. If we review the proceedings of the last editions,
a scarcely number of the presented papers refer to questions
related to the challenges DMIs have to face [12], [3], [20],
[13], [14]. We believe that the reasons for the short-life cycle
of most of the DMIs are essentially three: the lack of an in-
strumental technique dedicated to the musical instrument,
the necessity of a new form of musical notation more suit-
able for DMIs, and the non-existence of a repertoire. All of
this problems are interconnected.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’14,June 30 – July 03, 2014, Goldsmiths, University of London, UK.
Copyright remains with the author(s).
DMIs present an ensemble of characteristics that bring
new considerations in music. We will not focus on the sep-
aration between the sound control and sound generation
system, since this had already been treated extensively by
other authors [5],[24],[4]. From an overall review on the
literature on this subject, we understand that a DMI is pre-
sented, most of the times, as an hybrid object. On one hand,
DMIs appear as musical toys, musical gadgets or interfaces
on a sound installation, functions that require a straightfor-
ward strategy to allow everyone to be able to play them [6],
[12]. In several other situations DMIs are related to musical
instruments, with a great amount of expressivity, and conse-
quently demanding a learning stage [6], [13]. Although this
characteristic does not need to be negative per se, since it
opens the music universe to unsuspected performers (mu-
sic amateurs, public in general), it also introduces some
confusion in the deﬁnition of the roles of the performers,
instrument designers and composers.
The concept of DMI has been, in our view, strongly con-
nected with performative music, where improvisation has
a marked position. Chadabe [6] refers that an interactive
instrument1 combines performance and composition. In
consequence of these mixed roles (performer - composer)
both Chadabe [6] and Toeplitz [23] support the idea that
no notation is needed. Toeplitz states that the computer
already uses a symbolic language, which makes musical no-
tation pointless. The function of music sharing and preser-
vation would be performed by the computer itself. This
presents some problems, especially with obsolete or discon-
tinued software2. Thus, musical notation still has a func-
tionality on contemporary practices. We can, however, en-
visage new ways of notation, more suitable for composing
music with DMIs. A gestural notation where the idiomatic
gestures [14] [22] of the instrument would be represented,
can be a good solution. The existence of a gestural gram-
mar reﬂects the creation of an instrumental technique that
is particular to the instrument and can be shared between
performers. It also makes it easy to learn the instrument,
and allows the organization of a community of performers
and composers.
1The idea of interactive instrument for Chadabe relies on
the mutual reaction of the instrument to the performers
actions and vice-versa. The performer does not have the
total control over the instrument. However, not all DMIs
fall in this classiﬁcation, interactive instruments would be
a particular case in the universe of the DMIs.
2Digital conservation is one of the main issues on digital art
disciplines.
Proceedings of the International Conference on New Interfaces for Musical Expression
509
The method we propose and exemplify below can be im-
plemented using as a reference the study of instrumenta-
tion[1]. In this, the student learns the properties and re-
sources of each instrument, as well as examples of its usage.
We propose a collaborative work between composers and in-
strument designer that will deﬁne an instrumentation and
patterns of notation for the new DMI.
We designed a DMI - Intonaspacio, in which we observed
the most common gestures performed with the instrument,
from an experiment with several participants. These will
make part of what we believe would be a gestural grammar
of Intonaspacio, allowing the construction of gesture based
scores. Finally, we collaborated with a composer, who wrote
Entoa, the ﬁrst written piece for Intonaspacio. The ﬁnal
goal is to create a repertoire for Intonaspacio and to estab-
lish a gestural grammar emerging from the experience with
several composers and performers.
Intonaspacio
Intonaspacio [19] is a digital musical instrument, in which
sound ambiance is integrated on the process of sound gen-
eration, in real-time. It allows the creation of site-speciﬁc
sound, adding the room as an extra parameter in music
composition. The gestural interface is a sphere with an
ensemble of sensors that allow the control of the sound ma-
terial, Figure 1. The sensors are: an Inertial Measurement
Unit (IMU) with a three-axis accelerometer, gyroscope and
magnetometer that sense orientation and impact; two piezo-
electric sensors with diﬀerent sensibilities for detecting im-
pact on the structure, an infrared (IR) sensor to measure
distance, and ﬁnally a wireless mic to capture and record
the sound ambiance of the room. The information collected
by the sensors is sent to a computer using an Xbee. Once
the instrument is completely wireless, the performer could
explore space restricted only by the range of the Xbee it-
self. We decided to use sensors with diﬀerent sensibilities
in order to have diﬀerent degrees of freedom and thus an
instrument with more expressive potentialities. Mapping
Figure 1: Intonaspacio
was divided in two main layers, one corresponding to the
extracted features of the instrument, and the other to the
sound control of the instrument. This allow us to have a
ﬁrst layer, immutable, which would correspond to the be-
havior of the instrument, and hand over the second layer -
the sound generation, to composers. Although Intonaspacio
has a variable voice, the gestural interaction is stable. This
helps in the learning process and to engage the performer.
The extracted features were calculated from the infor-
mation retrieved from the sensors. These are not always
directly proportional to the sound control, i.e., the relation
between the sensed gesture and the sound reaction is not a
linearly one. Instead we use derivative (rate of change of a
variable) relations, as well as integration (how much time a
performer maintains the same action). It creates complex
relations and the performer, after a learning period, is able
to control Intonaspacio more accurately.
Entoa: relate sensors to sound
Entoa (2013) is the ﬁrst musical work for Intonaspacio.
Composed especially to the instrument, the work explores
its built-in features and aims to create an expressive and
intuitive performance that emphasizes the spherical shape
of the instrument. The composition process started by the
deﬁnition of groups of sensors, based on range and mea-
sured action (distance, impact, and so on) associated to
each sensor. Groups of gestures were cataloged and associ-
ated to sound events, in order to establish action-reaction
couplings that could be recognized by the audience as in-
tuitive [2] and expressive [15], [11]. Classiﬁcation of sound
events was based on the spectral typologies of sounds and
its dynamic morphologies [21].
The sound design in Entoa explores a timber palette of
metallic sounds. Sound events are triggered and processed
in real-time, based on the interactions between performer
and instrument. Shape, size and speed of gesture control
sound features as sound intensity, superposition of sound
layers, variation of spectral content through sound process-
ing and displacement of sound sources in spatial diﬀusion.
Notation of Entoa
The notation of Entoa comprises a text-based indicative
score, consisting of verbal signs suggesting how the work
can be performed. 3.
The score of Entoa provides simple guidelines to gestures
and expressive possibilities to the performer (details about
velocity, size, duration or overlaying of gestures are pur-
posely left to the responsibility of the performer). This op-
tion causes Entoa to become an indeterminate performance.
As they are, these instructions allow a large range of inter-
active possibilities with the instrument. Nevertheless, the
overall form of the composition, the articulation of its sec-
tions, dynamics and expressive possibilities, are bounded by
a set of speciﬁc indications set by the composer.
Finite-state machine model as formal structure
To organize the formal development of the music, we imple-
mented a composition structured as a ﬁnite state-machine
model [9]. In this approach, each state in the music pro-
gression corresponds to a new section. Each section has an
independent group of procedural rules, which in our case
comprises diﬀerent mapping designs for each section. Tran-
sition among states is established based on predeﬁned rules
with schematic outlines that smooth these transitions.
The designed mappings provide a rigid structure for data
processing and have a limited range of interaction for se-
lected sensors and sound events. This model introduces a
creative process grounded on bounded sections that have a
speciﬁc sound design, mapping and formal function inside
the work. Below, we present details on the mapping of all
sections.
3Examples of this notation are the works of Steve Reich
Pendulum Music [17] and Clapping Music [18], or perfor-
mance instructions used by some members of Fluxus [7]
Proceedings of the International Conference on New Interfaces for Musical Expression
510
The state Sof the work increases from 1 to 5, conditioned
by a fast sequential playing of piezos P2 and P1, in a time
span measured in milliseconds :
S = Si+1 if 0 <tP2 −tP1 <500ms (1)
A three-dimensional model of sound spatialization is imple-
mented in all states, and it is based on the rotation of the
instrument and the distance at which the instrument is from
the performer. The model uses a linear mapping for rotation
data, set after a number of experimental tests. This aims to
increase space control and make clear to audience the rela-
tion between gestures of rotation and sound diﬀusion. The
performer can control displacement of sound sources by ro-
tating Intonaspacio around its vertical axis within a range
of 0<θ< 360 degrees. Rotating around the horizontal axis
of the instrument will cause the sound sources to displace
vertically within a range of−90 <ϕ< 90 degrees. The spa-
tialization model is based on a 3D Vector Based Amplitude
Panning. The IR sensor measures the distance between per-
former and instrument. The sensor tracks distances within
a range of 0 < d <30 centimeters. The extracted dis-
tance is converted to a spatial distance ds within a range of
0 <ds <5 meters. Diﬀerent venues would ask for diﬀerent
conﬁguration of speakers in the patch, in order to correct
or simulate space in diﬀerent rooms.
Section 1
Section 1 uses recordings of singing bowls pre-processed
with ring modulation. This eﬀect creates a sound environ-
ment composed by notes with an inharmonic spectrum and
an open attack-decay proﬁle [21]. Dynamically, this sec-
tion starts with a pianissimo, and progressively increases to
a mezzo-forte before reaching next section. Interaction in
this section consists on triggering sounds with both piezo-
electric sensors and spatialization control. The dynamics
of the triggered sounds are retrieved from the analysis of
the acceleration time interval when the sensors are tapped.
Our model analyses time intervalt in milliseconds between
crossing of a noise threshold n and an attack threshold a.
Amplitude values eare constant, settled according to a cer-
tain sensibility chosen by performers. This model relates
velocity of attacka to sound level:
a= e
tn −ta
(2)
Since the distance between the sound source and the lis-
tener changes the perceived dynamics, we use this spatial
cognitive feature to control sound intensity between each
triggering. To make this feature credible, all sounds events
were slightly compressed, which increased the resonance in-
tensity. Thus, the performer can continuously control pres-
ence, direction and intensity of sounds combining rotation
and distance of the instrument to his/her body.
Section 2
In the second section, a track of sequenced sound events is
reproduced. This track contains sounds with closed attack-
decay combinations and continuously graduated dynamic
morphologies, as well as sounds with spectral typologies
of both harmonic and inharmonic notes [21]. This track
is processed in real-time by an harmonizer. Interaction
with Intonaspacio controls the parameters of the harmo-
nizer, changing the spectral content of sounds.
In section 2 we use the Roll descriptor to control the am-
plitude of the sound eﬀect. For each sound channel we have
deﬁned a diﬀerent conﬁguration for the harmonizer, increas-
ing perception of space and preserving musical appeal by
response to movement. Direction of rotation for Roll φcon-
trols the intensity of harmonizer eﬀect for left hl and right
hr channels:
hl = |φ| if φ< 0
hr = φ if φ> 0 (3)
Mapping of hl and hr values considers that changes in higher
frequencies are more noticeable by audience due to masking
eﬀects [8]. We progressively accentuate curves when map-
ping these values to higher frequencies of the harmonizer.
Section 3
Section 3 was conceived as the central apex for the work.
The previous section concludes in a mezzo-piano or in si-
lence (according to the performer’s criteria) giving an ex-
pectation of continuity. The conceptual schema of this piece
expects a rupture induced by a strong interaction of the per-
former with the instrument. We expect a dynamic range
from forte to fortissimo. In this section, a track of se-
quenced sounds constituted by notes with an inharmonic
spectrum [21], is continuously played. An amplitude enve-
lope controls the dynamics of the sound according to the
information retrieved from the gyroscope, in the x and y
axes. When the performer moves the instrument and the
angular velocityα surpasses a certain threshold u, it initi-
ates the sound with an amplitude L, otherwise the sound is
muted, as follow:
Lk =
{
0 if α k <uα
αk if α k ≥uα
(4)
The envelope has a 10 milliseconds fade in and fade out to
smooth the appearance of sound. This is particularly useful
when the instrument is moved very fast.
Section 4
Section 4 combines sounds of singing bowls from Section 1
with a track of percussive sounds with node spectral typol-
ogy and attack-impulse morphology [21]. In this section,
we return to the same idea of controlling sound events by
tapping on the piezoelectric sensors, while sound diﬀusion
is controlled with Yaw and Pitch angles, for both the con-
tinuous sound and the triggered sounds. Pitch also controls
a chorus eﬀect, in a similar mapping to the one presented
in Equation 3 for Roll. Conceptually, at this moment of the
work, we expect a reduction of the sound level and gestural
interaction of the performer with Intonaspacio.
Section 5
The last section of the work conducts a largecrescendo that
concludes the work in a sforzando followed by a short coda.
The singing bowls of the previous section are still present,
together with a sound track constituted by sounds of nodal
spectrum and gradually continuous morphology. Jab ges-
tures are introduced, that trigger brief inharmonic sounds
with low pitch, high amplitude and a sharp closed attack-
decay morphology [21]. The sum of the accelerometer val-
ues for the three coordinate planes retrieves jab gestures.
In this section all sensors are used simultaneously. At this
point, the performer had explored, in previous sections, the
features of the instrument individually. Section ﬁve is when
the performer can explore the combination of all the expres-
sive possibilities of Intonaspacio.
Conclusion
DMIs face certain diﬃculties in order to have historical
continuity. Most of the novelties presented every year are
not able to arouse the attention of enough musicians or
Proceedings of the International Conference on New Interfaces for Musical Expression
511
Figure 2: State 5: sensor’s activity
composers, which could prevent its disappearance. Several
causes could be pointed to explain this condition, includ-
ing the idiosyncrasy of DMIs [16] [10] or their obsolescence.
In this paper, we point three important characteristics that
we believe contribute to the preservation of DMIs: the exis-
tence of a musical notation, the creation of an instrumental
technique for the musical instrument and ﬁnally, the cre-
ation of a dedicated repertoire. We focus especially in the
last one, presenting an example of a music written especially
for a DMI - Intonaspacio.
Traditional musical notation are not suitable for most of
these new instruments. Also, DMIs are more centered in
a performance and improvisation situation than the tra-
ditional composition. Most of DMIs don’t produce single
note entities which make it diﬃcult to adapt to traditional
musical notation. Thus, we propose the construction of a
gestural grammar of the instrument that would be the ba-
sis for the formation of a new musical notation, focused in
gestures, and more suitable for DMIs.
We present our collaborative experience around the com-
position of Entoa as a case study that explains chosen strate-
gies exploring the shape and resources of the instrument, a
gestural grammar resulting from composition and perfor-
mance, as well as mapping conﬁgurations that implement
our usage of the sensors of Intonaspacio. We propose here
a process of composition that includes exploration and doc-
umentation, allowing in the future to establish a grammar
comprising the idiomatic usage of the instrument.
Acknowledgments
This research is supported by the Foundation for Science
and Technology (FCT, Portugal) and by the S˜ ao Paulo
Research Foundation (FAPESP), grants 2011/01553-8 and
2012/21039-0.
1. REFERENCES
[1] S. Adler. The study of orchestration . W. W. Norton &
Company, 2002.
[2] M. A. Boden. Mind as Machine: A history of
cognitive science. Oxford University Press, 2006.
[3] J. Butler. Creating pedagogical etudes for interactive
instruments. In Proccedings of the 2008 Conference on
New Instruments for Musical Expression , Genova
Italy, 2008.
[4] C. Cadoz. Interfaces Homme - Machine et cr´ eation
musicale, chapter Continuum ´ energ´ etique du geste au
son - simulation multisensorielle interactive d’objets
physiques. Hermes, 1999.
[5] C. Cadoz and M. M. Wanderley. Trends in Gestural
Control of Music , chapter Music - Gesture. IRCAM -
Centre Pompidou, 2000.
[6] J. Chadabe. The limitations of mapping as a
structural descriptive in electronic instruments. In
Proccedings of the 2002 Conference on New
Instruments for Musical Expression , Dublin Ireland,
2002.
[7] K. Friedman. The Fluxus Reader. Academy Editions,
1998.
[8] S. A. Gelfand. Hearing: An Introduction to
Psychological and Physiological Acoustics. Informa
Healthcare, 2010.
[9] A. Gill. Introduction to the theory of ﬁnite-state
machines. McGraw-Hill, 1962.
[10] R. Gluck. Live electronic music performance:
innovations and opportunities.
http://www.ciufo.org/classes/sonicart_sp09/
readings/gluck_liveelectronics.pdf, Fall 2007.
[11] D. Huron. Sweet anticipation: music and the
psychology of expectation. MIT Press, 2006.
[12] S. Jord´ a. New musical interfaces and new
music-making paradigms. In Proceedings of the
International Conference on New Interfaces for
Musical Expression, 2001.
[13] S. Jorda. Digital instruments and players: Part i -
eﬃciency and apprenticeship. In Proccedings of the
2004 Conference on New Instruments for Musical
Expression, Hamamatsu Japan, 2004.
[14] J. Malloch and M. M. Wanderley. The t-stick: from
musical interface to musical instrument. In
Proccedings of the 2007 Conference on New
Instruments for Musical Expression, New York USA,
2007.
[15] L. B. Meyer. Emotion and Meaning in Music .
Chicago University Press, 1956.
[16] N. Orio and M. M. Wanderley. Evaluation of input
devices for musical expression: Borrowing tools from
hci.Computer Music Journal , 26(3):62–76, Fall 2002.
[17] S. Reich. Pendulum music. Musical score, 1968.
[18] S. Reich. Clapping music. Music Score, 1972.
[19] M. G. Rodrigues, M. M. Wanderley, and
P. Ferreira-Lopes. Intonaspacio: A digital musical
instrument for exploring site-speciﬁcities in sound. In
Proceedings of CMMR, Marseille France, 2013.
[20] N. Schnell and M. Battler. Introducing composed
instruments, technical and musicological implications.
In Proccedings of the 2002 Conference on New
Instruments for Musical Expression , Dublin Ireland,
2002.
[21] D. Smalley. The language of electroacoustic music ,
chapter Spectro-morphology and structuring
processes, pages 61–93. Macmillan, London, 1986.
[22] D. A. Stewart. Digital musical instruments
composition: Limits and constraints. In Proceedings
of the EMS , Buenos Aires, Argentina, 2009.
[23] K. Toeplitz. L’ordinateur comme instrument de
concert. In ADERIM-GMEM, editor, Actes des
neuvi` emes Journ´ ees d’Informatique Musicale, pages
199–207, Marseille, 2002.
[24] M. M. Wanderley. Gestural control of music. In
International Workshop Human Supervision and
Control in Engineering and Music, pages 623–644,
September 2001.
Proceedings of the International Conference on New Interfaces for Musical Expression
512