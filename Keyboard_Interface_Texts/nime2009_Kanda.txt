Mims: Interactive Multimedia Live Performance System
Ryo Kanda
School of Science &
Technology
Kwansei Gakuin University
Sanda 669-1337 JAPAN
RyoKanda@kwansei.ac.jp
Mitsuyo Hashida
School of Science &
Technology
Kwansei Gakuin University
Sanda 669-1337 JAPAN
hashida@kwansei.ac.jp
Haruhiro Katayose
School of Science &
Technology
Kwansei Gakuin University
Sanda 669-1337 JAPAN
katayose@kwansei.ac.jp
Abstract
We introduce Mims, which is an interactive-multimedia live-
performance system, where pieces rendered by a performer’s
voice are translated into ﬂoating objects called voice objects.
The voice objects are generated from the performer’s cur-
rent position on the screen, and absorbed by another ﬂying
object called Mims. V oice sounds are modulated by the be-
havior of Mims. Performers can control these objects and
sound effects by using their own gestures. Mims provides
performers and their audiences with expressive visual feed-
back in terms of sound manipulations and results.
Keywords: Interaction, audience, performer, visualize, sen-
sor, physical, gesture.
1. Introduction
A performer’s physical appearance plays an important role
in live musical performances using laptops [5]. If the per-
former just stands behind the computer and manipulates his
or her keyboard or mouse to generate sound, the audience
may not understand what is happening on stage. The audi-
ence may also not be able to recognize whether this perfor-
mance is in real time or is a pre-recording and may become
bored. Reeve et al. called laptop performancesʞ secretive ʟ
interaction [10]. They claimed that the way interaction is ex-
pressed between the performer and the computer affects the
audience’s experience and various artists use their voice and
gestures to directly demonstrate their physical interaction
with the computer to their audiences. Such voice perform-
ers [4][11] apply sensor devices to control the parameters
of sound-processing programs. As these sound-processing
programs, e.g., granular synthesis [8], automatically modu-
late pitch and the panning position, performers can assign
complex-sound manipulations to these. In these live per-
formances, audiences can appreciate modulated sounds and
performers’ movements at the same time. However, accord-
ing to the ﬁrst author’s experience, both performers and au-
dience ﬁnd it hard to accurately recognize complex-sound
conditions. For instance, it is difﬁcult to identify the cor-
rect panning position for granular synthesized sound only
from speaker output. In such situations, the audience may
assume these sounds are just randomized because they are
not always familiar with sound-processing programs. A per-
former may make a mistake in the next manipulation be-
cause he or she is not aware of the current-sound condi-
tions. Fels et al. emphasis on the transparency of these
sound manipulation [5]. We considered that only providing
sound feedback is inadequate for live performance system,
and some visual feedback would be necessary to enable au-
diences and performers to recognize process and result.
Based on these reasons, we developed Mims, which is
an interactive system providing the performer and the au-
dience with expressive visual feedback of sound manipula-
tion. The system generates various graphical objects that
symbolize the performer’s voice on a screen from his or
her standing position. There are other objects called mims
to on the screen, whose positions and shapes are manipu-
lated by the performer’s gestures. When voice objects ﬂy
into mims, the mims hold them into themselves and modu-
late their sound outputs along with their own positions and
shapes. As the panning parameter for the performer’s raw
voice is determined from his current standing position, the
audience can easily identify the relationship between the vi-
suals and the performer. The performer recognizes the cur-
rent pitch and panning position or the modulation effects to
the sounds from the visuals.
2.1. Overview
The system consists of a projector, two speakers, wearable
sensors, vision sensors, and a wireless microphone system.
All sound and video processing was programmed, using Max
/Msp/Jitter [3]. Figure 1 shows an overview of the system
architecture.
2.2. Hardware
Some performers use their original wearable instruments to
manipulate their particular performance [7] [9] . We also
2. System Architecture 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists 
requires prior specific permission and /or a fee. 
NIME09, June 3-6, 2009, Pittsburgh, PA 
Copyright remains with the author(s).  
NIME 200945
Figure 1. System Architecture
Figure 2. Hardware
made our original sensor glove to control mims. Develop-
ment is still in progress. We present our prototype version
in this paper.
We implemented buttons, a 3-axis accelerometer, and an
infrared (IR) detector on the left glove to obtain information
on the performer’s gestures. An Arduino microcontroller
board [1] received all the sensing data and sent them to the
computer via an Xbee module [12] . The Xbee receiver was
connected via a serial-to-USB port to the computer that fed
the information to Max/Msp/Jitter.
2.3. Vision Sensor
We used a Web camera and bright white LEDs on the per-
former’s right glove to track the position of the performer.
The thresholding process enables the Web camera to track
the correct location of the bright white LEDs on the per-
former’s glove without being distracted by visible light. All
video processing was done using Jitter with the cv.jit library
[2].
2.4. Sound and Visual Manipulation
The voice sounds of performer is sampled per 50 millisec-
onds. All sampled voices convert into voice objects ﬂying
inside Mims objects. For instance, mims does not lock voice
objects inside of them, they just fade out and the simple
sound of the raw voice is played back. This causes delay-
like effects. If mims lock the voice objects inside of them,
voice sounds are played back in loop mode until they re-
lease them. The performer determines whether mims will
Figure 3. Snapshot of Performance
lock the voice objects or not by using a button on his or her
glove. The performer uses the 3-axis accelerometer and IR
detector to change the shape of mims, which means the type
and depth of sound modulation. For example, if the per-
former’s left palm face the right hand, mims become jagged,
and the sound playback positions and directions are random-
ized. The sound’s pitch and panning position are controlled
by the position of mims. The performer changes the posi-
tion of mims by using the position of his or her left hand.
We also implemented a multiple and automatic Mims mode
to create sounds with complex layers.
We introduced Mims, which is an interactive-multimedia
live-performance system that enables performers and audi-
ences to obtain expressive visual feedback for interaction
with computers. We now discuss the appropriate gesture
mapping, and are currently working on creating even smaller
sensor devices. We also plan to create multiplayer version
of Mims system for installation.
The video of the latest version of Mims system is avail-
able on http://www.vimeo.com/3036685.
References
[1] Arduino, http://www.arduino.cc/
[2] cv.jit, Computer vision for jitter by Pelletier, J. M.,
http://www.iamas.ac.jp/ʙjovan02/cv/
[3] Cycling74, http://www.cycling74.com
[4] Chikashi. Miyama, Keo Improvisation for sensor instrument
Qgo, InProceedings of the Conference on New Interfaces for
Musical Expression (NIME-08), p. 383, 2008.
[5] Fels, Gadd and Mulder, Mapping Transparency through
Metaphor: Towards More Expressive Musical Instruments,
Human Communication Technologies Laboratory, 2003.
[6] K. Jo, Note: To be Called as Music, No. 2Information Bion-
omy: Media to be Alive, Site Zero / Zero Site, MDR, pp. 98
ɹ 107, 2008.
3. Conclusion and Futurework 
46
[7] Laetitia Sonami, Lady’s glove no.5, http://www.sonami.net/
[8] MSP Granular Synthesis Patch v2.5,
http://web.mac.com/nsakonda/sakoweb/download.html
[9] Paradiso. J, Electronic Music: New Ways to Play. IEEE
Spectrum, V ol. 34, No. 12, 1997.
[10] Reeves, Stuart and Benford, Steve and O ʟMalley, Claire
and Fraser, Mike, Designing the Spectator Experience. In:
SIGCHI Conference on Human Factors in Computing Sys-
tems (CHI), 4/2005, Portland, Oregon.
[11] Tomomi. Adachi, http://www.adachitomomi.com/
[12] Xbee Wireless Transmitter, http://www.maxstream.com
47