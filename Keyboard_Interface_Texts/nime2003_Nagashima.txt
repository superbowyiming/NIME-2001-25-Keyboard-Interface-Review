Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
Bio-Sensing Systems and Bio-Feedback Systems for
Interactive Media Arts
Yoichi Nagashima
Shizuoka University of Art and Culture / Art and Science Laboratory
1794-1 Noguchi-cho, Hamamatsu, Shizuoka, Japan
430-8533
nagasm@computer.org
ABSTRACT
This is a report of research and some experimental appli-
cations of human-computer interaction in multi-media per-
forming arts. The human performer and the computer sys-
tems perform computer graphic and computer music interac-
tivelyin real-time. In general, many sensors areused for the
interactive communication as interfaces, and the performer
receives the output of the system via graphics, sounds and
physical reactions of interfaces like musical instruments. I
have produced many types of interfaces, not only with phys-
ical/electrical sensors but also with biological/physiological
sensors. This paper isi n t ended as an investigation of some
special approaches: (1) 16-channel electromyogram sensor
called “MiniBioMuse-III” and its application work called
“BioCosmicStorm-II” performed in Paris, Kassel and Ham-
burgi n 2001, (2) sensing/reacting with “breathing” in per-
forming arts, (3) 8-channel electric-feedback system and its
experiments of “body-hearing sounds” and “body-listening
to music”.
1. INTRODUCTION
As the research called PEGASUS project (Performing En-
vironment of Granulation, Automata, Succession, and Uniﬁed-
Synchronism), I have produced many systems of real-time
performance witho r i g i n a ls e nsors, and have composed and
performed many experimental works at concerts and festi-
vals. The seco nd step of the project is aimed ”multime-
dia interactive art” by the collaboration with CG artists,
dancers and poets.
Figure 1: Conceptual system block diagram of PE-
GASUS project, multimedia interactive art.
Fig.1 shows the concept of the keyword: “listen to the
graphics, watch the music”. The third step of the project
is aimed “biological or physiological interaction between hu-
man and system”. I had produced (1) Heart-beat sensor
by optical information at human earlobe, (2) Electrostatic
touch sensor with metal contacts, (3) single/dual channel
electromyogram sensor with direct muscle noise signals. And
now I report the newer sensors in this paper.
2. MINI BIO MUSE - III
At ﬁrst I reportt h edevelopment of a compact/light 16-
channels electromyogram sensor called “MiniBioMuse-III”
(Fig.2). This sensor is developed as the third generation of
my research in el ectromyogram sensing, because there are
many problems in high-gain sensing and noise reduction on
stage (bad condition for bio-sensing).
Figure 2: “MiniBioMuse-III” (only for one arm)
2.1 Development of “MiniBioMuse-III”
The front-end sensing circuit (Fig.3) is designed with heat-
combined dual-FETs,a n dc ancels the common-mode noises.
Therea re 9 contacts on one belt, one is common ”ground”.
Figure 3: Front-end circuits of “MiniBioMuse-III”
NIME03-48
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
Figure 4: High-density assembled front-end circuits
(8ch)
Each 8-channel electromyogram signals for one arm/hand
(ﬁg.4) is demultiplexed and converted to digital information
by 32bits CPU, and converted to MIDI information for the
system (ﬁg.5). This system alsog e nerates 2 channel Analog
voltage outputs for general purpose from MIDI inputs.
Figure 5: 32bits CPU circuits of “MiniBioMuse-III”
This CPUa lsow orks as software DSP to suppress the
Ham noise of environmental AC power supply. Figure 6
shows its algorythm of ”Notch Filter” for noise reduction.
Figure 6: Noisereduction algorythm of CPU
2.2 Output of “MiniBioMuse-III”
Figure 7/8 shows the output analog signals of front-end
circuits of “MiniBioMuse-III”. Figure 7 is the “relax” state
of the performer, and ﬁgure 8 is “hard-tension” state of the
performer.
Figure 9 is the Max/MSP s creen of the M IDI output of
this sensor. 8channels + 8cha nnels electromyogram signals
are all displayed in real time,and used for sound generating
parameters. The maximun sampling rate of this sensor is
about 5msec, but this sampling rate is changed by the host
system for its ability of MIDI receiving.
2.3 Performance with “MiniBioMuse-III”
Ih a v ec o m p o s e do n enew work with this “MiniBioMuse-
III”f or my Europe Tour in September 2001. The title of the
Figure 7: Sensing output signals of “relax”
Figure 8: Sensing output signals of ”hard tension”
Figure 9: MIDI output of ”MiniBiouse-III”
(Max/MSP)
work is“BioCosmicStorm-II”, and the system is constructed
in Max/MSP environments. 16 channel electromyogram sig-
nals are all displayed in Max/MSP screen and projected on
stage,s oa udience can easily understand the relations be-
tween sound and performance. This work was performed in
Paris (CCMIX), Kassel and Hamburg. Generated sounds
aret hree types in scenes : (1) 8+8 channels bandpass-ﬁlters
with white noises, (2) 16 individual-pitch sine-wave genara-
tors, and (3) 3+3 operators and 10 parameters of FM syn-
thesis generators. All sounds are real-time generated with
the sensor.
Figure 10: Performance of “BioCosmicStorm-II”
NIME03-49
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
2.4 Parameters Mapping in Composition
Figure 11 shows the main patch of Max/MSP for the work
“BioCosmicStorm-II” in running mode. There are 16 real-
time display windows of 16 channel EMG inputs. Figure
12 shows the same patch in e diting mode. The 16 sensor
outputs were mapped into “sinusoidal synthesis” part of this
work is programmed in this main patch using the Hide-mode
objects.
Figure 11: Main Patch of “BioCosmicStorm-II”
Figure 12: Edit Mode of Main Patch
Figure 13 shows the sub-patch of the work for the “FM
synthesis” part. This FM al gorithm is designed by Suguru
Goto and distributed by himself in DSPSS2000 in Japan.
The multiband ﬁlters were also used in this work.
Figure 13: SubPatch for FM Synthesis Block
3. “BREATHING MEDIA”
In computer music p erformance, the human performer
generates many information which computer system can de-
tect, but “sound” and “image” of performance have fatal
problems of its delay. The ﬁnal sound of the performance
and image of the movements of performer are detected just
after its generation, and the system has limited conversion
time and limited computation time, so the performer feels
the delay of response in every time. Thus I have developed
two types of news e n s o r sw i t hwhich computer system can
detect the actions before by sound or by image of the per-
formance.
3.1 Vocal Breath Sensor
Vocal performer acts with heavy breathing, and her(his)
breast and b elly repeats expansion and contraction. So I
used rubber tube sensor (Fig.14) which changes its resis-
tance with the tension, and produced the Vocal Breath Sen-
sor system to convert the breathing information to MIDI in
real-time (Fig.15).
Figure 14: Rubber Tube Sensor
Figure 15: “Vocal Breath Sensor” System
The sensing information (Fig.16) is used to change signal
processing parameters of her voices and to arrange param-
eters of real-time computer graphics on stage. The audi-
ence can listen to her voice and watch her behavior with
thet ight/exaggerated relation eﬀected and generated by the
system whichd etects the changes before the sound.
Figure 16: Output of “Vocal Breath Sensor”
3.2 SHO Breath Sensor
NIME03-50
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
Ir e p o r tt h edevelopment of a compact/light bi-directional
breath pressure sensor for SHO(Fig.17). SHO is the Japanese
traditional musical instrument, a mouth organ. The SHO
player blows into a hole in the mouthpiece, which sends the
air through bamboo tubes which are similar in design and
produce a timbre similar to the pipes in a western organ.
The bi-directional breath pressure is measured by an air-
pressure sensor module, converted to digital information by
32bits CPU, and converted to MIDI information.
Figure 17: SHO Breath Sensor
The authoring/performing system displays the breathing
information in real-time, and helps the performer for delicate
control and eﬀective setting of the parameters (Fig.18). The
output of this sensor shows not only (1) the air-pressure
inside the SHO,a n d( 2 )the volume of SHO sound of course,
but also (3) preliminary preparation operation and mental
attitude of the performer, so it is very important for the
system to detect this information before sound starts.
Figure 18: Example of SHO Breath Data
3.3 Performances with SHO Breath Sensor
Figure 19: Performance of “Visional Legend” (Kas-
sel)
This bi-directional SHO breath sensor is originally pro-
duced by myself, and used by Japanese SHO performer/composer
Tamami Tono Ito. The “Breathing Media” project is her
own. I have composed interactive multimedia art called “Vi-
sional Legend” which was performed in Kassel (2 concerts)
and Hamburg in September 2001 (Fig.19). She composed
some works using this sensor, and Figure 20 shows the work
“I/O”. Her breath controls both sound synthesis and live
graphics.
Figure 20: Performance of “I/O” (SUAC Japan)
4. BIO-F EEDBACK SYSTEM
Finally, I report the newest development of a compact and
light 8-channel biological feedback system (Fig.21). The
feedback signal is high voltage (10V-100V) electric pulses
like “low frequency massage” device (Fig.22-23)). The wave-
shape, voltage and density of pulses are real-time controlled
with MIDI fromthe system. The purposes of this feedback
are: (1) detecting performer’s cues from the system with-
outb eing understood by audience, (2) delicate control of
sounds and graphics with the feedback feeling in virtual en-
vironment, (3) live performan ce of outside of anticipation
with the electric trigger.
Figure 21: Bio-Feedback System
Figure 22: Example of Bio-Feedback signal
NIME03-51
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
Figure 23: Bio-Feedback contacts
4.1 Application Example (1)
Figure 24 shows the performance of the work “It was going
better If I would be sadist truly.” composed and performed
by KenF urudachii nF ebruary 2002 in Japan. There were
2D J( s c r a t c h ing discs) performers on stage, and the DJ
sounds generates many types of bio-feedback signals with
Max/MSP and this system. T he performer shows the re-
lation between input sounds and output performance just
by his body itself. This work is the ﬁrst application of the
system.
Figure 24: Performance of “It was going better If I
would be sadist truly.”
4.2 Application Example (2)
Figure 25 shows the performance of the work called “Flesh
Protocol” composed by Masayuki Akamatsu and performed
by Masayuki Sumi in February 2002 in Japan. The per-
former is a professional dancer, so he can receive two times
bigger electronic pulses with his strong and well-trained body.
Thec omposerp roduces many noises and sounds with Max/MSP,
and the converted signals control the body of the performer
on stage. The relations of them are well shown in real-time
with the screen and motions on stage.
Figure 25: Performance of “Flesh Protocol.”
4.3 Application Example (3)
Figure 26 shows the performance of the work called “Ryu-
sei Raihai” composed by Masahiro Miwa in March 2002 in
Japan. The four perfo rmers connected to the system are
”instruments” of the special message in Internet with the
composer’s ﬁltering program.W h e n one special data occurs
in the network, one of the performers is triggered by the
system, then he/she plays bell on the hand in real-time.
Figure 26: Performance of “Ryusei Raihai”
4.4 Possibility of “Hearing pulse”
Iw ant to discuss about a possibility of ”hearing pulse”
without using ears. In experiments during development of
this sy stem, I found many interest ing experiences to de-
tect ”sounds” without acoustic method (speaker, etc). The
NIME03-52
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
numbed achefrom this Bio-feedback system is diﬀerent with
thew aveshape, frequency etc. This shows the possibility for
hearing-impaired person that a sound can be perceived with-
out using an ear. Another experiment, the sourse is changed
from simple pulses to musical signals also shows the possi-
bility of listening to the music with this feedback.
4.5 Combined force display system
It is well known that EMG sensor is a good instrument,
but there is a weakpoint compared with other “mechanical”
instruments. EMG sensor does not have the “physical reac-
tionof performance” like guitar, piano, etc. To resolve this,
one musician generates very big sound as the body-sonic
feedback of the performance. I have just started research-
ing the combination of EMG sensor and bio-feedback sys-
tem with the same electrode using time-sharing technique.
Figure 27 shows the block diagram for the combination.
Card-size controller “AKI-H8” contains 32bits-CPU, A/D,
D/A, SIO, RAM, FlashEEPROM and many ports. Elec-
trode channels (max:8ch) aremultiplexed for A/D and D/A.
“Highi mpedance of separation, high input voltage” analog
switch is controlled by the CPU ports synchronizing with
the system.
Figure 27: Blockdiagramof “Combined System”
5. CONCLUSIONS
Some researches and experimental applications of human-
computer interactioni nm u l t i - m e d i ap erforming arts were
reported. Interactive multi-media art is the interesting lab-
oratory of human interfaces and perception/cognition re-
searches. So I will continue these researches with many ex-
periments.
6. REFERENCES
[1] Y. Nagashima. PEGASUS-2: Real-Time Composing
Environment with Chaotic Interaction Model. In
Proceedings of ICMC,I CMA, 1993.
[2] Y. Nagashima. Multimedia Interactive Art : System
Design and Artistic Concept of Real-Time
Performance withC o mputer Graphics and Computer
Music. In Proceedings of Sixth International
Conference on Human-Computer Interaction,
ELSEVIER, 1995.
[3] Y. Nagashima. A Compositional Environment with
Interaction and Intersection between Musical Model
and Graphical Model – Listen to the Graphics, Watch
the Music. In Proceedings of ICMC,I CMA, 1995.
[4] Y. Nagashima. Real-Time Interactive Performance
with Computer Graphics and Computer Music In
Proceedings of 7th IFAC/IFIP/IFORS/IEA
Symposium on Analysis, Design, and Evaluation of
Man-Machina Systems,I FAC, 1998.
[5] Y. Nagashima. BioSensorFusion: New Interfaces for
Interactive Multimedia Art In Proceedings of ICMC,
ICMA, 1998.
[6] Y. Nagashima. ’It’s SHO time’ – An Interactive
Environment for SHO(Sheng) Performance In
Proceedings of ICMC,I CMA, 1999.
[7] Y. Nagashima. Composition of ”Visional Legend”
http://nagasm.suac.net/ASL/kassel/index.html
[8] C. Bahn, T. Hahn, and D. Trueman. Physicality and
Feedback: A Focus on the Body in the Performance of
Electronic Music In Proceedings of ICMC,I C M A ,
2001.
[9] R. Boulanger and M. V. Mathews. The 1997
Mathews’ Radio Baton and Improvisation Modes In
Proceedings of ICMC,I CMA, 1997.
[10] A. Camurri. Interactive Dance/Music Systems In
Proceedings of ICMC,I CMA, 1995.
[11] J. Chadabe. The Limitations of Mapping as a
Structural Descriptive in Electronic Instruments In
Proceedings of NIME, 2002.
[12] C. Chafe. Tactile Audio Feedback In Proceedings of
ICMC,I CMA, 1993.
[13] R.J.K. Jacob. Human-Computer Interaction: Input
Devices In Input Devices, ACM Computing Surveys
28(1), 1996.
[14] S. Jorda. Improvising with Computers: A Personal
Survey( 1989-2001) In Proceedings of ICMC,I C M A ,
2001.
[15] T. Kanamori, H. Katayose, S. Simura, and S.
Inokuchi. Gesture Sensor in Virtual Performer In
Proceedings of ICMC,I CMA, 1993.
[16] T. Kanamori, H. Katayose, Y. Aono, S. Inokuchi, and
T. Sakaguchi. Sensor Integration for Interactive
Digital Art InProceedings of ICMC,I CMA, 1995.
[17] T. Kanamori, H. Katayose, Y. Aono, S. Inokuchi, and
T. Sakaguchi. Virtual Performer In Proceedings of
ICMC,I CMA, 1993.
[18] R. B. Knapp and H. S. Lusted. A Bioelectric
Controller for Computer Music Applications In
Computer MusicJ o u r nal 14(1), 1990.
[19] R. B. Knapp and H. S. Lusted. The Computer Music
Tutorial Cambridge, MA: The MIT Press, 1996.
[20] R. Rowe. Interactive MusicSystems - Machine
Listening and Composing Cambridge, MA: The MIT
Press, 1993.
[21] A. Tanaka. Musical Technical Issues in Using
Interactive Instrument Technology with Application
to the Biomuse In Proceedings of ICMC,I CMA, 1993.
NIME03-53