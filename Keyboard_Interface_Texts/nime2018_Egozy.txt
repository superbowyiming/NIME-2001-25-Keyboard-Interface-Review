*12*: Mobile Phone-Based Audience Participation in a Chamber Music Performance   Eran Egozy Music and Theater Arts  Massachusetts Institute of Technology  77 Massachusetts Avenue, Cambridge, MA 02139 egozy@mit.edu 
 Eun Young Lee  Music Composition The Boston Conservatory at Berklee 8 Fenway, Boston, MA, 02215 elee@bostonconservatory.edu   ABSTRACT *12* is chamber music work composed with the goal of letting audience members have an engaging, individualized, and influential role in live music performance using their mobile phones as custom-tailored musical instruments. The goals of direct music making, meaningful communication, intuitive interfaces, and technical transparency led to a design that purposefully limits the number of participating audience members, balances the tradeoffs between interface simplicity and control, and prioritizes the role of a graphics and animation display system that is both functional and aesthetically integrated. Survey results from the audience and stage musicians show a successful and engaging experience, and also illuminate the path towards future improvements.  Author Keywords Mobile music, audience participation, chamber music, visualization  CCS Concepts • Applied computing → Performing arts; • Applied computing → Sound and music computing;  1. INTRODUCTION Various audience participation models have been explored for some time, in particular by exploiting the connected mobile devices available to almost all concert goers. This ubiquitous technology gives composers and technologists opportunities to create a meaningful bi-directional relationship between audience and performer by allowing audience members to influence the live music performance process. Much has been written about audience participation pieces. Many publications (as does this one) describe specific instances of enabling technologies and accompanying works such as Dialtones [10], McAllister’s Wireless PDAs [12], TweetDreams [2], echobo [9], Constellation [11], or Performances Without Borders [5]. Some describe generalized systems for audience participation like massMobile [16], Mood Conductor [4], or Open Symphony [17]. Some recent work takes a broader view attempting to generalize design methodologies and concepts in technology-mediated audience participation [6]. Most works use mobile devices as the audience interaction device, yet a variety of design choices must still be made when creating a mobile-based audience participation experience: 
                                                             1 http://radiusensemble.org 
• Sound Source: does the sonic output come from the mobile devices themselves, from speakers on stage, and/or from live musicians on stage? • Interaction Capabilities: what is the level of musical control offered to the audience member, from fine control (playing individual notes) to high-level control (voting on a musical mood). • Audience Skill: how much musical ability is assumed by a typical participating audience member? • Scope of Influence: to what extent do audience actions affect the resulting musical output? How does the overall size of the audience affect an individual audience member’s influence? • Stage Performers: Are there live performers and if so, how are they influenced by the audience? • Visual Feedback: What visual reinforcements exist to aid or enhance the experience for both audience and stage performers? There can be many satisfying audience-participation models, yet the above parameters must be chosen carefully to best achieve the design and artistic goals of a given work. 2. DESIGN GOALS *12* was commissioned by Radius Ensemble1 to be performed in the group’s regular concert season. Radius Ensemble is a chamber music group consisting of nine instrumentalists who form into subgroups (i.e., trios, quartets) to play a variety of classical music – from eighteenth century masterworks to modern compositions and world premieres. Unlike most audience-participation pieces which take place in an academic setting, *12* would be performed for a more traditional classical music audience unaccustomed to experimental music. It would also feature all the Radius Ensemble musicians who are classically trained and generally do not participate in freeform improvisation. Thus, we avoided participation models that rely primarily on stage-musician improvisation such as Wireless PDAs, echobo, Mood Conductor, or Open Symphony. Our inspiration for *12* came from the project “Look, Sound!” [8], a video game for music education and composition that encourages everyone to be a composer regardless of specialized tools or expertise. We want to blur the distinction between composer and listener, letting the audience actively engage in contemporary music. Therefore, our main goal was for audience participants to feel like true contributors to the outcome of the piece, to have creative control, meaningful interaction, and to be direct participants in the music making. This goal is in contrast, for example, to audience-voting pieces where audience members have control in aggregate by influencing the average value of certain 
NIME Proceedings Template for LaTeX
Ben Trovato⇤
Institute for Clarity in
Documentation
1932 Wallamaloo Lane
Wallamaloo, New Zealand
trovato@corporation.com
G.K.M. Tobin†
Institute for Clarity in
Documentation
P .O. Box 1212
Dublin, Ohio 43017-6221
webmaster@marysville-
ohio.com
Lars Thørväld‡
The Thørväld Group
1 Thørväld Circle
Hekla, Iceland
larst@afﬁliation.org
Lawrence P . Leipuner
Brookhaven Laboratories
Brookhaven National Lab
P .O. Box 5000
lleipuner@researchlabs.org
Sean Fogarty
NASA Ames Research Center
Moffett Field
California 94035
fogartys@amesres.org
Anon Nymous
Redacted
8600 Datapoint Drive
San Antonio, Texas 78229
cpalmer@prl.com
ABSTRACT
This paper provides a sample of a LATEX document for the
NIME conference series. It conforms, somewhat loosely, to
the formatting guidelines for ACM SIG Proceedings. It is an
alternate style which produces atighter-lookingpaper and
was designed in response to concerns expressed, by authors,
over page-budgets. It complements the documentAuthor’s
(Alternate) Guide to Preparing ACM SIG Proceedings Us-
ing LATEX2✏ and BibTEX. This source ﬁle has been written
with the intention of being compiled under LATEX2✏ and
BibTeX.
To make best use of this sample document, run it through
LATEX and BibTeX, and compare this source code with your
compiled PDF ﬁle. A compiled PDF version is available to
help you with the ‘look and feel.’The paper submit-
ted to the NIME conference must be stored in an
A4-sized PDF ﬁle, so North Americans should take
care not to inadvertently generate letterpaper-sized
PDF ﬁles.This paper template should prevent that from
happening if thepdflatex program is used to generate the
PDF ﬁle.
The abstract should preferably be between 100 and 200
words.
Author Keywords
NIME, proceedings, LATEX, template
CCS Concepts
•Applied computing! Sound and music comput-
ing; Performing arts; •Information systems! Music
retrieval;
⇤Dr. Trovato insisted his name be ﬁrst.†The secretary disavows any knowledge of this author’s ac-
tions.‡This author is the one who did all the really hard work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
Please read the comments in the nime-template.tex
ﬁle to see how to create the CCS Concept Classiﬁ-
cations!
1. INTRODUCTION
The proceedingsare the records of a conference. ACM seeks
to give these conference by-products a uniform, high-quality
appearance. To do this, ACM has some rigid requirements
for the format of the proceedings documents: there is a
speciﬁed format (balanced double columns), a speciﬁed set
of fonts (Arial or Helvetica and Times Roman) in certain
speciﬁed sizes (for instance, 9 point for body copy).
The good news is, with only a handful of manual set-
tings,1 the LATEX document class ﬁle handles all of this for
you.
The remainder of this document is concerned with show-
ing, in the context of an “actual” document, the LATEXc o m -
mands speciﬁcally available for denoting the structure of a
proceedings paper, rather than with giving rigorous descrip-
tions or explanations of such commands.
2. THE BODYOF THE PAPER
Typically, the body of a paper is organized into a hierar-
chical structure, with numbered or unnumbered headings
for sections, subsections, sub-subsections, and even smaller
sections. The command\section that precedes this para-
graph is part of such a hierarchy.2 LATEX handles the num-
bering and placement of these headings for you, when you
use the appropriate heading commands around the titles of
the headings. If you want a sub-subsection or smaller part
to be unnumbered in your output, simply append an aster-
isk to the command name. Examples of both numbered and
unnumbered headings will appear throughout the balance
of this sample document.
Because the entire article is contained in thedocument
environment, you can indicate the start of a new paragraph
with a blank line in your input ﬁle; that is why this sentence
forms a separate paragraph.
1Two of these, the \numberofauthors and \alignau-
thor commands, you have already used; another,\bal-
ancecolumns, will be used in your very last run of LATEX
to ensure balanced column heights on the last page.2This is the second footnote. It starts a series of three
footnotes that add nothing informational, but just give an
idea of how footnotes work and look. It is a wordy one, just
so you see how a longish one plays out.
7
performance parameters, yet any particular individual does not have direct influence. We chose to satisfy this primary goal by imposing the following design guidelines: • Limit the number of audience participants at any one time so that each audience voice can be individually heard, and whose influence is not diluted by group averaging. • Create instruments that are intuitive to use by non-musicians (interfaces should not presume music-theoretical knowledge like “chord tones” or “eighth notes”). • Reinforce audience sound production with graphics and animation. This goal provides two benefits in that the visual display is not only informational but also contributes to the artistic expression of the piece. • Create transparency in what the technology does and how it unfolds over the course of the performance. Reducing the number of audience participants creates an additional group of stakeholders at the concert. While most audience-participation pieces focus on two groups – the stage performers and the audience performers, we have three roles: performing stage musicians, performing audience members, and non-performing audience members. As we designed and composed *12*, we kept this third group in mind, especially with the goal of creating transparency. The control interface for the mobile device and its music production capabilities go hand in hand. We limit the degrees of freedom of expression so that audience participants cannot play incorrect notes or create sounds discordant with the music played by the stage musicians. The interface is simple and intuitive, such that audience participant need not worry about “making mistakes”. We are motivated by systems that encourage music-making by non-musicians such as the generative music control scheme described by Rigopulos in [14], Harmonix Music System’s The Axe [3], and Jordà’s work on intuitive and efficient instruments [7]. More recent examples include casual music creation apps like those by Smule [15], or web apps like Chrome Music Lab: Kandinsky2. In other words, we design instrument models that are simple enough to learn easily, yet still possess reasonable musical expression capabilities. After first considering device-originated sound production (where audience-participant music is generated by the mobile device itself), we instead designed the mobile device to act purely as a controller, leaving sound production to a synthesis device connected to on-stage speakers. One concern was that audience participants may receive audible phone calls or text messages, thereby disturbing the performance. Another was our desire to have more control over the resulting sound. Mobile phone speakers have limited bass response, and the quality and volume of the sound cannot be guaranteed. For example, we recorded custom percussion samples for *12* such as deep bass drum rolls which greatly benefitted from playback on high-quality speakers. But this decision presents a trade-off. Without local sound production, the mobile device feels less like a musical instrument. The participant may feel less influential and less directly connected to the performance.  To ameliorate this problem, we introduced a responsive visualization system that is projected on a large screen during the performance. As audience participants play their instruments, animations appear on the screen in perfect synchrony with their triggered sounds. This combination of visual and auditory response reinforces the causality between input gesture and resultant action. But this visualization system also confers two                                                              2 https://musiclab.chromeexperiments.com/kandinsky 
additional advantages. First, it increases transparency. Just as mobile players benefit from the audiovisual connection between simultaneous sound and animation events, so too can the non-participating audience members. Second, the visualization system offers an additional axis of artistic expression, adding a multi-sensory dimension to the work. 3. ARTISTIC GOALS *12* is based on the twelve signs of the zodiac and therefore has twelve movements, each inspired by (and named) one of zodiac signs. Compositionally, the acoustic chamber music timbres, the audience-performed sounds, and the mood of each movement are designed to be as distinctive as possible. In fact, each movement is played by a unique subgrouping of the chamber musicians. Four movements are designated as audience-participation movements, spread evenly across the whole piece, while the remaining eight movements are performed only by the stage musicians. This choice kept the project in scope yet also creates an overarching structure, where the work as a whole moves back and forth between the traditional and the experimental. Though the eight non-interactive movements are interesting in their own right, this paper focuses on the technical and artistic issues related to the four audience-participation movements. To satisfy the individuality goal, each interactive movement features only three audience participants, for a total of 12 participants in the whole piece. While this constraint limits the number of participants, it creates considerable design opportunities that are difficult to achieve when the audience size is unknown in advance or unbound (as is the typical assumption for most previous work, with the exception of, say, McAllister, where a fixed number of devices were handed out). Each mobile instrument is custom-crafted to the specific compositional needs of the movement in which it plays. This helps maintain artistic intent and cohesion of the interactive movements while also increasing the individuality and influence of the audience participants. Two movements feature larger stage ensembles with a high density of sound, rhythmic activity, and harmonic motion: Taurus, scored for clarinet, horn, cello, and piano, and Leo, scored for bassoon, French horn, and viola. The other two are solo instrument movements: Scorpio, for solo piano, and Aquarius for solo flute. Despite the variation in mood across these four movements, we desired some coherence in the musical role of the mobile devices by choosing a common sound design. In addition, we needed to create instruments that could be comfortably played by non-musicians – where any produced output is musically plausible. This led us to choose custom-recorded percussion sounds for the audience-controlled instruments. For the large ensemble pairings, we use unpitched percussion to avoid harmonic inconsistencies between the stage ensemble and triggered sounds. During the solo movements, additional pitched percussion sounds are used that are sonically distinct yet harmonically blend with the soloist’s musical lines, thus encouraging playful interaction between stage musician and audience participants. 4. IMPLEMENTATION The technology components of *12* are depicted in Figure 1. A local server supports connections from four client types: the audience mobile device client (the Instrument), a simple operator client for switching between operational modes of the piece (the Manager), a graphics client for displaying visuals on a large projection screen (the Visualizer), and a music synthesis client (the Synth). In addition, a set of remotely addressable LED 
8
necklaces are worn by audience players. These components are described in detail below.  
Server: The server (javascript / Node.js) accepts connections from the various clients and uses Websockets and OSC messages for real-time communication. It manages configuration, program state, and shuttles data between clients as appropriate. The server computer (Mac/OSX) is connected via hardwire to a WIFI access point (Apple Airport Extreme). We tested several communication topologies and concluded that this configuration presents the lowest possible latency characteristics. A dedicated WIFI network with wired server (as opposed to public WIFI, wireless only, or 3G/LTE mobile networks) provides extremely fast communication between clients and server such that the latency between a tap on the mobile device and resulting sound is essentially imperceptible (i.e., less than 30ms). In addition, this client-server topology allows for great hardware flexibility. Each component can run on a dedicated machine, or components can be grouped, depending on the speed and availability of computing platforms. In our performance, two Macs were used, one for the Visualizer, and one for the Server, Synth, and Manager. Instrument: The instrument client (javascript / HTML5 / p5.js) is a web page accessed by the mobile device’s browser. This design choice has the disadvantage of less programmatic control of the device than a native app (such as volume level, or the automatic sleep timeout). However, the advantages of easy setup (no app download), target platform independence (works on iOS and Android), as well as rapid development and instantaneous deployment, were deemed worthwhile tradeoffs.  When an Instrument client connects to the server, it presents a choice of movement (one of the four interactive movements) and a part (one of three available parts for that movement). After these selections are made, the proper interface appears. With twelve Instrument types, all interfaces share some common design elements, yet are custom-coded to the needs of that particular Instrument.  After a number of prototypes and experiments, we settled on a 2D rectangular control surface. We avoided representational graphics (such as a piano keyboard) or even standard GUI elements (like buttons or sliders). Instead we adhered to a simple fundamental concept – the entire control surface is the interface. Using a single finger, the user presses anywhere on the screen to initiate a sound. For sustaining or looping sounds, pressing and holding will sustain the sound and releasing the finger will stop the sound. During a hold, the user can slide their finger in the X and Y directions to continually modify and shape the resultant musical output. Generally, the X direction is mapped to volume 
while the Y direction controls the “activity” or “complexity” of the sound. The actual mapping of the Y control axis is based on the particular needs of that Instrumental. See below for two examples. As the user taps or slides, a colored trail is left behind as a short-term memory of the recent finger gesture. The trail’s color is the “player color” which is unique to each active player and matches their visualizations on the display, similarly to the multiplayer display system used in [13]. Two interface examples are shown in Figure 2. In Leo, the temple block Instrument can trigger either an ascending or descending scale of 4 notes. The course Y position (top half vs bottom half) selects the direction of the notes. Within the selected portion, Y position maps continuously to the tempo of the played notes. In Scorpio, the Y axis maps to three discrete samples: a single gong hit, rapid successive gong taps, or a pitch-bending gong (recorded as the gong was hit and submerged in water). 
Manager: The Manager client (javascript/ HTML5) allows the operator to enable a particular movement of the piece (i.e., when the stage musicians are about to begin that movement). Enabling a movement causes the proper Instruments to become active and configures the Visualizer to display the proper graphical elements for that movement. The Manager also provides information to the operator such as the number and types of connected clients and various debugging and statistical information. Visualizer: The visualizer client (javascript/ p5.js) displays real-time graphics and animations based directly on the actions of the Instrument clients. The visual elements are designed to complement the distinctive mood of each movement and to provide a unifying look for the whole piece. Taking a cue from the zodiac theme, a star-field is drawn as background and divided into three vertically arranged zones. Each zone reacts to the music created by one of the three active mobile devices. A single musical gesture (quick tap or tap-and-hold) by a mobile device triggers a graphical animation that originates in the zone assigned to that Instrument and is tinted to the color of the Instrument’s interface. Emergent graphical effects occur when these animations overlap. 
Figure 1. System diagram.  
Figure 2. Instrument interface examples for Leo temple block (a) and Scorpio gong (b). The moving circle follows the finger position, with color corresponding to the color of the graphical animation in the Visualizer. X-axis maps to volume while Y-axis mapping depends on the instrument type.   
9
Two examples are shown in Figure 3. In Aquarius, each triggered Instrument sound initiates an animation of concentric circles emanating outwards from an origin star. This animation has the feel of overlapping ripples caused by pebbles tossed into a lake. In Taurus, jet-like streams of color shoot out from an origin star in a random cardinal direction. The overlapping streams creates rectangular patterns that appear and disappear along with the corresponding Instrument sounds. 
Synth: A real-time python app handles sound production. It receives OSC control messages from the Instruments (via the Server) and responds according to their particular configurations. Each of the twelve Instruments is individually designed, taking advantage of various features of the sound engine. At its core, the Synth plays sampled sounds. Sounds can be triggered as one-shots, looped until release, or cross-faded to other sounds. Each sound also has independent volume and pan controls. A simple sequencer can create rhythmic patterns of samples at variable tempi as well as loop and seamlessly switch to other patterns. This basic set of functionalities allows for a rich variety of Instrument types. Some examples are: Maracas (fast and slow shaking sounds), Temple Block (ascending or descending patterns at different speeds), Toy Hose (looping sounds of a spinning hose, cross-fading between different speeds/pitches), and Bass Drum (drum rolls at different speeds). LED Necklaces: To further aid the transparency goal (in addition to the projected visualizations), we assembled twelve remote-controlled LED necklaces to be worn by the audience                                                              3 http://musictech.mit.edu/twelve 
participants. As participants begin to play, their LED necklaces illuminate for the duration of their movement. While they stay seated and are thus conceptually part of the audience, their special role as audience participants is highlighted to the rest of the crowd. 5. EVALUATION We selected audience participants the week prior to the concert based on responses to a “call for participants” email to the Radius Ensemble mailing list. Participants arrived one hour before the concert for a brief explanation, setup, and dress rehearsal. Participants agreed ahead of time which movement and Instrument they would play. In the dress rehearsal, each audience member tried their Instrument to see how it works, feels, and sounds. We ran each interactive movement twice, letting both the audience participants and the stage musicians become familiar with the musical material. The performance itself went well and presented no technical difficulties. Video excerpts from the concert are available online.3 Non-participating audience reaction was favorable. Though there was no formal survey, anecdotally many in the audience liked the visual responses and the interplay between live musicians and electronically produced sounds. A few audience members wished to have the opportunity to perform as well, though most were content to see the events unfold passively. This sentiment is not surprising given that this audience is generally unaccustomed to experimental music and the number of respondents in the call-for-participation was not very high (fewer than 15). The mobile instrumentalists were asked to complete an online survey with quantitative questions on a Likert seven-point scale, and qualitative open-response questions. We wanted to determine how engaged the users felt in the music making process, how comfortable their Instruments felt, and how much influence they perceived themselves to have. Of the twelve audience participants, nine responded to the survey. See Table 1 for the quantitative questions and Figure 4 for a summary of the results.  Question Likert Scale Range 1 7 1 How musical are you? Tone-deaf Professional 2 How clear / easy-to-use was the interface for controlling your phone instrument? Confusing Clear 
3 How much musical impact did you feel you had in the movement that you played? Very little Quite a lot 
4 How nervous did you get? Not at all Very much 5 Were you able to look up from your phone while you played? Always looked at phone Always looked up 6 Did you have fun? No Yes    The overall interface design was clear and easily understood (question 2). Users felt comfortable with their Instruments, and appeared engaged in the performance; they could look up from their device (question 4) and they had fun (question 6). The impact (question 3) they felt to have on the overall piece was moderate on average, though variation in responses seems related to the particular movement played: Taurus scoring an 
 
Figure 3. Visualization examples for Aquarius (a) and Taurus (b). The star-field background is common to all movements, while the triggered animations are unique.  Animation origin and color corresponds to one of the active mobile Instruments.  
Table 1. Quantitative survey questions   
10
average of 3.5, Aquarius and Leo scoring an average of 4, and Scorpio scoring an average of 4.7. See Figure 5. This corresponds to the general feeling amongst some non-participating audience members and musicians that Scorpio (solo flute movement) was the most successful interactive movement of the four. Taurus, with the lowest impact score, has the most stage instrumentalist (four), and the most complicated rhythmic patterns. 
Two other questions (question 1, “how musical are you?”, and question 2, “how nervous did you get?”) aim to assess if the participants’ musical knowledge and performance experience are correlated with any of the other answers. Most users considered themselves quite musical even though the Instruments were designed with non-musicians in mind. The only factors showing correlation to level of musicianship are nervousness (r = -.47, p = .19) and the ability to look up from the phone (r = -.50, p = .16). The rest of the questions appear uncorrelated. However, due to the small sample size, we do not show sufficient confidence in these results, and thus further study would be required.  The qualitative questions are: • What did you think about the large projected graphics? • Describe your experience playing your phone • What do you think could improve the experience for the smartphonist? • Any other comments? Responses about the projected graphics were generally positive (7 of 9 had positive responses). Some quibbled with the visual aesthetic (“I think the graphics would suit the music better if they had a more organic look to them”, “the sonic variety was not reflected visually”). Some liked the artistic direction (“they were 
beautiful”, “I would find myself excited to see the art that came out from my playing”). Many found them functionally useful, thereby helping to satisfy our transparency goal (“it was good to have the visual feedback for the audience”, “sometimes, I couldn't hear my part, but it helps me to see myself playing”, “it enhanced the musical experience”). Freeform response to the overall experience was also very positive (8 of 9 had positive comments). One non-musician found it a very fulfilling experience (“I have never been part of a performance before, and I was very moved by the chance to take part in a communicative way”). A secondary theme emerged as well – that of wanting more control (“an instrument with more freedom would have been more enjoyable to interact with”, “the maracas were tricky to control, I wish I could have had the ability to make faster rhythms”, “was impressed with the simplicity and low latency. I wanted more control, though.”). Indeed, simplicity was a design goal. However, since many of the participants considered themselves highly musical, they may have desired more of the freedom and control afforded by traditional musical instruments. The two soloists (flutist in Scorpio, pianist in Aquarius) were surveyed about their experience playing their movements with audience participation. Again, responses were positive. The pianist felt that the movement was very atmospheric and allowed for “noticing what was going on around me and respond[ing] to some degree”. Though when asked about soloist-audience communication, she said “I think I could respond more readily than an audience member could. Not sure it was symbiotic.” The flutist had a very positive and engaging experience. In particular, she commented on how additional run-throughs helped improve communications between soloist and audience: “first [run] was a bit heavy handed; they were constantly playing, not very sympathetic to my nuances or breaks. The second run was much better but I have to say that the performance was the best. They were truly collaborative, sensitive to my silences and got louder when I got louder like a real chamber music performer.” 6. CONCLUSION AND FUTURE WORK Though *12* is not the first audience participation piece to incorporate mobile devices, stage performers, and large screen visualization, it does explore some new territory. We aimed to create a deep interaction model between audience and performer. We intentionally diminished the size of the participating group in order to increase the impact of the individual voice and its importance in the work. This design choice allowed the audience participant to seek the same level of musical influence as the stage musician. As our flutist said, “like a real chamber music performer.” Designing an audience performance system where the precise number of participants is known a priori creates a new model of audience participation system. While it reduces the number of participants from the whole audience, it affords the composer more intentionality. Unlike some audience participation pieces where the audience’s sound tends to provide only a textured background (such as [9]), in *12* each audience participant’s instrument is individualized and has a unique and important voice in the work. We introduced this system into a traditional chamber music concert series. Instead of being consumed by technology, *12* is a tasteful integration of interactive design into an existing medium – contemporary classical chamber music. Only 1/3 of the movements are interactive. We consciously interwove the traditional with the experimental. Many related works rely on the stage musicians’ ability to improvise (such as [4] and [17]) because their music is free-form or the structure is not known in advance. *12* represents a 
Figure 4. Average values of quantitative survey results from audience participants. Error bars show standard error of the mean.  
Figure 5. Results of question 3 (Musical Impact), shown as average value by movement.  
11
through-composed audience participation pieces – the classically trained musicians play music “as written”, while the audience participants play the role of improvisers. This does beg the question of why the audience should take on the role of improviser. Indeed, an additional area to explore is providing more guidance to the audience players. We could introduce more directed instruction such as graphical notation or videogame motivational structures for musical expression as discussed in [2]. We are heartened by the success of the projected visualization system. It helps both the participating and non-participating audience understand the piece, yet just as importantly, the visualization system is an integral part of *12*, providing for a cohesive and artistically integrated multi-sensory experience. Lastly, we note the (obvious) importance of rehearsals to improve the performance itself. Professional chamber musicians rehearse many times before a concert. If we want audience participants to have an engaging and communicative music-making experience, they should be “elevated” to the status of performing musicians and be considered as musical equals. The work and analysis described herein suggest a few directions for continued exploration. Though having twelve participants worked well for this piece, we wonder how the present model can scale to a larger number of audience participants. Is it possible to keep the individualized importance of a particular person while also increasing the number of participants?  The system architecture as described is moderate in complexity, requiring two laptops, a dedicated WIFI router, and local python installation. With some additional investigation, we could make the system easier to deploy, for example by having 100% of the code be browser-based, removing the dedicated router, and running the server in the cloud. Such a setup could make it easier for other groups to perform *12* without the presence of the original creators. We also want to investigate the mobile Instrument control paradigm. How do we provide more control to the player while keeping the interface simple and intuitive? And more broadly, how can composers create engagements that encourage a higher degree of musical communication between stage musicians and audience participants.  7. ACKNOWLEDGMENTS Our thanks to Jeff Hesser, for animation and graphic design of the visualization system, Aaron Trant, for recording the percussion samples, Nathan Gutierrez for audio production, Will Bartlett for building and operating the LED necklaces, the musicians of Radius Ensemble, Jennifer Montbach, music director, and John and Ann-Mara Lanza for their generous financial support of this project. 8. REFERENCES [1] M. Boltz, B. Ebendorf, and B. Field. Audiovisual Interactions: The Impact of Visual Information on Music Perception and Memory. Music Perception: An Interdisciplinary Journal, Vol. 27 No. 1, September 2009, pp. 43-59. [2] L. Dahl, J. Herrera, and C. Wilkerson. TweetDreams: Making Music with the Audience and the World Using Real-Time Twitter Data. In Proceedings of the International Conference on New Interfaces for Musical Expression (NIME). Oslo, 2011 [3] E. Egozy. Approaches to musical expression in Harmonix video games. In J. B. L. Smith, E. Chew, & G. Assayag (Eds.), Mathemusical Conversations: 
Mathematics and Computation in Music Performance and Composition. Singapore: Imperial College Press and World Scientific. 2016 pp. 20–36 [4] G. Fazekas, B. Mathieu, and M. B. Sandler. Novel Methods in Facilitating Audience and Performer Interaction Using the Mood Conductor Framework. In International Symposium on Computer Music Modeling and Retrieval, pp. 122-147. Springer, Cham, 2013. [5] M. Gimenes, P. Largeron, and E. Miranda, Frontiers: Expanding Musical Imagination with Audience Participation. In Proceedings of the International Conferences on New Interfaces for Musical Expression (NIME). Brisbane, Australia, 2016, pp. 350-354.  [6] O. Hödl, The Design of Technology-Mediated Audience Participation in Live Music, PhD Thesis, Vienna University of Technology, 2016. Available from: http://repositum.tuwien.ac.at/obvutwhs/download/ pdf/1375209 [7] S. Jordà. Interactive music systems for everyone: exploring visual feedback as a way for creating more intuitive, efficient and learnable instruments. En Proceedings of the Stockholm Music Acoustics Conference, August. 2003. [8] E. Y. Lee, Look! Sound! Available from: http://eunyoungleemusic.com/EYL/Projects.html [9] S. Lee and J. Freeman. Echobo : A Mobile Music Instrument Designed for Audience to Play. In Proceedings of the International Conference on New Interfaces for Musical Expression (NIME), Daejeon, South Korea, 2013. [10] G. Levin, Dialtones (A Telesymphony) 2001 Available from: http://www.flong.com/projects/telesymphony/ [11] J. Madhavan, N. Snyder. Constellation: A musical exploration of phone-based audience interaction roles. In WAC Web Audio Conference. Georgia Institute of Technology, 2016. [12] G. McAllister, M. Alcorn, and P. Strain. Interactive performance with wireless PDAs. In Proceedings of the International Computer Music Conference. San Francisco, USA, 2004. [13] O. Perrotin and C. d’Alessandro, Visualizing gestures in the control of a digital musical instrument. In Proceedings of the International Conference on New Interfaces for Musical Expression, London, United Kingdom, 2014, pp. 605-608.  [14] A. Rigopulos, Growing Music from Seeds: Parametric Generation and Control of Seed-Based Music for Interactive Composition and Performance. Master's thesis, MIT Media Lab, 1994. [15] G. Wang, Game design for expressive mobile music. In Proceedings of the International Conferences on New Interfaces for Musical Expression (NIME). Brisbane, Australia, 2016, pp. 182-187. [16] N. Weitzner, J. Freeman, S. Garrett, and Y. Chen. massMobile – an Audience Participation Framework. In Proceedings of the International Conferences on New Interfaces for Musical Expression (NIME). Ann Arbor, MI, USA, 2012. [17] L. Zhang, Y. Wu, and M. Barthet, A Web Application for Audience Participation in Live Music Performance: the Open Symphony Use Case. In Proceedings of the International Conferences on New Interfaces for Musical Expression (NIME). Brisbane, Australia, 2016, pp. 170-175. 
12