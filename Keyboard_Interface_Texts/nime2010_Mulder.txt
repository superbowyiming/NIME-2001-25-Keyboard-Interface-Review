 
The Loudspeaker as Musical Instrument  
 
Jos Mulder 
University of Technology Sydney, Faculty of Arts and Social Sciences  
Room 16.22 P.O. Box 123 Broadway  
NSW 2007 Australia 
+61 295142019 
Johannes.Mulder@uts.edu.au 
 
 
ABSTRACT 
With the author’s own experien ces in mind, this paper argues 
that, when used to amplify musical instruments or to play back 
other sonic material to an audience, loudspeakers and the 
technology that drives them, can be considered as a musical 
instrument. Particularly in situations with acoustic instruments 
this perspective can provide insight into the often cumbersome 
relation between the –technology orientated – sound engineer 
and the –music orientated – performer. Playing a musical 
instrument (whether acoustic, electric or electronic) in volves 
navigating often complicated but very precise interfaces. The 
interface for sound amplification technology in a certain 
environment is not limited to the control surface of a mixing 
desk but includes the interaction with other stakeholder, i.e. the 
performers and the choice of loudspeakers and microphones 
and their positions. As such this interface can be as accurate and 
intimate but also as complicated as the interfaces of 'normal' 
musical instruments. By zooming in on differences between 
acoustic and electronic sources a step is taken towards inclusion 
in this discussion of the perception of amplified music and the 
possible influence of that amplification on performance 
practise. 
Keywords 
Sound technology (amplification), musical instruments, multi 
modal perception, performance practice.  
1. INTRODUCTION 
Considering the loudspeaker a musical instrument is not a very 
novel idea. It goes back to as early as Luigi Russolo’s 
“Intonarumori”, as described in, for instance, Simon 
Emmerson’s Living Electronic Music  [8] p.151 -160. Emmerson 
is one of the few authors who writes about the amplification of 
music other than from a technological point of view. This paper 
regularly references to his work, because of the lack of 
literature about this subject. A more technological appro ach can 
be found in a recent book on Sound System Design. This 
comprises the design of systems of loudspeakers, microphones 
and paraphernalia to play back or amplify sounds to audiences 
in a specified acoustic environment. In Sound Systems: Design 
and Optimization Bob McCarthy [11] writes:  
“A central premise of this book is that the loudspeaker is not 
given any exception for musicality. Its Job is as dry as the wire 
that feeds it an input signal: track the waveform.” 
From a technological point of view this  premise makes a lot of 
sense. But from a musical performance point of view there is 
more to converting electronic waveforms into audibly moving 
air: the process of translating creative ideas into applied 
technology make loudspeakers a lot more than just a  transducer.  
1.1 Musical Instruments 
In this paper I zoom in on how we use the “ instrument” 
Loudspeaker, or rather the whole system of input devices, 
infrastructure, control surfaces, signal processors and 
loudspeakers to amplify acoustic instruments or other  sources. 
Just like a musical instrument the loudspeaker instrument 
comes with its own interface. And like musical instruments a 
loudspeaker comes with its own relation to an acoustic 
environment. A big difference is of course the fact that musical 
instruments like the violin have their sound source attached to 
the interface. But like earlier examples of instruments with their 
interfaces detached from the sound source (organ, carillon) the 
advent of MIDI and other technologies allowed modern 
instruments to make the separation of interface and sound 
source a common one (see for instance Bert Bongers [2] p.55 or 
Chabade [4] p.185). At first sight the interface of the 
loudspeaker –or amplification– instrument would be the mixing 
desk as this is where usually al l inputs and outputs come 
together enabling a mixer’s or sound projectionist’ interaction 
connected to a performance. But the results of those interactions 
at the mixing desk are heard through a loudspeaker system that 
functions in a larger system of input  devices and room 
acoustics. This larger system creates a whole range of different 
feedback loops that extends beyond sonic feedback. Other 
feedback comes from (for instance) interactions with 
performers, audience and other stakeholders such as concert 
organizers such as a venue’s commercial, artistic and 
production staff.  
1.1.1 Ecological approach  
In the process of writing this paper I was directed to a 
comparable holistic approach to amplified performance by 
Owen Green [9], after Agostino DiScipio [7] stressin g that in an 
“Audible Ecosystem Interface” all components are structurally 
coupled. In connection to that argument Green makes another 
important point (quoting amongst others, Simon Waters [16]).  
Our current (electronic) performance practice is contiguous  
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advant age and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee.  
NIME2010, 15-18th June, 2010, Sydney, Australia 
Copyright remains with the author(s).   
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
13
with the history of musical performance: new technological 
developments and possibilities haven’t necessary changed the 
nature or the intricacies, including the importance of social 
context, of musical performance. However, with the separation 
of visual ( a performer) and aural (sound produced by that 
performer) sources, from a perception point of view something 
has changed indeed after the introduction of the loudspeaker. I 
will discuss this interesting notion further in section 3.  
1.1.2 Remote instruments  
The use of MIDI to separate interface and sound source serves 
as a good example of how sound sources can be controlled 
remotely. With digital mixing desks, in practice for live 
amplification since the mid -nineties, something similar 
happens. For instance all t he processing and ADDA 1 is done in 
one unit on stage (where usually most inputs and outputs come 
together), the processes are controlled from a networked control 
surface in the auditorium or any other location. As an analogous 
example: some, mostly jazz -rock, keyboardist of the seventies 
would hook their Fender Rhodes or Wurlitzer electronic piano 
up to outboard gear just like a guitarist hooks up his instrument 
to many different effect processors. Recently I witnessed 
keyboardist John Medeski playing his W urlitzer through, what 
sounded like a ring modulator. As he was playing the piano he 
made changes to the ring modulator’s settings using a dedicated 
control surface on top of his piano. The sounds he generated on 
his Wurlitzer (playing its familiar interfa ce of black and white 
keys) functioned as one of the sound sources for the ring 
modulator he controlled with its own interface. I would like to 
combine this notion of different interfacing controlling 
instruments that feed into each other with the aforemen tioned 
separation of interface and sound source. We can compare a 
sound system (again, microphones, mixer, loudspeakers and 
infrastructure) to a controller controlling a remote sound source. 
The sound system is the interface and the source(s) it controls 
are the microphones, signal generators, play back devices or 
other electronic and digital devices that can act as a source to 
such a system. In other words: we are using the interface of the 
instrument loudspeaker to control other instruments that are 
generating sounds controlled by their own interfaces.  
1.1.3 An instrument on an instrument  
With the example of a keyboardist playing his electronic piano 
and controlling his external processing at the same time (using 
one hand on each, limiting his ability to play hi s keyboard but 
extending his expressiveness) I hope to underline the 
interdependency of users of complex systems of sound sources, 
controllers and transducers. If a guitarist in a band cranks up his 
amp, for whatever reason, he is adjusting the balance tha t a 
mixer at a desk is trying to maintain. Or if a mixer decides to 
work at a higher, louder level, again, for whatever reason, he 
may disturb the balance on stage (by creating more or different 
reflections in the room or simply from leakage of the PA syst em 
onto the stage. Rather than suggesting an instrument on an 
instrument as a metaphor for processes in amplified music it is 
perhaps better to think of the whole complex system as one big 
instrument comprising of: a room’s acoustics, an (silent, noisy 
or screaming) audience, a sound system (with loudspeaker, 
mixing desk, microphones and paraphernalia) a monitoring or 
fold back system (with its own loudspeakers, mixing desk and 
paraphernalia) and musical instruments controlled by people, 
remotely or directl y, generating acoustic and or electronic 
sounds. In other words, this instrument is not just played by a 
                                                                    
1 Analog to digital, and digital to analog conversion.  
sound engineer! As a premise, all involved should learn how to 
play that instrument, as an addition to playing their own 
instruments. Or as part of lea rning their instruments, 
amplification being a very regular part of most performance 
practices. 
1.2 Technological interaction is codependent 
The complicated interaction between artist and technologists 
comes to the fore not just in live sound engineering, it i s part of 
many contemporary artistic practices. This complexity is not 
limited to artist/technologist relations: for a flutist in an 
orchestra it is not necessary to learn to play the hobo or clarinet, 
but to be able to play ‘in balance’ with the other pla yers in the 
wind section a working understanding of other instruments is 
essential. Although not as intimate as in an orchestra section, a 
similar codependent and interconnected relation between 
performer and technologist could be beneficial.  
1.2.1 Good bad and bad good practice  
On a different note, music and musicians or compositions and 
composers function very well without engineers or other 
learned technicians. Many fantastic musical ideas and 
developments stem from serendipitous accidents, coincidences 
or sometimes straightforward ‘bad’ practice. See for instance in 
Bert Bongers’ Interactivation [2] describing the effect of a 
loose wire in Michel Waisvisz’ “The Hands” (p.57). Or on a 
similar note Nicolas Collins [5] on the topic feedback (p.41): 
“Feedback became the ur -sound of chance: it erupted whenever 
composers hooked up sound systems without the benefit of 
technicians;”. Working as a sound engineer for composers and 
performers of contemporary music I have learned to (when 
needed) let go of my own practic al technology based 
experiences and (schooled) knowledge but to let 
counterintuitive notions of how to use sound technology 
prevail. Some of  the performers I have worked with showed 
highly personal concepts  of how to apply sound amplification 
technology. Concepts that were, or appeared to be, based much 
more on experience rather than laws of physics, but 
nevertheless defining for a performance’ result or presentation. 
This led me to believe that there should be room for ‘authentic 
performances’ of electroni c and electro -acoustic music 2. 
Considerations for these authentic performances should deal 
with questions like type and placement of loudspeakers (not so 
much period loudspeakers although I would really appreciate 
such an effort), mixing desks and playback  technology. I see 
this argument as an extension of the question whether, in the 
digitizing process of analog tapes these should be sterilized to 
comply with modern standards of noise free and crackle free 
‘CD-quality’ digital sound. See also Simon Emmerson [8] about 
authenticity (p. 170). 
1.2.2 Acoustic and electronic sources  
With the exclusion of some electric and electro -mechanical 
instruments (see for instance Bert Bongers [2] p. 50) all 
electronic music relies solemnly on loudspeakers to be heard. 
This signi fies an important difference with electronic 
amplification of acoustic instruments, as the acoustic source 
always coexists with the sound from one or more loudspeaker 
introducing new sources for sounds relating to the same (inter -) 
actions. The flexibility  of the loudspeaker as the actual sound 
                                                                    
2 In November 2009 I presented a paper with that title at a post 
graduate student conference at City University London: 
“Outside the Box: practice, participation and method in live 
electronic music”. 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
14
source for electronic music allowed for the extension of 
spatiality as a musical parameter. With flexibility I mean the 
possibility of placing that loudspeaker in a different spot than a 
performer, creating different  ways of relating to the localization 
of a sound. Or, in other words, freedom in the diffusion of 
sounds in a space. Works like Pierre Schaeffer’s Pupitre 
d’Espace (1951) or Karlheinz Stockhausens Gesang der 
Jünglinge (1956) are amongst the first to explor e this flexibility. 
However, this flexible separation of source and transducer, of 
the visible interaction of performer and instrument raises some 
new questions. In the case of a church organ, the pipes produce 
the actual sound whereas the very location of  the performer is 
not an important musical parameter. He or she is often hidden 
from sight! In electronic music, live or support based  this 
separation is structural and the position of the actual transducers 
becomes a musical parameter. Obviously this is w hat makes a 
big difference between the amplification of acoustic instruments 
whose mechanical source co -exists with the electro -dynamic 
source (a loudspeaker), depending of the overall level of 
amplification. 
1.2.3 Mixed Music 
In Mixed Music (performances of aco ustic instruments with 
support based or live electronic music) these two different 
source relations come together. For instance in Bruno 
Maderna’s Musica su Due Dimensioni  (1952) or Luigi Nono’s 
La Fabbrica Illuminata  (1964). Or a work like Stockhausens 
Kontakte (1960) where the four -track pre recorded 
“Elektronische Musik” is projected onto a four channel 
loudspeaker system to ‘make contact’ with the acoustic 
instrumental music. It is common practice (and often prescribed 
in a score) to electronically ampl ify the acoustic instruments to 
allow for better blending with the electronic music. For a 
thorough discussion of mixed music and the problems around 
mixing acoustic and electronic sources, see (again) Emmerson 
[8] p.104 -106. In section 3.1 I will describe  the importance of 
the relations between different sources in greater detail.  
1.2.4 Loudspeaker orchestras  
If the loudspeaker is a musical instrument, multiple 
loudspeakers can form an ensemble or an orchestra. Mentioned 
before was Luigi Russolo’s Intonarumori: the “noise intoners” 
were part of a “Futurist Orchestra” that performed as an 
ensemble of noise machines, occasionally combined with 
acoustic musical instruments. A more literal example is the 
“Orchestre de Haut -Parleurs” that originated in France. For 
instance the Gmebaphone invented by Christian Clozier and the 
Acousmonium (1974) created by François Bayle, both at the 
Groupe de Recherche Musicale (see Chabade [4] p.68). 
Different approaches towards the source of such an orchestra 
are possible. For each of  the channels in the orchestra a 
dedicated tape track could be used. Or sounds from a much 
smaller number of tracks (or even from only one track) could be 
divided over the loudspeakers by frequency, spatially, or by 
other sonic parameters. 
 
2. PERCEIVING AMPLIFICATION 
Not unlike singing in the bathroom or playing a pipe organ in a 
church, amplifying acoustic instruments make the relation of 
that instrument to a room’s acoustic more explicit. Whereas 
singing in a bathroom is enjoyable because of it’s acoustic 
properties (and it makes you think you can sing), church organs 
sound good as they were specifically developed for the acoustic 
conditions of churches. Very few auditoriums are specifically 
designed for performing amplified music 3, causing problems in 
the relation the different sound sources to a venue’s acoustic. At 
a concert where amplification is used, that amplification should 
not be in the way of aesthetic enquiry or musical enjoyment. 
When it is, due to level, technical faults, or feedback the 
question that arises is: are these technical faults intentional or 
are things just going wrong? (See 1.2.1)  
2.1.1 The amplified singer’s dilemma 
A problem that occurs very often in many different practices of 
contemporary music is that the musicians are on stage during 
the concert (and usually during a sound check or rehearsal). 
Normally they are not in a position to hear what the audience is 
hearing. For instrumentalists this problem can be overcome (to 
some extend) by having someone else play for you and listen in 
the hall. But this only gives a suggestion of what you sound like 
during a concert, as conditions with or without audience can 
vary enormously, not in the least if you consider the difference 
in atmosphere feeding back to the performers. For singers this 
problem is a bit harder to overcome, as you can’t ask someone 
else to ‘play’ your instrument for a while. Walking out in the 
room while singing (with a wireless mike for instance) is hardly 
representative, as singers still perceive a lot of their voice 
through their own body. In older music traditions such as 
orchestras there is usually an assistant conductor or someone of 
the ‘artistic’ staff in the audience (and in the hall during 
rehearsals) to give feedback to a conductor. With most sorts of 
amplified music these strategies are rare and musicians have to 
rely on the person at the mixing desk. Often bands and groups 
perform with their own trusted sound person. Friends or 
relatives in the audience, reporting back on the sound person’s 
“mix”, occasionally monito r such a person’s work. Remarks 
from related people are then fed back to the sound person: (for 
instance) “my brother told me he thought my voice sounded 
thin/fat/big/small/bad”. At occasions the faith performers have 
in their dedicated engineer exceeds th e engineer’s capability, 
leading to interesting situations (and often less interesting 
results). Some bands choose to travel without a dedicated 
engineer (or simply can’t afford one). They are relying on the 
abilities of a venue’s own sound crew. A sound check and often 
a (annotated) set -list informs the engineer at the mixing desk 
about how a band ‘wants to sound’ for each song. 
2.2 Functions 
Emmerson is one of the first authors to create a systematic 
overview of when and how we amplify music. In Living 
Electronic Music  [8] he lists a number of common (musical) 
applications of amplification technology, identifying six 
functions of amplification that he considers vital to discussing 
“live music requiring electronic technology for its 
presentation”. As I have men tioned before, systematic research 
into this area is very scarce making Emmerson’s overview very 
important and an excellent basis for further research. 
                                                                    
3 The Radio City Music Hall  in New York (1936) comes to 
mind and the Heineken Music Hall  in Amsterdam (2001), the 
latter wa s constructed to have a minimal acoustic of its own, 
leaving it for the user to create acoustics (or the suggestion of 
acoustics) electronically. 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
15
Table 1 Six functions of amplification (Emmerson)  
1 Balance Correcting acoustic imbalances  
2 Blend Integrating timbres 
3 Projection/ 
Spatialisation 
Zooming in perceptually/intentional (ac 
ousmatic) dislocation. 
4 Perspective Suggesting different acoustics 
(dislocation of space and time)  
5 Coloration Intentional distortion 
6 Resonance Use of feedback 
For the argument presented in this paper, I would like to add a 
very common function that for now I would like to describe as 
reinforcement. At a pop or rock concert (or even an opera or 
André Rieu’s Wiener Waltz circus) in a stadium, the scale of 
amplification goes beyond the subtle functions in Emmerson’s 
list. Beginning sporadically in the 1950’s with certain jazz 
concerts4 getting out of hand, but certainly with the Beatle -
mania in the early sixties, loud amplification became 
necessarily to allow for the  music to be heard over the 
singing/clapping/screaming audience 5. In the Oxford concise 
dictionary we find for amplify: increase the amplitude of (an 
electrical signal or other oscillation). For reinforce we find: 
extra personnel sent to increase the stren gth of an army or 
similar force. For reinforcement we find: the action or process 
of reinforcing or strengthening. Increasing the amplitude of an 
electrical system is what we usually do when we make music 
louder; this is no different if we call it reinforc ement. But I like 
the notion of strengthening and supporting in the definition for 
reinforcement. Strengthening the sound to make it available for 
many. Strengthening also suggests an addition, causing a 
change to the original. Festivals with unprecedented  numbers of 
visitors required larger sound systems. A recent book by David 
Collison [6] is one of the most concise overviews of the use of 
sound in the theatre (mostly in England), including 
amplification. He describes how in England in 1967 the first 
sound system in excess of 1000 watts was installed for the 
annual jazz festival at Windsor. All of a sudden the balance 
problem of pop bands was reversed: the acoustic instruments 
and vocals had microphones and were amplified but now the 
guitars and drums had problems coming through using just their 
own dedicated amplifiers.  
2.2.1 Functions of amplification depending on level  
A more overall idea I would like to add to Emmerson’s 
overview is that the working of these functions is dependant of 
the level of amplificati on. Acoustic imbalances cannot be 
corrected with very loud amplification nor is reinforcement in a 
stadium at a very low level very useful. In projection and 
spatialization, amplification is not so much used to make things 
louder, but to bring details to t he surface (zooming in) or 
change the perceived location of a sounding event. The same 
goes for perspective, although a certain level of loudness must 
be reached before effects (such as reverb) can be heard in 
                                                                    
4 For instance the concerts of Lionel Hampton in Europe in the 
fifties that would occasionally have crowds goi ng wild and end 
in rows. See for instance Hamptons biography [5] p.108.  
5 Or if we want to stay in the realm of electronic music: a 
performance of Schaeffer’s and Henry’s Orphée 53  at 
Donaueschingen with an offended audience making more noise 
than the loud speakers. As recalled by Henry quoted in Chabade 
[2]. 
balance with the acoustic sound of an instrume nt (see Mulder 
[12]).  
2.2.2 Role of sound engineer depending on level  
Apart from (to some extent) the functions, the amplification 
level also changes the role of the sound engineer, sound diffuser 
or projectionist at the mixing desk. When electronically 
adjusting acoustic balances or blending acoustic and electronic 
timbres the person at the desk functions as a conductor in a 
conductor’s role of balancing an ensemble or orchestra. Same 
as for a conductor, the control over a balance is limited to 
physical acousti c possibilities of the involved instruments 
(unless players are asked to not play or leave the stage). This is 
different in situations where the acoustic source plays no 
audible role: when the amplification becomes so loud that it 
drowns out the sources be ing amplified the mixer’s job 
becomes much more like that of a producer in a recording 
studio. He is solemnly responsible for what the audience hears 
of the intentions of the musicians on stage. Much like in a 
recording studio there are many options (filte ring, compression, 
reverbs, delays or even muting parts) to adjust a performance, 
with the obvious difference that a concert this will have to be 
done in real time. The use of click -track (sometimes combined 
with whole pre -recorded orchestra sections) sugg est that these 
big, what Dennis Smalley [15] (after Auslander [1]) calls 
“mediatized” concerts become more like an instantaneous play 
back of a recording rather than a live performance 6. 
3. Sound sources for amplification 
3.1 Acoustic sound sources 
Microphones pi ck up acoustic sound sources such as 
instruments, the human voice or a guitar amplifier and transfer 
the acoustic energy into an electronic sound source, existing 
independently of the “original” acoustic source. If we zoom into 
the microphone as a sound so urce we find that its membrane is 
moving with the sound waves it comes across at its particular 
position. We can consider the membrane in a microphone as a 
resonating body that is set in motion by the sound waves it 
comes across. The membrane traces the ac oustic waves, not 
unlike a stylus following the grooves on a record.  
3.1.1 The microphone as musical instrument  
A microphone, when handled by a skilled performer, can be an 
excellent musical instrument with several very direct 
sensitivities. Probably the most fa mous example of such 
application would be Stockhausen’s Mikrophonie I  (1964) (see 
Emmerson [8] p. 128 -9 or Chabade [4] p.83 -4). A less famous 
but equally expressive example is Dick Raaimakers’ Twelve 
ways to silence a microphone  (1992). Amplified microphon es 
are destroyed in twelve different ways, while “reporting their 
own doom through loudspeakers” [3]. Some of these destructive 
ways include the use of acid or straightforward ‘drowning’ in 
water. 
3.1.2 Electronic sound sources  
Examples of electronic sound sourc es are numerous. Bongers 
[2] (p. 61) describes the work of Wart Wamsteker a Sonology 
student at the Royal Conservatory in The Hague in the nineties. 
He used a glove with sensors to control a set of filters, using 
feedback to turn the filters into signal ge nerators. Before this 
Wamsteker used to play “no -input mixer” by connecting 
outputs to input, feeding channels back into itself and thus 
                                                                    
6 Latency in the signal chain, introduced by using digital 
equipment, strengthens this idea, in my opinion.  
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
16
generating sound. Electronic sound sources come in an endless 
variety, especially if we include playback devices such a s 
record players, tape decks, CD -players or hard disks. These last 
two may use digital storage, but after DA conversion their 
output is as electronic as any other electronic source.  
3.1.3 Spatial displacement  
One very important difference between amplifying acou stic 
instruments, and amplifying or playing back electronic sources 
is the change in perception of the acoustic source. Not only will 
it sound like appear to be coming from a different direction 7, 
the “detached” electronic sound will have a different relat ion to 
the acoustics of the same space. This detachment of the sound, 
not at all unlike Pierre Schaeffer’s [13] “Acousmatic” 
condition8 or R. Murray Schafer’s [13] “Schizophonia”, appears 
as a function of  the level of amplification. In the case of 
reinforcement this displacement is complete as the acoustic 
source drowns in the sound of the loudspeakers. At lower levels 
the displacement can become an interesting parameter, as both 
acoustic and electronic sources are audible.  
3.1.4 Hearing and seeing  
The connection between seeing and hearing in the perception of 
visible sound sources is very well established. For instance, in 
room acoustics optimal sightlines help understanding of speech. 
Dennis Smalley [15] takes this idea a bit further when he writes 
about seeing proprioception at work. Not only do we see and 
hear a musician at work, according to Smalley we recognize 
how the musicians utilizes proprioception in playing an 
instrument. To support his claim and to show how powerful 
‘just the movements’ possibly can be , we can think of the 
popularity of “Air Guitar” contests, or the successful Guitar 
Hero computer  games. The detachment of visible and audible 
stimuli from the one source when amplifying sound makes this 
connection between aural and visual perception more apparent, 
and as such a musical parameter.   
3.2 The loudspeaker as a multiplayer 
musical instrument 
By zooming in from the general complexities of amplified 
sound into the minute details of different sources I hope to 
demonstrate the complex relation between a coustic sources and 
microphones, electronic sources, and loudspeakers sounding in 
an acoustic space. The microphone becomes an expressive tool 
with an interface that stretches beyond its placement in the 
proximity of an instrument. A musician interfaces wi th it 
(although some choose not to), while a person at a mixing desk 
interacts with the source(s), relating it to a space through 
projections on a sound system. In a less temporary sense 
interactions with audiences (complaining or complimenting 
during or a fter a performance), production and artistic staff at 
venues, friends or relatives of the performer, generate 
complicated feedback loops that extend beyond the stage and 
the timeframe of a performance.  
                                                                    
7 Delaying a sound to invoke the famous Haas or precedence 
effect, as is common practice, only  works within relatively 
small margins. 
8 For a discussion of a possible relation between the 
amplification of acoustic sound and the Acousmatic condition 
see Jos Mulder: Sound amplification technology and the 
Acousmatic experience [12]. 
4. CONCLUSION 
In the above paragraphs I have tried to pro vide insight in a 
systematic approach to amplifying sound in the context of 
musical performance. To clarify the complicated interrelation 
between stakeholders, technology I propose using the idea of a 
musical instrument, subdivided into a sound source and an 
interface. When broken down as such the sensitivities of 
technological and artistic interaction can be addressed. 
Although not a novel idea, I have tried to operationalize this 
metaphor to stress the interdependency of performers, 
technologists and othe r stakeholders. Again this 
interdependency is nothing new in musical performance, but by 
focusing on the differences between electronic and acoustic 
sources when amplifying music I have suggested that there is a 
break with the history of musical performanc e since the 
introduction of amplification. Awareness of this changing 
relationship, although going on since the thirties, is important 
and calls for new strategies in musical performance and 
collaboration between stakeholders, on different levels. From 
policy makers and architects, music critics, programmers and 
production managers at venue’s to performers and 
technologists. 
5. RESEARCH 
My own research will continue in the direction of trans -modal 
perception, trying to establish how the source displacement, 
when amplifying an acoustic sound source, influences our aural 
and visual perception of that sound source. Other research that 
is needed in this area is a broad history of sound amplification 
both from a technological and a musical perspective.  
6. REFERENCES 
[1] Auslander, P. Liveness : performance in a mediatized 
culture. 2nd edition, Routledge, London ; New York 2008.  
[2] Bongers, Bert. Interactivation. Ph.D thesis Vrije 
Universiteit Amsterdam 2006.  
[3] Brouwer, Joke and Mulder, Arjen (eds.). Dick 
Raaymakers, A Monograph .V2, Rotterdam, 2008. 
[4] Chabade, Joel. Electric Sound. Prentice Hall, New Jersey 
1997. P32. 
[5] Collins, Nicholas. Live electronic music. In Nick Collins 
and Julio d’Escriván (eds) Electronic Music. Cambridge 
University Press 2007. 
[6] Collison, David. The Sound of Theatre. Professional 
Lighting and Sound Association, London 2008.  
[7] DiScipio, Agostino. Sound is the Interface: From 
interactive to ecosystemic signal processing. In Organized 
Sound 8/3/ (2003) pp 269-77. 
[8] Emmerson, Simon. Living Electronic Music . Ashgate 
Publishing Limited, Aldershot, 2007.  
[9] Green, Owen. Pondering Value in the Performance 
Ecosystem. In eContact 10 (4), 2008. 
[10] Hampton, Lionel en Haskins, James. Hamp. New York: 
Warner Books, 1989. 
[11] McCarthy, Bob. Sound Systems: Design and Optimization . 
Focal Press, Burlington, 2007. 
[12] Mulder, Johannes. Sound Amplification Technology and 
the Acousmatic Experience. In: Proceedings of the second 
international conference of students of systematic 
musicology 2009. IPEM, Ghent 2009. 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
17
[13] Schaeffer, Pierre. Treaté des Objets Musicaux. Éditions du 
Seuil, Paris 1966. 
[14] Schafer, Raymond Murray. The Tuning of the World . 
Random House, New York, 1977.  
[15] Smalley, Dennis. Space-form and the acousmatic image. in 
Organized Sound vol. 12, no.1, 2007, pp. 35-58. 
[16] Waters, Simon. Performance ecosys tems: Ecological 
approaches to musical interaction. In: Proceedings of 
Electroacoustic Music Studies Network Conference 2007 , 
2007. 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
18