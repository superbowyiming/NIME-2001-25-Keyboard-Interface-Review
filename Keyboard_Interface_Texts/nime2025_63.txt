Designing Sensory NIME for Autism   Aditya Arora  Swinburne University  John Street  Hawthorn, Australia adityaarora070@hotmail.com  
Erica Tandori Monash University 19 Innovation Walk  Clayton, Australia erica.tandori@monash.edu 
James Marshall Swinburne University  John Street  Hawthorn, Australia jgmarshall@swin.edu.au  
Stuart Favilla Swinburne University  John Street  Hawthorn, Australia sfavilla@swin.edu.au
  ABSTRACT This paper explores how sensory NIME design principles may inform the design of musical interfaces tailored for children with Autism Spectrum Disorder (ASD), focusing on their sensory processing challenges. Given the prevalence of sensory over-responsivity (SOR) and under-responsivity (SUR) in ASD, traditional sensory interventions often fail to accommodate the highly individualized and fluctuating sensory needs of autistic individuals. The authors highlight the potential for multisensory NIME to address the diverse range of sensory needs, promoting emotional regulation and sensory balance through new creative musical opportunities and activities. This paper presents research in the form of a narrative review and comparative case study of recent NIME and sensory intervention research, exploring emerging approaches, rhythm-based interventions, generative algorithms, play-centered designs and other possibilities for enhancing sensory engagement and emotional regulation. Drawing on insights from 30 recent NIME papers, this research explores the boundaries of current approaches and seeks to establish an understanding of multisensory NIME for ASD. The research underscores the profound variability in sensory profiles for ASD, necessitating a shift from clinician-directed interventions to creative, inclusive, multisensory solutions. Finally, a set of sensory NIME design principles are offered, emphasizing the importance of sensory perception, sensory equilibrium and the promotion of emotional regulation for ASD.    Author Keywords NIME, proceedings, MS Word, template  1. INTRODUCTION  In recent years, the prevalence of Autism Spectrum Disorder (ASD) has significantly increased due to improved awareness, advancements in diagnostic criteria prompting a more nuanced understanding of its diverse manifestations. This evolution of understanding highlights the importance of addressing sensory processing challenges experienced by many individuals with ASD. These challenges profoundly impact emotional regulation, social interaction, and daily functioning, (themes reflected in the concept of Entangled NIME) thereby making developing practical sensory modulation tools essential, including the potential benefits of autism specific musical interfaces for younger users.   Autism Spectrum Disorder (ASD) any one of a group of disorders with an onset typically occurring during the preschool years and characterized by varying but often marked and persistent deficits in social communication and social 
interaction, including difficulties with social-emotional reciprocity, nonverbal communication behaviors, and social relationships, along with restricted and repetitive patterns of interests, behaviors, and/or activities [1]. Among these, sensory modulation difficulties are particularly pervasive, with up to 95% of individuals with ASD experiencing either sensory over-responsivity (SOR) or sensory under-responsivity (SUR) [2].   SOR manifests as heightened sensitivity to auditory, visual, or tactile stimuli, often leading to overstimulation, anxiety, and avoidance behaviors [3]. In contrast, SUR is characterized by a diminished awareness of sensory inputs, resulting in disengagement or repetitive sensory-seeking behaviors [4].   These distinct sensory profiles highlight the need for interventions that are both adaptable and responsive to real-time sensory needs. Traditional sensory interventions, such as Sensory Integration Therapy (SIT), rely on clinician-directed exposure to structured sensory inputs, limiting their effectiveness in accommodating the diversity of sensory experiences in ASD [5].  By integrating user-centered, adaptive technologies that cater to both SOR and SUR, musical interfaces can provide real-time flexibility, enhancing sensory modulation and emotional regulation.    Given these variations in ASD there are significant challenges in addressing sensory modalities, including auditory, tactile, proprioceptive, and visual systems, emotional regulation, social functioning, and daily life activities. For example, individuals with hyper-responsivity may perceive ordinary sounds or textures as distressing, while those with hypo-responsivity may require heightened stimuli to engage with their surroundings. The research underscores the profound variability in sensory profiles among individuals with ASD, necessitating tailored intervention strategies to address these diverse sensory needs.   2. SENSORY INTERVENTIONS  Sensory processing challenges in Autism Spectrum Disorder (ASD) are highly complex and require targeted interventions. Elwin et al. [6] highlight the coexistence of hypersensitivity and hyposensitivity in individuals with high-functioning autism, with external senses such as vision and touch more prone to hypersensitivity, while internal modalities like proprioception and pain perception are often hyposensitive. This variability disrupts daily functioning and underscores the need for personalized interventions that address these distinct sensory needs. Similarly, Tomchek and Dunn [7] found that 95% of children with ASD experience sensory dysfunctions, including sensory-seeking behaviors, auditory filtering difficulties, and tactile sensitivities, all of which hinder attention and social engagement.   The biological basis of these sensory challenges is linked to genetic and neural factors. Schaffler et al. [8] identify mutations in ASD-related genes such as Shank3, Mecp2, and Fmr1 as contributors to sensory irregularities, particularly in tactile processing, suggesting that the peripheral nervous system plays a significant role in sensory abnormalities and may present therapeutic intervention targets. Kern 

et al. [9] further demonstrate global sensory dysfunction across auditory, tactile, visual, and multisensory domains, with a strong correlation between sensory challenges and ASD severity in childhood, though this association diminishes with age, potentially due to sensory adaptation or neural maturation.   Sensory modulation difficulties also profoundly impact emotional regulation and social interactions. Sensory over-responsivity is closely linked to anxiety and avoidance behaviors, with Ben-Sasson et al. [10] reporting that toddlers exhibiting both over- and under-responsivity are more likely to experience heightened anxiety, withdrawal, and depressive symptoms. These findings indicate that sensory challenges extend beyond physical responses, affecting emotional well-being and social participation. Additionally, Baum et al. [11] identify reduced neural connectivity between brain regions as a key factor underlying sensory integration difficulties in ASD, further impairing social communication and cognitive function. These findings emphasize the urgent need for adaptive sensory interventions that accommodate the diverse and evolving sensory profiles of individuals with ASD.    2.1 Limitations of Conventional Musical Interfaces for ASD   Traditional sensory interventions, such as Sensory Integration Therapy (SIT) [12] and Multisensory Integration Theory (MIT) [13], have been widely used to address sensory processing challenges in individuals with Autism Spectrum Disorder (ASD). However, these approaches often fail to accommodate the highly individualized and fluctuating nature of sensory needs across the spectrum. SIT, developed by A. Jean Ayres [12], relies on structured sensory activities to enhance sensory processing and integration. While it has shown some success in improving adaptive behaviors and self-regulation, its rigid, clinician-directed framework limits its effectiveness in addressing the dynamic and diverse sensory profiles of individuals with ASD. The SenITA randomized controlled trial [14] found no statistically significant improvements in primary outcomes such as behavior and quality of life when comparing SIT to usual care. Although caregivers reported increased satisfaction and perceived progress in specific goals, these findings highlight that SIT’s benefits are context-dependent rather than universally effective. Additionally, SIT remains resource-intensive, making widespread accessibility and scalability challenging in broader public health settings.  Similarly, Multisensory Integration Theory (MIT) [13] emphasizes engaging multiple sensory modalities simultaneously to enhance sensory processing and emotional regulation. Multi-Sensory Environments (MSEs) have been developed to offer children control over sensory variables such as light, sound, and tactile inputs, promoting engagement and reducing defensive sensory behaviors. While this autonomy aligns with the preferences of many children with ASD, MSEs are not without limitations. Their reliance on specialized equipment and complex setups restricts their accessibility outside of clinical environments, and their effectiveness varies due to a lack of standardization across studies.   De Domenico et al. [15] reported that while MSEs facilitated self-regulation and sensory engagement in specific domains, they did not consistently outperform traditional therapies in achieving broader developmental outcomes. Moreover, conventional interventions, including SIT and MSEs, lack the flexibility to adjust sensory inputs in real time, a crucial limitation given the fluctuating and individualized sensory responses of those with ASD.  These limitations underscore the need for sensory interventions that are adaptive, user-centered, and capable of 
responding dynamically to the moment-to-moment sensory fluctuations experienced by individuals with ASD. Conventional approaches, with their static and pre-determined frameworks, often fail to support the full range of sensory processing differences. The design of musical interfaces for ASD must prioritize real-time adaptability, personalization, and user autonomy, moving beyond the constraints of traditional methodologies to create more inclusive and effective sensory engagement tools. 2.2 Emerging Adaptive Alternatives  Given the highly individualized and fluctuating sensory needs of individuals with Autism Spectrum Disorder (ASD), conventional sensory interventions often fall short in providing effective, sustainable support. Emerging adaptive approaches seek to address these limitations by incorporating flexibility, real-time adaptability, and user-centered design. In the context of musical interfaces, these innovations present a compelling opportunity to enhance sensory engagement, self-regulation, and emotional well-being for young users with ASD.   Rhythm-based interventions have demonstrated significant promise in supporting sensory processing challenges in ASD. The structured yet flexible nature of rhythmic auditory patterns provides a predictable sensory input that enhances focus, reduces anxiety, and improves motor coordination [16]. Unlike conventional interventions, which often impose fixed sensory stimuli, rhythm-based therapies offer an adaptable framework that can be tailored to individual sensory thresholds. This aligns with the need for dynamic, user-responsive systems capable of addressing hypersensitivity and hyposensitivity in real time.   Similarly, algorithmic modulation presents a breakthrough in sensory-based therapies by allowing real-time adjustments to auditory, visual, and tactile stimuli. De Domenico et al. [15] emphasize how algorithmic systems can actively modify sensory inputs in response to user feedback, promoting sensory balance and reducing overstimulation. This adaptability is particularly valuable in ASD, where sensory experiences fluctuate and rigid, pre-defined interventions may fail to accommodate moment-to-moment needs. By fostering self-regulation and sensory autonomy, these systems empower individuals with ASD to engage with their environments in ways that traditional interventions cannot.   Play-centered interventions further highlight the importance of flexible, engaging, and child-led approaches to sensory modulation. O’Keeffe and McNally [17] found that interactive, play-driven sensory experiences improve sensory tolerance, emotional regulation, and social communication in children with ASD. These interventions leverage intrinsic motivation and creativity, ensuring that sensory engagement occurs in a low-pressure, enjoyable environment. By prioritizing personalization and adaptability, play-based approaches move beyond the limitations of static interventions, offering a more inclusive and responsive method for addressing sensory challenges.   Musical interfaces for children with ASD exist at the intersection of sensory intervention and play-based engagement. While not designed as rigid therapeutic tools, they may provide structured, flexible environments and accommodate diverse sensory needs, supporting self-regulation, emotional expression, and social interaction. Unlike traditional interventions, which often impose external frameworks, these interfaces empower children to explore sound, rhythm, and multisensory experiences on their own terms. By integrating principles from adaptive sensory interventions—such as real-time modulation, rhythmic structure, and interactive play—these systems create an inclusive and enjoyable medium that naturally fosters developmental benefits while prioritizing autonomy and creativity.  
 These contemporary developments underscore the critical need for adaptive, interactive, and user-driven design in musical interfaces for children with ASD. The limitations of traditional interventions highlight the urgency of developing systems that respond to the diverse and evolving sensory needs of autistic individuals. Building on these principles, the following NIME papers provide a foundation for proposing a new set of design guidelines tailored to autism-specific musical interfaces. By integrating rhythm-based structures, real-time modulation, and play-centered engagement, these interfaces can better support sensory regulation, emotional expression, and creative exploration in young users with ASD. 3. METHOD  This study employs an integrative review methodology to synthesize existing research on New Interfaces for Musical Expression (NIME) designed for individuals with Autism Spectrum Disorder (ASD). The scope of this review is limited to peer-reviewed NIME conference papers, reflecting the most recent and relevant innovations in musical interface design for autism.    The corpus of literature analyzed consists of 30 NIME papers selected through a structured synthesis matrix. Inclusion criteria required that studies explicitly address sensory or cognitive engagement within the context of musical interfaces and autism-related interventions. literature that focused solely on technical innovations and considering user-centered interaction or ASD-specific adaptations. The primary objective of this review is to critically assess how existing musical interfaces accommodate sensory diversity, with particular attention to real-time adaptability, rhythmic structuring, and multimodal interactions.   A synthesis matrix was prepared to facilitate a comparative analysis, highlighting methodological strengths, limitations, and research gaps. In addition, thematic coding was employed to identify dominant intervention strategies, including rhythm-based engagement, adaptive sensory modulation, and play-centered interactivity. Given the complexity of sensory processing in ASD, the analysis places particular emphasis on how musical interfaces address hypersensitivity, hyposensitivity, and fluctuating sensory thresholds.  4. RESULTS The review of 30 NIME studies is presented in table 1 overleaf. The synthesis matrix reveals three primary themes in the design of musical interfaces for autism: (1) Rhythm-based interventions for sensory modulation, (2) Real-time adaptive systems for sensory engagement, and (3) Play-centered multisensory interfaces. These themes encapsulate the predominant approaches adopted within the NIME community to address sensory and cognitive challenges in ASD.  Rhythm-based interventions emerged as a recurring strategy across several studies, reflecting the intrinsic relationship between rhythmic predictability and sensory self-regulation [16]. Multiple studies emphasize the role of rhythmic entrainment in stabilizing attention, reducing anxiety, and promoting motor coordination. Unlike traditional sensory integration therapies, which rely on structured sensory exposure, rhythm-based interfaces offer flexible, user-driven interaction, allowing individuals with ASD to modulate their engagement based on personal sensory thresholds.  The second major theme involves real-time adaptive sensory interfaces, which adjust auditory, visual, and haptic stimuli in response to user interaction. These interfaces leverage algorithmic modulation to dynamically alter sensory inputs, ensuring that the interface remains responsive to moment-to-moment changes in sensory tolerance. This approach aligns with neurodiversity-informed principles [15], which emphasize 
customization and self-directed sensory engagement as opposed to rigid, externally imposed sensory protocols.  A third critical theme is the integration of play-centered multisensory engagement within musical interfaces. Studies indicate that play-based interaction fosters intrinsic motivation, leading to greater sensory tolerance and enhanced social communication [17]. Many NIME interfaces designed for ASD incorporate game-like mechanics, interactive soundscapes, and exploratory sound synthesis, enabling children to engage with music and sensory stimuli in a non-threatening, enjoyable environment.  While these approaches demonstrate significant promise, the review also highlights key gaps and limitations. Many studies focus primarily on auditory and visual modalities, with less emphasis on proprioceptive and vestibular engagement, which are critical for sensory integration in ASD [7]. Moreover, the majority of NIME interventions rely on short-term experimental evaluations, with limited longitudinal data on sustained sensory and behavioral impact. RESULTS  The review of 30 NIME studies reveals three primary themes in the design of musical interfaces for autism: (1) Rhythm-based interventions for sensory modulation, (2) Real-time adaptive systems for sensory engagement, and (3) Play-centered multisensory interfaces. These themes encapsulate the predominant approaches adopted within the NIME community to address sensory and cognitive challenges in ASD.  Rhythm-based interventions emerged as a recurring strategy across several studies, reflecting the intrinsic relationship between rhythmic predictability and sensory self-regulation [16] Multiple studies emphasize the role of rhythmic entrainment in stabilizing attention, reducing anxiety, and promoting motor coordination. Unlike traditional sensory integration therapies, which rely on structured sensory exposure, rhythm-based interfaces offer flexible, user-driven interaction, allowing individuals with ASD to modulate their engagement based on personal sensory thresholds.  The second major theme involves real-time adaptive sensory interfaces, which adjust auditory, visual, and haptic stimuli in response to user interaction. These interfaces leverage algorithmic modulation to dynamically alter sensory inputs, ensuring that the interface remains responsive to moment-to-moment changes in sensory tolerance [15]. This approach aligns with neurodiversity-informed principles, which emphasize customization and self-directed sensory engagement as opposed to rigid, externally imposed sensory protocols.  A third critical theme is the integration of play-centered multisensory engagement within musical interfaces. Studies indicate that play-based interaction fosters intrinsic motivation, leading to greater sensory tolerance and enhanced social communication [17]. Many NIME interfaces designed for ASD incorporate game-like mechanics, interactive soundscapes, and exploratory sound synthesis, enabling children to engage with music and sensory stimuli in a non-threatening, enjoyable environment.  While these approaches demonstrate significant promise, the review also highlights key gaps and limitations. Many studies focus primarily on auditory and visual modalities, with less emphasis on proprioceptive and vestibular engagement, which are critical for sensory integration in ASD [7]. Moreover, the majority of the selected NIME rely on short-term experimental evaluations, with limited longitudinal data on sustained sensory and behavioral impact.      
Table 1. Synthesis Matrix of selected NIME papersTitle Date of Article Authors Study Type Summary Methods Results Gaps Perception vs Sensation Discussion
VESBALL: A Ball-Shaped 
Instrument for Music Therapy 2023 Ajit Nath
Case Study, Therapeutic Design – 
Explores the impact of tactile, ball-
based music therapy on users with 
ASD.
Describes the VESBALL, a tactile, 
spherical digital musical instrument 
designed for use in music therapy 
sessions.
Focused on participatory design 
with therapists and patients, 
emphasizing intuitive interaction 
and adaptability.
Enhanced engagement and 
participation in therapeutic 
settings. Demonstrated adaptability 
across different patient needs.
Limited evaluation beyond music 
therapy. No long-term impact 
studies.
Sensation: Tactile interaction with 
a soft ball, motion-based sound 
generation.
Perception: Recognition of cause-
effect relationships between 
movement and sound, supporting 
social and musical engagement.
Suggests tactile DMIs can enhance 
therapy, supporting multisensory 
engagement and emotional 
expression.
Music Making to Scaffold 
Social Playful Activities and 
Self-Regulation
2019 Antonella Nonnis, Nick Bryan-
Kinns Case Study, Experimental
Investigates the impact of Olly, a 
musical textile tangible user 
interface (TUI), on social 
interaction and sensory regulation 
in autistic children. A 5-week study 
in a SEN school demonstrated its 
effectiveness in fostering 
collaborative play and emotional 
engagement.
Observational study with five 
autistic children in a SEN school 
over 5 weeks. Sessions lasted 30 
minutes using Olly, an inflatable 
gym ball with elastic ribbons that 
triggered musical responses. 
Teacher observations and 
behavioral analysis (eye contact, 
engagement, shared play) were 
recorded.
Children showed increased social 
engagement (smiling, singing, 
shared play). Teachers noted Olly’s 
accessibility and effectiveness in 
encouraging interaction and 
sensory relief. The study supports 
the role of TUIs in enhancing social 
and emotional 
Small sample size (n=5), short study 
duration (5 weeks). Future work 
should include diverse participants, 
long-term impact analysis, and 
comparisons with other interactive 
musical tools.
Sensation: Tactile interaction with 
elastic ribbons and auditory 
feedback. Perception: Recognizing 
musical cues, linking physical 
interaction to social and emotional 
engagement.
Highlights the role of multisensory 
engagement in fostering social 
interaction and self-regulation in 
autistic children. Music-based 
interfaces like Olly enhance joint 
attention, collaborative play, and 
emotional expression.
SnoeSky and SonicDive: 
Accessible Digital Musical 
Instruments for SEN Schools
2020 Andreas Förster, Christina 
Komesker, Norbert Schnell
Educational Research, Case Study 
– Evaluates the impact of 
accessible musical instruments in 
special education.
Presents two ADMIs—SnoeSky 
and SonicDive—designed for 
students with intellectual 
disabilities in a SEN school, 
aiming to improve accessibility and 
self-efficacy.
Multidisciplinary participatory 
design involving students, teachers, 
and researchers. Iterative 
prototyping tailored to the school’s 
needs and infrastructure.
Demonstrated effectiveness in 
fostering relaxation, activation, and 
engagement in SEN students.
Limited to one school with specific 
needs; scalability and diverse 
applications need further 
exploration.
Sensation: Touch-sensitive and 
motion-based musical interactions 
for SEN students.
Perception: Users develop 
structured engagement with sound, 
fostering cognitive and emotional 
learning.
Highlights self-efficacy through 
ADMIs, encouraging broader 
adoption in SEN settings.
Participatory Design of 
Collaborative Digital Musical 
Interfaces for Children with 
Autism Spectrum Condition 
(ASC)
2023 Balázs András Iványi
Participatory Design, Case Study, 
User Research – Incorporates ASD 
children in co-design workshops.
Investigates collaborative DMIs 
designed to enhance social skills in 
children with ASC through 
participatory design.
Co-design approach with 
educators, therapists, and children. 
Evaluations based on social 
interaction metrics.
Increased social interaction and 
engagement in group music-
making. Positive feedback from 
educators.
Limited by small sample size. Long-
term social impact not evaluated.
Sensation: Tactile and auditory 
engagement with musical interface; 
immediate sensory feedback from 
touch.
Perception: Children develop 
social and collaborative skills 
through structured musical 
interaction.
Emphasizes the importance of 
collaborative DMIs in therapeutic 
and educational contexts for 
children with ASC.
Participatory Design of a 
Collaborative Accessible Digital 
Musical Interface
2023 Balázs András Iványi
Participatory Design, Autism 
Research – Focuses on 
collaborative accessibility-based 
instrument development.
Investigates the development of 
collaborative accessible digital 
musical interfaces (CADMI) for 
children with autism spectrum 
condition (ASC).
Co-design approach with children, 
educators, and therapists. Focused 
on iterative prototyping and 
evaluations in classroom settings.
Enhanced collaboration and 
communication among children 
with ASC. Positive feedback from 
educators and therapists.
Limited exploration of scalability 
beyond the specific user group. Long-
term social impact not evaluated.
Sensation: Tactile and auditory 
feedback from an accessible 
musical interface.
Perception: Users recognize 
interaction patterns, fostering 
collaborative and social 
engagement through music.
Highlights the potential for 
collaborative DMIs to foster social 
interaction and inclusivity in 
therapeutic contexts.
dB: A Web-Based Drummer 
Bot for Finger-Tapping 2023 Çağrı Erdem
AI and Computational Music 
Research, Usability Study – 
Evaluates AI-generated rhythm 
interactions with human input.
Presents dB, a web-based drummer 
bot that uses finger-tapping as input 
for real-time rhythm generation.
Web-based interface with machine 
learning algorithms for detecting 
and replicating finger-tapping 
rhythms. Iterative user testing 
included.
Demonstrated effectiveness in 
capturing and reproducing 
rhythmic inputs. Received positive 
feedback from testers for usability.
Limited testing with diverse musical 
styles. Integration with other musical 
systems not explored.
Sensation: Tactile interaction with 
keyboard tapping; auditory 
feedback from AI-generated 
rhythms.
Perception: Users interpret 
rhythmic variations, recognizing 
groove patterns shaped by AI 
modulation.
Highlights potential for 
democratizing rhythm creation 
through simple and intuitive 
interfaces.
Kinesynth: A Hybrid 
Kinesthetic Synthesizer 2023 Don Derek Haddad, Joseph A. 
Paradiso
Experimental Design, Interactive 
Performance Study – Examines 
how capacitive sensing influences 
sound perception.
Introduces the Kinesynth, a 
synthesizer combining kinesthetic 
input with sound modulation.
Utilized a hybrid of motion 
tracking and traditional synthesizer 
interfaces. Tested with performers 
and educators.
Enabled expressive sound control 
through kinesthetic movements. 
Received positive reviews for 
performance usability.
Complexity of motion tracking may 
limit accessibility. Long-term impact 
on performance techniques not 
studied.
Sensation: Tactile and electrical 
sensations from capacitive skin 
conduction.
Perception: Users develop 
awareness of bioelectric 
modulation, linking bodily 
interaction with sound control.
Suggests potential for blending 
traditional and gestural inputs in 
music creation and performance.
ClimaSynth: Enhancing 
Environmental Perception 
through Sonic Interaction
2023 Eleni-Ira Panourgia
Experimental Design, Human-
Computer Interaction (HCI) Study 
– Investigates how users engage 
with sound-driven climate models.
Explores ClimaSynth, a sonic 
interface designed to enhance 
awareness of climate change 
through interactive soundscapes.
Iterative design based on 
environmental data mapping to 
sound. User testing focused on 
perceptual engagement.
Increased user awareness and 
emotional connection to climate 
data. Demonstrated engagement 
through interactive soundscapes.
Limited long-term evaluation of 
behavioral changes. Requires 
scalability for wider audience 
integration.
Sensation: Auditory and tactile 
engagement with climate-related 
soundscapes, manipulated through 
a web-based interface.
Perception: Users interpret climate 
change effects through sonic 
variations, linking environmental 
changes to acoustic perception.
Highlights the potential for 
sonification to evoke emotional 
responses and inspire 
environmental action.
Accessible Digital Musical 
Instruments: A Review of 
Musical Interfaces in Inclusive 
Music Practice
2023 Emma Frid
Systematic Review, Literature 
Analysis – Surveys 113 
publications on accessible digital 
instruments.
Systematic review of accessible 
digital musical instruments 
(ADMIs) in inclusive music 
practices.
Reviewed literature on ADMIs, 
focusing on usability, accessibility, 
and inclusivity. Identified gaps and 
best practices.
Provided a comprehensive 
overview of trends and challenges 
in ADMI development. Identified 
best practices for inclusivity.
Lack of standardized evaluation 
frameworks across studies. Limited 
focus on long-term impacts.
Sensation: Interaction with ADMIs 
via touch, gaze, and haptic 
feedback, providing alternative 
sensory pathways.
Perception: Users develop an 
understanding of sound control 
through multimodal feedback, 
expanding access to music-making.
Emphasizes the importance of 
iterative, user-centered design in 
developing inclusive ADMIs.
MusiCane: An Accessible 
Digital Instrument Inspired by 
the White Cane
2023 Emmanouil Dimogerontakis
Applied Research, Human-
Centered Design Study – 
Investigates how blind and visually 
impaired users engage with 
vibrotactile music interfaces.
Introduces the MusiCane, a digital 
instrument for blind and visually 
impaired users that combines 
mobility and music-making 
capabilities.
Iterative prototyping with visually 
impaired users. Evaluation based 
on accessibility, usability, and 
musical output.
Effective in combining mobility 
with creative expression. Users 
reported increased confidence and 
enjoyment.
Limited sample size for evaluation. 
Scalability in broader contexts 
unexplored.
Sensation: Haptic vibrations and 
auditory cues provide spatial 
awareness through cane movement.
Perception: Users construct an 
auditory map of their surroundings 
and engage in collaborative music-
making, transforming raw sensory 
input into structured interaction.
Highlights the potential for 
accessible DMIs to transform 
mobility aids into creative tools.
SnakeSynth: New Interactions 
for Generative Audio Synthesis 2023 Eric Easthope
Computational Music Research, AI-
driven Usability Study – Analyzes 
user engagement with AI-powered 
generative synthesis.
Explores SnakeSynth, a generative 
audio synthesizer emphasizing 
gestural interactions for dynamic 
sound design.
Utilized motion-based interaction 
via a tangible interface. Iterative 
testing with sound designers and 
musicians.
Enhanced engagement through 
physical interaction. Positive 
feedback for creative possibilities 
in sound exploration.
Limited testing across diverse user 
groups. Integration with existing 
DAWs not fully explored.
Sensation: Touch-based interaction 
with a generative synthesis system; 
gestures mapped to musical 
expressions.
Perception: Users develop an 
understanding of how gestures 
modulate sound, learning emergent 
generative patterns.
Highlights the potential for 
tangible interaction in expanding 
expressive capabilities in sound 
design.
BioSynth: An Affective 
Biofeedback Device 2023 Erin Gee
Experimental, Case Study – Uses 
physiological data as a generative 
music method and examines 
biofeedback interactions.
Explores the BioSynth, an 
instrument leveraging biofeedback 
for emotional expression through 
sound synthesis.
Combined physiological sensors 
with sound synthesis algorithms. 
Iterative design process with 
testing on diverse user groups.
Successfully captured emotional 
states and transformed them into 
musical outputs. Users found the 
experience engaging.
Limited by sensor accuracy and 
latency issues. Scalability to 
performance contexts needs further 
exploration.
Sensation: Physiological feedback 
(heart rate, respiration, skin 
conductance) generates real-time 
sound output.
Perception: Users interpret 
biofeedback sounds as emotional 
expressions, blurring the line 
between felt and externalized 
emotion.
Discusses the role of biofeedback 
in enhancing emotional connection 
in musical interactions.
Evaluation of Timbre-Based 
Control of a Parametric 
Synthesizer
2023 Jeff Gregorio, Youngmoo E. Kim
Experimental Design, Usability 
Study – Examines how timbre-
based control influences user 
experience in parametric synthesis.
Evaluates user interaction with a 
parametric synthesizer using timbre-
based controls.
Conducted user studies comparing 
traditional and timbre-based 
interfaces. Statistical analysis of 
user performance and satisfaction.
Demonstrated improved usability 
and creative potential of timbre-
based controls.
Limited exploration of user 
diversity. Scalability for complex 
sound synthesis scenarios not 
addressed.
Sensation: Tactile control of timbre 
parameters and auditory feedback 
from spectral changes.
Perception: Users develop 
cognitive models of timbre 
interaction, refining their 
understanding of sound 
transformation.
Highlights the potential for timbre-
based interfaces to simplify 
complex synthesis tasks.
The Harvester: A DIY Sampler 
and Synthesizer Demo 2023 Johann Diedrick
User Research, Experimental, 
Workshop-Based Study – 
Combines questionnaires, field 
research, and participatory design 
workshops.
Describes The Harvester, a DIY 
sampler and synthesizer enabling 
creative sound manipulation.
Developed through hands-on 
workshops with musicians and 
hobbyists. Iterative prototyping and 
feedback.
Empowered users to create unique 
soundscapes. Accessible to non-
experts through its DIY nature.
Limited evaluation in professional 
contexts. No exploration of long-
term user engagement.
Sensation: Capturing everyday 
sounds via tactile and motion-
based interaction; hearing raw 
audio snippets. Perception: 
Spatially manipulating recorded 
sounds to shape them into 
structured music, altering auditory 
meaning.
Highlights the democratization of 
music technology through 
accessible DIY tools.
The Timbre Explorer: A 
Synthesizer Interface for 
Educational Purposes and 
Perceptual Studies
2023 Joshua Ryan Lam
Educational Research, Usability 
Study – Explores how users engage 
with timbre learning through an 
interactive interface.
Presents the Timbre Explorer, a 
synthesizer designed for exploring 
sound timbre for educational and 
research purposes.
Developed through iterative 
prototyping with educators and 
students. Evaluated in classroom 
and laboratory settings.
Enabled intuitive exploration of 
sound timbre. Effective as an 
educational tool for sound 
perception studies.
Limited application beyond 
educational contexts. Scalability to 
other instruments not explored.
Sensation: Tactile and auditory 
manipulation of timbral 
parameters.
Perception: Users develop 
cognitive models of timbre through 
interactive sound exploration.
Discusses the role of simplified 
interfaces in fostering sound 
understanding and exploration in 
educational settings.
An Embedded Wavetable 
Synthesizer for the Electronic 
Bandoneon
2023 Juan M. Ramos
Applied Research, Technical 
Sound Design – Studies how 
digital wavetable synthesis 
recreates acoustic bandoneon 
timbres.
Describes an embedded wavetable 
synthesizer integrated into the 
electronic bandoneon for enhanced 
musical expression.
Combined acoustic measurements 
with parameter mapping. Iterative 
testing with bandoneon players.
Achieved seamless integration of 
acoustic and electronic elements. 
Improved expressiveness in 
performances.
Limited exploration beyond 
traditional bandoneon players. 
Scalability for other instruments not 
studied.
Sensation: Auditory experience of 
synthesized bandoneon tones 
shaped by bellows pressure and 
keystrokes.
Perception: Musicians compare and 
interpret synthesized vs. acoustic 
sounds, adapting their playing 
techniques accordingly.
Suggests potential for enhancing 
traditional instruments through 
embedded electronic synthesis.
 5. DISCUSSION The findings underscore a crucial paradigm shift in sensory intervention: moving from static, interventions to dynamic, user-responsive adaptive systems. The limitations of conventional sensory therapies—such as Sensory Integration Therapy (SIT) and Multisensory Environments (MSEs)—stem from their 
inflexibility and lack of real-time adaptability [14]. NIME-based interventions, by contrast, prioritize self-regulation, sensory autonomy, and interactive engagement, offering a more inclusive and sustainable approach.   The review also suggests that rhythm may serve as a powerful regulatory mechanism for individuals with ASD. The 
AI-terity 2.0: An Autonomous 
Music Agent 2023 Koray Tahiroğlu
AI and Computational Music 
Research, Experimental Study – 
Studies how machine learning 
models generate music and interact 
with human improvisation.
Explores AI-terity 2.0, an 
autonomous AI-driven music agent 
designed for live performance and 
collaboration.
Developed using AI frameworks 
and tested in live performance 
contexts. Iterative refinements 
based on user feedback.
Demonstrated ability to improvise 
and adapt in live performances. 
Users appreciated its collaborative 
potential.
Limited evaluation of AI’s 
adaptability across genres. Ethical 
implications of AI creativity not 
fully addressed.
Sensation: Algorithmic sound 
transformations driven by machine 
improvisation.
Perception: Users interpret AI-
generated patterns, adjusting 
expectations of musical form and 
authorship.
Highlights the evolving role of AI 
as a collaborator in music creation 
and live performance.
NAKANISYNTH: An Intuitive 
Freehand Drawing Waveform 
Synthesizer Application for iOS 
Devices
2023 Kyosuke Nakanishi
Experimental Design, Usability 
Study – Evaluates how users 
interact with the freehand 
waveform synthesizer.
Introduces NAKANISYNTH, an 
iOS-based synthesizer that 
generates waveforms through 
freehand drawing.
Developed and tested on iOS 
devices with iterative user 
feedback. Focused on intuitive 
gesture-to-sound mapping.
Provided a highly intuitive 
interface for waveform creation. 
Positive feedback for its creative 
possibilities.
Limited evaluation beyond iOS 
devices. Scalability to other 
platforms not explored.
Sensation: Tactile drawing of 
waveforms and real-time auditory 
feedback.
Perception: Users associate drawn 
waveforms with sound 
characteristics, refining their 
understanding of synthesis 
parameters.
Discusses the potential for intuitive 
interfaces in democratizing sound 
design.
BoomBox: A Haptic Instrument 
for Deaf and Hard-of-Hearing 
Children
2023 Lloyd May
Co-Design Study, Accessibility 
Research – Evaluates physical 
accessibility in drum machine 
interfaces.
Introduces BoomBox, a haptic 
instrument designed to make music 
accessible for children who are 
deaf or hard of hearing.
Developed using participatory 
design with educators and students. 
Evaluated for tactile feedback and 
accessibility.
Enhanced inclusivity in music-
making for users with hearing 
impairments. Received positive 
feedback for tactile interaction.
Limited exploration of integration 
with broader educational tools. Long-
term user impact not evaluated.
Sensation: Tactile engagement with 
capacitive sensing, magnetic 
placement of pieces, and auditory 
feedback.
Perception: Users construct 
rhythmic structures, understanding 
groove and tempo through 
interactive sequencing.
Discusses the importance of 
multisensory DMIs in expanding 
accessibility for marginalized 
communities.
BLIKSEM: An Acoustic 
Synthesis Fuzz Pedal 2023 Lloyd May
Experimental Design, Technical 
Performance Analysis – Studies 
nonlinear acoustic synthesis effects 
in fuzz pedals
Presents BLIKSEM, a fuzz pedal 
with a focus on acoustic synthesis 
for expressive sound manipulation.
Prototyped with iterative testing by 
guitarists and sound designers. 
Evaluated for expressiveness and 
usability.
Provided a unique acoustic 
synthesis experience. Received 
positive feedback for expressive 
control.
Limited testing beyond experienced 
musicians. Scalability for 
commercial production not 
addressed.
Sensation: Tactile foot pedal 
control over a driven cantilever, 
modifying electromechanical 
vibrations.
Perception: Users shape distortion 
and timbre dynamically, 
experiencing material-driven 
acoustic synthesis.
Suggests potential for novel 
synthesis approaches in guitar-
based performance tools.
Co-Designing Haptic 
Instruments With Deaf and 
Hard-of-Hearing Children
2023 Lloyd May
Participatory Design, Accessibility 
Research – Engages DHH children 
in haptic music interaction.
Explores the co-design of haptic 
instruments to make music creation 
accessible for children who are 
deaf or hard of hearing.
Participatory design involving 
children, educators, and engineers. 
Evaluations focused on usability 
and inclusivity.
Enabled engaging and accessible 
music creation. Received positive 
feedback from children and 
educators.
Limited long-term impact studies. 
Scalability in educational settings 
not fully addressed.
Sensation: Direct tactile experience 
of haptic vibrations with varying 
frequencies and intensities.
Perception: Interpretation of 
vibrotactile stimuli as musical and 
artistic expressions, forming 
structured compositions.
Highlights the role of participatory 
design in creating inclusive musical 
tools for marginalized groups.
Design and Evaluation of 
Accessible Digital Musical 
Instruments for Pupils with 
Neurodevelopmental Disorders
2024 Matteo Olivo, Florence Carrouel, 
Emily Darlington, Laurent Pottier
Accessibility Research, Case Study 
– Evaluates the usability of 
assistive musical technology in 
SEN classrooms.
Highlights participatory design of 
Accessible Digital Musical 
Instruments (ADMIs) for students 
with neurodevelopmental disorders 
(NDDs). Evaluated in a special 
education setting.
Participatory and iterative design 
approach with input from students, 
teachers, and assistants. Conducted 
evaluations in a French middle 
school, focusing on usability and 
engagement.
Enhanced accessibility and 
engagement for students with 
NDDs. Iterative refinements 
improved user experience and 
teacher involvement.
Limited scalability beyond the 
specific school environment. 
Evaluation was qualitative, with 
minimal quantitative data.
Sensation: Multisensory interaction 
through touch, vision, and sound in 
accessible musical interfaces.
Perception: Users develop 
structured learning of musical 
patterns, enhancing cognitive and 
motor skills.
Emphasized the importance of 
involving multiple stakeholders in 
ADMI design to improve usability 
and social engagement.
GrooveTransformer: A 
Generative Drum Sequencer 
Eurorack Module
2024 Nicholas Evans, Behzad Haki, 
Sergi Jordà
Computational Music Research, 
Experimental Design – Examines 
AI-driven generative drum 
sequencing.
Presents the GrooveTransformer 
module, which uses a Variational 
Auto-Encoder (VAE) for 
generative drum sequencing. 
Designed for real-time 
performance and user-defined 
exploration of generative models.
Built around a VAE generative 
model adapted to Eurorack 
modular synthesis format. Used 
iterative prototyping and 
integration tests with musicians.
Demonstrated potential for 
redefining generative processes, 
enabling users to explore latent 
spaces for novel musical patterns.
Hardware complexity in real-time 
contexts. Limited testing with non-
expert users. Scalability challenges 
for diverse Eurorack setups.
Sensation: Tactile control over 
generative drum sequencing, with 
real-time auditory feedback.
Perception: Users analyze rhythmic 
patterns, distinguishing between AI-
driven and manually programmed 
grooves.
Highlights opportunities for 
expanded generative model use in 
creative performance, emphasizing 
user adaptability and exploratory 
design.
The Effectiveness of Mirroring- 
and Rhythm-Based 
Interventions for Children with 
Autism Spectrum Disorder
2021 Phoebe Morris, Edward Hope, 
Tom Foulsham, John P. Mills
Therapeutic Case Study, 
Neurodevelopmental Research – 
Investigates rhythm-based 
interventions in autism therapy.
Systematic review evaluating 
mirroring and rhythm interventions 
for improving communication and 
social development in children 
with Autism Spectrum Disorder 
(ASD).
Reviewed 11 studies using 
SPIDER and PICO frameworks, 
focusing on communication and 
social skills outcomes in children 
under 12 years old.
Both interventions were effective, 
with mirroring enhancing social 
connectedness and rhythm 
improving communication skills.
Lack of standardization in 
methodologies, duration, and 
outcome measures. Need for 
quantitative studies and long-term 
evaluations.
Sensation: Tactile and auditory 
feedback from rhythmic mirroring 
exercises.
Perception: Children improve 
motor coordination and social 
interaction through structured 
rhythmic engagement.
Suggested integrating these 
techniques in broader educational 
and therapeutic practices, such as 
physical education.
SketchSynth: A Browser-Based 
Sketching Interface for Sound 
Control
2023 Sebastian Löbbers, György 
Fazekas
Usability Study, Interaction Design 
Research – Examines how visual 
sketching translates into sound 
synthesis control.
Describes SketchSynth, an 
interface for mapping visual 
sketches to sound characteristics 
using cross-modal perception 
research.
Developed using iterative 
prototyping with real-time visual 
feature extraction linked to sound 
synthesis via Open Sound Control 
(OSC).
Enabled meaningful and creative 
gesture-to-sound mappings. 
Demonstrated in musical contexts 
with positive user feedback.
Overstimulation for users with 
sensory sensitivities. Limited 
exploration of diverse synthesis 
types.
Sensation: Visual sketching of 
waveforms with real-time auditory 
feedback.
Perception: Users associate drawn 
shapes with sound characteristics, 
linking visual forms to musical 
structure
Highlights the potential of 
perceptual research in designing 
engaging and accessible sonic 
interfaces.
About TIME: Textile Interfaces 
for Musical Expression 2024 Sophie Skach, Victor Shepardson, 
Thor Magnusson
Experimental Design, Human-
Computer Interaction Study – 
Studies textile-based tactile 
interfaces for music-making.
Explores textile-based interfaces 
for musical expression, 
emphasizing the potential of 
malleable and soft materials for 
interaction.
Developed textile prototypes tested 
with musicians using qualitative 
user studies. Combined insights on 
textile interaction and sound 
mapping.
Showed promise in creating 
expressive, tactile, and engaging 
musical interactions. Provided 
insights for future textile-based 
designs.
Limited evaluation on diverse 
demographics. More iterative 
prototyping needed for refinement.
Sensation: Tactile interaction with 
textile surfaces to control sound 
parameters.
Perception: Users interpret fabric 
deformations as expressive musical 
input, redefining instrument 
interaction.
Suggests textiles as a promising 
material for expanding inclusivity 
and interaction in musical 
interfaces.
Buttons, Sliders, and Keys – A 
Survey on Musical Grid 
Interface Standards
2022 Beat Rossmy Survey, UI Design Analysis
This paper investigates the 
presence of emerging design 
standards in musical grid interfaces 
by analyzing 40 applications, 
instruments, or controllers. It 
identifies 18 UI elements and 
proposes three design theses for 
standardizing UI elements in 
musical grids.
Survey-based analysis of 30 
independent user scripts from the 
monome norns community and 10 
commercial grid-based musical 
devices. Examined UI elements, 
interactions, and usability 
heuristics.
Identified recurring patterns in 
musical grid UI designs, such as 
common button interactions (click, 
hold, double-click), standard UI 
elements (buttons, sliders, 
keyboards, drum matrices), and 
navigation structures (pages, tabs). 
Proposed a formalized 
classification of grid UI 
components.
Lack of formalized, universally 
agreed-upon standards. Usability 
issues in some designs due to 
inconsistent feedback mechanisms. 
Limited exploration of alternative 
UI paradigms beyond WIMP 
(Windows, Icons, Menus, Pointer) 
influences.
Sensation: Users engage with 
tactile button presses, visual grid 
illuminations, and haptic feedback.
Perception: Users recognize 
patterns in UI interactions, 
understand grid-based navigation, 
and develop an intuition for step-
sequencing and note entry.
The study suggests that while grid 
UIs lack formalized design 
standards, common conventions 
have emerged. It proposes 
structured design principles for 
improving usability, visibility, and 
interaction consistency in grid-
based musical tools. Highlights the 
importance of balancing 
accessibility with expressive 
musical interaction.
User-Friendly MIDI in the Web 
Browser 2022 Jean-Philippe Côté
Usability Study, API Evaluation, 
Open-Source Software 
Development
This paper explores the usability of 
the Web MIDI API, its limitations, 
and the improvements introduced 
by the WEBMIDI.js library. It 
discusses the challenges of 
interacting with MIDI devices via 
browsers
Conducted a usability survey with 
58 respondents, analyzing their 
challenges with the Web MIDI 
API. Iterative design cycles, user 
feedback, and real-world testing 
informed the development of the 
WEBMIDI.js library.
Findings indicated that the Web 
MIDI API is too low-level for most 
users. 93% of survey respondents 
supported the need for a higher-
level library. WEBMIDI.js 
introduced 
Limited browser support (Apple 
does not support the Web MIDI 
API). Web MIDI API 
documentation was incomplete for 
several years, delaying adoption. 
Further work needed on accessibility 
and cross-platform compatibility.
Sensation: Users interact with 
MIDI devices via web-based 
interfaces, sending and receiving 
MIDI messages in real time.
Perception: Developers benefit 
from semantic abstractions, making 
MIDI integration in web 
applications more intuitive.
It emphasizes the importance of 
high-level abstractions in fostering 
accessibility, usability, and 
developer engagement. The 
research sets the stage for further 
innovations in web-based music 
technology.
Towards User Interface 
Guidelines for Musical Grid 
Interfaces
2022 Beat Rossmy, Maximilian Rauh, 
Alexander Wiethoff
Experimental Study (pre-study and 
main study)
Musical grid interfaces are 
becoming an industry standard for 
interacting with music software and 
instruments. However, the absence 
of unified design standards creates 
challenges due to competing design 
approaches. 
Pre-study: Online survey with 23 
participants focusing on challenges 
in using grid interfaces. Key 
themes: navigation, button 
functionality, visual feedback.
Main Study: Online interactive 
grid interface with 24 participants. 
Metrics included task completion 
time, errors, distance from targets, 
and perceived workload.
1. Color Use: Multicolored UIs 
improved recognition and reduced 
errors; monochromatic UIs caused 
more errors in complex setups.
2. Movement Emphasis: Amplified 
movements improved accuracy; 
non-amplified movements were 
faster but less accurate.
3. Animated Transitions: Preferred 
by users for spatial context but 
slightly slower navigation.
Limited sample diversity; focused 
only on statistical measures; did not 
include cultural and creative 
experiences.
Enhanced visual and tactile 
perception improves cognitive 
mapping and memory retention.
Adaptive color schemes improve 
usability but need personalization 
options. Amplified movements 
reduce focus strain but require 
optimization. Animated transitions 
improve spatial understanding but 
may slow navigation.
Squeeze, Twist, Stretch: 
Exploring Deformable Digital 
Musical Interfaces Design 
Through Non-Functional 
Prototypes
2022 Jianing Zheng, Nick Bryan-Kinns 
Qualitative design exploration 
study with a remote workshop 
(n=23)
Investigates how deformable 
interfaces can be used in digital 
musical instruments (DMIs). A 
workshop study invited designers 
to create non-functional prototypes 
of deformable DMIs. The study 
identifies key design approaches, 
interaction styles, and mappings 
between gestures and sound.
Conducted eight remote design 
workshops with 23 participants. 
Participants created non-functional 
prototypes of deformable DMIs 
using everyday materials. 
Open-ended tasks explored 
mappings between gestures and 
sound. Thematic analysis of 
qualitative data (transcribed 
discussions and observation
 Mappings between deformable 
input (e.g., stretch, twist, press) and 
sound output emerged naturally. 
Designers categorized their 
prototypes into four interaction 
styles: handheld, surface-based, 
wearable, and hybrid. 
 Unexpected affordances and 
gesture-based and form-driven 
design approaches emerged as 
primary methods for developing 
deformable DMIs.
Study lacks real-world 
implementation; findings are based 
on non-functional prototypes. 
No technical validation of how 
deformable DMIs would perform in 
live settings. 
Limited discussion on how such 
interfaces would integrate with 
existing digital music workflows.
Explores tactile and proprioceptive 
sensation in musical interaction. 
Highlights how material properties 
shape perceived affordances and 
user expectations. 
Study suggests that physical 
sensation plays a crucial role in 
intuitive sound control.
Demonstrates that open-ended, 
material-driven design fosters 
creativity in DMI development. 
Highlights the potential for 
deformable DMIs to enable novel 
musical expression. 
Raises questions about how such 
interfaces would be perceived by 
audiences versus musicians.
neurocognitive basis for rhythmic entrainment suggests that predictable, structured auditory inputs can enhance sensory processing and motor coordination, making rhythm-based musical interfaces particularly well-suited for ASD interventions. However, while rhythmic predictability enhances engagement, excessive rigidity may limit creative exploration. Future NIME designs must balance structured rhythmic input with elements of free-form exploration to ensure a flexible yet supportive interactive experience.   The integration of algorithmic sensory modulation represents another significant advancement, allowing interfaces to adjust to individual sensory profiles in real time. This adaptability is critical, given that sensory needs fluctuate dynamically in ASD. Unlike traditional interventions, which often impose fixed sensory frameworks, real-time adaptive NIME systems empower users to self-regulate sensory input, thereby reducing sensory distress and promoting engagement. Therefore, adaptive NIME may offer new solutions to assist with the challenges of catering for SOR and SUR sensory profiles for ASD.   6. CONCLUSION This review establishes the foundation for a new paradigm in NIME-based sensory intervention for autism, emphasizing real-time adaptability, rhythmic structure, and play-based engagement. While traditional approaches to sensory intervention have relied on rigid, clinician-driven frameworks, emerging research in NIME highlights the potential of interactive, user-centered interfaces to accommodate individualized sensory needs. Based on the study’s findings we offer the following concise list of design recommendations for NIME practitioners interested in engaging the Autism space and its challenges for SOR and SUR sensory profiles.   6.1 Design Recommendations for Sensory NIME for Autism   1. Real-Time Adaptive Sensory Modulation   • Incorporate algorithmic modulation to dynamically adjust auditory, visual, and tactile stimuli based on user interaction.  • Implement biofeedback-driven adjustments to respond to physiological and behavioral cues in real time.  • Support both sensory over-responsivity (SOR) and sensory under-responsivity (SUR) by offering customizable thresholds for sensory input.   2. Rhythm-Based Sensory Regulation   • Use structured rhythmic elements to enhance focus, reduce anxiety, and improve motor coordination.  • Balance predictability with flexibility, allowing users to engage with structured rhythmic patterns while exploring variations.  • Leverage rhythmic entrainment to promote sensory stability and social synchronization.   3. Play-Centered, Multisensory Engagement   • Design interactive, game-like experiences that foster intrinsic motivation and encourage sensory exploration.  • Integrate multisensory feedback (auditory, haptic, and visual) to support diverse sensory profiles.  
• Allow child-led exploration, prioritizing autonomy and low-pressure engagement.   4. Inclusive and Accessible Interface Design   • Develop low-cost, scalable solutions to ensure accessibility beyond research settings.  • Offer customizable interfaces that accommodate individual sensory preferences and needs.  • Use open-source platforms to foster broader adoption and collaborative development.   5. Interdisciplinary and Longitudinal Research Integration  • Collaborate with occupational therapists, neuroscientists, and human-computer interaction specialists for evidence-based design.  • Conduct longitudinal studies to assess the sustained impact of sensory NIME interventions.  • Establish standardized sensory terminology for clarity in interface design and evaluation.  7. NEXT STEPS  Future research should prioritize longitudinal studies to assess the sustained impact of sensory NIME interventions. The integration of biofeedback-driven sensory interfaces, which adapt to real-time physiological and behavioral responses, presents a promising avenue for future development. Additionally, interdisciplinary collaboration with occupational therapists, neuroscientists, and human-computer interaction specialists will be critical in refining evidence-based design principles.   Expanding accessibility remains a key challenge. Many existing NIME prototypes require specialized hardware, limiting their application beyond research settings. Open-source development initiatives should be encouraged to foster scalability and inclusivity. By addressing these next steps, the field of NIME can pioneer a new era of interactive, sensory-responsive musical interfaces, fundamentally transforming the landscape of autism-specific interventions.  8. ETHICAL STATEMENT This research follows NIME’s ethical guidelines for responsible and inclusive practice in music interface design. We aim to contribute to accessible, user-centered tools that respect sensory diversity. We acknowledge that we are not autistic or neurodivergent.  9. REFERENCES [1] APA Dictionary of Psychology (2023). Autism Spectrum Disorder. Retrieved 30.1.2025 from https://dictionary.apa.org/autism-spectrum-disorder   [2] Ben-Sasson, A., Gal, E., Fluss, R., Katz-Zetler, N., & Cermak, S. A. (2019). Update of a meta-analysis of sensory symptoms in ASD: A new decade of research. Journal of Autism and Developmental Disorders, 49, 4974-4996. https://doi.org/10.1007/s10803-019-04180-0  [3] Ashburner, J., Bennett, L., Rodger, S., & Ziviani, J. (2013). Understanding the sensory experiences of young people with autism spectrum disorder. Australian Occupational Therapy Journal, 60(3), 171–180. https://doi.org/10.1111/1440-1630.12025   [4] Pfeiffer, B. A., Koenig, K., Kinnealey, M., Sheppard, M., & Henderson, L. (2011). Effectiveness of sensory 
integration interventions in children with ASD: A pilot study. American Journal of Occupational Therapy, 65(1), 76-85. https://doi.org/10.5014/ajot.2011.09205    [5] Case-Smith, J., Weaver, L. L., & Fristad, M. A. (2015). A systematic review of sensory processing interventions for children with ASD. Autism, 19(2), 133–148. https://doi.org/10.1177/1362361313517762 [6] Elwin, M., Schröder, A., Ek, L., Wallsten, T., & Kjellin, L. (2017). Sensory clusters of adults with and without autism spectrum conditions. Journal of Autism and Developmental Disorders, 47(3), 579-589. https://doi.org/10.1007/s10803-016-2976-1 [7]  Tomchek, S. D., & Dunn, W. (2007). Sensory processing in children with and without autism. American Journal of Occupational Therapy, 61(2), 190-200. https://doi.org/10.5014/ajot.61.2.190   [8] Schaffler, M. D., Middleton, L. J., & Abdus-Saboor, I. (2019). Mechanisms of tactile sensory phenotypes in autism. Current Psychiatry Reports, 21(12), 134. https://doi.org/10.1007/s11920-019-1122-0   [9] Kern, J. K., Trivedi, M. H., Grannemann, B. D., Garver, C. R., et al. (2007). Sensory correlations in autism. Autism, 11(2), 123–134. https://doi.org/10.1177/1362361307075702   [10] Ben-Sasson, A., Cermak, S. A., Orsmond, G. I., et al. (2008). Sensory clusters of toddlers with ASD: Differences in affective symptoms. Journal of Child Psychology and Psychiatry, 49(8), 817-825. https://doi.org/10.1111/j.1469-7610.2008.01899.x [11] Baum, C., Christiansen, C., & Bass, J. (2015). Person-Environment-Occupational Performance (PEOP) Model. In Occupational Therapy: Performance, Participation, Well-being (4th ed.). Thorofare, NJ: Slack.  [12] Ayres, A. J. (2005). Sensory Integration and the Child (25th anniversary ed.). Los Angeles, CA: WPS.  [13] Marks, W. N., Parker, M. E., & Howland, J. G. (2018). Variants of the spontaneous recognition procedure. In Handbook of Object Novelty Recognition (Vol. 27). Elsevier. https://doi.org/10.1016/B978-0-12-812012-5.00007-0 [14] Randell, E., Wright, M., Milosevic, S., et al. (2022). Sensory integration therapy for children with autism. Health Technology Assessment, 26(29). https://doi.org/10.3310/TQGE0020   [15] de Domenico, C., di Cara, M., Piccolo, A., et al. (2024). Multi-sensory environment and sensory behaviors in children with ASD. Journal of Clinical Medicine, 13(14), 4162. https://doi.org/10.3390/jcm13144162   [16] LaGasse, B., Yoo, G. E., & Hardy, M. W. (2024). Rhythm and music for sensorimotor organization in autism. Frontiers in Integrative Neuroscience, 18. https://doi.org/10.3389/fnint.2024.1403876   [17] O'Keeffe, C., & McNally, S. (2021). Play-based interventions for social communication in children with ASD. Review Journal of Autism and Developmental Disorders, 10, 51-81. https://doi.org/10.1007/s40489-021-00286-3  [18] Nath, A., & Young, S. (2015). Vesball: A ball-shaped instrument for music therapy. New Interfaces for Musical Expression. DOI: 10.5281/zenodo.1179146 [19] Nonnis, A., & Bryan-Kinns, N. (2020). Όλοι: Music making to scaffold social activities in ASD. New Interfaces for Musical Expression. DOI: 10.5281/zenodo.4813186 
[20] Ivanyi, A. B. (2024). Participatory design of a collaborative digital musical interface for children with ASD. Masters Dissertation, Aalborg University. http://www.aau.dk.  [21] Ivanyi, A. B., Tjemsland, T. B., May, L., et al. (2024). Participatory design of a collaborative digital musical interface for ASD. NIME’24. DOI: 10.5281/zenodo.13904778 [22] Gee, E. (2023). The BioSynth: An affective biofeedback device grounded in feminist thought. NIME’23. DOI: 10.5281/zenodo.11189254 [23] Erdem, Ç., & Griwodz, C. (2024). dB: A web-based drummer bot for finger-tapping. NIME’24. DOI: 10.5281/zenodo.13904788 [24] Haddad, D. D., & Paradiso, J. A. (2018). Kinesynth: A hybrid kinesthetic synthesizer. NIME’18. DOI: 10.5281/zenodo.1302669 [25] Panourgia, E., Usabaev, B., & Brennecke, A. (2024). ClimaSynth: Enhancing environmental perception. NIME’24. DOI: 10.5281/zenodo.13904937 [26] Dimogerontakis, E., Overholt, D., & Serafin, S. (2024). MusiCane: An accessible digital instrument inspired by the white cane. NIME’24. DOI: 10.5281/zenodo.13904852 [27] Easthope, E. (2023). SnakeSynth: New interactions for generative audio synthesis. NIME’23.  DOI: 10.5281/zenodo.11189319 [28] Gregorio, J., & Kim, Y. E. (2021). Timbre-based control of a parametric synthesizer. NIME’21. DOI: 10.21428/92fbeb44.31419bf9 [29] Diedrick, J. (2023). The Harvester: A DIY sampler and synthesizer. NIME’23. DOI: 10.5281/zenodo.11189333 [30]  Ramos, J., Calcagno, E., Vergara, R. O., et al. (2022). An electronic bandoneon with dynamic sound synthesis. Computer Music Journal, 46, 40-57. https://doi.org/10.1162/comj_a_00636 [31] Tahiroglu, K., Kastemaa, M., & Koli, O. (2021). AI-terity 2.0: A GAN-based deep learning model for NIME. NIME’21. DOI: 10.21428/92fbeb44.3d0e9e12 [32] Morris, P. O., Hope, E., Foulsham, T., & Mills, J. P. (2021). The effectiveness of mirroring- and rhythm-based interventions for ASD. Review Journal of Autism and Developmental Disorders, 8, 541-561. DOI:10.31234/osf.io/xbvj2 [33] Löbbers, S., & Fazekas, G. (2023). SketchSynth: a browser-based sketching interface for sound control. New Interfaces for Musical Expression. DOI: 10.5281/zenodo.11189331 [34] Skach, S., Shepardson, V., & Magnusson, T. About TIME: Textile Interfaces for Musical Expression. 10.5281/zenodo.13904939 [35] Rossmy, B. (2022). Buttons, Sliders, and Keys - A Survey on Musical Grid Interface Standards. NIME 2022.  DOI: 10.21428/92fbeb44.563bfea9 [36] Côté, J. (2022). User-Friendly MIDI in the Web Browser. NIME 2022. DOI: 10.21428/92fbeb44.388e4764 [37] Rossmy, B., Rauh, M., & Wiethoff, A. (2022). Towards User Interface Guidelines for Musical Grid Interfaces. NIME 2022. DOI: 10.21428/92fbeb44.db84ecd0 [38] Zheng, J., & Bryan-Kinns, N. (2022). Squeeze, Twist, Stretch: Exploring Deformable Digital Musical Interfaces Design Through Non-Functional Prototypes. NIME 2022.  DOI: 10.21428/92fbeb44.41da9da5 
  