Women Who Build Things: Gestural Controllers, Augmented Instruments, and Musical Mechatronics   Sarah Reid  California Institute of the Arts  Valencia, CA  sreid@calarts.edu 
 Sara Sithi-Amnuai  California Institute of the Arts  Valencia, CA  sarasithiamnuai@alum.calarts.edu  
 Ajay Kapur  California Institute of the Arts  Valencia, CA  akapur@calarts.edu    ABSTRACT This paper presents a collection of hardware-based technologies for live performance developed by women over the last few decades. The field of music technology and interface design has a significant gender imbalance, with men greatly outnumbering women. The purpose of this paper is to promote the visibility and representation of women in this field, and to encourage discussion on the importance of mentorship and role models for young women and girls in music technology.   Author Keywords Gestural controller, Augmented instrument, Mechatronic, Women  CCS Concepts • Social and professional topics → Women; • Hardware → Emerging technologies; • Applied computing → Sound and music computing;   1. INTRODUCTION Over the last few decades, a large number of new digital music controllers and instruments have been built. Many of these new projects and advances—ranging from wearable gestural controllers to mechatronic music-makers—were designed and built by women. Nevertheless, there remains a significant gender imbalance in the field of music technology and interface design, with men far outnumbering women. Evidence of this can be seen in academic conferences [5][8], higher education music technology programs [2][3], and throughout music technology-related job fields [25].   It is important to note that this inequality is more than strictly an issue of participation: many of the contributions and accomplishments of women in this field are reduced in value or otherwise overshadowed by their male counterparts, resulting in a lack of equal visibility and representation—even for those who are actively contributing [15]. Author Tara Rodgers remarks that both ‘technology’ and ‘music’ are typically thought of as being predominantly male, and that the “trenchancy of associated gendered stereotypes seems to gain force when these fields converge” [37]. Lack of equal visibility and the presence of gendered stereotyping makes it difficult for young women to feel confident in their learning environment, find role models and mentors [15], and results in biased educational and creative programming. In turn, this may dissuade many young women from cultivating their interests and pursuing studies in technology-related fields [13].  
 In Women Composers and Music Technology in the United States [15], Elizabeth Hinkle-Turner provides a detailed account of the work and contributions of over one hundred women composers working with technology in the United States. She emphasizes that these women—and others around the world—have been largely omitted from concert programming, festivals, conferences, educational textbooks, and curricula. In exploring the relationship between gender and technology, one can trace the evolution of the word ‘technology’ from a term used in the 19th century to refer generally to the “skills of bakers, farmers, and teachers, as well as engineers” [6] to one that in the early 20th century came to be representative of male-identity and masculinity [32]. Regrettably, this gendered relationship between technology and masculinity persists today, and women are often—even if unconsciously—reduced to “mere onlookers” or “consumers” of technology while the men are the active participants, inventors, and builders [32] [6].   As educators in the field of music technology, the importance of acknowledging this potential bias and actively researching and documenting women’s achievements in music technology cannot be understated. In doing so, we are able to make steps toward establishing a more diverse and truly representative field [15][11]. The purpose of this paper is to expand on the work being done to promote and celebrate women’s contributions to music technology, focusing on a particular subset of the field that is not yet thoroughly documented: women who build hardware-based technologies for live performance.  The authors acknowledge that this collection of individuals and projects is by no means comprehensive. Due to the fact that there is not a substantial body of pre-existing research in this area (and in an attempt to expand beyond what has already been published) the authors solicited suggestions on projects and artists incorporating new hardware-based technologies from their personal and academic communities. This yielded a substantial collection of work, albeit one that is biased toward the authors’ community and geographical location. The authors acknowledge that this bias is present, and are committed to mitigating it as much as possible for future work.   The work presented in this paper is organized by project-type into three main categories:  1. Gestural Controllers: new interfaces with which to trigger, synthesize, or otherwise control sound; 2. Augmented Instruments: acoustic instruments that have been electronically augmented through the use of sensors; and 3. Mechatronic Instruments: instruments that produce sound using automatic mechanical parts, such as motors and gears. A number of the individuals presented span multiple categories and have prolific bodies of work. For the purpose of this paper, only one or two projects per artist will be highlighted.  2. GESTURAL CONTROLLERS The gestural controllers presented are broken down into three main categories: instrument-inspired controllers, alternate gestural con-
NIME Proceedings Template for LaTeX
Ben Trovato⇤
Institute for Clarity in
Documentation
1932 Wallamaloo Lane
Wallamaloo, New Zealand
trovato@corporation.com
G.K.M. Tobin†
Institute for Clarity in
Documentation
P .O. Box 1212
Dublin, Ohio 43017-6221
webmaster@marysville-
ohio.com
Lars Thørväld‡
The Thørväld Group
1 Thørväld Circle
Hekla, Iceland
larst@afﬁliation.org
Lawrence P . Leipuner
Brookhaven Laboratories
Brookhaven National Lab
P .O. Box 5000
lleipuner@researchlabs.org
Sean Fogarty
NASA Ames Research Center
Moffett Field
California 94035
fogartys@amesres.org
Anon Nymous
Redacted
8600 Datapoint Drive
San Antonio, Texas 78229
cpalmer@prl.com
ABSTRACT
This paper provides a sample of a LATEX document for the
NIME conference series. It conforms, somewhat loosely, to
the formatting guidelines for ACM SIG Proceedings. It is an
alternate style which produces atighter-lookingpaper and
was designed in response to concerns expressed, by authors,
over page-budgets. It complements the documentAuthor’s
(Alternate) Guide to Preparing ACM SIG Proceedings Us-
ing LATEX2✏ and BibTEX. This source ﬁle has been written
with the intention of being compiled under LATEX2✏ and
BibTeX.
To make best use of this sample document, run it through
LATEX and BibTeX, and compare this source code with your
compiled PDF ﬁle. A compiled PDF version is available to
help you with the ‘look and feel.’The paper submit-
ted to the NIME conference must be stored in an
A4-sized PDF ﬁle, so North Americans should take
care not to inadvertently generate letterpaper-sized
PDF ﬁles.This paper template should prevent that from
happening if thepdflatex program is used to generate the
PDF ﬁle.
The abstract should preferably be between 100 and 200
words.
Author Keywords
NIME, proceedings, LATEX, template
CCS Concepts
•Applied computing! Sound and music comput-
ing; Performing arts; •Information systems! Music
retrieval;
⇤Dr. Trovato insisted his name be ﬁrst.†The secretary disavows any knowledge of this author’s ac-
tions.‡This author is the one who did all the really hard work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
Please read the comments in the nime-template.tex
ﬁle to see how to create the CCS Concept Classiﬁ-
cations!
1. INTRODUCTION
The proceedingsare the records of a conference. ACM seeks
to give these conference by-products a uniform, high-quality
appearance. To do this, ACM has some rigid requirements
for the format of the proceedings documents: there is a
speciﬁed format (balanced double columns), a speciﬁed set
of fonts (Arial or Helvetica and Times Roman) in certain
speciﬁed sizes (for instance, 9 point for body copy).
The good news is, with only a handful of manual set-
tings,1 the LATEX document class ﬁle handles all of this for
you.
The remainder of this document is concerned with show-
ing, in the context of an “actual” document, the LATEXc o m -
mands speciﬁcally available for denoting the structure of a
proceedings paper, rather than with giving rigorous descrip-
tions or explanations of such commands.
2. THE BODYOF THE PAPER
Typically, the body of a paper is organized into a hierar-
chical structure, with numbered or unnumbered headings
for sections, subsections, sub-subsections, and even smaller
sections. The command\section that precedes this para-
graph is part of such a hierarchy.2 LATEX handles the num-
bering and placement of these headings for you, when you
use the appropriate heading commands around the titles of
the headings. If you want a sub-subsection or smaller part
to be unnumbered in your output, simply append an aster-
isk to the command name. Examples of both numbered and
unnumbered headings will appear throughout the balance
of this sample document.
Because the entire article is contained in thedocument
environment, you can indicate the start of a new paragraph
with a blank line in your input ﬁle; that is why this sentence
forms a separate paragraph.
1Two of these, the \numberofauthors and \alignau-
thor commands, you have already used; another,\bal-
ancecolumns, will be used in your very last run of LATEX
to ensure balanced column heights on the last page.2This is the second footnote. It starts a series of three
footnotes that add nothing informational, but just give an
idea of how footnotes work and look. It is a wordy one, just
so you see how a longish one plays out.
178
trollers, and audience-participation controllers. Each controller type is briefly defined at the start of the section.  2.1 Instrument-Inspired Controllers According to Miranda and Wanderly [30], instrument-inspired gestural controllers are controllers that take inspiration from pre-existing instruments, but do not try to reproduce their features exactly. For this section, the authors are considering standard audio equipment such as microphones and speakers as ‘instruments’ in addition to traditional musical instruments.  Inspired by turntables and hip hop music, Myriam Bleau1 created a series of acrylic tops with embedded sensors called Soft Revolvers (Figure 1). As the tops are spun, data captured by the sensors is used to activate and control different audiovisual processes. This work received an honorable mention for the Prix Ars Electronica in 2015 [34] and 1st prize in the “digital art” category of the 2015 Sonic Arts Award.  
 Figure 1. Overhead view of Soft Revolvers  The Abacus (Figure 2), built by Kristina Warren2 (in consultation with Bussigel, et al.), attaches onto a standard vocal microphone to give the performer easily-accessible control over live processing. The design of this controller was influenced by Warren’s desire for an integrated performance practice and a stronger connection with her audience. The Abacus uses an Arduino microcontroller, a series of toggles for control input, and LEDs for visual feedback [42].  
Figure 2. Abacus (version 4)         Figure 3. Feedback instruments  Lesley Flanigan3 builds and performs with feedback instruments inspired by traditional speakers, incorporating suspended piezo discs and found speakers (Figure 3). In 2007 she created a “Feedback Synth,” a wooden instrument containing five small speaker and piezo pairings. She uses subtle hand gestures and microphone movement to explore controlled feedback and the unique character of each object [28].                                                                       1 http://www.myriambleau.com/softrevolvers 2 http://kmwarren.org 3 http://lesleyflanigan.com 4 https://rucyl.com 
2.2 Alternate Gestural Controllers In contrast to instrument-inspired controllers, alternate gestural controllers do not bear a resemblance to pre-existing instruments, nor do they necessarily seek to reproduce any of their sonic features [30].  Laurie Anderson, a pioneer of live performance with technology, designed a wireless gestural controller called the Talking Stick in collaboration with Interval Research Corporation and Rob Bielecki. The Talking Stick is an approximately six-foot-long baton outfitted with sensors to capture movements and hand positions, which are used to interface with sample playback and granular synthesis processes [23]. Anderson used the Talking Stick in her performance of Songs and Stories of Moby Dick [16].  The Chakakhantroller (Figure 4) is a wearable MIDI controller fabricated out of snakeskin and leather by Rucyl Mills.4 Wearing the Chakakhantroller, the performer is able to manipulate and trigger video, a bank of samples, and 4 channels of audio, while maintaining freedom of movement away from the laptop computer [29].  Laetitia Sonami5 built the Lady’s Glove to encourage freer collaboration between her imagination and her instrument, and to allow audience engagement to occur “through the witnessing of the gestures [20].” Beginning with the first prototype in 1991, the Lady’s Glove contains numerous embedded sensors, giving Sonami the ability to control sonic and visual parameters of her live performance with subtle gestures and finger movements [37]. More recently, Sonami developed the Spring Spyre (Figure 5), a circular spring-based interface that uses Rebecca Fiebrink’s machine learning software Wekinator [6]. Sonami’s mentor and electronic music pioneer Eliane Radigue composed a piece for the Spring Spyre titled “OCCAM IX”, which was premiered in 2013 [40].  
 Figure 4. Chakakhantroller     Figure 5. Spring Spyre  Another glove controller, the eXo SkeL, was designed and built by Lyn Goeringer.6 In Goeringer’s solo performance of “Atrium,” she utilizes the eXo SkeL in tandem with movement and intricate hand gestures to control multiple channels of video and audio.   Glove-based controllers have also been adopted by popular music artists, such as Imogen Heap. Heap worked with a team of engineers and designers to create the Mi.Mu Gloves,7 a wireless glove controller with orientation and flex sensors, as well as vibration motors to transmit haptic feedback to performers. Since their public release in 2015, the gloves have been used by a number of different artists and musicians including Kris Halpin and Ariana Grande.  Dancer and ethnomusicologist Tomie Hahn8 developed SSpeaPer in collaboration with Curtis Bahn [10]. SSpeaPer is a wireless interface that is worn by a dancer and uses tilt sensors to detect motion. This gestural information is transmitted to a computer running Max/MSP and used to control sound synthesis and sample playback. These sounds are then sent back to the performer and played through speakers attached to the performer’s body. Hahn performs with 
5 http://sonami.net 6 http://www.lyngoeringer.com 7 https://mimugloves.com 8 http://www.arts.rpi.edu/~hahnt 
179
SSpeaPer as PikaPika, “a character influenced by anime and manga, Japanese pop animation and comics [10].”  Michela Pelusio created SpaceTime Helix, an audiovisual project that features “a giant spinning standing wave in a white string.”9 When SpaceTime Helix is active, it produces an illuminated helicoid extending vertically from floor to ceiling, up to 12 meters in length (Figure 6). According to Pelusio, this work “is a metaphor for the return to the root of things, and to the simplicity and the complexity of the macro and microcosmos.” In collaboration with Mika Satomi10 and Hannah Perner-Wilson,11 Pelusio built the Corset Breathing Sensor (Figure 7) to control the Helix in live performance. This wearable controller wraps around the performer’s waist and uses fabric stretch sensors to detect inhalation and exhalation.  
 Figure 6. Pelusio performing with SpaceTime Helix  Pamela Z12 also performs with a wearable controller called the BodySynth, developed by Chris Van Raalte and Ed Severinghaus. The BodySynth uses electrodes attached to the performer’s body to detect muscle contractions and movement. Using this controller, even subtle gestures can be used to trigger samples and manipulate sound in real time.   
 Figure 7. Corset Breathing Sensor     Figure 8. Lilypad Arduino  A significant contribution to the field of wearable technology was made by Leah Buechley,13 who developed the LilyPad Arduino (Figure 8) [4]. This microcontroller can be sewn directly into clothing and other electronic textile projects. Buechley founded the High-Low Tech group at MIT Media Lab, and received the Edith Ackerman award for Interaction Design and Children in 2017.  Margaret Schedel14 has done extensive work in the area of interactive media and live performance, exploring a variety of different gestural controllers and modes of interaction. In one project from 2011, Schedel collaborated with Phoenix Perry15 and Rebecca Fiebrink on an audiovisual performance called Monster. The project                                                                      9 http://maakali.org 10 http://www.nerding.at 11 http://www.plusea.at 12 http://www.pamelaz.com 13 http://leahbuechley.com 
used the Wekinator machine learning software, Kinect motion sensor, and Keith McMillen K-Bow to create an interactive environment that can be controlled through physical gestures in real time [38].  2.3 Participation Controllers   For the purpose of this paper, the term “participation controller” is being used to refer to gestural controllers that require some degree of audience or public interaction to be experienced, instead of being used primarily by a performer in a more traditional setting.   Kaffe Matthews transforms objects such as park benches, beds, bicycles, and kayaks into musical instruments that respond to physical input [9][37]. Since 2012, Matthews has been developing a series of interactive works for Sonic Bike:  bicycles outfitted with speakers and sensors that produce sound based on how—and where—people ride them (Figure 9). According to Matthews, “…the sonic bike creates an outdoor listening experience for everyone – reaching new audiences on the street or hillside, far beyond the confines of concert halls and galleries.” 17  
 Figure 9. Sonic Bike Kaffe Matthews was the first woman to receive the Edgar Varèse Guest Professorship at the Technical University of Berlin (summer 2016), where she taught and continued development on the Bicrophonic Research Institute (BRI) [26].  The PlaySoundGround, designed by Sasha Leitman and Michael St. Clair, is an adult-sized playground consisting of swings, a see-saw, and a climbing structure all containing embedded sensors [22]. The sensors register the interactions and movements of participants as they explore the playground, and translate them into digital sounds. Sasha Leitman18 currently teaches public workshops on new interface design, fabrication, and computer music at Stanford University and heads the Max Lab at Stanford University’s Center for Computer Research in Music and Acoustics (CCRMA).   Charlotte Parallel built a portable “light-to-sound transducer” that enables her to explore outdoor spaces while transforming light and electrical energy into sound. By connecting a solar panel to an audio jack, Parallel is able to convert light from illuminated street signs and shop windows into “a time-varying electrical signal. This small electrical signal can then be fed to an audio signal amplifier and is then converted into sound [34].” Parallel notes that these explorations through city streets and public spaces often become performative acts, in which the site itself and the people moving through it inform and interact with the sonic experience [34].    
14 http://schedel.net 15 http://phoenixperry.com 17 http://www.kaffematthews.net 18 http://www.sashaleitman.com 
180
3. AUGMENTED INSTRUMENTS Augmented instruments—also known as hyper, hybrid, meta, or extended instruments—are acoustic instruments that have been augmented through the addition of sensors and electronics, to provide extended control during live performance [30][35].  The Koto Monster is an electronically augmented Koto that was developed by Miya Masaoka19 and colleagues at STEIM and CNMAT. Rings with embedded ultrasound sensors and a series of foot pedals allow Masaoka to trigger samples and process sounds in real time. Developing out of the Koto Monster, Masaoka’s Laser Koto utilizes four vertically stacked laser beams as well as light sensors and proximity sensors to detect hand position and interaction with the beams (Figure 10) [24].  Brenda Hutchinson20 has been performing with the Long Tube—a 9.5-foot aluminum tube instrument—for over two decades [15]. In 2000, she augmented the acoustic Long Tube by adding a gestural controller onto it (Figure 11), which enables her to incorporate playback of pre-recorded material as well as live sampling and processing into her performances [17].  
 Figure 10. Laser Koto       Figure 11. Long Tube interface  In 2006, violinist, composer and researcher Mari Kimura21 was introduced to the IRCAM Real Time Interaction Team’s Augmented Violin system, which included a six-axis motion sensor attached onto a violin bow [1][11]. Interest in this project led to an ongoing collaboration between Kimura and the IRCAM team as well as numerous new compositions using the Augmented Violin system. More recently, Kimura has developed µgic (in collaboration with Liubo Borissov): an Arduino-based glove worn on the violinist’s bow hand containing a nine-axis motion sensor to capture gestural information [21].  Cléo Palacio-Quintin developed the Hyper-Flute (Figure 12), an acoustic flute augmented with pressure sensors, magnetic field sensors, ultrasound transducer, tilt sensor, and a series of buttons [33].   
 Figure 12. Hyper-Flute           Figure 13. Augmented flute prototype During a residency at STEIM in 2010, MaryClare Brzytwa23 also built an electronically augmented flute (Figure 13). The prototype featured three multi-functional buttons mounted onto the flute to control various parameters in Max/MSP.                                                                      19 http://www.miyamasaoka.com 20 http://www.sonicportraits.org 21 http://www.marikimura.com 23 http://www.maryclarebrzytwa.com 
 Nicole Carroll24 built an Arduino-based augmented bassoon incorporating a series of pressure and vibration sensors. The data collected by the sensors is used to trigger and control sampling, playback, and effects processing in Max/MSP.   The author draws inspiration from all of the work presented for the design and development of MIGSI: The Minimally Invasive Gesture Sensing Interface for trumpet (Figure 14). MIGSI attaches onto the valve casing of any standard B-flat of C trumpet and uses a series of optical, pressure, and motion sensors to capture gestural information from the performer and instrument during live performance [36]. Since the completion of the prototype in 2015, the author has been performing extensively with MIGSI and developing a body of repertoire for augmented trumpet, including a new work in which MIGSI controls a series of mechatronic percussion instruments built by Kapur, Trimpin, and students at California Institute of the Arts [19].  
 Figure 14. MIGSI on a B-flat trumpet  4. MECHATRONIC INSTRUMENTS Mechatronic musical instruments—also referred to as robotic instruments—use mechanical components such as motors, gears, and solenoids to pluck and bow strings, strike membranes, and otherwise produce sound [19].  Bridget Johnson created speaker.motion (Figure 15), a mechatronic loudspeaker instrument designed to facilitate dynamic spatialized performances through automated speaker movement [18]. The speaker.motion system consists of four loudspeaker units that can be independently rotated and tilted using a servomechanism and stepper motor.   Stephanie Smith25 built a motor-controlled instrument called the Bell Controller (Figure 16), which uses an array of 15 motors to control small bells. Each bell has a single LED attached to it, which lights up when activated by the motor, resulting in delicate jingles and flickers [39].  
 Figure 15. speaker.motion      Figure 16. Bell Controller  Ensemble Robot, co-founded by Leila Hasan and Christine Southworth, is group of musicians and technologists who build new musical robots for live performance and collaboration. Ensemble 
24 http://nicolecarrollmusic.com/lunchbox 25 http://music.stephiescastle.com 
181
Robot has built and performed with numerous mechatronic instruments including the Heliphon, the Bot(I)Cello, the Beatbot, the Blobot, and the Whirlybot [41]. Additionally, Hasan created the Termenova (in collaboration with Yu and Paradiso), a hybrid mechatronic/gestural controller that uses lasers and optical sensors to detect hand movements (Figure 17). The lasers are all independently controllable with servo motors, allowing for fine tuning and calibration for each performer [12].   
 Figure 17. Termenova  Motivated by the desire to facilitate access to the inside of a piano for preparations and extended techniques, Sarah Nicolls created the Inside-Out Piano (Figure 18). The Inside-Out Piano is equal in sound and size to a grand piano, but features an open body with exposed strings standing vertically and perpendicularly to the keys. In addition to modifying the layout of the piano, Nicolls installed motors and solenoids to activate the strings independent from the performer [31].  
 Figure 18. Inside-Out Piano  5. CONCLUSION Throughout history, women have played pioneering roles in the development of computer science and electronic music. Nevertheless, these fields remain predominantly male, with many accomplished female scholars, technologists, and musicians failing to receive adequate acknowledgment and visibility for their work. Recent studies [5][8] have shown that men greatly outnumber women in terms of authorship at academic conferences such as New Interfaces for Musical Expression (NIME), International Computer Music Conference (ICMC), and Sound and Music Computing (SMC), with female participation ranging from 9.5–14.3% between 2004 and 2016.  This issue is not limited to academia: in the field of audio engineering, for example, women make up less than 15% of the population [25]. A recent study conducted by Born and Devine looking at undergraduate music technology programs in Britain found that nearly 90% of music technology students are male [2].   Without visible and accessible teachers, mentors, and role models, many young girls interested in technology-related fields end up abandoning the field of study well before reaching college, instead choosing a focus in which they will receive more support and mentorship [13][27]. For the relatively small number of women who do enter into these programs, having access to a female mentor can provide “a valuable ally and friend, giving specific guidance and encouragement that even the most understanding male cannot provide [15].” In an interview with Tara Rodgers, Laetitia Sonami speaks 
about how the mentorship she received from Eliane Radigue was vital to the development of her electronic music practice, providing her with support and opportunities that she was unable to find elsewhere [36].  In her article A Tool is a Tool [43], Pamela Z discusses the importance of mentorship for young women and the bias that women working with technology experience regularly: “…our culture has always socialized women to feel less confident working with mechanical or electronic devices, and people in general continue to have less confidence in women's abilities with them.” Pamela Z remarks that she was motivated to offer workshops specifically for low-income women after realizing that a large number of women around her felt intimidated and discouraged to pursue their studies due to “unequal treatment by instructors, classmates, fellow musicians [43].” Her workshops created a safe, supportive learning environment for these students in which they could take risks and flourish.  In his 2003 study [5], Essl noted that in addition to a participation-based gender imbalance in the field of music technology and interface design, there is also a “marked absence of documented gender awareness in the field.” Research shows that fostering a learning environment in which women feel supported and encouraged has a positive impact on their likelihood to persist and succeed in technology-related fields [13]. Educators at all academic levels can work toward creating supportive classroom environments and providing meaningful opportunities for their students to find female role models by actively integrating the contributions, music, writing, and histories of the women presented here—and others around the world—into their own curricula and course materials.  6. FUTURE WORK The authors would like to reiterate that this list of individuals is far from exhaustive. The collection of projects presented in this paper represents only a small portion of the contributions made by women in the field of hardware-based technology for live performance. The majority of the work included was discovered through word of mouth within the authors’ academic communities, as well as through published papers and self-published documentation on personal artist websites.   Future work will expand on this research in an effort to be more inclusive. The authors endeavor to move well beyond their own communities, with the goal of representing a larger population and broader geographical and cultural reach. Additionally, plans for future research include contacting each artist personally to include interviews and quotes from the artists about their work.   Another area of interest for future work involves taking a deeper look into mentorship and education in the field of music technology, examining trends and connections between artists as they receive and offer mentorship and support among one another. Many of the individuals included in this paper have contributed to this field not only creatively but as mentors and educators themselves, through academic positions, workshops and meet-ups, or as private instructors. Future work will endeavor to discuss these roles and initiatives with the artists, and continue the discussion on how we can work toward not only creating an inclusive, supportive learning environment for students, but a generally inclusive educational space in which women are able to take on leadership and mentorship roles.   7. ACKNOWLEDGMENTS Thank you to the reviewers for their valuable feedback, which was instrumental in pushing this paper forward. Special thanks to Lauren Pratt, Kathy Carbone, Ryan Gaston, David Rosenboom, Robert Wannamaker, and Perry Cook for their endless support and collaboration.  
182
8. REFERENCES [1] F. Bevilacqua, et al. The Augmented Violin Project: Research, Composition and Performance Report. In Proceedings of the 2006 Conference on New Interfaces for Musical Expression, (Paris, 2006), 402–406. [2] G. Born and K. Devine. Music technology, gender, and class: Digitization, educational and social change in Britain. Twentieth-Century Music, 12, 2 (2015),135–172. [3] G. Born and K. Devine. Gender, Creativity and Education in Digital Musics and Sound Art. Contemporary Music Review, 35, 1 (2016), 1–20. [4] L. Buechley and M. Eisenberg. The LilyPad Arduino: Toward Wearable Engineering for Everyone. Wearable Computing Column in IEEE Pervasive, 7, 2 (2008): 12–15. [5] G. Essl. On Gender in New Music Interface Technology. Organised Sound 8, 1 (April 2003), 19–30. [6] R. Farrugia. Beyond the Dance Floor: Female DJs, Technology and Electronic Dance Music Culture. Bristol; Chicago, Ill.: Intellect Ltd, 2012. [7] R. Fiebrink. Machine Learning as Meta-Instrument: Human-Machine Partnerships Shaping Expressive Instrumental Creation. In: T. Bovermann, et al. (eds) Musical Instruments in the 21st Century. 2016, 137-151. [8] E. Frid. Sonification of Women in Sound and Music Computing: The Sound of Female Authorship in ICMC, SMC and NIME Proceedings. In Proceedings of the International Computer Music Conference, (2017). [9] A. Griffiths, et al. Sonic Kayaks: Environmental monitoring and experimental music by citizens. PLoS Biol, 15, 11 (2017). [10] T. Hahn and C. Bahn. Pikapika—the collaborative composition of an interactive sonic character. Organised Sound, 7, 3 (2002) 229–238. [11] T. Hall. “Artist Focus: Mari Kimura.” Available at https://cycling74.com/articles/artist-focus-mari-kimura. [12] L. Hasan, et al. The Termenova: A Hybrid Free-Gesture Interface.  In Proceedings of the International Conference on New Interfaces for Musical Expression, (Dublin, 2002). [13] C. Hill, C. Corbett, and A. St Rose. Why so few? Women in Science, Technology, Engineering, and Mathematics. ERIC, 2010. [14] E. Hinkle-Turner. Women and music technology: pioneers, precedents and issues in the United States. Organised Sound, 8, 1 (2003) 31–47. [15] E. Hinkle-Turner. Women Composers and Music Technology in the United States: Crossing the Line. Ashgate Publishing, Ltd., 2006. [16] C. Horsley. Laurie Anderson: Songs and Stories from Moby Dick. Available at: http://www.thecityreview.com/laurie.html [17] B. Hutchinson. ‘Let’s Just See What Happens’ for Long Tube and Gestural Interface. In Proceedings of the International Conference on New Interfaces for Musical Expression, (2007), 438–438. [18] B. Johnson, M. Norris, and A. Kapur. speaker.motion: A Mechatronic Loudspeaker System for Live Spatialisation. In Proceedings of the International Conference on New Interfaces for Musical Expression, (2016), 41–45. [19] A. Kapur. A History of Robotic Musical Instruments. In Proceedings of the International Computer Music Conference, (2005). [20] E. Karp. Artist Interview: Laetitia Sonami. Available at: http://www.somarts.org/laetitiasonami/ [21] M. Kimura, N. Rasamimanana, and F. Bevilacqua. Extracting Human Expression for Interactive Composition with the Augmented Violin. In Proceedings of the International Conference on New Interfaces for Musical Expression, (Michigan, 2012). 
[22] S. Leitman, et al. PlaySoundGround: An Interactive Musical Playground. In Proceedings of the International Conference on New Interfaces for Musical Expression, (Pittsburgh, 2009), 293-296. [23] D. Levitin, S. McAdams, and R. Adams. Control Parameters for Musical Instruments: A Foundation for New Mappings of Gesture to Sound. Organised Sound, 7, 2 (2002): 171–189. [24] M. Masaoka. Things in an open field (for laser koto). In Proceedings of the International Conference on New Interfaces for Musical Expression, (New York, 2007). [25] M. Mathew, J. Grossman, and A. Andreopoulou. Women in Audio: Contributions and Challenges in Music Technology and Production. Audio Engineering Society 141st Convention, (Los Angeles, Sept. 29–Oct. 2, 2016). [26] K. Matthews. Bicrophonic Research Institute. 2017. Available at: http://sonicbikes.net/sonic-bike/#sensory-bike [27] A. McCartney and E. Waterman. In and Out of the Sound Studio: Introduction. Intersections: Canadian Journal of Music/Intersections: Revue Canadienne de Musique. 26, 2 (2006): 3–19. [28] T. Miller. The Speaker is Present: A Conversation with Lesley Flanigan. Impose Magazine, 2016. Available at: http://www.imposemagazine.com/features/lesley-flanigan-hedera-interview [29] R. Mills. Chakakhantroller. Available at: https://rucyl.com/_Chakakhantroller [30] E. Miranda, and M. Wanderley. New Digital Musical Instruments: Control and Interaction Beyond the Keyboard. A-R Editions, Inc., 2006. [31] S. Nicolls. Inside Out Piano I. Available at: http://sarahnicolls.com/inside-out-piano/ [32] R. Oldenziel. Making Technology Masculine: Men, Women, and Modern Machines in America, 1870-1945. Amsterdam University Press, 2010. [33] C. Palacio-Quintin. The hyper-flute. In Proceedings of the International Conference New Interfaces for Musical Expression, (2003), 206–207. [34] C. Parallel. Transductions: Transforming Light into Sound. eContact! 19.2, (2017) [35] D. Radford. Myriam Bleau Spins Soft Revolvers. Music Works. Available at: https://www.musicworks.ca/featured-article/visions-sound/myriam-bleau-spins-soft-revolvers [36] S. Reid, R. Gaston, C. Honigman, and A. Kapur. Minimally Invasive Gesture Sensing Interface (MIGSI) for Trumpet. In Proceedings of the International Conference on New Interfaces for Musical Expression, (Brisbane, 2016), 419–424.  [37] T. Rodgers. Pink Noises: Women on Electronic Music and Sound. Duke University Press, 2010. [38] M. Schedel, P. Perry, and R. Fiebrink. Wekinating 000000Swan: Using Machine Learning to Create and Control Complex Artistic Systems. In Proceedings of the International Conference on New Interfaces for Musical Expression, (Oslo, 2011), 453–456. [39] S. Smith. Bell Controller (motor array). Available at: http://music.stephiescastle.com/music/bell-controller-motor-array [40] L. Sonami. Program notes: CCRMA Presents Laetitia Sonami. CCRMA Stage, Stanford, 13 October 2016. [41] C. Southworth and L. Hasan. Ensemble Robot. Available at: http://www.ensemblerobot.org/robots.shtml [42] K. Warren. Composing and Performing Digital Voice Using Microphone-Centric Gesture and Control Data. In Proceedings of the International Computer Music Conference, (2016). [43] P. Z. J. Malloy (ed.) A Tool is a Tool. Women, Art, and Technology, MIT Press, 2003: 348–361
183