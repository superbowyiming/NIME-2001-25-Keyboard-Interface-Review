 Music Aid - Towards a Collaborative Experience for Deaf and Hearing People in Creating Music  Ene Alicia Søderberg, Rasmus Emil Odgaard, Sarah Bitsch, Oliver Høeg-Jensen, Nikolaj Schildt Christensen, Søren Dahl Poulsen and Steven Gelineck*  Department of Architecture, Design & Media Technology, Aalborg University CPH A.C. Meyers Vænge 15, 2450 Copenhagen SV {esader, rodgaa, sbitsc, ohaegj, nsch, sdpo}15@student.aau.dk, *stg@create.aau.dk     ABSTRACT This paper explores the possibility of breaking the barrier between deaf and hearing people when it comes to the subject of making music. Suggestions on how deaf and hearing people can collaborate in creating music together, are presented. The conducted research will focus on deaf people with a general interest in music as well as hearing musicians as target groups. Through reviewing different related research areas, it is found that visualization of sound along with a haptic feedback can help deaf people interpret and interact with music. With this in mind, three variations of a collaborative user interface are presented, in which deaf and hearing people are meant to collaborate in creating short beats and melody sequences. Through evaluating the three prototypes, with two deaf people and two hearing musicians, it is found that the target groups can collaborate to some extent in creating beats. However, in order for the target groups to create melodic sequences together in a satisfactory manner, more detailed visualization and distributed haptic output is necessary, mostly due to the fact that the deaf test participants struggle in distinguishing between higher pitch and timbre.   Author Keywords Musical Collaboration, Hearing Impaired, Multi-touch, Multi-modal, Screen Based Interaction, Design for all, Exploratory Research.   ACM Classification H.5.2 [Information Interfaces and Presentation] User Interfaces—Input devices and strategies, H.5.5 [Information Interfaces and Presentation] Sound and Music Computing.  1. INTRODUCTION The different ways in which deaf and hearing people perceive and experience music create a social barrier between the two groups when interacting in certain musical contexts. Although deaf people do not experience music in the same way as hearing people they still perceive certain aspects of the music that collectively provide them with an overall artistic experience. Extending this we wish to not only explore how deaf people themselves can construct these experiences (i.e. creating music), but also to explore how the musical creation can take place in collaboration with hearing people, thus breaking the social boundaries found between the deaf and 
hearing. One particular context where such collaboration could be beneficial is in early education, where development of communication and collaboration between hearing and deaf children could lead to increased inclusivity from a very early age. The motivation for exploring this problem area originates from the fact that there are not many simple and usable ways in which deaf and hearing people can create music together.  Initial empirical work has been carried out in order to investigate how deaf people understand and experience music. Furthermore the investigation explored what needs deaf people might have in terms of an interface for creating music in collaboration with hearing people. This lead to the development of three alternative touch-screen based prototypes that focused on various forms of visual audio representation and explored different layouts for collaboration. These prototypes were finally evaluated in order to assess which elements are important for both target groups. The paper is organized as follows: Section 2 discusses related works within musical perception for deaf people, within visual and haptic representation of sound, and within collaborative user interfaces. Section 3 presents the overall goals and methodology used to explore the problem area. Section 4 shortly presents a user study carried out on deaf and hearing target groups, leading to Section 5, which describes the design and implementation of the initial interface. Section 6 describes the qualitative evaluation carried out. Finally, Section 7 discusses the results and elaborates on how to extend these towards future research within the field. 2.  RELATED WORKS As sound is not only transmitted through air, but also through vibrations in the floor, walls and other physical objects, it can be sensed by the whole body and not only through our auditory system. This plays a particular role in how deaf people experience music and it has been found that these vibrations are sensed in the same part of the brain used for hearing [1]. Nanayakkara et al. [2] conducted a survey asking 19 profoundly deaf and 22 partially deaf participants a series of questions regarding their experience with listening to music and about how their musical perception could be enhanced by additional visual and haptic stimuli. When asked which factors were most dominant for them when engaging in musical activity, 32% of the participants answered that they prefer to watch a visual display, while 27% prefer to feel vibrations. Others factors included watching an artist and their body language (13%) and hearing sound even if only partially (15%). More than half of the participants reported having used assistive graphical and/or haptic devices and of those 94% found them useful.  2.1 Visual feedback The enhancement of auditory stimuli with visual representation of sound is used extensively in audio manipulation software – see for 
321
instance [3], or in visualizers such as Windows Media Player [4]. Traditionally we distinguish between symbolic representation (for instance sheet music) and analogic representation of sound (level meters, frequency spectra, etc.).  Examples of systems that visualise musical output include work by Ferguson et al. [5], who visualised harmonic content, noisiness, pitch and loudness using spheres with various sizes, direction of particle flow, slant and length of sphere group respectively, in order to provide visual feedback for training musicians. Likewise, Nijs et al. [6] explored visualization of both audio and player movement for increasing immersion and flow and thus creativity, while learning to improvise using a musical instrument. Fourney & Fels [7] explored representations of MIDI data using different animation types for visualizing music for hearing disabled and Perrotin & d’Alessandro [8] explored visual feedback for improving audiences’ understanding of musical intend and expressivity at concert performances.  Nanayakkara [9] presented two overall forms for displaying audio visualisation: (1) the piano roll represents both past, current and future sounds as seen in most MIDI editing software, and (2) the movie roll, which only visualizes what sounds are played at any given moment. The movie roll was evaluated to be the more natural form of representation because it mimics how sound is also perceived in the moment it occurs [9]. However, since we are dealing with a music creation tool we consider the piano roll to provide a better overview.  A combination of the two would be beneficial.  When creating a music visualizer for increasing understanding and enjoyment of music among hearing disabled, Pouris & Fels [10] argued that perceived musical emotion (valence, arousal and tension) is dependant on pitch, tempo and loudness. They created a visualizer that mapped pitch, tempo and loudness of different MIDI instruments to various 3D graphics properties such as colour brightness, size and position in order to visualise emotion in the music.  2.2 Haptic Feedback Besides visualisation of sound, deaf people’s music perception can be enhanced or somewhat substituted by haptic or vibrotactile feedback. Several forms of haptic feedback exist for enhancing audio-visual content – see [11] for an extensive review of haptic-audiovisual content creation.  Examples of devices using haptic feedback include haptic chairs [2][12], haptic body suits [13] or other types of haptic wearables [14][15]. A typical strategy is to filter the audio content into sub-bands and distribute vibrations spatially on the body [12] in order to provide a sensation of distributed frequency content. This also helps when trying to perceive higher frequencies, which are not as easily perceived through haptic vibrations as lower frequencies. Birnbaum and Wanderley [16] present a thorough review of how sound perception properties such as pitch, loudness and brightness might be substituted or mapped to vibrotactile sensation. Unfortunately, the project presented here has not put emphasis on exploring various strategies for delivering vibrotactile feedback and thus a simple setup was implemented using a bass amplifier that produced vibrations through the table on which the touch screen interface was standing. In that sense we have not as such augmented the sound with vibrotactile feedback, but rather provided inherent vibrotactile feedback of the sound being produced.  2.3 Collaborative Musical Instruments Several attempts at creating collaborative musical instruments have been made where probably the most prominent example is the Reactable [17], that lets every collaborating musician have an equal influence on the sound being produced. When creating music in a collaborative environment, communication is important. There needs to be a mutual understanding of what  
 Figure 1. Left: Screenshot of Patatap. Right: Screenshot of ToneMatrix. the participants are doing together and what they want to do next. This can be achieved by simply talking with each other.  Here we have a challenge as the deaf and the hearing individual may not be able to communicate through language, which is why it is important that both individuals are aware of what  happens on the screen and that they know what every interface element does – i.e. clear affordances. This also means that the interface needs to be intuitive and easy to work with for all users [18]. Jordá [19] argues that each user should have equal influence on the output and should at all times be able to see/hear what the other user is doing. This creates motivation for more interplay and flexibility, which will influence the output and therefore user experience in a unique way depending on each individual.  3. GOALS & METHODOLOGY The overall goal of the research was to explore how to design an interface that firstly would let a deaf person create music and secondly how that interface would support collaboration between hearing and deaf people. In order to gain an understanding of this somewhat novel research area a bottom up approach was used. An initial interview with a deaf music performer (sign language performer) was used to inform the design requirements from the point of view of a deaf participant. Additionally, a survey was sent to 19 musicians with varying musical background assessing the potential of such an interfaces from the point of view of the hearing participant. Three alternative prototypes were developed based on needs elicited from the above activities. These prototypes were then qualitatively evaluated, in a controlled environment where hearing and partially deaf participants (almost completely deaf) explored the prototypes, followed by in-depth interviews with the two target groups. The goal of the exploration test and the interviews was to gain an understanding about which elements (visual/haptic feedback or collaborative elements) worked to enhance the deaf’s possibility of understanding and collaborating on making music, on the same level as the hearing. 4. USER NEEDS As mentioned above, an initial interview was carried out with a deaf music performer. The interview, which took around 1.5 hours was semi structured with the goal of understanding how she experienced and perceived various musical properties such as different sounds, instruments, rhythms, pitch, and vibrations. Furthermore we were interested in understanding how especially visual feedback could aid in that experience. Different interfaces were shown during the interview to spark the imagination of the participant in terms of what might / might not work in different contexts.  The participant explained how especially low pitched sounds and visualizations of the sound help her experience music and how most instruments are not distinguishable from each other. Rhythms are easier to hear and feel and she describes how she sometimes takes off her hearing aid to focus solely on bass and rhythm. Visuals are helpful since they provide information on what she cannot feel, but she also points 
322
out that visuals can be too abstract – she asks whether it is possible to just use pictures of the instruments that are playing. The participant found especially two interfaces intuitive, ToneMatrix1 and Patatap2 (see Figure 1). These have been an important inspiration for designing the main interface described in Section 5. A short qualitative survey was sent to 19 musicians in order to establish an understanding of some basic needs on behalf of the target group of hearing musicians and to see whether and how hearing musicians would be motivated to work with deaf people in general.  Responses were generally positive emphasizing how it would be interesting to gain a whole different perspective on the musical output, potentially taking the participants to places musically, they would not go on their own. Participants expressed basic needs of being able to construct melodies and beats, to be able to have many different sounds at one’s disposal and to have the opportunity to record one’s own instruments into the music tool.  5. DESIGN & IMPLEMENTATION From the analysis and empirical user study the following design criteria were outlined:   • Be able to make simple beats • Be able to make simple melodic sequences • Include several choices of sound • Explore Visual Feedback o Movie roll and/or piano roll representations o Tempo o Pitch o Type of instrument • Haptic feedback through vibrations • Explore split or shared screen • Each user’s input has equal impact on the output  In order to evaluate the importance of different elements we decided to develop three variations of the same overall interface – see Figure 2, 3 & 4. We considered mapping spectral analysis features to visual properties, but since the empirical research pointed us towards a simple music creation tool we chose a simple symbolic representation of different percussive instruments (represented by simple animations) and pitch represented by colour brightness. Further considerations included mapping the overall mood of the music to ambient changes in background colour, however, a suitable implementation was not developed in time for the evaluation.  All three prototypes were programmed in Processing 3.0 using pre-recorded samples and deployed on a 27” touch screen using a PQ Labs G43 infrared multi touch frame on top of a flat screen LCD. 5.1 Interface 1 – shared interaction space Interface 1 (see Figure 2) contained a 2-dimensional matrix of on/off buttons used to toggle audio samples on/off as seen in many musical sequencers. The matrix was divided into two sections, the upper for melodic sequencing, and the lower for beat sequencing. The prototype contained animated visuals that were triggered for each of the beat section instruments. The colors of the buttons for the melodic sequence varied in brightness corresponding to the pitch of the represented sample.                                                                     1 http://tonematrix.audiotool.com 2 http://www.patatap.com 3 http://multitouch.com 
Figure 2. Overall layout of the graphical user interface. The two 2D-grids in the centre represent melody and beats section, while large icons in the sides are animated to provide visual feedback connected to the beats section.  
 Figure 3. Interface 2 is the same as interface 1 but with no animated visuals. 
 Figure 4. Interface 3 explores dividing the grid between the users. The users was able to toggle between three different sound libraries in order to explore more variety. A bass amplifier was placed so its vibrations were sent through the table upon which the touch screen was standing. 5.2 Interface 2 – limited visual feedback Interface 2 (see Figure 3) worked in exactly the same way as Interface 1 – however, it did not implement the animated visuals representing the beats section. This interface variation was included to let the users experience whether those visuals had an actual effect and provided information for the deaf users.  5.3 Interface 3 – separate interaction space Interface 3 (see Figure 4) was also a variation of Interface 1, but this time the same grid was divided into two sections representing half of the loop each. We wanted to explore whether the test participants preferred their own separate interaction space, or a shared interaction space. 
323
  
Figure 5. Left: handheld camera, filming user reactions. Right: mounted camera, filming close-up interaction.6. EVALUATION We want to emphasize here that the reason for including three variations of the interface in the evaluation was not to test which one was best. Rather, it was to explore the impact of certain elements implemented in the three variations. The overall goal of the evaluation was to explore the following: ● Does it make sense to either of the target groups to collaborate on the interface? ● Which elements implemented in the three prototypes are important for musical collaboration between the two target groups? ● Did the representations of rhythm, pitch, as well as the enhanced vibrational output, provide sufficient information to the deaf user? 6.1 Test Procedure The test was set up in a band rehearsal room and lasted for approx. two hours in total. Present were two deaf participants (D1 and D2) and two hearing musicians (H1 and H2), four members of the research team, as well as a sign language translator. Both deaf participants were partially deaf. They were allowed to choose if they wanted to use their hearing aid during the different parts of the evaluation session. Each participant received a bottled of wine for participating. The participants were divided into two random groups with a deaf person and a hearing musician in each. Then group 1 tested for 25 minutes, while group 2 waited outside of the room. When group 2 was testing, a semi-structured qualitative interview was conducted with group 1 regarding their test experience and vice versa. After both groups had tested and been interviewed the deaf and hearing participants were interviewed separately.  6.2 Results The interviews were transcribed and coded, thus answers and statements from the test subjects concerning similar themes could easily be compared and contrasted together with observations. The three most important themes were: (1) Collaboration, (2) Visuals and Sounds, and (3) Functionality. 6.2.1 Collaboration and Communication The test showed that collaboration between a deaf and a hearing was indeed possible by the aid of body language. There is still a communication barrier though, and this is heightened when the subjects do not feel comfortable around each other (participants did not know each other beforehand). To work with this issue perhaps a future interface would have to implement some sort of means of communication.  Often the deaf participants used the hearing participant to monitor the audio output and explain sounds they could not understand. This seemed to have a good influence on their 
understanding of what they were creating as well. This shows the benefits of having a deaf and a hearing create something together, rather than the deaf trying to create something on their own – but future evaluation could examine this more in depth.  The evaluation showed that in terms of collaboration it was beneficial for the participants to have their own work area, as this removed insecurities for the deaf participant, and prevented the hearing from “taking over”. Working on separate aspects additionally lowered the need for direct communication as the attention to the others’ work was heightened, and it was more natural to observe the work of the others and supplement it.  6.2.2 Visuals and Sound The deaf participants were somewhat reliably able to work with beats but they struggled to create melodic structures. The reason for this was that the deaf participants were able to feel the beat through vibrations in the table and visually understand when the different percussive instruments were playing. This was not the case for the melody, which they had difficulties perceiving. Even though the deaf participants were able to provide some inspirational input on the melody, the hearing participants at times found the deaf participants’ contribution unpleasing.  Those visuals that clearly resembled the actual instruments were the easiest for the deaf to understand (Hihat and Kick). The fact that the melody was only visually represented by brightness, and thus had no animation, seemed to have made it a lot harder for the deaf participants to understand the sounds, which they additionally had a hard time hearing. The melody also did not produce much vibrational output so they could not gather information from vibration either. It is clear that for the deaf to contribute on an equal level to the hearing in creating a melody, the melody would need to be represented more explicitly – for instance through more representative animations or spatially distributed vibrotactile feedback as described earlier in the paper. In relation to the prototype it might be beneficial to design visual representations for other instruments, which are less abstract and closer to the look of the actual instrument. The sound library for the melody part might additionally have to be more carefully designed to produce a stronger vibrational output, which was requested by both deaf participants.   6.2.3 Functionality There were evidently some functionality issues, which influenced the participants. A possibility to hear and feel the sounds individually was requested by both deaf participants as a beneficial addition for their understanding, as many sounds at 
324
the same time created confusion in their perception, and required additional clarification from their hearing partner.   7. DISCUSSION & FUTURE WORK The evaluation has showed that there is definitely potential for creating an interface that lets deaf and hearing people collaborate in making music. However, some things have to be thought of, for them to be able to contribute equally, including a more advanced visual and haptic feedback for especially the melodic aspect of a music creation program, so that the understanding of the sound can be put on a somewhat equal level. Collaboration between a hearing and a deaf person was possible and successful even though there was a communicative barrier. It was possible for both the deaf and the hearing to communicate through the use of their body language In a broader context the knowledge gained through this project, can contribute with inspiration and ideas to help deaf people be part of the normal hearing segment in relation to the creation of music.  Something this evaluation does not provide an answer to, is if the creations the test participants made can be defined as music, or if it is pleasant to listen to. Additionally, the time given to complete the evaluation was limited, therefore it is also hard to say if the test participants would have created different musical outputs, if they had more time to get to know the interface. Looking at the results from the evaluation, they give certain indications as to what to focus on when designing such a collaborative interface. But the evaluation also suggests new areas where unanswered questions might be explored further. Some questions that emerged after conducting the evaluation are:   • How can graphical animations help deaf people identify melody sequences if they cannot hear nor feel the sounds used for said melody?  • How can the mood in certain sounds be mapped intuitively to visual or haptic feedback? • Can the deaf and hearing users find common grounds in evaluating the actual musical outcome of such an interface? (See interesting work on this by Schubert et al. [20]) • Is it possible to uncover processes that are exclusive for this type of musical creation where collaborators do not have equal entry-points?  • Do profoundly deaf and partially deaf have different experiences with such interfaces and what are these differences?  • The evaluation showed that the deaf test subjects may have been overwhelmed with too much sound, when several instruments were playing at the same time. What is the threshold for how many different sounds can be perceived at a given time? • Are some instrument sounds better than others for a deaf person to hear and feel?  The prototype presented in this paper only used four different animated visual elements and in reality only two of those worked optimally. Visually, we recommend using somewhat fast animations coupled to how the sounds are produced in the real world, focusing on the transients in the music. This way attention is constantly shifted in a more intuitive way to what is sounding at any given time. Perhaps also completely different visuals to each sound library could help the deaf user understand the new sounds and the overall mood and theme of the library better.    Finally, improvement of the haptic feedback would be a natural next step for the research area. A haptic wearable such as the belt used in [15], or another vibrotactile display, mapping information such as pitch, loudness and rhythm to different parts of the body, could provide a nuanced sound perception for the deaf test subject. 
8. ACKNOWLEDGMENTS The authors would like to thank all the test participants.   9. REFERENCES [1] D. Shibata. Brains of deaf people “hear” music. In International Arts-Medicine Association Newsletter, 16, 4, 2001.  [2] S. C. Nanayakkara, E. Taylor, L. Wyse & S. H. Ong. An enhanced musical experience for the deaf: design and evaluation of a music display and a haptic chair. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Boston, USA, April 4-9, 2009). ACM Press, New York, NY, 2009, 337-346.  [3] C. Cannam, C. Landone, M. B. Sandler & J. P. Bello. The Sonic Visualiser: A Visualisation Platform for Semantic Descriptors from Musical Signals. In ISMIR (pp. 324-327), 2006. [4] http://windows.microsoft.com/en-us/windows/windows-media-player, online, accessed January 2016. [5] S. Ferguson, A. V. Moere & D. Cabrera. Seeing sound: Real-time sound visualisation in visual feedback loops used for training musicians. In Proceedings of the International Conference on Information Visualisation, (pp. 97-102). IEEE. 2005. [6] L. Nijs, B.  Moens, M.  Lesaffre & M. Leman. The Music Paint Machine: Stimulating self-monitoring through the generation of creative visual output using a technology-enhanced learning tool. Journal of New Music Research, 41(1), 79-101. 2012. [7] D. W. Fourney, & D. I. Fels. Creating access to music through visualization. In Science and Technology for Humanity (TIC-STH), IEEE Toronto International Conference, 939(944). IEEE, 2009. [8] O. Perrotin & C. d'Alessandro. Visualizing Gestures in the Control of a Digital Musical Instrument. In Proceedings of NIME (pp. 605-608), 2014. [9] S. C. Nanayakkara. Enhancing musical experience for the hearing- impaired using visual and haptic displays. Doctoral Thesis, National University of Singapore. 2009. [10] M. Pouris & D. I. Fels. Creating an entertaining and informative music visualization. In ICCHP, Part I, LNCS 7382, (pp. 451–458), Springer Berlin Heidelberg, 2012.  [11] F. Danieau, A. Lécuyer, P. Guillotel, J. Fleureau, N. Mollet & Marc Christie. Enhancing audiovisual experience with haptic feedback: a survey on HAV. In IEEE Transactions on Haptics. 6 (2), 193-205, 2013. [12] A. Baijal, J. Kim, C. Branje, F. Russo & D. I. Fels. Composing vibrotactile music: A multi-sensory experience with the Emoti-chair. In Haptics Symposium (HAPTICS), 2012 IEEE (pp. 509-515). IEEE, 2012. [13] E. Gunther & S. O’Modhrain. Cutaneous grooves: composing for the sense of touch. Journal of New Music Research, 32(4), 369-381. 2003. [14] M. Giordano & M. M. Wanderley. Follow the Tactile Metronome: Vibrotactile Stimulation for Tempo Synchronization in Music Performance. In Proceedings of the Sound and Music Computing Conference. (2015). [15] M. Schumacher, M. Giordano, M. M. Wanderley & S. Ferguson. Vibrotactile notification for live electronics performance: A prototype system. In Proceedings of the International Symposium on Computer Music Multidisciplinary Research (CMMR) (pp. 516-525). 2013. [16] D. M. Birnbaum & M. M. Wanderley. A systematic approach to musical vibrotactile feedback. In Proceedings of the International Computer Music Conference (ICMC) (Vol. 2, pp. 397-404). 2007   
325
[17] M. Kaltenbranner, S. Jordà, G. Geiger & M. Alonso. The reactable*: A collaborative musical instrument. In the 15th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises, (pp. 406-411). IEEE. 2006 [18] N. Klügel, M. R. Frieß, G. Groh & F. Echtler. An approach to collaborative music composition. In Proceedings of New Interfaces for Musical Expression (pp. 32-35). 2011. 
[19] S Jordà. Multi-user instruments: models, examples and compromises. In Proceedings of New Interfaces for Musical Expression. 2005.  [20] E. Schubert, J. Marozeau, CJ. Stevens, H. Innes-Brown. ‘Like pots and pans falling down the stairs’. Experience of music composed for listeners with cochlear implants in a live concert setting. Journal of New Music Research 43(2), 237-249. 2014.
 
326