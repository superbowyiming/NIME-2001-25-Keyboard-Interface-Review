Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
NIME03-95
Force Feedback Gesture Controlled
Physical Modelling Synthesis
David M Howard
Media Engineering Research Group
Department of Electronics
University of York, Heslington, York
YO10 5DD, UK
+44 1904 432405
dh@ohm.york.ac.uk
Stuart Rimell
Media Engineering Research Group
Department of Electronics
University of York, Heslington, York
YO10 5DD, UK
+44 1904 432407
smr12@ohm.york.ac.uk
Andy D Hunt
Media Engineering Research Group
Department of Electronics
University of York, Heslington, York
YO10 5DD, UK
+44 1904 432375
adh@ohm.york.ac.uk
ABSTRACT
A physical modelling music synthesis system known as ‘Cymatic’
is described that enables ‘virtual instruments’ to be controlled in
real-time via a force-feedback joystick and a force-feedback
mouse. These serve to provide the user with gestural controllers
whilst in addition giving tactile feedback to the user. Cymatic
virtual instruments are set up via a graphical user interface in a
manner that is highly intuitive. Users design and play these virtual
instruments by interacting directly with their physical shape and
structure in terms of the physical properties of basic objects such
as strings, membranes and solids which can be interconnected to
form complex structures. The virtual instrument can be excited at
any point mass by the following: bowing, plucking, striking,
sine/square/sawtooth/random waveform, or an external sound
source. Virtual microphones can be placed at any point masses to
deliver the acoustic output. This paper describes the underlying
structure and principles upon which Cymatic is based, and
illustrates its acoustic output.
Keywords
Physical modeling, haptic controllers, gesture control, force
feedback.
1. INTRODUCTION
The incorporation of computer technology in electronic musical
instrument has enabled musicians to push back the creative
boundaries within which they work, but despite enjoying the
resulting freedom from physical constraints, musicians are still
searching for virtual instruments that come closer to their physical
counterparts. The widespread availability of gestural controllers
which now incorporate a tactile element by means of
proprioceptive force feedback, following in the wake of PC
gaming developments, offers a cost effective route to restoring the
musician’s sense of working with a true physical instrument in the
natural world. The acoustic output from computer instruments is
often described as ‘cold’ or ‘lifeless’ whereas that from real
instruments may be described as ‘warm’, ‘intimate’ or ‘organic’.
These criticisms can be addressed in two ways: (i) by using
physical modelling to create organic sounds that more closely
resemble those of physical causality, and (ii) by creating new user
interfaces that enable musicians to interact with the computer in
more intuitive and intimate musical ways.
An ew instrument, known as ‘Cymatic’, is described in this
paper that incorporates both of these approaches to create an
instrument which provides an immersive, organic and tactile
musical experience that is more commonly associated with
acoustic instruments and rarely found with computer-based
instruments. Cymatic takes inspiration from physical modeling
sound synthesis environments such as TAO [1], Mosaic [2] and
CORDIS-ANIMA [3], through the use of resonating structures
that can be interconnected to create complex and musically
versatile virtual instruments playable in real-time using one or
more acoustic excitation methods.
Musicians interact with Cymatic’s virtual instruments via tactile
and gestural interfaces, thereby providing a route to enabling them
to interact with the computer in more intuitive and intimate
musical ways. In the virtual domain, the player is physically
detached from the sound source and therefore is interacting with it
indirectly via interface peripherals such as a mouse, MIDI
keyboard or musical instrument controller. Second to audition
itself, the haptic senses provide the most important means for
observing and interacting with the behaviour of musical
instruments [4]. Developments in computer-based musical
instruments have prioritised visual stimuli over tactile control,
with the result that the haptic senses have been left seriously
undernourished. It is only possible to realise complex and realistic
musical expression when both tactile (vibrational and textural)
and proprioceptive cues are available in combination with aural
feedback [5, 6]. Previous attempts to rectify this unsatisfactory
situation include: electronic keyboards that have a ‘feel’ close to a
real piano [7] the provision of tactile feedback [8], haptic
feedback bows that simulate the feel and forces of real bows [9],
and the use of finger fitted vibrational devices in open air gestural
musical instruments [10].
Existing haptic control devices are generally one-off devices
that are restricted to implementation with specific computer
systems, and are thereby inaccessible to the musical masses. In
contrast, Cymatic exploits the musical interface potential of
inexpensive and widely-available PC gaming devices, as its real-
time gestural control and haptic feedback is provided by a force
feedback joystick and a tactile feedback mouse.
2. Overview of Cymatic
Figure 1: Schematic depicting the flow of operations in
Cymatic’s core mechanics functions
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
NIME03-96
dtdttvtxdttx )2()()( ++=+
dttadttvdttv )()2()2( +-=+
Cymatic is implemented in C under Windows on a standard PC
machine. It utilises the mass-spring paradigm of physical
modelling to synthesise resonating structures in real time, and the
calculations are carried out for each mass cell in terms of updating
its position, velocity and acceleration based on the forces acting
on that cell. For real-time operation, this set of calculations has to
be completed for every cell in the instrument within time dt (the
reciprocal of the sampling rate selected), otherwise audible output
clicks are likely to result since the output waveform will not be
fully defined. It can be shown that [11] the position (x), velocity
(v) and acceleration (a) can be calculated from equations 1, 2, and
3 respectively.
(1)
  (2)
(3)
Where: k is the spring constant, m is the mass of the cell, r is the
viscosity as given by the damping parameter of the cell, F external is
the force on the cell from any external excitations e.g. plucking or
bowing, pn is the position of the n th neighbour, and p 0 is the
position of the current cell.
Cymatic is implemented in C++ on a PC machine. It performs
all the calculations required to run the model in a core mechanics
function as illustrated in figure 1. The new cell position, velocity
and acceleration values are calculated from equations 1, 2, and 8
respectively.
Users design instruments by means of an intuitive graphical
user interface (GUI) to create resonant structures of irregular
shapes and multiple dimensions. These can be interconnected (any
point mass on one structure to any point mass on another
structure) to form highly complex virtual instruments. Figure 2
shows an example complex Cymatic instrument. Virtual
excitation mechanisms are available (bowing, plucking, oscillators
and live-audio input) which can be applied to any point mass
within the instrument under real-time or off-line control. Any of
the physical parameters can be altered in real time as desired.
During synthesis, real-time animation of the instrument is
available to provide visual feedback of instrument vibrations.
Control over instrument parameters and excitation functions is
gestural via a force feedback joystick and force feedback mouse.
Figure 2: An example complex Cymatic instrument
consisting of an interconnected string, membrane and solid.
The resulting sound can be heard by placing any number of
virtual microphones that can be placed at user-defined points on
the instrument (see figure 3). The output is the sampled
displacement of the mass-spring cell to which it is attached. A
selection of standard sampling rates from 96kHz to 8kHz are
available, giving the user scope to trade off frequency resolution
against the number of point masses that can be incorporated in the
instrument for real-time operation.
Figure 3: Cymatic’s dialog box enabling placement of virtual
microphones at arbitrary cell coordinates
Figure 4 shows a series of time frames from Cymatic’s
animated GUI interface to illustrate the motion of a bowed string.
The animations provide immediate feedback as to the authenticity
of the physically modelled excitations in terms of the design
intention and the interaction between various elements.
Figure 4: Time snapshots from a Cymatic animation showing
Helmholtz motion of the bowed string model
Due to the mass-spring nature of the physical modelling
process, the instrument can be modified in real-time during
synthesis. For example, excitations and virtual microphones can
be moved in real-time, ‘virtual scissors’ can be applied to strings,
membranes or solids, and mass or spring parameters can be
adjusted.
))()()(1()( 0 externaln Ftvppkmta +--= Â r
Time
String Length
t1
t2
t3
t4
t5t6
t7
t8
t9
t10
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
NIME03-97
3. Real-Time Control
Real-time control of Cymatic is currently achieved using the
joystick and mouse to vary the physical parameters within the
instrument components, including mass, tension and damping,
excitation force and velocity, excitation point and virtual
microphone point.
Cymatic’s main controllers are a Microsoft Sidewinder Force
Feedback Pro Joystick and a Logitech iFeel mouse. The joystick
offers four degrees of freedom (x-movement, y-movement, z-twist
movement and a rotary “throttle” controller) and eight buttons. It
also provides tactile and proprioceptive feedback with a high
degree of customizability, boasting the potential to output six
forces simultaneously.F orce feedback implementation for
Window’s devices is normally achieved via DirectX and the
‘Immersion Foundation Classes.’ The Microsoft Sidewinder Force
Feedback Pro joystick receives its force instructions via MIDI
through the combined MIDI/joystick port on most PC sound
cards. Cymatic can stimulate the joystick’s haptic capabilities by
simply outputting the appropriate MIDI messages.
The Logitech iFeel mouse is a simple optical mouse which
implements Immersion’s [12] iFeel technology, containing a
vibrotactile device to produce tactile feedback over a range of
frequencies and amplitudes. It offers two degrees of freedom and
three buttons. Stimulating the iFeel mouse’s tactile feedback is
achieved through Immersion’s, “Immersion Touchsense
Entertainment” software which converts any audio signal to tactile
sensations on the iFeel mouse. The parameter assigned to each
controller function is fully customizable so the controllers can be
adapted to the type of instrument or excitation method that
Cymatic is running through arbitrary mapping by the user.
The Microsoft joystick can simulate a wide range of time- and
position-based haptic sensations. Time-based effects include
periodic oscillations of a variety of waveforms and a wide range
of amplitudes and frequencies (from 1Hz to approx 300Hz).
Constant forces, pulses and recoils also come under this category.
Position-based effects include sensations of friction, inertia, solid
surfaces and damping as well as spring like forces which increase
or decrease as a function of the displacement of the joystick
handle. The forces are identified by the joystick as system-
exclusive messages and can be played, stopped and altered
parametrically in real-time with MIDI control change and
aftertouch messages.
Figure 5: The haptic mapping of the joystick for
Cymatic’s bowed string model
Using these methods it is possible to design haptic sensations
to suit the instrument and excitation type running on Cymatic.
Figure 5 shows the haptic mapping for a Cymatic bowed string, in
which the Y axis of the joystick is mapped to the force of the bow
and the rate of change of the x-axis is mapped to the velocity of
the bow. The user will feel no force until the virtual bow contacts
with the string, after which the more force placed upon the string,
the greater the force that is felt. A friction force in the x direction
will also increase with respect to increasing the force placed on
the string. The iFeel mouse is useful to simulate the velocity of the
virtual bow while the joystick Y-axis takes care of the force
parameter. This ensures that the user has to provide energy to the
instrument via a gestural input in order to achieve an output. Other
controller functions can be used to alter different parameters e.g.
the ‘twist’ function of the joystick can change the tension of the
string to be used as a vibrato effect and the ‘throttle control’ can
be mapped to the bow position on the string, microphone position
etc. A periodic force related to the amplitude and frequency of the
audio output is felt through the iFeel mouse and the joystick’s
handle.
Figure 6: Spectrograms of the outputs from an acoustic
double bass (upper) and a Cymatic bowed virtual instrument
tuned to the same pitch.
Figure 6 enables the acoustic outputs from Cymatic and an
acoustic double bass to be compared spectrographically,
illustrating something of the potential organic nature of the
Cymatic output sound. This is one of the key features of physical
modelling. The Cymatic instrument has been bowed and its string
set to produce the same fundamental frequency as that obtained
from the acoustic double bass to enable comparison. There are
clear similarities between the nature of the note onset and offset in
each case, and it should be remembered that every note played on
a Cymatic virtual instrument will be acoustically subtly different
since each will have its origins in a unique gesture. Haptic and
gestural mappings can be readily implemented to suit the
individual instrument’s needs.
4. Discussion
Cymatic was conceived with the desire to create new sounds
through the use of new instruments, which may not be physically
realisable in the real world, and to provide the player with a more
engaging musical experience when performing with the
instrument. The use of a physical modeling paradigm enables
intuitions gained through training and performance with acoustic
instruments to be immediately transferred to Cymatic instruments.
The addition of tactile feedback reinforces the available visual and
aural feedback cues, helping the player to develop internal models
that are physically rooted in the manner in whcih the instrument
responds to gesture.
Cymatic made its concert debut in December 2002 to universal
audience acclaim at a public performance in York in a specially
written piece by Stuart Rimell for a small (13 strong) SATB choir
and Cymatic. Here the choir provided a backdrop over which a
Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03), Montreal, Canada
NIME03-98
three sheet Cymatic instrument performed an obbligato. Extracts
from this piece are available at [13].
Cymatic provides a new environment within which new
musical instruments can be implemented, explored, and interacted
with in live performance and composition. Only a very small
subset of the potential possibilities has been explored to date, and
next steps will focus on the creative side in an exploration of
Cymatic’s wider timbral potential. Cymatic instrument
possibilities have the potential to feed musical imagination for a
considerable time to come.
5. ACKNOWLEDGMENTS
This work was supported by EPSRC grant number
GR/M94137.
6. REFERENCES
[1] Pearson, M.D., and Howard, D.M. (1996). Recent
developments with TAO physical modelling system
Proceedings of the International Computer Music
Conference, ICMC-96, 97-99.
[2] Morrison, J.D. and Adrien, J.M. (1993). MOSAIC: A
Framework for Modal Synthesis, Computer Music Journal,
17, (1), 45-56.
[3] Cadoz, C., Luciani, A., and Florens, J.L. (1993). CORDIS-
ANIMA: A Modelling system for sound and image synthesis,
the general formalism, Computer Music Journal 17, (1), 19-
29.
[4] Cook, P.R. (1999). Music, Cognition and Computerised
Sound: An Introduction to Psychoacoustics, London: MIT
Press, pp. 229.
[5] MacLean, K.E. (2000). Designing With Haptic-Feedback,
www.cs.ubc.ca/~maclean/publics/icra00-DesignWithHaptic-
reprint.PDF
[6] Howard, D.M., Rimell, S., Hunt, A.D., Kirk, P.R., and
Tyrrell, A.M. (2002). Tactile feedback in the control of a
physical modelling music synthesiser, In: Proceedings of the
7th International Conference on Music Perception and
Cognition, Stevens, C., Burnham, D., McPherson, G.,
Schubert, E., and Renwick, J. (Eds.), Adelaide: Casual
Publications, 224-227.
[7] Gillespie, B. (1992). Proc ICMC San Jose, CA. pp. 447-448.
[8] Cadoz, C., Luciani, A. & Florens, J.L. (1984). Responsive
Input Devices and Sound Synthesis by Simulation of
Instrumental Mechanisms: The Cordis System . Computer
Music Journal 8, (3), 60-73
[9] Nichols, C. (2001). The vBow: Development of a Virtual
Violin Bow Haptic Human Computer Interface, Proceedings
of the NIME-02 Conference on New Interfaces for Musical
Expression, ISBN (CD-ROM): 1-87465365-8, 29-32.
[10] Rovan, J. (2000) Typology of Tactile Sounds and their
Synthesis in Gesture-Driven Computer Music Performance.
In Trends in Gestural Control of Music. Wanderley, M.,
Battier, M. (eds). Editions IRCAM, Paris.
[11] Rimell, S., and Howard, D.M. (2003). Tactile controlled
physical modelling music synthesis, paper submitted to IEEE
Transactions on Speech and Audio Processing.
[12] www.immersion.com
[13] www-users.york.ac.uk/~dmh8