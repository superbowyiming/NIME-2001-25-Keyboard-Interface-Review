Use of Body Motion to Enhance Traditional Musical
Instruments
A Multimodal Embodied Approach to Gesture Mapping, Composition and
Performance
Federico Visi
Interdisciplinary Centre for
Computer Music Research
(ICCMR)
Plymouth University, UK
federico.visi@plymouth.ac.uk
Rodrigo Schramm
Institute of Informatics
Federal University of Rio
Grande do Sul, Brazil
rodrigos@caef.ufrgs.br
Eduardo Miranda
Interdisciplinary Centre for
Computer Music Research
(ICCMR)
Plymouth University, UK
eduardo.miranda@plymouth.ac.uk
ABSTRACT
This work describes a new approach to gesture mapping in a
performance with a traditional musical instrument and live
electronics inspired by theories of embodied music cognition
(EMC) and musical gestures. Considerations on EMC and
how gestures aﬀect the experience of music inform diﬀerent
mapping strategies. Our intent is to enhance the expressive-
ness and the liveness of performance by tracking gestures
via a multimodal motion capture system and to use mo-
tion data to control several features of the music. We then
describe an application of such approach to a performance
with electric guitar and live electronics, focusing both on
aspects of meaning formation and motion capturing.
Keywords
NIME, gesture, multimodal, mapping, embodied music cog-
nition, gesture recognition, liveness, expressiveness, guitar.
1. INTRODUCTION
In the last two decades, the study of music and gesture has
been subject to increasing cross-disciplinary interest, mov-
ing away from a primarily musicological context and engag-
ing multiple disciplinary ﬁelds. Extensive interdisciplinary
research has recently been carried out [9, 10, 11] giving rise
to new paradigms for the understanding of gesture in music.
In particular, insights from research on embodied music cog-
nition [13] inspired new viewpoints that required a rethink
of the foundations of musical gestures. Within this theoretic
framework, music perception isembodied (i.e. closely linked
with bodily experience) and multimodal, in the sense that
music is perceived not only through sound but additionally
with the help of both visual cues and sensations of motion,
eﬀort and dynamics [8]. Hence gestures become a core no-
tion as they act as a bridge between bodily movement and
meaning formation.
Concurrently, motion sensing technologies have consid-
erably developed and become pervasive as an increasing
number of consumer-grade electronics employ motion data
for accomplishing diﬀerent functions. Such technologies are
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
NIME’14,June 30 – July 03, 2014, Goldsmiths, University of London, UK.
Copyright remains with the author(s).
easily accessible and are being employed in both academic
research and artistic practice. In musical contexts, there
is a long and proliﬁc tradition of electronic interfaces that
exploit gestures and motion as means of control of musi-
cal parameters [19]. As Miranda and Wanderley also note,
adopting an eﬀective mapping strategy is crucial for the ex-
pressiveness of the interface, being the relationship between
gestural variables and musical parameters often far from ob-
vious. Mapping has therefore become a key topic in digital
musical instrument research and several approaches have
been documented in the proceedings of NIME conferences
through the years.
Within this context – where a considerable cross-discipli-
nary theoretical apparatus has come of age while continu-
ously developing and motion sensing technologies have be-
come ubiquitous – this work aims at giving an account of
approaching gesture mapping in a performance with a tra-
ditional musical instrument and live electronics, taking into
consideration recent theories of embodied cognition and mu-
sical gesture and employing multiple motion-sensing tech-
nologies.
2. GESTURES AND EMBODIED MUSIC
COGNITION
The concept of gesture is used across multiple disciplines
and contexts and its deﬁnition is quite broad and sometimes
vague. Jensenius et al. [12] give a comprehensive look at
the term and its uses in music research in order to present
a clearer overview. While initially problematic to pinpoint,
the notion of gesture has considerable potential in modern
music research as it works as a bridge between movement
and meaning and, consequently, bypasses the boundary be-
tween physical world and mental experiences. This is at the
core of the embodied music cognition paradigm [13]. In this
context, “musical gestures can be described in an objective
way as movement of body parts, but they have an impor-
tant experiential component that is related to intentions,
goals, and expressions” [15]. The body is thereby under-
stood as a mediator between the physical environment (e.g.
music as sound waves moving in the air) and the subjective
musical experience (e.g. one’s feelings in response to that
music). By acting as a mediator, the body will build up
a repertoire of gestures and gesture/action consequences,
or what Leman calls agesture/action-oriented ontology[15].
This repertoire can be considered as a collection of move-
ments made to achieve a particular goal (actions) linked
with the experiences and sensations resulting from such ac-
tions. Musical gestures are at the core of this repertoire.
Proceedings of the International Conference on New Interfaces for Musical Expression
601
The coupling of actions and perceived sensations forms an
engine that guides our understanding of music. Through
this mechanism, the listener is able to relate physical as-
pects of movement in space to expressive qualities, inten-
tions and inner feelings. Conversely, perceived patterns of
musical expression recall previously learned knowledge of
the corresponding body movement. This continuous two-
way mirroring process allows the listener not only to at-
tribute intentions and feelings to music but also to predict
the outcomes of actions and project them onto the music
[17]. This is what Leman callsaction-perception coupling
system; it forms the basis of musical intentional communi-
cation and expressiveness, which then elicits several social
phenomena such as empathy and social bonding [13, 14, 15].
Overall, being at the core of embodied music cognition,
we could say that gestures are a vehicle for the construc-
tion of musical meaning . This means not only that music
perception is embodied, but that it is also multimodal in
the sense that we perceive it using multiple senses; through
sound but also with the help of visual images and feelings
of motion, such as kinaesthetic sensations and kinematic
images [8].
From the perspective of embodiment, it is clear that ges-
tures play an important role in how we perceive and at-
tribute meaning to music and how we express ourselves
through music. To better understand the functions of mu-
sical gestures in performance, Jensenius et al. [12] identify
four functional categories; sound-producing gestures, sound-
facilitating gestures, sound-accompanying gestures and com-
municative gestures. While their boundaries are usually
blurry – mostly because in actual musical performances ges-
tures naturally tend to have multiple functions – functional
categories are a useful starting point for making sense of the
role a gesture may have in a performance. Further practice-
led research may bring more detailed insights on the func-
tions gestures may have in performances with a particular
instrument or within diﬀerent musical contexts.
3. TRADITIONAL MUSICAL
INSTRUMENTS
In this context, the term traditional musical instrument
(henceforth TMI), is used to deﬁne instruments that have
a rich idiomatic repertoire that ranges across both popu-
lar music and classical music. Examples include electric
guitar, violin, electronic keyboards, etc. Such a term is pre-
ferred to acoustic instrument because it refers to the use
and repertoire rather than to technological aspects. The
choice of focusing on TMI performance is motivated by the
vast knowledge that listeners have of the gestural and sound
aspects of each instrument, which is learnt through expe-
rience. This can be summarised as theecological knowl-
edge [8] of an instrument; listeners have, and in some cases
share, a repertoire of sound-producing gestures . TMIs also
have fairly explicit and known aﬀordances [6] that can de-
lineate action relationships between the instrument and the
musician, inform expectations in the listeners and be used,
along with sound aﬀordances, to devise mapping strategies
for controlling electronic aspects of the performance. Using
embodied music cognition terminology, instruments have a
rich action/gesture repertoire that the listeners can recog-
nise during the performance. With this in mind, using the
instrumentalist’s gestures may have a considerable expres-
sive potential in performance as well as in composition as
composers would be able to draw from a gestural palette of
the instrument when writing a piece.
Figure 1: Multimodal motion capture setup featuring opti-
cal motion capture and wearable sensors.
4. APPLYING THE THEORY TO A CASE
STUDY: THE ELECTRIC GUITAR
Given this background scenario, it is clear that gestures
have a signiﬁcant inﬂuence on how music is experienced
and traditional musical instruments are a rich repository
of shared gestural information. In recent years, there have
been applications of EMC within interactive multimedia en-
vironments [2] and singing performance [16].
Here, we are proposing an initial approach to the de-
velopment of a performance with electric guitar and live
electronics involving multiple motion capture technologies,
namely ﬂex sensors and accelerometers located on hands
and wrists of the guitarist and a Microsoft Kinect to mon-
itor full-body movements. The choice of placing sensors
on the performer’s hands is motivated by the need to have
more accurate and detailed data of hand movements since
sound-producing gestures of guitar playing – as with many
other instruments – typically involve hands and arms and
such gestures are the most readily noticed by an observer
[3]. This was done using ﬂex sensors and 3-axis accelerom-
eters mounted on custom wristbands together with a cus-
tom Arduino-based board equipped with an XBee wireless
chip1. Conjointly, the Kinect allows the extraction of full
body movements in diﬀerent lighting situations, which is de-
sirable in a staged context. On the other hand, a drawback
of such technology, is that the quality of the skeleton track-
ing depends on factors like camera position and occlusion
of body parts. Indeed, the presence of a musical instrument
normally aﬀects the accuracy of the skeleton tracking algo-
rithm. Another limit of this sensor is its low frame rate (30
fps), which may be a problem for real time audio process-
ing. In this case, employing a multimodal motion capture
system allows to use the more stable signal of the wear-
able sensors to capture the subtle movements of hands and
wrists while the Kinect data, even at low frame rates or with
jittery joint tracking, allows for the interpretation of more
complex gestures from distinct parts of the body, enabling
the recognition of high-level expressive movements.
Parameter mappings are arranged in a layered conceptual
framework (low-level and high-level), reﬂecting the layered
approach to musical meaning formation as described by Le-
man [14].
5. MAPPINGS: LOW-LEVEL LAYER
These are low level mappings related to low-level charac-
teristics of the movement and they are tracked using sin-
gle skeleton points captured by the Kinect and the output
values of the accelerometers and ﬂex sensors. This layer
1http://www.sensestage.eu
Proceedings of the International Conference on New Interfaces for Musical Expression
602
comprises the use of one-to-one and one-to-many mapping
strategies [19] relating a single gesture to sonic events in a
transparent fashion. The motion data goes through sim-
ple signal processing – including rescaling, smoothing and
compression – using OSC and MIDI in a data routing ap-
plication2. The processed data is then used to control vari-
ous DSP patches. These mappings are devised using func-
tional categories of gestures in order to reinforce and play
around the meaning and expectations associated to each
gesture. For example, turning the palm of the strumming
hand upwards and therefore interrupting one of the most
evidentsound-producing gestures (string plucking) triggers
a granular sampler controlled via the wearable sensors that
freezes and sustains the sound of the guitar, reﬂecting the
suspension in the performance.Sound-facilitating phrasing
gestures, like the movement of the head to express entrain-
ment and follow features of the music, are instead captured
via the Kinect and mapped onto parameters of a ring mod-
ulator which modify the overall timbre of the instrument.
6. MAPPINGS: HIGH-LEVEL LAYER
This layer contains mapping strategies relating more com-
plex gestures to structural changes in the arrangement of
the electronic parts of the piece and in the global sound.
Motion data is therefore used to recognise gesture trajecto-
ries and semantic and cultural aspects of the performance
are also taken in consideration.
6.1 Gesture recognition
Speciﬁc gesture trajectories are recognised and are useful
for controlling structural features of the performance, such
as the introduction of loops and samples and other more
complex parts of the piece. In addition, gesture trajecto-
ries are employed to assign low-level gestures to other mu-
sical parameters, allowing the performer to control diﬀer-
ent features later in the piece. In this case, the skeleton
joints tracked with Kinect are used to deﬁne a set of N-
Dimensional temporal features, which are then evaluated in
real time to map gestures. To realise this task, temporal
classiﬁcation algorithms like the Dynamic Time Warping
- DTW [20] and Hidden Markov Models-HMM [5, 1] were
extensively studied and experimented.
An implementation based on the N-Dimensional DTW
technique described by Gillian et al. [7] was used in this
work. As a result, the proposed system can learn speciﬁc
gestures through a machine learning system and recognise
them during the performance. Figure 2 illustrates an ex-
ample where two diﬀerent types of gestures are recognised.
6.2 Context (social, semiotic)
As noted by Leman [14], social context contributes indi-
rectly to musical meaning formation. In live guitar per-
formance, playing seated is a typical feature of acoustic
performances while playing standing is usually associated
with jazz and rock contexts where a more engaged perfor-
mance body is expected [21]. Playing standing or seated
has a meaning for the audience that relies upon previously
learned social and aesthetic aspects. This mapping under-
lines these aspects by varying the overall timbre of both the
guitar and the electronic parts according to the posture of
the performer.
Neither a direct low level mapping nor the gesture recog-
nition procedure are appropriate to interpret the posture of
the performer. For a better mapping in this situation, two
measures were used by Fenza et al. [4]:
2http://steim.org/product/junxion/
Gesture 1
 Gesture 2
0 500 1000 1500 2000 2500 3000 3500
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Frames
DTW measure
gesture 1
gesture 2
threshold
Figure 2: Gesture recognition task. The dashed and solid
lines show the DTW distance from each gesture template.
The dashed horizontal line shows the classiﬁcation thresh-
old. In this case, gesture 1 and gesture 2 were detected at
frame 567 and at frame 2396, respectively.
100 200 300 400 500 600 700
50
100
150
200
fram e 185 fram e 280 fram e 380 fram e 475 fram e 650
Figure 3: Contraction Index variation (solid line) of per-
former’s body along a recorded time sequence. Skeletons
(on top) illustrate the posture on each time stamp (marks).
• Contraction Index - CI: This measure is the sum
of the distances between the skeleton barycenter and
each skeleton point and provides information about
the contraction and expansion of the body.
• Quantity of Motion - QoM: The QoM is propor-
tional to the distance of movement, that is, it gives
high values when the skeleton body is moving fast and
low values when the it is more stationary.
These two values, measuring the contraction/expansion
and the quantity of motion of the body, can be used to con-
trol structural features of the piece being performed. For
example, intense physical engagement can trigger new sam-
pled parts of the piece or assign the low-level gestures to a
new set of musical parameters.
To illustrate this, Figure 3 shows a time-sequence during
which the performer changes its posture, from standing up
(frame 185) to sitting down (frame 280) and ﬁnally with the
arms wide open (frame 650). The line shows the respective
Contraction Index along the time.
7. CONCLUSION AND FUTURE WORK
The paradigm of embodied music cognition provides inter-
esting insights on musical gestures and their role in our ex-
perience and understanding of music. The assumption that
Proceedings of the International Conference on New Interfaces for Musical Expression
603
music is based on a tight relationship between sounds and
experiences that are mediated by the body have many sig-
niﬁcant implications. From this perspective, gesture is not
subordinated to sound by simply acting as a means of con-
trol, but rather it contributes to the formation of meaning,
acting on the same level of sound.
If we take into account that the action/perception cou-
pling is at the core of embodied music cognition, mapping
can even more be considered as a translation layer that must
be carefully designed. An eﬀective mapping strategy is in
fact pivotal to obtain good results in terms of expressiveness
and liveness of performance and it can also be said that it
constitutes an integral part of composition, as gestures con-
vey meaning analogously to sounds.
Within this context, it is evident that technology plays an
important role. However, diﬀerent systems can be employed
to reach the same expressive result working on the same
gesture/sound relations, showing that the approach is to
some extent hardware-agnostic.
Future work may be done to explore the gestural reper-
toire of other instruments. Functional categories are a good
starting point for understanding the role of individual ges-
tures. However, their boundaries are very blurry due to
the multi-functional nature of gestures, therefore the devel-
opment of more precise tools for semantic analysis may be
helpful.
Embodied music cognition is still a relatively young re-
search ﬁeld and new contributions to the understanding of
some of its inner mechanisms – such as mirroring, com-
mon coding and enaction – have been recently published
[17, 18]. In addition, there are few examples of practical
applications outside academic research (e.g. [16]). Gath-
ering more empirical evidence about the formation of em-
bodied musical meaning through academic experimentation
may result in new notable insights. Nonetheless, intuitions
from more practice-led, artistic environments may give con-
siderable contributions to the research.
As Godøy notes in [8] Western musical thought has not
been well equipped for thinking the inclusion of musical el-
ements within the context of a gesture. Further research
into gestural and embodied aspects of music and practice
of a moregesture-aware music might bring about unprece-
dented insights and perhaps lead to the emergence of new
aesthetic categories.
References
[1] F. Bevilacqua, B. Zamborlin, A. Sypniewski,
N. Schnell, F. Gu´ edy, and N. Rasamimanana.
Continuous realtime gesture following and recognition.
InProceedings of the 8th International Conference on
Gesture in Embodied Communication and Human-
Computer Interaction, GW’09, pages 73–84, Berlin,
Heidelberg, 2010. Springer-Verlag.
[2] A. Camurri, G. D. Poli, M. Leman, and G. Volpe. A
multi-layered conceptual framework for expressive ges-
ture applications. InProceedings of MOSART: Work-
shop on Current Directions in Computer Music , pages
29–34, 2001.
[3] S. Dahl, F. Bevilacqua, R. Bresin, M. Clayton,
L. Leante, I. Poggi, and N. Rasamimanana. Gesture
in performance. In R. I. Godøy and M. Leman, edi-
tors, Musical Gestures: Sound, Movement, and Mean-
ing. Routledge, 2010.
[4] D. Fenza, L. Mion, S. Canazza, and A. Rod` a. Physical
movement and musical gestures: a multilevel mapping
strategy. In Proceedings of Sound and Music Comput-
ing Conference, Salerno, 2005.
[5] G. A. Fink. Markov Models for Pattern Recognition:
From Theory to Applications . Springer-Verlag, Berlin
Heidelberg, 2008.
[6] J. J. Gibson. The theory of aﬀordances. In Perceiving,
Acting, and Knowing. Erlbaum, 1977.
[7] N. Gillian, B. Knapp, and S. O’Modhrain. Recognition
Of Multivariate Temporal Musical Gestures Using N-
Dimensional Dynamic Time Warping. In Proceedings
of the International Conference on New Interfaces for
Musical Expression, pages 337–342, 2011.
[8] R. I. Godøy. Gestural aﬀordances of musical sound. In
R. I. Godøy and M. Leman, editors, Musical gestures:
Sound, movement, and meaning . Routledge, 2010.
[9] R. I. Godøy and M. Leman, editors. Musical Gestures:
Sound, Movement and Meaning . Routledge, 2010.
[10] A. Gritten and E. King. Music and Gesture. Ashgate,
2006.
[11] A. Gritten and E. King. New Perspectives on Music
and Gesture. SEMPRE studies in the psychology of
music. Ashgate, 2011.
[12] A. R. Jensenius, M. M. Wanderley, R. I. Godøy, and
M. Leman. Musical gestures: Concepts and methods in
research. In R. I. Godøy and M. Leman, editors,Mu-
sical gestures: Sound, movement, and meaning , pages
12–35. Routledge, 2010.
[13] M. Leman. Embodied Music Cognition and Mediation
Technology. MIT Press, 2008.
[14] M. Leman. Music, gesture, and the formation of em-
bodied meaning. In R. I. Godøy and M. Leman, edi-
tors, Musical Gestures: Sound, Movement and Mean-
ing, pages 126–153. Routledge, 2010.
[15] M. Leman. Musical gestures and embodied cogni-
tion. In T. Dutoit, T. Todoroﬀ, and N. d’Alessandro,
editors, Actes des Journ´ ees d’Informatique Musicale
(JIM 2012), pages 5–7, Mons, Belgique, 9-11 mai 2012.
UMONS/numediart.
[16] P.-J. Maes, M. Leman, K. Kochman, M. Lesaﬀre, and
M. Demey. The ”one-person choir”: A multidisci-
plinary approach to the development of an embodied
human-computer interface. Computer Music Journal ,
35(2):22–35, 2011.
[17] P.-J. Maes, C. Palmer, M. Leman, and M. Wanderley.
Action-based eﬀects on music perception. Frontiers in
Psychology, 4, 2014.
[18] J. R. Matyja and A. Schiavio. Enactive music cogni-
tion: Background and research themes. Constructivist
Foundations, 8(3):351–357, 2013.
[19] E. R. Miranda and M. Wanderley. New Digital Musi-
cal Instruments: Control And Interaction Beyond the
Keyboard (Computer Music and Digital Audio Series).
A-R Editions, Inc., Madison, WI, USA, 2006.
[20] M. M ¨uller. Information Retrieval for Music and Mo-
tion. Springer Verlag, 2007.
[21] V. Verfaille, O. Quek, and M. M. Wanderley. Soniﬁ-
cation of musicians’ ancillary gestures. In ENACTIVE
Workshop, Montreal, May 2006.
Proceedings of the International Conference on New Interfaces for Musical Expression
604