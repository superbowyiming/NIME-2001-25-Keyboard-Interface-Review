Live Cinema: Designing an Instrument for 
Cinema Editing as a Live Performance 
 Michael Lew 
Media Lab Europe 
Crane Street 
Dublin 8, Ireland 
+353 1 474-2800 
lew@media.mit.edu 
 
 
 
 
 
 
ABSTRACT 
This paper describes the desi gn of an expressive tangible 
interface for cinema editing as a live performance. A short survey 
of live video practices is provided. The Live Cinema instrument is 
a cross between a musical instrument and a film editing tool, 
tailored for improvisational control as well as performance 
presence. Design specifications for the instrument evolved based 
on several types of observations including: our own performances 
in which we used a prototype based on available tools; an analysis 
of performative aspects of contemporary DJ equipment; and an 
evaluation of organizational aspects of several generations of film 
editing tools. Our instrument presents the performer with a large 
canvas where projected images can be grabbed and moved around 
with both hands simultaneously; the performer also has access to 
two video drums featuring haptic display to manipulate the shots 
and cut between streams. The paper ends with a discussion of 
issues related to the tensions between narrative structure and 
hands-on control, live and recorded arts and the scoring of 
improvised films. 
Keywords 
live cinema, video controller, visu al music, DJ, VJ, film editing, 
tactile interface, two-hand interaction, improvisation, 
performance, narrative structure. 
1. INTRODUCTION 
The recent years have witnessed an impressive surge of new 
live visual practices. As with every generation of moving image 
tools (film in the 1920’s, video in the 1960’s, graphics-enabled 
laptop in the 1990’s), democratization of access has allowed 
artists to invent languages and artforms that redefine what cinema 
can be. 
Today, in camera-based cinema, film has been freed from the 
inherent linearity of the physical tape or celluloid medium. Stored 
as chunks of data on a hard disk that can be randomly accessed, 
film no longer needs to be presented in a linear, deterministic 
way, as a static sequence of s hots on a one-dimensional timeline; 
rather it can be presented as a c onnected constellation of shots in 
a multidimensional narrative or performance space, that can be 
traversed in multiple ways, generating a different interpretation of 
the same film each time. 
With traditional cinema, the time flow is imposed. This means 
that in any polymedia collaboration (e.g. projection in opera, 
dance, music), film is the unchangeable part of the performance 
requiring all other performers (actors, dancers, musicians) to time 
themselves to it. With live cine ma, the film elements, controlled 
by association and algorithmic behaviors, exist as a loose 
structure adaptable to real-time conditions as the performer 
improvisationally creates the projection live. 
In live cinema, the performer is essentially editing and 
sequencing previously recorded digital video on the fly while at 
the same time being on view to an audience.  This situation 
introduces some real constraints on the design of a performance 
system.  This new practice, combining the nature of live and 
recorded arts, is part of the emerging time-based performance 
artforms called Live Cinema or Performance Cinema [24]. 
In this paper we present a novel instrument, for cinematic 
editing as a live performance.  Our Live Cinema instrument is the 
hybdrization of a tangible interface to perform electronic music 
and an editing tool for motion picture. The instrument must allow 
a performer to assemble a feature-length film from beginning to 
end in front of the audience, suggesting relevant shots at the right 
time, based on various time- and context-based organizations of 
the footage prepared in advance by the performer in the live 
cinema score. 
The design of video instruments, although not a new practice, is 
not an established field and has been the subject of very little 
writing. However, because of the many analogies that can be 
drawn between audio and video domains, all instruments for the 
production of time-based media in live conditions share common 
design attributes.   
This paper is organised as follo ws: In Section 2, we provide a 
short survey of live video practices and an overview of current 
available tools. In Section 3, we expose the problem and the 
design criteria, learning from the DJ in section 3.1 for the 
performative aspect, and from the motion picture editing tool in 
section 3.2 for the organisational aspect. In section 4, we describe 
the live cinema instrument and its implementation. 
2. BACKGROUND 
Live Cinema can be described as any performance involving the 
presence of a human performer  manipulating moving images 
projected for an audience. We attempt here to give a fragmentary 
survey of the different genres and communities that are 
represented today among these emerging experimental practices: 
- Abstract synthetic cinema 
Made with mathematical models  and procedural programming, 
abstract synthetic cinema becomes live when some of its 
parameters are controlled by a performer. Working with simple 
Figure 1. Abstract cinema 
elements of rhythm, colour, shape, it is also called light art , or 
visual music , aiming at “the development of a visual dynamic 
language”, a “pure graphic choreography” [33]. There is a long 
tradition of mechanical devices for such color-music 
performances, such as the clavecin oculaire , the pyrophone, the 
color organ, the Clavilux, the Musiscope and particularly 
Fischinger’s Lumigraph (for a history and bibliography on these 
instruments, see [4, 10]). The first electronic models were analog 
video synthesizers [28] ; followed by Laurie Spiegel’s digital 
programmable VAMPIRE [22] - although it is unclear whether 
they were ever used in a live context. Today artists use the live 
video software packages mentioned below with MIDI devices, or 
their own code or sensors, like Golan Levin [10] or Jasch. 
- Graphical cinema 
Emerging from the rave scene of th e 80’s, this genre belongs to 
club culture and music video aesthetics. Originating with the 
‘demo’, real-time fly-through 3D worlds that hackers competed to 
make in the smallest code size, its material now comprises flash 
animations and other fast-paced 2D/3D computer graphics. Video 
projectors have started to invade clubs, making the VJ (video 
jockey) a natural extension of the DJ in the visual domain. The 
nature of the visuals comes from the sampling culture, graphic 
design, pop art and psychedelia. For more information on the 
abundance of VJ tools, events and communities, see for example 
[28, 38]. 
- Camera-based cinema 
This category describes the work of experimental filmmakers or 
video artists whose raw material in a live performance is mostly 
footage captured with a camera obscura. Using personal or found 
footage, these artists compose time-based collages of an often 
dream-like surrealist nature. Others repurpose footage borrowed 
from commercial movies to revisit them, remix them or to make 
video pop art. On the most narrative end, some artists are starting 
to make film productions specifically for live shows, often 
performing simultaneously with narrators, musicians, actors, 
dancers. Our project is designed for this type of live cinema. 
 
The combination of recorded film and live performance is not a 
new concept. Before the “talkies”, film projection was always 
accompanied by musicians, from a pianist to a symphonic 
orchestra. Early manual techniques for film performance (still 
used today) consist in using multiple 16mm projectors, running 
loops of celluloid through them, moving filters and translucid 
objects in front of the lens, etc. The first interactive cinema 
experiments in the 60’s with audience voting had a human 
operator appearing on stage [14]. 
Steina Vasulka, founder of th e Kitchen, an experimental 
performance space in New York City, started in the mid-70’s to 
perform video by using her violin to control real-time image 
processing – technique she still us es today with a MIDI-enabled 
violin controller [25]. In the 80’s,  Nam June Paik created several 
live video editing performances. 
In the corporate television world, there exists a range of 
switching and effects consoles  developed for editing live 
broadcasts, which we will not expand on here; the role of the live 
TV editor as a narrator was analysed by Eco [6]. 
In the late 90s, the home computer became mature enough for 
live cinema, facilitated by powerful graphics cards, image 
compression standards, fast and large hard disks and the firewire 
standard. Many electronic artists started building their own 
software and later licensing it : Steim’s Image/ine [32], David 
Rokeby’s softVNS [37], Netochka Nezvanova’s nato [3], PD’s 
gem, Cycling74’s jitter [29], Marc Coniglio’s Isadora [34], 
Meso’s vvvv. For other tools and further details, ask these mailing 
lists [27, 34]. The latest tendencies can be witnessed in European 
electronic art festivals (club transmediale, sonar, onedotzero..) 
and new media festivals (deaf, ars electronica, FCMM, viper..). 
Two recent noteworthy events dedicated exclusively to the art of 
live cinema were Sonic Light 2003 in Amsterdam [21] and the 
San Francisco Performance Cinema Symposium [20]. 
Stage arts are also being transformed by live video : projections 
are increasingly becoming part of the scenography of theatre, 
danse and opera pieces [17]. Although most companies use static 
video (La Fura dels Baus, Wooster group, Station House Opera), 
some are increasingly using live dynamic video (Dumb Type, Cie 
Incidents Mémorables, Troika Ranch). Other strategies to have 
on-stage performers control vide o include vision-based motion 
tracking [25] or motion capture (Merce Cunningham). 
Alas, in the very conservative wo rld of narrative industrial film 
as we know it, only very few daring filmmakers have recently 
attempted to do live editing : Morten Schjødt controlling his 
interactive film “Switching” and Mike Figgis remixing the sound 
tracks of his 4-screen feature “Timecode”. 
3. DESIGN 
Our objective was to build an e xpressive tangible instrument 
that would allow a filmmaker to come to a venue with all the 
shots on a hard disk and assemble the film in front of the 
audience. Such an instrument has to be performative, allowing 
fast, accurate, expressive manipulations of the image; and have a 
capacity for organisation, so that shots can be easily browsed and 
retrieved. We adopted the following approach : 
1. Analyse the activities of the DJ and try to replicate them in the 
visual domain. 
2. Study industry-standard film  editing tools, determine their 
shortcomings and rethink them in a performance context. 
3. Build a simple prototype with existing products and perform 
extensively with it in real conditions. 
Figure 2. Graphical cinema 
Figure 3. Camera-based cinema 

Figure 4. Screenshot of the live cinema prototype.
3.1 The DJ analogy 
Skilled hip-hop DJs know so well their record collection that 
they are able to perform musical collages sample by sample [18]. 
If you replace these sound samples by film shots, you have a 
simple live cinema tool. The basic equipment of a DJ is : 
- a rack of records, sorted by genre, beats per minute or other 
personal classifications scheme s, where spatial memory and 
personal organisation are of utmost importance. 
- two turntables, each featuring random-access (the arm can be 
positioned anywhere on the disc by increments of a revolution) 
and speed control; a sound mixer, optionally with equalizers and 
effects. 
The task of a DJ can be decomposed into the following three 
operations : 
1. choose record (media retrieval) 
2. preview the record on the headphones and find the right track 
or sample within it; adjust speed , filters, effects (preview and 
adjustment) 
3. incorporate the material into the existing stream being played 
and manipulate the material by scratching, cutting, backspinning 
(live manipulation) 
Step 1 and 2+3 are two different problems, each one requiring 
their own interface. 
Step 1 is an organisational problem - a time-critical video 
retrieval problem, but not classical in the sense that the artist is 
very familiar with the raw material  and has classified the database 
according to her own criteria. The interface must provide some 
hierarchical classification and a browsing mechanism at least as 
efficient as thumbing through a rack of vinyl records. Finally, a 
search engine should behave more like a suggestion engine , 
featuring some fuzziness in orde r to stimulate improvisation and 
creative exploration rather than deterministic execution. Step 2+3 
are problems related to the design of a musical instrument [1]. 
The interface has to be hands-on, expressive and accurate for live 
manipulations of the media. 
3.2 First functional prototype 
We wanted to make a first prototype based on a simple mapping 
of the DJ activity into the visual domain, but robust enough to be 
used in a real performance context. There is a plethora of 
commercial VJ tools that can do exactly that [28] : video samplers 
where each sample can be triggered by MIDI. However, mapping 
each shot to a different key was not a useful solution. We decided 
to implement our functional prototype in a flexible environment 
and opted for Max with Jitter ; our patch is shown in Figure 4. 
We have a rack of clips at the top, two video players and a 
crossfader. For each player : timeline random acces, speed 
control, YRGB colour control, zoom and pan. In addition, we 
have a subtitle module that allows the performer to type live 
subtitles as a narrator. As a tactile interface, we used a wacom 
tablet (allowing fast and accurate movements for scratching), the 
computer keyboard (to start, stop, cut and change direction) and 
16 midi sliders for continuous controls (colour, zoom, pan). The 
PowerBook G4’s dual graphics card allows for the use of one 
screen as the workspace and the other to output the visuals. 
This prototype was used for a bout 20 performances in festivals 
and clubs across Europe and the US, accompanying DJs or 
electronic musicians. The limitations of these laptop performances 
can be summarized in the following two main problems : 
The first problem is related to the organization of the footage, 
which was not dealt with in this prototype; this directly 
determines the content of the performance. Visibility of all clip 
names on screen does not imply it is easy to find the right one ; 
besides, in the heat of a performance, decisions have to made 
extremely fast, with a high factor of serendipity. What the tool is 
missing is an understanding of the time and state of the 
performance, so that it would suggest a critical subset of relevant 
material at the right time. 
The other problem, familiar to electronic musicians, resides in 
using the laptop as an instrument. During our shows, most non-
specialist audience members assumed video was prerecorded and 
did not understand the performer’s role on stage. We concluded 
that the interface needs to be : transparent, because the audience 
wants to see the process. It wants to see the performer’s actions 
and understand what is happening behind the scene; and 
performative, so that the audience can be engaged in the 
performer’s effort and perceive how it is related to the images and 
sounds produced. 
3.3 Revisiting the editing tool 
The other line of research that this project inherits from can be 
traced in the genealogy of non-linear film editing tools, who 
started to appear with video in the late 80s. During the past five 
years, almost all film editors have moved from analog to digital 
postproduction tools. Before that, they were working with 
celluloid, scissors (film splicer) and glue (transparent tape). These 
mechanical editing machines (Steenbeck, Moviola, Kem) were 
essentially composed of a motor to carry the film and a projection 
system to watch the frames. After a generation of cumbersome 
tape-based tools (Ediflex, Montage I/II/III, EMC2), non-linear 
Figure 5. Laptop performance at Le Placard, Paris, 5 am.
editing tools started to appear thanks to image compression 
standards and large random-access storage devices such as the 
hard disks. Avid has been leading the market, followed by 
Lightworks and Apple Final Cut pro. 
This first generation of comput er-based postproduction tools is 
still a very literal translation of their analog equivalent; they were 
not redesigned to take advantage of the very distinct properties of 
digital media. In order to comprehend these problems, we 
interviewed the renowned French film editor Joële van Effenterre. 
We report here her main criticisms : 
- With the exception of Lightworks, software editing tools are 
based on a desktop interface, where shots are represented as files 
in bins with no tangible representation. Because they were made 
in existing operating systems, their GUI is limited to the WIMP 
conventions : mouse, menu, window, file, which are office work - 
not film – metaphors. 
- The screen real estate is obfuscated by too many one-use buttons 
and information not relevant to the editing tasks ; the only 
elements really needed are the images and a way to cut and paste 
them. Important elements shoulds be visually emphasized. 
- Dexterity is constrained by th e mouse (and to USB jog wheels, 
or Lightworks’ Steenbeck-style wheel controller). In physical 
space, organization of the material and spatial memory were 
much richer. The proprioceptive evaluation of the length of a 
celluloid strip gave a much more intuitive sense of time. 
- Images are far too small and look very different from what will 
appear on a cinema screen (the resolution of a frame of celluloid 
is orders of magnitude higher than that of a computer screen). 
- Last, the importance of the accident is underestimated : coming 
across lost rushes was one of the main sources for creative 
combinations in the task of editing. The technological drive for 
speed and efficiency has ignored that. 
In addition to these reflections, we took into account the key 
research questions posed at the MIT Media Lab Interactive 
Cinema group [5], where experiments in interactive editing 
systems have been ongoing since 1982. Key issues therefore 
include   : 
- How to think of novel visual representations of the footage, 
besides the traditional "bin and timeline" metaphors, in the 
organization, browsing and assembly of the footage ? 
- How can the editing tool have a sense of the narrative structure, 
have more intelligence about what cuts and shots mean, to assist 
the editing by suggesting shots ? What is the influence of the tool 
on the content ? 
4. DESCRIPTION 
Taking the above into account, we describe here the system we 
have built and for each of its components we explain our design 
choices and some implementation details. 
Our Live Cinema interface consists of a large translucid canvas, 
on which the performer can grab and move images using the two 
hands simultaneously. Images repr esenting shots float across the 
screen and react upon touch. By moving them to the bottom of the 
screen, they can be explored in detail and routed to the main 
projector by using the two tangible video drums featuring haptic 
display. 
4.1 Image canvas 
A general inspiration for the design was the editing system 
conceived by John Underkoffler for Spielberg’s film Minority 
Report : a large display area, a gestural choreography using the 
two hands. 
Since we wanted to retrieve and manipulate two-dimensional 
shots for projection on a two-dimens ional screen, it made sense to 
work with a flat interface surface. We wanted gestural control but 
haptic feedback (unlike Justin Manor [11]). We wanted the 
performer to be able to touch the images and m ove them around, 
taking advantage of the two hands for different operations. 
Because the canvas is translucid and held vertical, audience all 
around the room are able to see and have an understanding of the 
performer’s operations. The display is very large, sharp 
(depending on projector definition and distance to screen), and 
has a full viewing angle (unlike flat LCD screens). 
Using vision-based tracking for pointing, the two hands can be 
used with distinction between  left and right – a powerful 
additional degree of freedom for the interaction design. We 
designed small battery-operated LED thimbles that the performer 
has to wear on each pointing finger. To be able to discriminate 
between left and right hand, we use different color LEDs (blue for 
left, red for right). The offset between the LED and the fingertip 
is corrected in software during calibration. 
Implementation details 
We use rear-projection on a hi gh-contrast Reversa screen 
(extruded acrylic containing diffusing polymer beads). Finger 
tracking is vision-based. A color camera is pointing at the back of 
the screen, so the LEDs can be s een through when the fingers are 
very close to the screen. To re duce CPU load, we filter optically 
as much as possible : 1. by pe rpendicularly polarizing projection 
and camera with filters, so that  only LEDs are seen and not the 
projected image ; 2. by closing down the shutter to the minimum 
so that the captured image is black save for small clusters of 
Figure 6. our Live Cinema instrument.
Figure 7. Video drum.
colored dots for each pointer. 
Fast software tracking is  done by performing 4x4 RGB 
subsampling on the captu red image, identifying loci of highest 
brightness and comparing red and blue components for hand 
differentiation. The bandwidth of this pointing interface is limited 
by the sampling frequency of the camera and capture card (here 
50 interlaced frames per second - PAL). A 4-point calibration is 
performed at each system startup by asking the user to touch the 
four corners of the screen. 
4.2 Video drum 
While the image canvas is very adapted for exploring the 
footage, it is not very appropriate  for hands-on, fast, expressive 
and accurate live manipulations. Therefore once a shot has been 
selected for playback, it can be controlled by a physical video 
drum (Fig. 7). 
Time only has two 
topologies : linear or 
circular [9]. In our 
prototype, we were 
using a linear 
controller. Here, we 
decided to use a 
circular disc-based 
time controller, 
bringing back the 
looks of old-
fashioned Steenbeck 
editing decks. 
The video turntable has the following functions :  
- speed haptic control . There is a direct bidirectional relationship 
between the speed of the disc and the speed of the video it 
represents; if slowed down or sped up, the disc will continue 
spinning at this new speed (and so will the video). 
- frame-by-frame position control . The high-resolution motor 
encoder allows to navigate the shot at the frame increment, for 
accurate positioning, scratching or other non-linear time motions. 
- hit sensitivity. To switch (cut) to a video stream, one can hit on 
the corresponding drum. Hitting hard causes a subliminal cut 
(flash cut). 
- haptic display features. In future developments, we are working 
on haptically displaying other featur es in the shot, such as cuts 
(automatic scene change detection in the video) [19]. 
Note that there exist a few simple commercial systems that allow 
to turn a conventional turntable into a rough video controller, 
using a barcode disc and replacing the pickup head by an optical 
encoder [30, 36]. 
Implementation details 
 We use direct drive for the disc, using a Maxon motor mounted 
with an optical encoder. The encoder is used to measure 
movement and speed manipulations; to insure accurate speed 
control in a feedback loop; and for haptic display features. We 
designed our own electronics board to control and drive the 
motor, detect hits and communicate using RS-232 with the main 
CPU. Hits are picked up by an electret microphone, filtered and 
amplified on the board and proce ssed by a PIC microcontroller. 
The disc is covered with Kevlar for a good grip.   
4.3 Interaction design 
Interaction design for the image canvas is a work in progress. 
We present here the ideas in development. 
Our minimalist visual interface is only made of moving images 
– their relationship is represented by their size, colour, behaviour 
and haptic display. The cinema sc ore is represented as a network 
of shots drifting across the screen, where invisible springs 
between the icons will eventually represent a sense of narrative 
coherence. We are studying visu al representations of narrative 
structure and semantic classifica tion of shots [13], in order to 
display a dynamic mesh of shots clustered by space, time, 
character, emotion, narrative arc. The performer needs to be 
offered just in time a relevant subset of clips while keeping an 
overall sense of the unfolding narrative. We hope to go beyond 
hierarchical and timeline classifications (such as the concept of 
scenes that sound and lighting designers use on their consoles to 
store their presets for a live show). 
Two-handed interaction offers many options for intuitive 
manipulations [8] : e.g. one hand can hold an image and the other 
be used to rotate/scale it, or a shot icon can be expanded to its 
time representation by pulling on both its ends.  
Implementation details 
Vision tracking plus constant hard disk polls to display the 
movie icons require the need for a second computer in charge of 
the final projection output (see Fig. 8). While the interface 
(master) computer has low-resolution copies of the video clips 
(for preview on the image canvas), the projection (slave) 
computer has high-resolution copies for the big screen. This 
guarantees fluid projection to the audience at full frame rate. The 
master only sends control information when needed to the slave 
through UDP/IP : clip reference, frame number, speed, colour 
correction, zoom, pan, alpha value for each layer. On both 
machines, software is written in java with QuickTime and 
OpenGL. It is possible to connect additional MIDI or serial 
controllers. 
5. CONCLUSION 
With recording technologies, both music and film have lost the 
magic of the presence, the instant, the construction, the 
performance. But postmodern art practices show it is possible to 
make a performance art based on the playback of recorded 
Figure 8. General diagram. 

material. With Live Cinema, we want to bring back the filmmaker 
in touch with the audience; try to make film allographic, as art 
theorist Nelson Goodman would ca ll it [7], so that it would be 
different every time it is shown. As a true performance, the film 
would only exist in the presence of its author. Why live arts ? For 
the contact, the act of gift between artists and audience [12]; For 
the risk, the thrill of the audience toward the unexpected, the 
accident, the insight into the construction process; For 
improvisation and open forms, because text and recording are not 
holy and should be subject each time to reinterpretation and 
recreation. 
Finally, the live cinema instrume nt has sound-only applications, 
if video samples are replaced by sound samples. We hope to use it 
for the live creation of musique concrète soundscapes for theatre. 
This project raises questions about designing computer-based 
instruments working at high st ructural levels, where manual 
control is not mapped to the pr oduction of individual sounds but 
to the conducting of sound-producing processes. Between the 
key-to-sound mapping of the sampler and the rigid determinism 
of the sequencer, there is a range of instruments to be developed 
for live improvisation [15] ; the composition of material by the 
author on these instruments would be done in partnership with a 
generative program [23, 31] based on open structures breaking the 
linearity of the traditional written score ; the musician would steer 
through these structures during the live performance. 
For more information on the Live Cinema project, go to 
http://www.mle.ie/~michael/research/livecinema. 
6. ACKNOWLEDGMENTS 
We wish to thank all collaborators of the live cinema project, 
especially Stephen Hughes for elect ronics and Albert Quigley for 
industrial design. Thanks to Glorianna Davenport, Joële van 
Effenterre, Sile O’Modhrain, Marc Downie, James Patten and the 
Story Networks group for their inspiration. 
7. REFERENCES 
[1] Andersen, T. H. Mixxx: Towards Novel DJ Interfaces. In 
Proceedings of NIME, 2003. 
[2] Beekman, René. Composing Images. Lier en Boog, Series of 
Philosophy of Art and Art Theory, Volume 15 Screen-Based 
Art. http://www.xs4all.nl/~rbeekman/l&b.html 
[3] Bernstein, J. a discussion of NATO.0+55+3d modular. 
bootsquad.com, 2001. http://www.bootsquad.com/nato/ 
[4] Collopy, Fred. Designing an Instrument to Perform Abstract 
Animation in Real-Time. Proceedings of SFPCS, 2003. 
[5] Davenport, G. and Murtaugh, M. Automatist storyteller 
systems and the shifting sands of story. IBM Systems 
Journal, vol. 36, no. 3, pg. 446-56. November 1999. 
[6] Eco, Umberto. The Open Work. Cambridge: Harvard U.P., 
1989. 
[7] Goodman, Nelson. Languages of Art: An approach to a 
theory of symbols. Hackett, Indianapolis, 1976. 
[8] Kabbash, P., Buxton W. and Sellen A., 1994, Two-handed 
Input in a Compound Task. In Proceedings of CHI'94. 
[9] Le Poidevin, Robin. Relationism and Temporal Topology. In 
Travels. In Four Dimensions: The Enigmas of Space and 
Time. Oxford: Oxford University Press, 2003.  
[10] Levin, Golan. Painterly Interfaces for Audiovisual 
Performance. MS Thesis, MIT, 2000.  
[11] Manor, Justin. Cinema Fabriqué : A Gestural Environment 
for Realtime Video Performance. MS Thesis, MIT, 2003. 
[12] Mauss, Marcel. The Gift. New York: W.W. Norton, 1950. 
[13] Metz, Christian. Film Language: A Semiotics of the Cinema. 
New York: Oxford University Press, 1974. 
[14] Naimark, Michael. World’s First Interactive Filmmaker. 
Interval trip report, 1998. 
http://www.naimark.net/writing/trips/praguetrip.html 
[15] Nemirovsky, Paul and Watson, Richard. Genetic 
Improvisation Model : a framework for real-time 
performance environments. In Proceedings of EvoMusArt 
2003, Gloucester, UK. 
[16] Patten, J., Recht, B. and Ishii, H. Audiopad: A Tag-based 
Interface for Musical Performance. In Proceedings of NIME, 
2002. 
[17] Picon-Vallin, Béatrice. Les écrans sur la scène. L’Age 
d’Homme. Lausanne, Suisse, 1998. 
[18] Smith, Sophy. Compositional strategies for hip-hop 
turntablists. Organised Sound 5(2):75-79, Cambridge 
University Press, UK, 2000. 
[19] Snibbe, S. and MacLean, K. Haptic Techniques for Media 
Control. In Proceedings of UIST 2001. 
[20] San Francisco Performance Cinema Symposium. 
http://www.kether.com/SFPCS/index.html 
[21] Sonic Light 2003. http://www.sonicacts.com/ 
[22] Spiegel, Laurie. Graphical Groove: Memorium for a Visual 
Music System. Organised Sound 3(3): 187-191 1998 
Cambridge University Press.  
[23] Ward, Adrian and Cox, Geoff. The Authorship of Generative 
Art. Proceedings of Generative Art, 2004. 
[24] Warwick, Henry. Towards a Theory of Performance Cinema. 
In Proceedings of San Francisco Performance Cinema 
Symposium, 2003. http://www.kether.com/SFPCS/ 
[25] Winkler, Todd. Fusing Movement, Sound, and Video in 
Falling Up, an Interactive Dance/Theatre Production. In 
Proceedings of NIME, 2002. 
[26] Youngblood, Gene. A Meditation on the Vasulka Archive. 
Vasulka Archive. http://www.fondation-
langlois.org/e/collection/vasulka/archives/essais.html. 
[27] 0xff mailing list. 
http://www.music.columbia.edu/mailman/listinfo/0xff 
[28] Audiovisualizers. http://www.audiovisualizers.com 
[29] Cycling 74, jitter. http://www.cycling74.com 
[30] EJ. http://www.scientifikent.com/scratchTV/ 
[31] Generative. www.generative.net 
[32] Image/ine. http://www.image-ine.org/ 
[33] Iotacenter. www.iotacenter.org 
[34] Isadora. http://www.troikatronix.com/isadora.html 
[35] LEV mailing list. 
http://music.calarts.edu/~cchaplin/lev/lev.html 
[36] Miss Pinky. http://www.mspinky.com 
[37] SoftVNS. http://www.interlog.com/~drokeby/softVNS.html 
[38] VJcentral. www.vjcentral.com
 