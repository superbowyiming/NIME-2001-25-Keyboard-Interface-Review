SoundMorphTPU: Exploring Gesture Mapping in
Deformable Interfaces for Music Interaction
Zhen Wu
Hong Kong University of
Science and Technology
Hong Kong SAR
zwuch@connect.ust.hk
Ze Gao
Hong Kong University of
Science and Technology &
Hong Kong Polytechnic
University
Hong Kong SAR
zgaoap@connect.ust.hk
Hua Xu
Hong Kong University of
Science and Technology
Hong Kong SAR
epxuhua@ust.hk
Xingxing Y ang
Hong Kong University of
Science and Technology
Hong Kong SAR
xyangbx@connect.ust.hk
Tristan Braud
∗
Hong Kong University of
Science and Technology
Hong Kong SAR
braudt@ust.hk
ABSTRACT
Deformable interface is an emerging field with significant
potential for use in computing applications, particularly in
the design of Digital Music Instruments (DMIs). While
prior works have investigated the design of gestural input
for deformable interfaces and developed novel musical in-
teractions, there remains a gap in understanding the tangi-
ble gestures as input and their corresponding output from
the user’s perspectives. This study explores the relation-
ship between gestural input and the output of a deformable
interface for multi-gestural music interaction. Following a
pilot study to explore materials and their corresponding in-
tuitive gestures with participants, we develop a TPU fab-
ric interface as a probe to investigate this question in the
context of musical interaction. Through user engagement
with the probe as a sound control, we discovered that the
input-output relationship between gestures and the sound
can have meaningful implications for users’ embodied inter-
action with the system. Our research deepens the under-
standing of designing deformable interfaces and their capac-
ity to enhance embodied experiences in music interaction
scenarios.
Author Keywords
Material-Driven-Design, smart textile, sensorial interaction,
musical instruments, gestures, playability.
CCS Concepts
•Human-centered computing → Interface design prototyp-
ing; Sound-based input / output;
∗Corresponding author of this work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’24, 4–6 September, Utrecht, The Netherlands.
1. INTRODUCTION
Figure 1: SoundMorphTPU, the probe with which partici-
pants engaged during the study.
Gestures are essential in musical interaction and expres-
sion [38, 59, 43]. Musicians utilize specific gestures to pro-
duce sound when playing traditional acoustic instruments,
such as pressing a key on a piano or blowing air into a horn
to convey musical intention beyond sound through their
body language [12]. Digital Music Instruments (DMIs) free
the artist from the acoustic relationship between gesture
and sound, allowing the automation of music production
through sequencers, triggers, and predefined effectors [59].
As a result, electronic music artists have developed new
ways to leverage gestures during live performances, either
relying on automation to detach themselves from the in-
strument or associating effectors with highly visual inter-
actions [47, 48]. In the meantime, gestural interaction is
essential in material-based interaction [56], with the I/O
relation between user input and system output playing a
vital role in shaping meaningful experiences [49, 15, 42, 54,
20].
Deformable interfaces stand at the intersection between
materiality and gestural interaction, facilitating interactive
experiences unattainable with rigid interface [8, 5, 3, 53].
Material-oriented thinking has inspired the design of musi-
cal interfaces, specifically deformable materials [67, 68, 53].
Fabric, as a deformable material, has been applied in design-
ing DMIs and music performances by mapping different ges-
tures such as stretching, pressing, touching, and grabbing
to musical parameters [64, 63, 17, 39]. While existing stud-
ies show the potential of gestural inputs in providing rich
interaction, how to design the output based on the input
remains unclear.
To address this gap, this paper investigates the tri-modal
relationship between the characteristics of deformable ma-
terials, sound generation, and gestural input for music inter-
action. We aim to reintroduce gesture (input)-sound (out-
put) relationships while facilitating expressive interactions
in the design of DMIs applying deformable interfaces. We
formulate the following research questions to guide our in-
vestigation:
• RQ1: Can gestures mediated through the multi-gestural
deformable interface be significantly associated with
specific sound effects?
• RQ2: What constitutes the mapping between gestures
and sound effects?
• RQ3: How do users perceive using a deformable inter-
face with specific gestural (input) -sound effects (out-
put) relationships?
We particularly explore the potential of fabric materi-
als, which exhibit greater potential for multi-gestural sens-
ing and application in ubiquitous computing scenarios [51,
19, 32]. We approach these research questions through a
material-centric approach, relying on material probes [26].
We first conduct an exploratory workshop to establish how
users intuitively associate several fabric materials with sound
and gestures. We then selected one fabric material and de-
veloped a material probe using inkjet circuit printing that
can sense four types of gestures elicited from the pilot study:
sliding, tapping, poking, and patting. Using this probe, we
conducted a user experiment with 12 participants interested
in musical interaction to further explore the interactions.
This research yields the following findings:
• Although gesture-specific sound control matching could
be personal, common patterns were discovered, such
as sliding matched with continuous control and poking
and tapping to instant control.
• Common patterns were found in how users match the
material-mediated gesture with sound control. These
patterns involve making analogies with real-life expe-
rience, thinking of the direction and continuity of the
gesture, and imagining the sound deformation as the
deformation of the material. These observations in-
dicate the emergence of the concept of embodiment
within the context of deformable interaction.
• Multi-gestural deformable interfaces offer advantages
for musical interaction, including intuitiveness, learn-
ability, and playability, and provide a richer sensory
experience, with the potential for mapping richer sound
parameters than rigid input methods.
These findings highlight the importance of considering the
IO relation based on materiality when designing deformable
interfaces for music interaction, and its relationship with the
embodiment of the experience.
2. RELATED WORK
2.1 Materiality and Tangible Embodiment
The recent development of computational materials has made
designers and researchers rethink the relationship between
physical and digital [45]. Wiberg emphasizes the impor-
tance of materiality in interaction design and later proposes
materiality as a ”turn away from metaphors toward directly
with and via physical materials” [61, 62]. Karana et al.
propose Material Driven Design (MDD), emphasizing that
materials should elicit meaningful user experiences beyond
their utilitarian assessment [28]. Studies have shown that
different types of material afford different types of gestures
corresponding to material properties, with textures and ma-
terial sensations as the key influential factors [67, 25]. In the
case of music interaction, several researchers have stressed
the importance of materiality, highlighting the opportuni-
ties that arise from combining physical input through ma-
terials with virtual modalities [67, 53, 37, 36]. Tangible in-
teraction bridges physical user interfaces with cyberspace,
exploring the interplay between our bodies and the materi-
ality of objects [14, 23, 22]. Within this context, Embodied
Cognition (EC) emerges as a prominent framework [55, 30],
stressing the role of the body in shaping perceptual and
mental processes within the interaction loops [4, 31, 11, 35,
13].
2.2 Deformable Interfaces in DMIs
Deformable interfaces are made of soft and malleable mate-
rials, requiring physical input to be deformed for gestural in-
teraction that is unlikely with rigid interfaces [5, 3, 53]. The
tangibility of deformable interfaces enables a more organic
way for HCI, opening up the potential for gestural inter-
action that becomes analogous to sculpting and expressive
controls [8, 5]. Material property is important, as it links
with how the interface deforms, what input parameter can
be introduced by that deform, what gestures can act on it,
and finally, what sensing technology to use [5, 53]. A Dig-
ital Music Instrument (DMI) is characterized by having a
separate gestural interface, or gestural controller, from its
sound generation unit, with the two units being independent
yet connected through mapping strategies [59]. Deformable
interfaces have been applied to DMI design and show the
application of different materials to control different musi-
cal parameters, such as rubber [1], foam [29], and clay [60].
Fabric-like material presents interesting applications, espe-
cially in multi-gestural interactions. Examples include the
Knitted Keyboard, which employs touch, proximity, finger
pressure, and arm movements [64], ZPatch, a hybrid resis-
tive/capacitive eTextile input that can sense multiple ges-
tures [51], and Grabbing at an Angle , a fabric interface for
menu selection [19]. Several sensing techniques have been
put forward to support this type of material, including knit-
ting and weaving using electronic yarn and printing [6, 24,
46, 50, 24, 7, 16]. Moreover, the Machine Learning method
can be combined with the sensing surface for training and
recognizing different gestures [18]. The multi-gestural inter-
action observed in fabric material has inspired us to utilize
such materials as the focus of our research.
2.3 Gesture Mapping
Gestures are intertwined with the expressive aspect of elec-
tronic musical performance, the source of excitation mo-
ments [43]. DMIs allow for empty-handed, naked, or phys-
ical contact gestures [38], with the latter being typical for
instruments with a material-oriented perspective. Gestur-
ing with physical artifacts can be more meaningful from the
perspective of embodiment [54, 14, 2, 56, 27]. In the mean-
time, the relationship between the input and output of an
interactive system is called IO relation, and mapping is a
term that denotes the steps between them [49, 15, 21]. Dig-
ital Music Instrument(DMI) is an area interacting with ges-
tural control and placing an important focus on I/O map-
ping [54]. Compared with acoustic instruments, DMIs free
the artist from the acoustic relationship, allowing assigning
new sound mapping strategies to the gestural interaction.
Given the unique properties of deformable material, pre-
vious studies have contributed to examining how it is mean-
ingful for gesture-based interaction. Lee et al. focused
on user-defined gestures for flat deformable interfaces and
found that higher flexibility enhanced consensus and in-
tuitiveness of gestures [33]. Troiano et al. investigated
deformable interfaces for music performance and found a
stronger bond between gestures and effects compared to
rigid controllers [53]. However, they did not explore the
gesture mapping of deformable interfaces allowing multi-
gestural interaction. Zheng et al. studied the meaning of
material properties in a musical context but focused on de-
signing gestures rather than the IO relation [67]. This lack
of exploring the I/O relationship in the deformable interface
was also mentioned in [5].
2.4 Methods for Material Exploration
User-elicitation studies are a specific type of participatory
design methodology that involves end-users in the design
of gesture-sets [65, 57, 66]. For DMI design, it is applied
in the study from Zheng et al. [67] and Torani et al. [53].
This method can result in gestural inputs that are conceptu-
ally simpler [40]. The Material Probe method, proposed by
Jung and Stolterman [26], uses physical materials to prompt
participants to discuss, play with material samples, and
compare the material qualities of physical-digital artifacts.
The Material Speculation method, proposed by Wakkary et
al. [58], involves crafting artifacts that support and question
new possibilities that contradict the world around them, en-
couraging reflection on materiality instead of just function-
ality. These materials exploration methods can be applied
in investigating the design opportunities of unfamiliar sens-
ing materials [41]. They have also been applied in previous
DMI studies [53, 67] to understand deformable sensing ma-
terials and use them as a resource for design.
3. EXPLORATORY STUDY
The exploratory study investigates the relationship between
materiality and gesture in deformable interfaces for DMIs.
Specifically, it seeks to identify the primary gestures associ-
ated with different types of fabric used as deformable mate-
rials and the users’ sound imagination towards the different
materials. We used gestural elicitation (see Section 2.4) as
the primary method in this study to gather insights and un-
cover the unique gestures that emerge from each material.
We were also inspired by the method of material probe [26]
and the method of gathering user-defined gestures without
considering the sensing ability by previous a study [33].
3.1 Materials
Durability is a significant concern when selecting materials
for sensors in deformable interfaces [24, 5]. We thus chose
fabric materials that are both durable and capable of sup-
porting various deformations. We selected five high-density
and hydrophobic fabric materials similar to those used in
existing multi-gesture sensing sensors [52, 16]. These mate-
rials include elastic space cotton, PU(Polyurethane) fabric,
Tyvek, TPU(Thermoplastic Polyurethane) fabric, and Silky
down jacket fabric. Their surface appearance and material
specification are listed in Figure 2. These fabric samples
were purchased from the local market for garment material.
For each material, we prepared three different sizes: small
(10cm x 10cm), medium (A4 size), and large (A3 size).
3.2 Participants
Eight participants (4 females and 4 males, aged 23 to 25
years) were invited to this study. All participants had pre-
vious experience playing musical instruments at a level self-
described as amateur. In terms of their playing habits, three
participants played instruments regularly every week.
3.3 Procedure
This study used a within-subject design. Before the ex-
periment, participants were asked to complete a consent
form and a questionnaire that collected their demographic
background and previous music experience. All participants
underwent five sessions, each corresponding to a set of sam-
ples of one of the five types of material provided in the
three sizes. Participants were asked to perform gestures on
the material samples, imagining that the gestures would be
sensed and used to control sounds. Participants were en-
couraged to imagine and report the sounds they thought of
while performing the gesture. Participants’ gestures while
interacting with the material samples were recorded using
a webcam from the computer.
3.4 Analysis and Results
Figure 2 summarizes the findings observed from this ma-
terial exploration study. Fabric materials used in the pilot
study are placed at the top, and gestures elicited by the par-
ticipants during the study are placed in the middle, followed
by the description of the imagined sounds that the mate-
rial can generate if it is an instrument. For the gestures
and descriptions of sounds, the words appear larger, mean-
ing higher frequencies of occurrence during the study. Some
common gestures were shared among different material sam-
ples. For instance, the gesture drag. This gesture takes
advantage of the material’s elasticity, as it involves pulling
the material in the horizontal direction. Additionally, we
noticed that single-handed gestures were more prevalent in
small and medium-sized material samples. These gestures
included sliding with the palm, patting, and sliding with
the finger. Overall, the exploratory demonstrates that dif-
ferent fabric materials may evoke different types of gestures
to interact, and the feeling towards the sound produced by
the fabric is also unique to the materiality. This inspired
us to explore further the relationship between the applied
gestures and the type of sound control in our main study.
We chose TPU fabric as the proof-of-concept material.
Although other fabric samples present interesting insights,
compared with the other sample, the TPU fabric is effective
in being manufactured into a sensing unit, as demonstrated
in a previous study(see [9, 52]). Furthermore, the TPU fab-
ric evoked diverse gestures and elicited intriguing imagined
feelings toward sound. Our investigations revealed that par-
ticipants tended to poke the fabric when imagining sounds
associated with raindrops. Additionally, participants en-
gaged in patting, tapping, and sliding gestures. Partici-
pants also associated the fabric with soft sounds and sounds
produced by electronic instruments. Figure 3 shows some
gestures with which participants interacted with the TPU
fabric during the exploratory study.
4. SOUNDMORPHTPU
4.1 Probe design and fabrication process
Our exploratory study identified several interactive gestures
that can be applied to our chosen fabric materials, TPU.
Although flipping and flickering are interesting, the sens-
Figure 2: Fabric material used in the exploratory study (on
the top) with the corresponding name and specification, ap-
plied gestures (on the middle), and the imagined sounds ob-
served based on the corresponding fabric (on the bottom).
The words that appear larger represent a higher frequency
of occurrence during the study.
Figure 3: Participants employed diverse gestures to inter-
act with the TPU fabric samples, as captured in the video
recordings. Beside the captures are demonstrations of the
gestures.
ing mechanism was not easy to implement and was hard
for users to control. Gestures that are both easily sensed
and show the deformation quality: sliding with one finger
(index finger), patting, poking, and tapping were finally se-
lected to implement. We thus developed SoundMorphTPU,
a soft, thin and flat deformable interface for multi-gesture
recognition, as the key material probe for our main study.
To develop this material into a sensing surface, we aim to
avoid compromising the original materiality of the TPU fab-
ric by minimizing the impact of sensing unit attachment.
We characterize the TPU material as flexible, transparent,
lightweight, smooth, and nonporous. It is waterproof and
hydrophobic, while its shape can be sustained as flat af-
ter deformation. Considering the sensing technology, ap-
plying embroidered conductive thread onto its nonporous
and continuous surface is not appropriate, and attaching a
conductive fabric as another layer would compromise the
transparency and smoothness of this material. Islam [24]
has identified wettability and surface roughness as neces-
sary material requirements for conductive ink printing. Our
TPU fabric’s properties make it suitable for printing. In-
spired by Gong et al.’s [16] array pattern for sensing mul-
tiple gestures, we designed a five-vertical line pattern array
with two circles at the end of each line. The circle patterns
facilitate the attachment of the conductor wire to the cir-
cuit. The size of our sensing pattern and the circuit diagram
for its connection with the Arduino Uno board are shown
in Figure 4. This pattern is similar to the pattern used in
Silver Tape [7], further supporting the feasibility of using
this pattern to sense deformation. With the five sensing
units as input, we aim to apply a machine learning model
to train gestures, as described in previous work [18].
We printed the conductive pattern with a programmable
dispenser PC-400H with a piezoelectric nozzle PDV-8000.
We programmed the pattern using the dispenser’s software.
We used Sicrys ™ I60PM-116, a conductive ink based on
single-crystal silver nanoparticles, to print the pattern and
fill it in the dispenser. We stabilized the TPU fabric with an
embroidery hoop and tape during the printing process (see
Figure 5). We repeated the printing process twice to ensure
conductivity and then cured the printed ink in a chamber
at 85 degrees Celsius for 30 minutes. After confirming the
conductivity of the printed patterns, we connected the ends
of each pattern link with crocodile clips. We constructed it
into a circuit linked with an Arduino Uno board. The defor-
mation of each sensing pattern results in continuous changes
in the input pin’s analogue values. When the printed pat-
tern is deformed, its resistance and the analogue value out-
put in Arduino IDE change. We first tested the analogue
signal changes of each pattern using visualizers, as shown
in Figure 6; the deformation of each unit is distinguishable.
After that, we link the signal input to Wekinator1, an open-
source software for using machine learning to train different
gestures. Figure 7 shows the four gestures we aimed to sense
through the SoundMorphTPU. The researcher who devel-
oped the prototype trained these gestures in Wekinator un-
til they could be successfully differentiated from each other.
To protect our printing pattern when interacting with it,
we placed another TPU fabric above the sensing unit. Fig-
ure 8 shows the final printed pattern on TPU fabric and the
constructed SoundMorphTPU.
4.2 Software and Sound Design
We connected SoundMorphTPU with Wekinator for gesture
recognition. We pre-trained the four tested gestures and a
1http://www.wekinator.org/
Figure 4: The Sensing Pattern and Circuit Scheme of Sound-
MorphTPU consists of units with two circles placed at the
beginning and end of a line, each having a diameter of 10mm.
The distance between each unit is 17mm.
Figure 5: A programmable dispenser was used to perform
the conductive pattern printing process. To maintain a flat
surface during the printing, we placed the TPU material on
an embroidery hoop.
Figure 6: The usability of SoundMorphTPU in sensing ac-
tions applied to different conductive units was tested, and the
force exerted during the interactions was visualized using the
Processing programming language.
Figure 7: The Four Gestural Inputs of SoundMorphTPU,
including sliding with the index finger, tapping with two fin-
gers, poking with the index finger, and patting.
Figure 8: Final printed pattern on TPU fabric, which was
constructed and linked with an Arduino Uno board to create
the SoundMorphTPU system.
no-gesture status before the study. We linked Wekinator
with MaxMsp 2 via Processing 3 as a signal transmitter
to trigger different sound effectors. We select six effectors,
pitchshifting (in our example, shifting the sound sample in
a higher tone), reverb, flanger, chorus, EQ filter (in our
example a high pass filter) anddelay and programmed them
in MaxMsp.
To evaluate the sound control, we included five samples
based on the sound feeling towards the material in the pre-
vious exploratory study (Section 3), including a sine wave, a
drum playing sample, a jazz music piece, a piano loop, and
an electronic sound loop, which we produced using Garage-
band 4. A counter was set to ensure the effect remains active
for four seconds after the effector is successfully triggered.
For reverb, we set a fade in and fade out lasting eight sec-
onds to avoid sudden trigger effects. Figure 9 shows the Max
Patch in the programming view and the presentation inter-
face. In the presentation interface, the participants could
interact with different samples from the list to try the cor-
responding effects triggered by gestures.
5. MAIN STUDY
The main study investigated the relationship between in-
teractive gestures of deformable interface and music control
using SoundMorphTPU as the material probe. Specifically,
we aimed to explore (1) Whether certain gestures mediated
through the material can be significantly associated with
sound control. (2) What constitutes the mapping between
gestures and sound effects? and (3) How do users perceive
using a deformable interface with specific gestural (input)
-sound effects (output) relationships? We focused on typ-
ical sound effects that are essential elements for electronic
music, building upon previous studies that have indicated
that deformations are commonly employed to modulate or
apply effects to sounds [53].
5.0.1 Study Setup
Figure 10 shows the setup of the study. The studies were
conducted in a quiet environment, and we provided headsets
for participants to listen to the samples. SoundMorphTPU
was connected to a laptop, and the necessary software was
installed. Participants sat on a chair beside SoundMor-
phTPU and used various gestures to control the sound. We
also placed a traditional music controller, Launchpad5, near
the participants to facilitate discussion after the gesture-
matching activities.
2https://cycling74.com/products/max
3https://processing.org/
4https://www.apple.com/mac/garageband/
5https://novationmusic.com/launch
Figure 9: Interface in MaxMsp for the Six Effects. Left:
the patches constituting the effects. Right: the presenta-
tion interface for the participants to select sound control and
samples.
Figure 10: Demonstration of the study setup, including ex-
perimental hardware and equipment.
5.1 Study Design and Procedure
The study design is a combination of quantitative measure-
ment and interviews. We used a within-subject experimen-
tal design where all participants tried four conditions: slid-
ing, tapping, poking, and patting, which are the indepen-
dent variables for gestural inputs. The presence of these
conditions was counterbalanced among participants. The
matched sound effects, including six variables (pitch shift-
ing, reverb, flanger, chorus, EQ filter, and delay), were the
dependent variables investigated. After completing the ges-
tural input-sound effects matching tasks, we interviewed the
participants to gather qualitative data on their experiences.
Initially, participants were asked to complete a consent
form and a demographic questionnaire, which included age,
gender, country, and profession. They also answered ques-
tions about their previous music experience and favourite
music styles. Next, we introduced the study objectives and
digital music controller for electronic music. To facilitate
understanding, we presented the Novation Launchpad 6 as
an example. Participants were introduced to the six effects
and asked to wear the headset to try the different sound
effects using the MaxMsp interface. Once familiar with
the sound effects, participants proceeded to the first condi-
tion. The researcher first demonstrated the first gesture on
SoundMorphTPU, after which participants used this ges-
ture on SoundMorphTPU to trigger the effects. Partici-
pants could freely choose sound samples within each effect
group and apply the studied gesture to the fabric surface
to produce a corresponding sound effect. After trying all
the effects, participants were asked to select an effect they
thought fit the gestural input the most ( Q1) and indicate
their confidence level on a 5-point scale ( Q2). Next, the
researcher conducted a short open-ended interview to ex-
plore the reasons behind participants’ choices and detailed
thoughts on the gesture. Participants then moved on to the
following condition to explore another gesture. After com-
pleting all conditions, the researcher conducted a further
open-ended interview to gather participants’ perceptions of
using this deformable interface as input for music control
and the differences between this method and existing music
controllers. The study took approximately 30 minutes per
participant.
5.2 Participants
We invited twelve participants, mainly students and re-
searchers in human-computer interaction who had an in-
terest in musical interaction, to participate in our study.
Different from previous studies [67, 68, 53], we did not limit
the participants to musicians or DMI designers. Partici-
pants were recruited from institutional recruitment and ex-
hibition venues. There are nine females and three males
(25 ± 1.25 years, ranging from 21 to 38 years). All par-
ticipants had to fill in the ethics and consent forms before
participating in the study.
5.3 Data Collection and Analysis
We used Qualtrics 7 to record and manage participants’
answers. We calculate the frequency of each choice from
Q1 and the rating data distribution of Q2. For the results
obtained from the interview, the researchers transcribed
the recorded audio into notes. Then, several Affinity Di-
agrams [34, 44] were built using an online whiteboard tool,
6https://novationmusic.com/launch
7https://www.qualtrics.com/
Figure 11: Matching Level and Confidence Level Diagram
Miro 8 to conduct basic coding of the answers and summa-
rize topics originating from them.
6. FINDINGS
6.1 Gesture-sound Effects Matching
As for the gesture-sound matching ( Q1), for patting, the
most selected sound effectors (3 times) are the flanger and
EQ filter (3 times). For poking, the chorus was selected the
most (4 times), as the same case for tapping (4 times). For
sliding, flangers were selected the most(4 times). For the
confidence level, tapping reaches the highest confidence level
in matching with sound effectors (M = 4.417, SD= 0.493),
and sliding is the second ( M = 4 .250, SD= 0 .433). As
shown in Figure 11, most participants were confident or
very confident about the matching in these two conditions.
6.2 Perception of Gesture
The affinity diagram analysis identified several topics and
common grounds from participants’ thoughts about the ges-
tures. We present the findings for different gestures. Fig-
ure 12 provides a simplified version of the affinity diagrams.
6.2.1 Sliding
When trying to control sound effects by sliding, some par-
ticipants made analogies with real-life scenarios or nature.
Four participants thought of water (P1, P2, P7, P8). P7
chose ”chorus” and commented that interacting with the
material is like ”running your hand through the ripples”.
P2 chose ”reverb” and also thought of water ripple. P12
chose ”reverb” and described the gesture as ”pushing the
airflow”. P11 described the match with ’delay’ as ”sliding
appears similarly to trying to remove dust from the sur-
face of an object, equivalent to the delay sound effect of
bouncing the last few notes back”. Five participants em-
phasized sliding as a directional gesture and would think
of directional sound effects (P1, P2, P6, P8, P9). P9 said:
”I chose ’reverb’ because the sound is like moving from the
side close to me to the side away from me, corresponding
to the directional gestures I make”. Some participants as-
sociated sliding with continuous sound effects like ”reverb”
because of the perceived similarity between the ”continuous
nature of the sliding gestures and the continuous quality of
the sound effect” (P10).
6.2.2 Tapping
The most common thinking about this gesture is its instant
nature, and four participants (P1, P2, P3, P10) thought
that it should be matched with instant changes of the sound
as ”dots like (P10)” effectors. P2 chose the EQ filter as ”it
8https://miro.com/
provides me a feeling of start and stop, instant, ’on’ effect”.
In addition, three participants mentioned that the tapping
gesture gave them a strong sense of how the sound changed
through the movement of their fingers (P6, P7, P9). P7
described his feeling as ”adding new tracks through the fin-
ger, and is like cell sorting”. Two participants mentioned
the gesture as ”rush and messing up” and chose the effec-
tor based on this. P11 chose flanger since it ”produces a
messy feeling”. When attempting to trigger sound effec-
tors through tapping, some participants envisioned how the
gesture would physically and tangibly transform the sound.
For instance, P2 provided an example: ”The ’delay’ effect
is like repeating the sounds, and tapping on the sound fre-
quently may produce this effect.”
6.2.3 Poking
Like sliding, poking on the fabric-like surface also shows
a connection with other scenarios. Analogies with switch
buttons were most common in the associations, and par-
ticipants chose sound effects associated with switches (P1,
P4, P10). P1 commented, ”This gesture gives me a feeling
of pushing a button, and it is temporal, one time, fitting
the changes of sound by the EQ filter ”. Three participants
reported that the feeling of sound effects corresponding to
this gesture should be instant, which will disappear after re-
lease. P2 said, ”Keep poking means turning the effector on”,
and P4 said, ”This gesture does not have the feeling of slow
changes”. Another interesting finding is that three partici-
pants mentioned they think of poking people (P6, P9, P11)
and that people produce funny sounds, which is the effect of
pitch-shifting and flanger. P6 chose pitch-shifting and said,
”It is like a person singing, and I poke him, then he changes
the tone”. Thinking of movement exert on sound inspired
three participants (P8, P2, P5). P8 commented, ”like the
sound was jumping when I used poking. The sound seemed
to mimic my hand gesture”. P2 chose pitch-shifting as ”a
feeling of pressing on the sound and making it deform, very
intuitive with poking”.
6.2.4 Patting
The most apparent finding for the sound effects matching
with patting was the analogy between interacting with the
fabric surface and other things. This includes patting on
a ball and making some bouncing sounds(P4, P6), covering
the sound with something to filter out some noise (P1, P11)
and analogizing it with existing musical tools like patting
on cowbells or triangle irons (P5). The idea of sound defor-
mation also came up. P7 chose pitch-shifting and felt like
”patting on the sound and the notes shows distortion”. P12
chose EQ filter since it felt like ”pressing on sound and some
part of the sound is filtered out”.
6.3 Perceptions of multi-gestural deformable
interface for Sound Control
Four main common topics were identified through the in-
terview questions about using a multi-gesture deformable
interface as input for music control.
6.3.1 Perceived Advantages
Two participants (P1, P2) considered the TPU material
deformable and can be changed into different shapes, such
as clothing or carpet. P2 proposed a carpet that can use
feet to control music. They also mentioned that the TPU
material can be placed above other stuff or as ”another layer
Figure 12: Gesture-related data collection
of an existing instrument” (P1). Compared with a rigid
music controller, it is portable and flexible (P2) and can be
placed at different room corners (P1).
6.3.2 Intuitiveness and Playability
Four participants mentioned that deformable-based input
is simple and easy to use. Beginners interested in elec-
tronic music wanted to use this probe to explore and ex-
periment with sounds (P4, P6, P8. P9). P8 mentioned that
deformable-based input is ”more exploratory, more experi-
mental, more creative for beginner, and I would prefer this
deformable-based method”. P8 also complained about the
complexity of the existing music controller and synthesizer,
as ”they are limited in small interface and tiny buttons”.
Participant 4 discussed LaunchPad as ”very complex com-
pared with deformable input” and preferred to ”control dif-
ferent sound effects using a simple interface”. They also saw
the deformable-based input as ”free-minded and flexible for
playing”. When grasping the gesture, participants antici-
pate they can ”play it without looking it—like the Touch
Typing when using the computer” (P5). P4 also mentioned
playability as ”the deformable-based input method makes
me more willing to play it and trigger my feeling of explo-
ration”.
6.3.3 Richer Sensory Experience
Participants thought the deformable-based input method
triggered their desire to pay close attention to the sound.
P6 said, ”I will guess and think about whether sound effects
are being triggered, which requires me to listen closely”.
P11 mentioned that ”the deformable-based input provides
a stronger sense of connection and creates heavier emphasis
on the senses of hearing and interaction”. Participants also
described the deformable-based input as exploratory, sen-
sory, triggering exploration, and ”more natural to human
skin” (P11).
6.3.4 Controlling Richer Sound Parameters
The last type of finding is the potential for richer gestural
control away from the current gestures of SoundMorphTPU.
P7 suggested that the TPU fabric surface can provide non-
mechanical deformation and potentially use a 3-dimensional
variable (x,y,z) to change sound. Strength, direction, dis-
tance, and speed can influence the sound. P11 thought that
a deformable interface controller could be more interesting
than a traditional music controller. It can utilize more sub-
tle gestures, such as soft touch, caressing, or sliding. How-
ever, P11 was doubtful at sensing technology to differentiate
between soft and hard gestures, but it would be ”amazing”
if it did. P11 and P1 proposed the same further gestures as
drawing soft gestures (e.g., simple shapes such as circles) on
the deformable surface as an interesting input. Moreover,
P11 expressed his wish for more continuous input beyond
the on/off effects, such as the finger’s location, which can
be sensed to control detailed intensity.
7. DISCUSSION
This section discusses the emerging themes from the study
and summarizes their design implications and future re-
search directions.
7.1 Gestures-Sound Mapping
While the choice of effects for each gesture varied among in-
dividuals in our study, further interviews revealed common
themes. We identified several patterns across the percep-
tions of different gesture-sound effects matchings that sup-
ported the reasons for participants’ choices and provided
insights into their further thoughts about the gestures.
7.1.1 Sound Association Based on Analogy
In this study, all four gestures have answers based on an
analogy with other things. These include: (1) Considering
the deformable surface with other substances such as wa-
ter and airflow and interacting with them through gestures.
(2) Think of the same gestures utilized in existing musi-
cal tools that they have experienced and choose a sound
effector based on that. (3) Thinking of real-life scenarios,
such as poking a person or putting off dust. Our findings
are consistent with Troiano et al.’s study [53], which found
that deformable interfaces embody the parameters of the
musician’s control. The thinking observed in our study,
based on established experience, is also similar to the study
by Zheng et al. [67], which investigated the materiality of
different deformable materials. In their study, eight par-
ticipants suggested that the interaction implication of the
material for music context mainly came from their previous
experience with existing interfaces. Similarly, in Troiano
et al.’s study, six participants twisted an object to control
sound effects and reported their inspiration from previous
experience with knobs embedded in synthesizers and MIDI
controllers [53]. The analogy of the material surface and
the speculation of real-life scenarios based on performing
the gestures corresponds to the material-oriented thinking
in interaction design introduced in Section 2.1. Participants
felt the gestural interaction with the deformable interface,
generating imaginations away from the musical concept,
and then linked it with the sound feeling. Relating mu-
sic interaction to scenarios outside of the musical field has
rarely been mentioned in previous studies. Introducing spe-
cific gestures in the material exploration may lead to more
divergent speculations. In an interactive tactile-sonic art-
work [39], the author also utilizes the imaginative stories
and emotional aspects of the gestural interaction with tex-
tiles as inspiration. Our findings can provide some empirical
support to this art-making approach.
7.1.2 Thinking of Sound Deformation
Many participants in our study imagine that sound has a
physical form and use gestures to transform it, a pattern
similar to the ”thinking from material to sonic response”
found in Zheng et al.’s study [67]. In their study, the par-
ticipants’ thinking of sonic response was linked with mate-
rial properties such as texture and rigidity. Troiano et al.’s
study [53] also revealed mappings of physical properties of
the material related to music, such as shape-retaining mate-
rial to dynamic changes of sound parameters. In contrast,
our study presents the idea of thinking from the gestural
influence of sound, where sound is considered a material
morphed by the gesture.
7.1.3 Instant or Continuous
Comparing answers from different gestures, the descriptions
of ”instant”and ”continuous”appear frequently and point to
specific gestures. For sliding, the sound change was thought
to be continuous, and tapping and poking were mentioned
as instant control. Troiano et al. [53] discovered that par-
ticipants tended to use simple surface contact to generate
sounds and object deformation to modulate or apply effects
to sounds. Zhang et al. [67] also mentioned the apparent
deformation-continuous control matching. These can fur-
ther support our findings on instant and continuous control.
These previous findings may indicate that the difference be-
tween instant and continuous may have a close relationship
with whether the gesture will cause obvious deformation on
the TPU surface.
7.2 Perceived Benefits
Another important aspect of this study is finding the per-
ceived benefits of fabric-like deformable interfaces for multi-
gestural music control and its potential. Our study’s de-
formable input method is an intuitive design for electronic
music beginners. Rigid input systems such as MIDI con-
trol offer diverse gestural interaction but are regarded as
hard to learn and remember by our participants. In con-
trast, deformation-based input is viewed as attractive, easy,
and intuitive by participants who were beginners in elec-
tronic music, and they showed high interest in using it in
the future. Participants also pointed out that gesture-sound
control can be remembered more easily. The intuitiveness
and embodiment are also mentioned in previous studies in
deformable interface [67, 53]. However, since this last study
was conducted among DMI designers and musicians, the
value of establishing interest in electronic music for be-
ginners was not discussed. Moreover, Cibrian et al. [10]
have applied fabric-based interactive surfaces, playing pi-
ano sounds to support early classroom development. This
further proposes the potential of using deformation-based
music control for users who are not music experts. Another
benefit is that deformation-based input can provide a richer
sensory experience. This echoes the materiality aspect and
its application in expressive performance.
Deformable interface-based input shows great potential.
Firstly, it has the potential to be developed into other for-
mats, such as garments and carpets, or placed over other
stuff to have an add-on function. Secondly, the flexibility
of fabric-like deformable interfaces in their ability to be de-
formed dynamically can be used to map richer and more
complex musical parameters. However, concerns about af-
fordance and whether the technology is feasible were also
raised.
7.3 Limitations and Future Directions
The first limitation is our proposed study probe. The tech-
nology of printed sensors lacks maturity. As the worries
raised by one of our participants, although continuous and
precise control is interesting, the sensing technologies may
be a problem. Our current prototype is limited in recogniz-
ing certain types of gestures, and whether the direction and
intensity of gestures can be sensed needs to be explored.
Future research could explore several avenues in deformable
fabric-like interfaces as sensorial interfaces for controlling
musical instruments through gesture-based input. Firstly,
using material-driven design to inspire new musical inter-
faces could be investigated, along with how specific ges-
tures mediated by a particular material, such as silicone or
rubber, can vary in sound control. Additionally, the po-
tential for combining multiple materials in the design of
digital music instruments to create more complex and nu-
anced gestural interactions could be explored. Secondly,
the potential of a deformable flat interface for controlling
richer sound parameters could be studied, as well as the use
of machine learning and other sensing technologies to en-
able a deformable flat interface to sense different gestures.
This could lead to the creation of more sophisticated musi-
cal instruments that provide a greater degree of control over
sound. Furthermore, deformable flat interfaces in other do-
mains beyond music, such as gaming or healthcare, could
be explored to determine the potential of material-driven
design in these areas. This could lead to developing new
types of interfaces for various applications that prioritize
playability and rich gestural interaction.
8. CONCLUSION
This research explores the relationship between input and
output in deformable interfaces, focusing on gestural in-
put and sound output in the design of digital music in-
struments. This study examines common patterns that
emerge from user-centred studies by utilizing a deformable
material, TPU, and transforming it into a deformable in-
terface capable of sensing multiple gestures. These pat-
terns involve drawing analogies from real-life experiences,
considering gesture direction and continuity, and imagin-
ing sound deformation as a reflection of material deforma-
tion. These observations highlight the concept of embodi-
ment within the context of deformable interaction. Addi-
tionally, the study reveals the advantages of multi-gestural
deformable interfaces in musical interaction, including intu-
itiveness, learnability, playability, and a richer sensory ex-
perience. Such interfaces also enable the mapping of diverse
sound parameters compared to rigid input methods. These
findings emphasize the importance of considering the input-
output relationship when designing deformable interfaces to
facilitate meaningful and immersive gestural interactions.
9. ACKNOWLEDGEMENTS
The authors of this paper express their gratitude to EPACK
Lab (Electronic Packaging Laboratory) at Hong Kong Uni-
versity of Science and Technology (HKUST) for providing
the manufacturing facility for prototype production.
10. ETHICAL STATEMENT
All individuals who participated in our evaluation voluntar-
ily did so. Prior to participating in the study, we informed
the participants of the purpose of the study and obtained
their informed consent, ensuring their understanding of the
procedures involved. No individuals under the age of 18
were included in the study.
11. REFERENCES
[1] M. B. Alonso and D. V. Keyson. Musiccube: making
digital music tangible. In CHI ’05 Extended Abstracts
on Human Factors in Computing Systems , CHI EA
’05, page 1176–1179, New York, NY, USA, 2005.
Association for Computing Machinery.
[2] L. Angelini, D. Lalanne, E. V. d. Hoven, O. A.
Khaled, and E. Mugellini. Move, hold and touch: A
framework for tangible gesture interactive systems.
Machines, 3(3):173–207, 2015.
[3] T. Beven, T. Hoang, M. Carter, and B. Ploderer.
Handlog: A deformable tangible device for continuous
input through finger flexion. In Proceedings of the
28th Australian Conference on Computer-Human
Interaction, OzCHI ’16, page 595–604, New York, NY,
USA, 2016. Association for Computing Machinery.
[4] Z. Bilda, L. Candy, and E. Edmonds. An embodied
cognition framework for interactive experience.
CoDesign, 3:123–137, 06 2007.
[5] A. Boem and G. M. Troiano. Non-rigid hci: A review
of deformable interfaces and input. In Proceedings of
the 2019 on Designing Interactive Systems
Conference, DIS ’19, page 885–906, New York, NY,
USA, 2019. Association for Computing Machinery.
[6] P. Bosowski, M. Hoerr, V. Mecnika, T. Gries, and
S. Jockenh¨ovel. 4 - design and manufacture of
textile-based sensors. In T. Dias, editor, Electronic
Textiles, pages 75–107. Woodhead Publishing, Oxford,
2015.
[7] T. Cheng, K. Narumi, Y. Do, Y. Zhang, T. D. Ta,
T. Sasatani, E. Markvicka, Y. Kawahara, L. Yao,
G. D. Abowd, and H. Oh. Silver tape: Inkjet-printed
circuits peeled-and-transferred on versatile substrates.
Proc. ACM Interact. Mob. Wearable Ubiquitous
Technol., 4(1), mar 2020.
[8] V. Cheung, A. K. Eady, and A. Girouard. Deformable
controllers: Fabrication and design to promote novel
hand gestural interaction mechanisms. In Proceedings
of the Twelfth International Conference on Tangible,
Embedded, and Embodied Interaction, TEI ’18, page
732–735, New York, NY, USA, 2018. Association for
Computing Machinery.
[9] K. Y. Choi, N. ElHaouij, J. Lee, R. W. Picard, and
H. Ishii. Design and evaluation of a clippable and
personalizable pneumatic-haptic feedback device for
breathing guidance. Proc. ACM Interact. Mob.
Wearable Ubiquitous Technol., 6(1), mar 2022.
[10] F. L. Cibrian, N. Weibel, and M. Tentori. Collective
use of a fabric-based interactive surface to support
early development in toddler classrooms. In
Proceedings of the 2016 ACM International Joint
Conference on Pervasive and Ubiquitous Computing ,
UbiComp ’16, page 328–339, New York, NY, USA,
2016. Association for Computing Machinery.
[11] A. Clark. Being There: Putting Mind, Body, and
World Together Again. 01 1997.
[12] S. Dahl and A. Friberg. Visual perception of
expressiveness in musicians´body movements. Music
Perception, 24(5):433–454, 2007.
[13] P. Dourish. Where the Action is: The Foundations of
Embodied Interaction. MIT Press, Cambridge, MA,
USA, 2001.
[14] G. W. Fitzmaurice. Graspable user interfaces. 1996.
[15] E. v. d. Garde-Perik, S. Offermans, K. v. Boerdonk,
K.-M. Lenssen, and E. v. d. Hoven. An analysis of
input-output relations in interaction with smart
tangible objects. ACM Trans. Interact. Intell. Syst. ,
3(2), aug 2013.
[16] N.-W. Gong, J. Steimle, S. Olberding, S. Hodges,
N. E. Gillian, Y. Kawahara, and J. A. Paradiso.
Printsense: A versatile sensing technique to support
multimodal flexible surface interaction. In Proceedings
of the SIGCHI Conference on Human Factors in
Computing Systems, CHI ’14, page 1407–1410, New
York, NY, USA, 2014. Association for Computing
Machinery.
[17] B. Greinke, G. Petri, P. Vierne, P. Biessmann,
A. B¨orner, K. Schleiser, E. Baccelli, C. Krause,
C. Verworner, and F. Biessmann. An interactive
garment for orchestra conducting: Iot-enabled textile
& machine learning to direct musical performance. In
Proceedings of the Fifteenth International Conference
on Tangible, Embedded, and Embodied Interaction,
pages 1–6, 2021.
[18] B. Greinke, G. Petri, P. Vierne, P. Biessmann,
A. B¨orner, K. Schleiser, E. Baccelli, C. Krause,
C. Verworner, and F. Biessmann. An interactive
garment for orchestra conducting: Iot-enabled textile
& machine learning to direct musical performance. In
Proceedings of the Fifteenth International Conference
on Tangible, Embedded, and Embodied Interaction,
TEI ’21, New York, NY, USA, 2021. Association for
Computing Machinery.
[19] N. A.-h. Hamdan, J. R. Blum, F. Heller, R. K.
Kosuru, and J. Borchers. Grabbing at an angle: Menu
selection for fabric interfaces. In Proceedings of the
2016 ACM International Symposium on Wearable
Computers, ISWC ’16, page 1–7, New York, NY,
USA, 2016. Association for Computing Machinery.
[20] G. Herkenrath, T. Karrer, and J. Borchers. Twend:
Twisting and bending as new interaction gesture in
mobile devices. In CHI ’08 Extended Abstracts on
Human Factors in Computing Systems , CHI EA ’08,
page 3819–3824, New York, NY, USA, 2008.
Association for Computing Machinery.
[21] K. Hinckley, R. J. K. Jacob, C. Ware, J. O.
Wobbrock, and D. J. Wigdor. Input/output devices
and interaction techniques. In Computing Handbook,
3rd ed., 2014.
[22] E. Hornecker. The role of physicality in tangible and
embodied interactions. Interactions, 18(2):19–23, mar
2011.
[23] H. Ishii and B. Ullmer. Tangible bits: Towards
seamless interfaces between people, bits and atoms. In
Proceedings of the ACM SIGCHI Conference on
Human Factors in Computing Systems , CHI ’97, page
234–241, New York, NY, USA, 1997. Association for
Computing Machinery.
[24] G. M. N. Islam, M. Ali, and S. Collie. Textile sensors
for wearable applications: a comprehensive review.
Cellulose, 27:1–29, 07 2020.
[25] M. Jiang, V. Nanjappan, H.-N. Liang, and M. ten
Bh¨omer. Gesfabri: Exploring affordances and
experience of textile interfaces for gesture-based
interaction. Proc. ACM Hum.-Comput. Interact.,
6(EICS), jun 2022.
[26] H. Jung and E. Stolterman. Material probe:
Exploring materiality of digital artifacts. In
Proceedings of the Fifth International Conference on
Tangible, Embedded, and Embodied Interaction, TEI
’11, page 153–156, New York, NY, USA, 2010.
Association for Computing Machinery.
[27] M. Karam and m. Schraefel. A taxonomy of gestures
in human computer interactions. Electronics and
Computer Science, 01 2005.
[28] E. Karana, B. Barati, V. Rognoli, and A. Zeeuw
van der Laan. Material driven design (mdd): A
method to design for material experiences.
International Journal of Design , in press, 05 2015.
[29] C. Kiefer. A malleable interface for sonic exploration.
In New Interfaces for Musical Expression , 2010.
[30] D. Kirsh. Embodied cognition and the magical future
of interaction design. ACM Trans. Comput.-Hum.
Interact., 20(1), apr 2013.
[31] S. R. Klemmer, B. Hartmann, and L. Takayama. How
bodies matter: Five themes for interaction design. In
Proceedings of the 6th Conference on Designing
Interactive Systems, DIS ’06, page 140–149, New
York, NY, USA, 2006. Association for Computing
Machinery.
[32] P.-S. Ku, M. T. I. Molla, K. Huang, P. Kattappurath,
K. Ranjan, and H.-L. C. Kao. Skinkit: Construction
kit for on-skin interface prototyping. Proc. ACM
Interact. Mob. Wearable Ubiquitous Technol., 5(4),
dec 2022.
[33] S.-S. Lee, S. Kim, B. Jin, E. Choi, B. Kim, X. Jia,
D. Kim, and K.-p. Lee. How users manipulate
deformable displays as input devices. In Proceedings
of the SIGCHI Conference on Human Factors in
Computing Systems, CHI ’10, page 1647–1656, New
York, NY, USA, 2010. Association for Computing
Machinery.
[34] A. Lucero. Using affinity diagrams to evaluate
interactive prototypes. In J. Abascal, S. Barbosa,
M. Fetter, T. Gross, P. Palanque, and M. Winckler,
editors, Human-Computer Interaction – INTERACT
2015, pages 231–248, Cham, 2015. Springer
International Publishing.
[35] M. Merleau-Ponty and C. Smith. Phenomenology of
perception, volume 26. Routledge London, 1962.
[36] L. Mice and A. Mcpherson. From miming to nimeing:
the development of idiomatic gestural language on
large scale dmis. In New Interfaces for Musical
Expression, 2020.
[37] L. Mice and A. P. McPherson. Super size me:
Interface size, identity and embodiment in digital
musical instrument design. In Proceedings of the 2022
CHI Conference on Human Factors in Computing
Systems, CHI ’22, New York, NY, USA, 2022.
Association for Computing Machinery.
[38] E. R. Miranda and M. Wanderley. New Digital
Musical Instruments: Control And Interaction
Beyond the Keyboard (Computer Music and Digital
Audio Series). A-R Editions, Inc., USA, 2006.
[39] S. Mlakar, T. Preindl, A. Pointner,
M. Alida Haberfellner, R. Danner, R. Aigner, and
M. Haller. The sound of textile: An interactive
tactile-sonic installation. In 10th International
Conference on Digital and Interactive Arts , ARTECH
2021, New York, NY, USA, 2022. Association for
Computing Machinery.
[40] M. R. Morris, J. O. Wobbrock, and A. D. Wilson.
Understanding users’ preferences for surface gestures.
In Proceedings of Graphics Interface 2010, GI ’10,
page 261–268, CAN, 2010. Canadian Information
Processing Society.
[41] C. Nordmoen, J. Armitage, F. Morreale, R. Stewart,
and A. McPherson. Making sense of sensors:
Discovery through craft practice with an open-ended
sensor material. In Proceedings of the 2019 on
Designing Interactive Systems Conference, DIS ’19,
page 135–146, New York, NY, USA, 2019. Association
for Computing Machinery.
[42] D. Norman. The design of everyday things: Revised
and expanded edition. Basic books, 2013.
[43] D. G. Paine. Gesture and musical interaction:
Interactive engagement through dynamic morphology.
In Proceedings of the 2004 Conference on New
Interfaces for Musical Expression, NIME ’04, page
80–86, SGP, 2004. National University of Singapore.
[44] C. Plain. Build an affinity for kj method. Quality
Progress, 40(3):88, 2007.
[45] E. Robles and M. Wiberg. Texturing the ”material
turn” in interaction design. In Proceedings of the
Fourth International Conference on Tangible,
Embedded, and Embodied Interaction, TEI ’10, page
137–144, New York, NY, USA, 2010. Association for
Computing Machinery.
[46] J.-S. Roh, Y. Mann, A. Freed, D. Wessel, U. Berkeley,
A. Freed, A. Street, and D. Wessel. Robust and
reliable fabric, piezoresistive multitouch sensing
surfaces for musical controllers. In NIME, pages
393–398, 2011.
[47] J. C. Schacher. The body in electronic music
performance. In Proceedings of the Sound and Music
Computing Conference, pages 194–200, 2012.
[48] J. C. Schacher and D. Bisig. Watch this! expressive
movement in electronic music performance. In
Proceedings of the 2014 International Workshop on
Movement and Computing , pages 106–111, 2014.
[49] H. Sharp, Y. Rogers, and J. Preece. Interaction design
: beyond human-computer interaction . John Wiley &
Sons, Indianapolis, IN, fifth edition edition, 2019 -
2019.
[50] R. Stewart. Cords and chords: Exploring the role of
e-textiles in computational audio. Frontiers in ICT, 6,
2019.
[51] P. Strohmeier, J. Knibbe, S. Boring, and K. Hornbæk.
Zpatch: Hybrid resistive/capacitive etextile input. In
Proceedings of the Twelfth International Conference
on Tangible, Embedded, and Embodied Interaction,
TEI ’18, page 188–198, New York, NY, USA, 2018.
Association for Computing Machinery.
[52] K. Tahiro˘ glu, T. Svedstr¨om, V. Wikstr¨om,
S. Overstall, J. Kildal, and T. Ahmaniemi. Soundflex:
Designing audio to guide interactions with
shape-retaining deformable interfaces. In Proceedings
of the 16th International Conference on Multimodal
Interaction, ICMI ’14, page 267–274, New York, NY,
USA, 2014. Association for Computing Machinery.
[53] G. M. Troiano, E. W. Pedersen, and K. Hornbæk.
Deformable interfaces for performing music. In
Proceedings of the 33rd Annual ACM Conference on
Human Factors in Computing Systems , CHI ’15, page
377–386, New York, NY, USA, 2015. Association for
Computing Machinery.
[54] E. van den Hoven and A. Mazalek. Grasping gestures:
Gesturing with physical artifacts. AI EDAM,
25(3):255–271, 2011.
[55] J. van Dijk, R. van der Lugt, and C. Hummels.
Beyond distributed representation: Embodied
cognition design supporting socio-sensorimotor
couplings. In Proceedings of the 8th International
Conference on Tangible, Embedded and Embodied
Interaction, TEI ’14, page 181–188, New York, NY,
USA, 2014. Association for Computing Machinery.
[56] C. Vaucelle and H. Ishii. Picture this! film assembly
using toy gestures. In Proceedings of the 10th
International Conference on Ubiquitous Computing ,
UbiComp ’08, page 350–359, New York, NY, USA,
2008. Association for Computing Machinery.
[57] S. Villarreal-Narvaez, J. Vanderdonckt, R.-D. Vatavu,
and J. O. Wobbrock. A systematic review of gesture
elicitation studies: What can we learn from 216
studies? In Proceedings of the 2020 ACM Designing
Interactive Systems Conference, DIS ’20, page
855–872, New York, NY, USA, 2020. Association for
Computing Machinery.
[58] R. Wakkary, W. Odom, S. Hauser, G. Hertz, and
H. Lin. A short guide to material speculation: Actual
artifacts for critical inquiry. Interactions, 23(2):44–48,
feb 2016.
[59] M. Wanderley and P. Depalle. Gestural control of
sound synthesis. Proceedings of the IEEE,
92(4):632–644, 2004.
[60] E. Watanabe, Y. Hanzawa, and M. Inakage. Clay
tone: A music system using clay for user interaction.
In ACM SIGGRAPH 2007 Posters , SIGGRAPH ’07,
page 156–es, New York, NY, USA, 2007. Association
for Computing Machinery.
[61] M. Wiberg. Methodology for materiality: Interaction
design research through a material lens. Personal
Ubiquitous Comput., 18(3):625–636, mar 2014.
[62] M. Wiberg. The Materiality of Interaction: Notes on
the Materials of Interaction DesignNotes on the
Materials of Interaction Design . 02 2018.
[63] I. Wicaksono, D. D. Haddad, and J. Paradiso. Tapis
magique: Machine-knitted electronic textile carpet for
interactive choreomusical performance and immersive
environments. In Proceedings of the 14th Conference
on Creativity and Cognition , pages 262–274, 2022.
[64] I. Wicaksono and J. Paradiso. Knittedkeyboard:
Digital knitting of electronic textile musical
controllers. In Proceedings of the International
Conference on New Interfaces for Musical Expression ,
pages 323–326. Birmingham City University,
Birmingham, UK, 2020.
[65] J. O. Wobbrock, H. H. Aung, B. Rothrock, and B. A.
Myers. Maximizing the guessability of symbolic input.
In CHI ’05 Extended Abstracts on Human Factors in
Computing Systems, CHI EA ’05, page 1869–1872,
New York, NY, USA, 2005. Association for
Computing Machinery.
[66] J. O. Wobbrock, M. R. Morris, and A. D. Wilson.
User-defined gestures for surface computing. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems , CHI ’09, page
1083–1092, New York, NY, USA, 2009. Association
for Computing Machinery.
[67] J. Zheng, N. Bryan-Kinns, and A. P. McPherson.
Material matters: Exploring materiality in digital
musical instruments design. In Proceedings of the
2022 ACM Designing Interactive Systems Conference ,
DIS ’22, page 976–986, New York, NY, USA, 2022.
Association for Computing Machinery.
[68] J. Zheng, A. Mcpherson, and N. Bryan-Kinns. When
materials meet sound: Discovering the meaning of
deformable materials in musical interaction. In
Proceedings of the 2023 ACM Designing Interactive
Systems Conference, DIS ’23, page 312–325, New
York, NY, USA, 2023. Association for Computing
Machinery.