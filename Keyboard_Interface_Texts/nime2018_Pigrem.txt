Do We Speak Sensor? Cultural Constraints of Embodied
Interaction
Jon Pigrem
Centre for Digital Music, Queen Mary
University of London
j.m.pigrem@qmul.ac.uk
Andrew McPherson
Centre for Digital Music, Queen Mary
University of London
a.mcpherson@qmul.ac.uk
ABSTRACT
This paper explores the role of materiality in Digital Musical
Instruments and questions the inﬂuence of tacit understand-
ings of sensor technology. Existing research investigates the
use of gesture, physical interaction and subsequent param-
eter mapping. We suggest that a tacit knowledge of the
‘sensor layer’ brings with it deﬁnitions, understandings and
expectations that forge and guide our approach to interac-
tion. We argue that the inﬂuence of technology starts before
a sound is made, and comes from not only intuition of mate-
rial properties, but also received notions of what technology
can and should do. On encountering an instrument with ob-
vious sensors, a potential performer will attempt to predict
what the sensors do and what the designer intends for them
to do, becoming inﬂuenced by a machine centered under-
standing of interaction and not a solely material centred
one. The paper presents an observational study of interac-
tion using non-functional prototype instruments designed
to explore fundamental ideas and understandings of instru-
mental interaction in the digital realm. We will show that
this understanding inﬂuences both gestural language and
ability to characterise an expected sonic/musical response.
Author Keywords
tactile interaction, tacit understanding, musical gesture, dig-
ital musical instruments, NIME, sensor technology
CCS Concepts
•Applied computing→Sound and music computing;
•Human-centered computing→Interaction design the-
ory, concepts and paradigms;
1. INTRODUCTION
Designers of digital musical instruments (DMIs) spend con-
siderable eﬀort identifying new sensor technologies and in-
corporating them into their instrument designs [14]. Sen-
sors, together with mapping relationships, constitute some
of the principal concerns of many NIME creators [22].
However, excessive focus on sensors and mappings carries
risks for the designer, particularly falling into a viewpoint
that considers the available aﬀordances from a technology-
centred viewpoint rather than a player-centred one [19]. Put
another way, a technology-centred viewpoint might askwhat
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
the instrument can do rather than what the performer can
do with the instrument .
That digital sensors can inﬂuence not only the designer
but also the performer is well established. Magnusson [12]
has observed that performers interacting with DMIs explore
their constraints as much as they engage with their aﬀor-
dances. It has been shown that technological and material
factors guide patterns of human interaction [4, 18], and that
the limited richness of sensor inputs on a DMI can lead to a
progressive reduction in the variety of performance gestures
[9] as well as to appropriating behaviours [8, 23].
Although some aspects of performer adaptation to an in-
strument might be explained by their reaction to the in-
strument’s responsive behaviour, we propose that material
factors of the instrument can guide their expectations before
the ﬁrst sound is made. In particular, we hypothesise that
the very existence of an exposed sensor, even in the absence
of any functionality, biases performers toward a technology-
centred way of thinking in which the sensing capabilities of
the device, rather than the gesture language of the body,
comes to the foreground.
To test this hypothesis, we conducted a study of three
non-functional prototypes [17], investigating the performer’s
tacit understanding of interaction with each material and
their expectation of the sonic response. The results are
presented alongside a general discussion of the use of non-
functional prototypes as a DMI research tool.
2. BACKGROUND
Many authors have discussed the danger of limited or unin-
tuitive mappings [20, 12], going beyond the aﬀordances of
technology, to explore and expose their constraints. Jack et
al. [9] highlight the reductive nature of the commonly found
“input-output” model of DMIs, reducing the dimensionality
of rich gestural interaction through limitations in the sensor
layer. Magnusson [12] builds on Gibson [7], suggesting that
playing an instrument is a form of exploring its constraints.
Armitage [1] reviews numerous Digital Musical Instrument
design frameworks and taxonomies, however very few dis-
cuss the subject of materiality. Magnusson [11] calls for
DMI luthiers to “acknowledge the theoretical, cultural and
social context in which all tools are designed - and this im-
plies an awareness by designers of the responsibilities they
have in terms of aesthetic and cultural inﬂuence”.
2.1 Embodied Interaction
Magnusson [11] discusses the ‘symbolic’ relationship we have
with digital music instruments through the very nature of
their design, describing them as epistemic tools. The paper
references Davis Beard [2], stating “the instrument becomes
an expression in itself, an externalisation of knowledge in a
form that is not symbolic but material”.
M¨uller [16] also discusses the ‘symbolic’ nature of DMIs
382
questioning the design stage, and highlighting phenomeno-
logical considerations of embodiment via ‘skill acquisition’
through experience with an instrument, linking perception
and action “where action demands perception and percep-
tion eﬀects and informs action”.
Tuuri et al. [18] explores embodied control, question-
ing how “technologies actually control our moving bodies
and transform our lived spaces”. The author uses the term
‘human-technology choreographies’ to perform a theoretical
analysis of the multidimensional aspects that reside within
embodied interaction. The paper outlines three dimensions
of embodied control. Two of their domains are particularly
relevant to this research: (1) Instrumental Control - How
designs harness body gestures and movements as instru-
ments in control surfaces in HCI; (2) Experiential Control
- How designs participate in constituting a feeling of being
controlled and a feeling of being in control.
2.2 Enactivism and Enactive Design
Wessel [21] and Essl and O’Modhrain [5] propose an ‘enac-
tive’ approach to the design of new tangible musical instru-
ments, stating the design theory to “retain the familiar tac-
tile aspect of the interaction so that the performer can take
advantage of tacit knowledge gained through experience
with such phenomena in the real world”. Momeni [15] ex-
plores the themes of enactive approaches, control intimacy
and tactile interaction. Bennett and O’Modhrain [3] inves-
tigate tangible enactive interfaces, seeking to compare and
combine concepts found within enactive design and Tangible
User Interfaces (TUIs). The authors use Fishkin’s [6] taxon-
omy, which uses concepts of embodiment and metaphor [10]
to classify TUIs. The idea of embodiment requires an actual
output (which is beyond the scope of this study), however
there is some fruit in his concept of metaphor, which “looks
at how the use of the interface can relate through metaphor
to a real-world concept”. Bennet and O’Modhrain [3] state
“Enactive interfaces are desirable because they allow the
user to utilise their pre-conceived knowledge of interacting
with the world when using the interface”.
While Bennet and O’Modhrain highlight the power of this
‘preconceived knowledge’ as a point of leverage in enactive
interfaces, we question if these same preconceptions of in-
teraction when applied to the ‘sensor layer’ found within
many DMIs, are what hold back the variety and creativity
of interaction with sensor-based objects.
3. METHODOLOGY
To understand how the materiality of an unfamiliar instru-
ment shapes engagement, we conducted a study involving
three non-functional prototype instruments [17]. It was
considered that functionality would guide participants into
ways of interaction deﬁned by epistemic considerations, be-
yond the materiality of the interface. This form of experi-
mentation will highlight the tacit knowledge of participants,
before any ‘symbolic’ language can be exchanged. Pierce
and Paulos [17] discuss the use of non-functional prototypes.
Their concept of “imagined ﬁrsthand use” enables a greater
focus on the cultural constraints of embodied interaction
prior to the inﬂuence of actual ‘soundmaking’.
3.1 The Instruments
Three prototype instruments were built for the study (Fig
1). All three are the same size and shape, and styled in
a similar way. #1 has a surface made from wood, #2 a
surface made from malleable rubber and #3 has a large
2-axis capacitive touch sensor made from a printed circuit
board derived from [13] embedded in it.
Figure 1: Non-functional prototype instruments.
Clockwise from right: #1 (wood), #2 (foam rub-
ber), #3 (capacitive sensor)
3.2 Experiment Design & Implementation
3.2.1 Participants
The instruments were tested by a group of twelve partic-
ipants (nine male / three female). All participants were
doctoral candidates or post doctoral researchers in the de-
partment of Electrical Engineering and Computer Science
at Queen Mary University of London. Nine participants
played a musical instrument, experience ranging from 1 to
27 years, all of whom were familiar with Digital Musical In-
struments. The three non-musicians and had taken a mod-
ule focusing on sound recording and production techniques.
3.2.2 Procedure
Participants were given 5 minutes to explore each instru-
ment prototype, which were presented in a randomised or-
der. Participants were told:
” These are prototypes Digital Musical Instruments. Cur-
rently they are non-functioning and have no intended func-
tionality or expected mode/s of operation. Spend 5 minutes
interacting with each prototype, considering the two follow-
ing questions: What gestures or techniques would you use
to play these instruments? What subsequent sonic/musical
response would you expect from each gesture?”
3.2.3 Data collection and Interviews
Each session was audio and video recorded to facilitate the-
matic analysis. The session ended with a short focused in-
terview (5 minutes) where the participants were asked to
discuss their experience with each instrument.
Two key areas of interest were targeted. First, the type
and number of unique gestures used. Second, the imagined
sonic/musical response to each gesture.
4. RESULTS
4.1 Gestural Interaction
Table 1 presents a summary of unique gestures used with
each instrument prototype, and the prevalence of their use
within the participant group.
Gestures used with the wooden prototype were primarily
percussive in nature, such as tapping/beating with ﬁngers
or sticks (used by 10 of the 12 participants). The tactil-
ity of the material elicited gestures such as scraping (us-
ing ﬁngernails on the wooden surface), swiping (fast ﬁnger
movements with minimal pressure), sliding (slower ﬁnger
383
Gesture Wooden Rubber Sensor
Tap/beat with Fingers 10 4 1
Scrape 8 - -
Waving/Turning in air 5 1 2
Push/Press 2 12 1
Swipe 2 1 12
Beat with sticks 1 2 -
Touch (single location) 1 - 10
Rub 1 - -
Slide 1 - -
Drag - 2 9
Stroke - 1 -
Bow - - 1
Strum - - 1
Shake - - 1
Add Patch Pins - - 1
Table 1: Unique gestures used with each instrument
prototype. Numbers indicate the number of partic-
ipants who used each gesture
movements with minimal pressure) and rubbing (continu-
ous ﬁnger movements with minimal pressure). The majority
of participants cited the wooden nature of the interface as
the main rational for their interaction, commenting “wood
is intended to be hit”, “the grain of the wood implied an
interaction strategy”, and “I would like to use the natural
properties of the wood”.
Gestures used with the rubber prototype fostered a much
more ‘pressure’ based approach to interaction, with a high
prevalence of push and press gestures. There was some evi-
dence of percussive gestures such as tapping and beating. 4
participants experimented with drag (slow ﬁnger movement
with pressure), stoke (slow ﬁnger movements with no pres-
sure) and swipe (fast ﬁnger movement with no pressure)
gestures, however with much lower frequency. Participants
commented that “the texture makes you want to enjoy the
sensation”, “the surface invites me to interact with it” and
subsequently “the behaviour of the material lent to interac-
tion”. Others commented that “the tactility makes me want
to push”, and that “you look at that and know you can press
it”. The material fostered constraints: “I wouldn’t want to
scrape this one”, “this absorbs vibration so I wouldn’t have
so much precision”, and aﬀordances: “I would imagine this
to be velocity sensitive”.
Gestures used with the sensor prototype were primarily
pressure-less actions in two dimensional space. Swipe and
touch gestures were much more prevalent with this instru-
ment than with the wooden and rubber prototypes, and
there was a reduction in the number of taping/beating ges-
tures. A theme of fragility emerged, with participants re-
luctant to impart pressure or force on the surface of the
instrument, commenting “I don’t feel as comfortable to hit
this one”, “It looks like something electronic that can be bro-
ken easily”, “It feels more delicate”, “I wouldn’t hit this as I
don’t want to break it”. Many participants used terms such
as ‘mappings’, ‘XY location’ and ‘multi touch’: “it looks like
something that can track my ﬁngers”, “I feel like swiping or
scrolling”, “I would be inclined to use touch more than pres-
sure”, “I feel like deﬁning precise points”. These notions of
interaction ﬁrmly deﬁned both expectation of functionality
and subsequent gestural interaction with participants stat-
ing that “It looks like a touch pad”, and “it appears to me
more as a controller”. In general the interface was likened to
a “smartphone screen” and referred to more as a controller
than an instrument.
One participant predicted the presence of gyroscopic and
acceleration based sensor technology in all three instru-
ments, which led to some free movement in 3D space.
4.2 Imagined Sonic Response
Table 2 presents a summary of the imagined sonic/musical
response elicited by each instrument.
Sound Wooden Rubber Sensor
Percussive Sounds 11 7 1
Piano/Keyboard 1 1 -
Synthetic Sounds 2 3 3
Organic/Natural 3 - -
Any Sounds 1 1 3
Scratchy Noises 1 - -
Timbral changes 1 4 1
Dynamic Changes 1 5 1
Filtering 1 2 2
Smooth Sounds 1 - -
Amp/Freq.mod/Pitch bend 1 1 -
Pitch Changes - 3 2
Pitched Sounds/Tones 1 2 1
Envelope (ADSR) - 2 2
Wind Instruments - 1 -
Samples/Loops - 2 7
Guitar/Piano/Harp/String - 1 4
Sequencer - - 2
Theremin - - 2
Bowed/Bent Sounds - - 1
Electronic Sounds - - 1
Pads/Continuous Sounds 1 - 1
Physical Model - - 1
Between Analogue/Digital - 1 1
Gain/Pitch Slider - - 2
Eﬀect Trigger - - 1
X/Y - Pitch/Dynamics - - 2
X/Y - Timbre/Amp - - 1
Table 2: Imagined sonic/musical response from each
instrument prototype. Numbers indicate the num-
ber of participants referring to each category
The wooden prototype elicited predominantly percussive
expectations, with 11 participants suggesting it to be a per-
cussive instrument. Overall, the expectation was of natural
acoustic sounds, which were linked by most to the mate-
riality of the surface. As the instruments were stated to
be ‘digital instrument’, 6 participants highlighted synthesis
based features such as ﬁltering and frequency modulation,
4 of whom used these terms within the remit of percussion
based interaction. Overall the materiality shone through
with participants commenting: “I get an organic feeling”,
“being made of wood it should sound natural” and that “it
should sound woody”.
The rubber surfaced instrument featured a range of imag-
ined responses, with percussive and sustained morphologies
exempliﬁed. The malleable nature of the surface featured
heavily in the sonic expectations, with direct parallels made
between tactile interaction and modulation of sonic param-
eters. An overall theme of ‘shaping’ sound through tac-
tile interaction emerged from all participants. Expectation
tended to reside within a more electronic paradigm, with
less reference to ‘natural’ or ‘acoustic’ sounds.
The sensor prototype elicited much more ‘controller’ based
responses, with 10 participants perceiving it as a control
surface rather than an instrument. 7 participants imagined
the instrument as a trigger for samples, commenting “the
384
interface looks more electronic”, “It looks like a digital in-
strument” or “this is more like a controller for digital stuﬀ”.
Only 3 participants referenced an actual type or family of
sound, commenting that it could sound like ‘anything’ or
‘nothing speciﬁc’. 9 participants used synthesis type termi-
nology (pitch, amplitude, envelope, timbre) in their descrip-
tion, however no direct ‘sound’ was imagined or described.
Participants stated: “It’s ambiguous”, “Anything could hap-
pen with this one”, “It doesn’t inspire any speciﬁc sound”,
“I don’t feel like this would have its own sound” and “I have
a less clear idea of what it would sound like”.
5. DISCUSSION
5.1 Materiality in DMI design
It is clear from the study that materiality plays a huge role
in expectation of both gestural interaction and sonic re-
sponse in DMIs. The wood and rubber prototypes evoked
categorically diﬀerent responses, with one seen as percussive
and the other pressure based. This understanding inﬂu-
enced not only the choice of gesture used, but assumptions
of what they would sound like. In many cases participants
drew an explicit analogy between the natural sound of the
material and the kind of sound the instrument should make.
This conclusion was strengthened by the fact that the sen-
sor prototype was seen not to have a natural sound at all.
The diﬀerence in response between the wood, rubber and
sensor based instruments show that our assessment of po-
tential gesture is guided by the playing surface, and beyond
this our expectation of what the instrument does and how it
sounds is fundamentally linked to material considerations.
5.2 A machine-centered approach to DMIs
The outcome of the study highlights a limitation in rela-
tion to explicit sensor technology, which we attribute to
a ‘machine centered’ approach to instrumental interaction.
Participants exhibited a very diﬀerent understanding of the
sensor prototype, becoming preoccupied with the kind of
spatial representation they assumed the digital sensor would
produce. The prototype was seen as a controller and not an
instrument, and promoted XY-based thinking adopted from
smartphones rather than acoustic instruments. This under-
standing was also evident in the actions of a few participants
who assumed the prototypes might contain accelerometers
or gyroscopes, which subsequently inﬂuenced their gestures.
Notions of fragility, lack of feedback, and a two dimensional
pattern of interaction underpinned many responses. The
instrumental gestures used with the wood and rubber pro-
totypes gave way to notions of basic control, mappings and
triggering; losing sight of control intimacy, gestural rich-
ness, and sonic nuance, as desired by many writers and
researchers in the ﬁeld.
5.3 Non-functional prototypes
The use of non-functional prototypes as an experimental
methodology has proven to be fruitful. The ability to sep-
arate the expectations of a technology from the reality of
using a technology has enabled an assessment of the tacit
cultural elements that underpin our understandings and in-
ﬂuence our subsequent actions. This paper shows the value
of the approach in the context of DMI and NIME design.
The use of a non-functional prototype design stage in NIME
development could aﬀord an indication of what kinds of per-
formance techniques are likely to emerge, which could be
reinforced of deliberately thwarted in the development of
elements such as sensor choice, placement and mappings.
6. ACKNOWLEDGMENTS
This research is supported by EPSRC under grants EP/N005112/1
and EP/L01632X/1 (Design for Virtuosity; Centre for Doc-
toral Training in Media and Arts Technology).
7. REFERENCES
[1] J. Armitage et al. “the ﬁner the musician, the smaller
the details”: NIMEcraft under the microscope. In
Proc. NIME, 2017.
[2] D. Baird. Thing knowledge: A philosophy of scientiﬁc
instruments. Univ of California Press, 2004.
[3] P. Bennett and S. O’Modhrain. Towards tangible
enactive-interfaces. In 4th International Conference
on Enactive Interfaces 2007 , 2007.
[4] F. Dennis. Organology and material culture. 2017.
[5] G. Essl and S. O’modhrain. An enactive approach to
the design of new tangible musical instruments.
Organised sound, 11(03), 2006.
[6] K. P. Fishkin. A taxonomy for and analysis of tangible
interfaces. Personal and Ubiquitous Computing , 2004.
[7] J. J. Gibson. The ecological approach to visual
perception: classic edition. Psychology Press, 2014.
[8] M. Gurevich et al. Style and constraint in electronic
musical instruments. In NIME, 2010.
[9] R. Jack et al. Rich gesture, reduced control: the
inﬂuence of constrained mappings on performance
technique. In Proc. MOCO, 2017.
[10] G. Lakoﬀ and M. Johnson. Metaphors we live by .
University of Chicago press, 2008.
[11] T. Magnusson. Of epistemic tools: Musical
instruments as cognitive extensions. Organised Sound,
14(02), 2009.
[12] T. Magnusson. Designing constraints: Composing and
performing with digital musical systems. Computer
Music Journal, 34(04), 2010.
[13] A. McPherson. TouchKeys: Capacitive multi-touch
sensing on a physical keyboard. In NIME, 2012.
[14] C. B. Medeiros and M. M. Wanderley. A
comprehensive review of sensors and instrumentation
methods in devices for musical expression. Sensors,
14(8), 2014.
[15] A. Momeni. Caress: An enactive electro-acoustic
percussive instrument for caressing sound. In Proc.
NIME, 2015.
[16] A. M ¨uller. An embodied approach to digital tangible
musical interfaces. In Proc. NordiCHI, 2010.
[17] J. Pierce and E. Paulos. Making multiple uses of the
obscura 1c digital camera: reﬂecting on the design,
production, packaging and distribution of a
counterfunctional device. In Proc. CHI, 2015.
[18] K. Tuuri et al. Who controls who? embodied control
within human–technology choreographies. Interacting
with Computers, 29(4), 2017.
[19] D. Van Nort. Instrumental listening: sonic gesture as
design principle. Organised sound, 14(2), 2009.
[20] M. M. Wanderley and P. Depalle. Gestural control of
sound synthesis. Proceedings of the IEEE, 92(4), 2004.
[21] D. Wessel. An enactive approach to computer music
performance. In Rencontres Musicales
Pluridisciplinaires, 2006.
[22] D. Wessel, M. Wright, and J. Schott. Intimate musical
control of computers with a variety of controllers and
gesture mapping metaphors. In Proc. NIME, 2002.
[23] V. Zappi and A. McPherson. Dimensionality and
appropriation in digital musical instrument design. In
NIME, 2014.
385