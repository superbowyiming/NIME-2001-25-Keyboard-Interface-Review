SoundMorpheus: A Myoelectric-Sensor Based Interface for Sound Spatialization and Shaping    Christopher Benson, Bill Manaris, Seth Stoudenmier  Computer Science Department College of Charleston, South Carolina, USA {bensonca@g. | manarisb@ | stoudenmiersh@g.} cofc.edu  
 Timothy Ward  Music Department American College of Greece   Athens, Greece  tjward@acg.edu  ABSTRACT We present an innovative sound spatialization and shaping interface, called SoundMorpheus, which allows the placement of sounds in space, as well as the altering of sound characteristics, via arm movements that resemble those of a conductor.  The interface displays sounds (or their attributes) to the user, who reaches for them with one or both hands, grabs them, and gently or forcefully sends them around in space, in a 360° circle.  The system combines MIDI and traditional instruments with one or more myoelectric sensors.  These components may be physically collocated or distributed in various locales connected via the Internet. This system also supports the performance of acousmatic and electronic music, enabling performances where the traditionally central mixing board, need not be touched at all (or minimally touched for calibration).  Finally, the system may facilitate the recording of a visual score of a performance, which can be stored for later playback and additional manipulation.  We present three projects that utilize SoundMorpheus and demonstrate its capabilities and potential.  Author Keywords Novel interface, musical expression, myoelectric sensor, sound spatialization, MIDI, audio, open sound control.  ACM Classification Human-centered computing~Gestural input, Human-centered computing~Pointing, Applied computing~Sound, and music computing  1. INTRODUCTION SoundMorpheus is a musical interface that allows a performer to grasp, hold, manipulate and position sound in a 3D space using their hands (see Figure 1).  The sound being manipulated may originate from live sources such as MIDI or traditional instruments (e.g., MIDI keyboard, flute, or guitar), or prerecorded sources (e.g., WAV files). SoundMorpheus accomplishes this through the integration of Myo sensors.   The Myo is a sensor-equipped armband that utilizes a combination of an accelerometer, magnetometer and gyroscope, all containing three axes, to track arm motion and position in space.  It also contains various EMG sensors to capture electric neuron impulses that are involved in creating contractions of the arm muscles.  Myo sensors are wireless and communicate with a computer via Bluetooth, which results in very reliable, yet untethered transfer of data, thus providing freedom of movement within a large performance area.  In our system, we utilize Myo’s gyroscope, accelerometer and EMG sensors, and map them to higher-level gestures, which are created 
through different combinations of the inertial sensing and factory gestures of the Myo..  These higher-level gestures are then communicated to various loosely-coupled components, such as JythonMusic and PureData, via the Open Sound Control (OSC) protocol to affect joint performances over various geographic arrangements.  These arrangements may range from all performers collocated in the same room, to performers distributed around the globe and being assisted via various video and sound sharing protocols (which are discussed herein, but are not considered part of our system).    SoundMorpheus may also be considered as an instrument in its own right, since the ability to manipulate arbitrary sounds (e.g., from a traditional instrument without having to physically alter what the actual performer is playing) acts to create a separate musical persona, which is distinct but closely related to the sound source it is built upon. The musical result could be seen to resemble a duet built from a single sound source (e.g., instrument) in real time.  A demonstration of SoundMorpheus is provided here: http://bit.ly/soundmorpheus2. 2. BACKGROUND A very early example of sound spatialization through movement is the potentiomètre d'espace (see Figure 2), It was developed by Jacques Poullin as an early performance system for musique concrète [1].  It consisted of four large coils and a hand-held magnetic unit, which distributed sound to the four coils.  Each of the coils were connected to a speaker.  The four speakers were distributed around the performance space (two on the stage, one in the center of the ceiling, and one on the back wall).  Figure 2 shows Pierre Henry performing with this early system in 1952.    The next three subsections explore relevant research in (a) gestural interface / instruments, (b) sound spatialization / diffusion, and (c) in the intersection of the two research areas.   
  Figure 1. Third author demonstrating SoundMorpheus.     
 
332
2.1 Gestural Input Gestural interface research is based on the realization that there is a direct and almost instinctual connection between music and movement.  This includes basic pulse relationships, such as a person tapping their foot unconsciously to music, or moving their head to a beat. It also includes more complex relationships, such as movements involved in playing musical instruments, e.g., the violin bow. This connection has been the inspiration for many gestural interfaces that seek to map natural movement to the creation and performance of musical ideas.    Tanaka and Knapp [12] present an early system using EMG and position sensing for building musical interfaces.  Exploring a related idea, Nymoen, et al. [10], designed the MuMyo interface, which utilizes the Myo controller to construct a musical instrument. This instrument facilitates creation and manipulation of musical pitches via the user’s arm and hand motions (somewhat resembling a Theremin). The user may swipe over GUI controls to select among pitch creation, tone modulation, or loading of instrument audio files (such as various drums), and through gestures play a virtual representation. Our system is similar to MuMyo in that it utilizes the Myo device’s position tracking and EMG sensors to manipulate sounds with intuitive gestures. However, MuMyo’s sounds originate from positional mapped MIDI pitches.  Our system, on the other hand, separates sound gestural control from sound generation – the latter originating from an instrument or pre-recorded sound files (as mentioned earlier). Additionally, our system distributes the tasks of performance and sound manipulation across two or more performers; for instance, one performer may generate sounds on a traditional instrument (e.g., a flute) captured through a microphone, and a second performer may then shape and transform these sounds via SoundMorpheus.  Unlike MuMyo, our approach is not constrained to scalar and arpeggiated passages, but instead allows for and encourages the creation of more complex or varied musical material normally generated through traditional instruments.   Another related system is Crossole by Senturk, et al. [11].  This is a gesture control system based on the Kinect.  Crossole represents musical chords as blocks that the user may place on a virtual grid to create a path (i.e., a chord progression). The user may then manipulate the internal pattern of notes within a chord block to mute specific notes, generate arpeggiations of chord tones, and vary rhythms using hand gestures.  To change the internal pattern of a block the user enters into the block (by stepping forward, towards the 
Kinect sensor), makes desired changes, and then exits the block when finished (by stepping backwards, away from the Kinect).  Similarly to the Crossole, our system allows users to manipulate a sound in finer detail by reaching into it (fully extending their arm in the direction of the sound), grabbing a piece of the sound (making a fist), pulling out a part of that sound (bringing their fist back towards themselves), shaping its timbral characteristics (twisting their fist), and then placing it back into its initial context (extending their arms back out in the direction of the sound and unclenching their fist).  While Crossole places sounds into a grid producing monophonic sound, our system places sound around a two-dimensional plane (utilizing an Ambisonics renderer) thus generating a spatialized 3D sound environment.  This is a significant difference, which allows the creation of a virtual soundscape that is easily manipulatable in real time by the SoundMorpheus user.   In terms of musical elements, the SoundMorpheus system allows for significant freedom in rhythmic exploration, as the separation of the gestural interface performer from the musical instrument performer allows for the creation and manipulation of dynamic, organic rhythms, as opposed to the restrictions inherent in any imposed metronomic rhythm shell.    Finally, Kinect-based interfaces constrain the user / performer in the funnel / triangular sensing area of the Kinect (e.g., see Manaris, et al. [9]), unless special space cross-stitching is provided, by combining more than one Kinect devices, as in Johnson et al. [8].  Our system provides increased mobility (within the confines of calibration with the screen projection of sound material, as described above), is hands free, and gives the performer greater freedom of movement to explore the performance space, while manipulating and experimenting with sounds.  2.2 Sound Spatialization Sound spatialization allows composers and performers to specify how sounds are positioned in the listener’s audio field, and so to create immersive musical textures and environments that become integral parts of musical works.  Although quite impressive and enabling, sound spatialization has, for the most part, played a minimal or secondary role in musical creation and performance. However, with the advent of electronic technology (and genres like acousmatic and electronic music, for example), sound spatialization has become increasingly more significant as a creative musical parameter.   Johnson, et al. [7], present tactile.motion, an iPad-based interface for sound diffusion / spatialization. The performer / user interacts with an application on a smartphone (or tablet) to place sounds in various trajectories on a 2D plane, through the use of a speaker array.  This system is mainly a tool for performers to spatialize pre-existing sound, as opposed to a musical instrument generating new sound.  SEPTAR by Graham, et al. [5] is designed to separate the sound generated by the different strings on a specially constructed 7-string guitar, and to individually position these sounds on a multichannel speaker array. A special pickup and electronics circuit takes audio input from the guitar and separates each string into a different audio channel.  Then, different algorithms (e.g., boids) are used, in conjunction with Ambisonics rendering patches in PureData, to spatialize these sounds over various arrays of speakers. This form of spatialization is unique since it takes the sound directly from the guitar itself, rather than working with any kind of MIDI representation, or microphone sources. In our system, we utilize Graham’s Ambisonics rendering algorithms.  Kuatro by Johnson, et al. [8] combines sound spatialization and motion-sensing via a dual-Kinect interface.  As users move through an installation, images and sounds move with them, and are “shaped” based on a predefined audio-visual composition. In particular, in terms of sound spatialization, each user is assigned a unique sound, which “attaches” to the user and changes as it “follows” the user around, through the use of vector-based amplitude panning (VBAP) 
  Figure 2. Pierre Henry performing a musique concrète piece with the potentiomètre d'espace in Paris, 1952.     
333
on a two-dimensional, four-speaker array.  Our system is loosely based on this concept, with the exception of utilizing Ambisonics, as opposed to VBAP-based spatial rendering.  2.3 Sound Spatialization through Movement In addition to the potentiomètre d'espace described at the beginning of this section, two more systems emerge that use movement for sound spatialization.  Donnarumma, et al. [3] describe a sonification study, where they combine electromyogram (EMG) and mechanomyogram (MMG) biosignals, to explore their use for interactive music applications.  By mapping these biosignals to sound, they demonstrated that novice performers could distinguish the two modalities, and easily learn to independently control them with simple or complex gestures.  For the purposes of our work, this suggests that it is possible to design usable, easy-to-learn, sound-spatialization interfaces, by creating meaningful, natural mappings between (as exemplified by the early potentiomètre d'espace example).   Finally, Donato, et al. [2], present gSPAT, a system similar to ours, which was developed to test the usability of the Myo armband for constructing musical interfaces.  The objective of gSPAT is to explore how gestural data from the Myo may be mapped as spatialization parameters, while appearing musically meaningful to both performer and audience.  They report on three usability experiments, which show that indeed it is possible to derive such mappings.  The rest of our paper discusses how we have extended these results.  3. SYSTEM ARCHITECTURE The SoundMorpheus system consists of three main components, the Myo controller, JythonMusic, and PureData.   3.1 Myo Controller As mentioned earlier, the Myo armband incorporates various EMG sensors, a gyroscope, an accelerometer, and a magnetometer.  These sensors can measure muscle activity as well as movement in 3D space.  This device communicates with a computer via Bluetooth for a wireless, low-energy transfer of data.    Our system, SoundMorpheus, allows for the connection of one or more Myo devices concurrently.  It makes use of the Myo gyroscope, accelerometer, and EMG sensors, and maps their activity to higher-level gestures assisted via a graphical user interface.    Low-level movements from a particular Myo device are communicated to a Myo hub (see Figure 3), which allows the connection of one or more Myo armbands to a single computer. The hub recognizes gestures and passes them to the JythonMusic component (described in the next section), which synthesizes higher-level gestures and communicates high-level information to other components via OSC messages.  The Myo API’s default language is C, which, for SoundMorpheus purposes, we are accessing via a Java-Myo wrapper.  The Myo API provides event data organized in 3 different types - spatial, gestural, and auxiliary - as outlined below: • Spatial Events: These events are captured and communicated continuously, at fixed, user-specified intervals. The communicated information involves two types of data, namely orientation and movement of a user’s arm.   • Gestural Events: These events capture and communicate higher-level information about what a user is doing with their hands.  As these events depend on user gestures, they are triggered and communicated irregularly, depending on the occurrence and timing of these gestures.  The Myo hub identifies various factory gestural poses, including fist, spread fingers, wave in, wave out, rest and double tap.  These gestures are recognized via a machine learning model (incorporated into the hub), involving complex patterns of EMG data.  
• Auxiliary Events: These events, similarly to gestural events, occur irregularly.  Auxiliary events are used for one-of-a-kind occurrences, such as the Myo getting connected or disconnected. The hub, once the connection is established, will try to maintain that connection between Myo devices and the computer, and generate such events if, say, an armband moved out of Bluetooth range and/or became disconnected, etc. 3.2 JythonMusic JythonMusic (http://jythonmusic.org) is a software environment with libraries for creative programming focusing mainly on music making, image manipulation, building graphical user interfaces (GUIs), and connecting computers to external MIDI and OSC devices (such as digital pianos, smartphones, and tablets).   JythonMusic is written in Jython, which is desirable for its expressive efficiency (short, easy-to-follow-and-maintain programs), and supports music transcription and playback, audio looping, computer-aided music composition, development of computer-based musical instruments (including hybrid instruments), and live performance.  In SoundMorpheus, JythonMusic is used to provide the glue between the different components of the architecture (see Figure 4).  For instance, a SoundMorpheus performance (as mentioned earlier) allows for the connection of multiple different devices in multiple different manners, such as traditional (audio) instruments via local connections and microphone, MIDI instruments via long-distance connections (i.e. JackTrip), and MIDI instruments and Myo 
 Figure 3. Myo portion of system architecture.     
  Figure 4.  An 8-channel spatialization arrangement for SoundMorpheus components in a local performance.   
334
armbands providing gestural data, mapped to control commands.  In order to facilitate communication of all of these different components, SoundMorpheus utilizes the MIDI, OSC and Timer APIs of the JythonMusic system, as follows:1   • MIDI API: This provides primitives for loading and looping MIDI files, and for connecting to external MIDI devices such as guitars, pianos, and synthesizers. SoundMorpheus uses this component to grab pitches as well as velocity (volume) data from MIDI instruments, engaged in performance.  This data will then be sent to a PureData synthesizer renderer (see next section).  • OSC API: This provides Open-Sound-Control primitives for connecting via the Internet to other devices, such as smartphones, computers, tablets, and synthesizers. SoundMorpheus uses this component to establish communication between the positional and gestural data being sent by the Myo armband (hub) and the PureData components being utilized in a performance (see next section). •  Timer API:  This provides ways to schedule individual or repeated tasks into the future, providing a time delay and repetition interval.  SoundMorpheus uses this component to schedule communication among various components.  One such example is polling positional and EMG data from the Myo armband whenever a MIDI pitch is produced in a performance.  By waiting until a MIDI event arrives to poll the Myo, this reduces data flow (which can be voluminous) to, say, the PureData component, since there is no reason to acquire such data if there is no current musical event on which they could operate.  Several other useful applications exist. 3.3 PureData PureData is built specifically for audio and MIDI rendering (and with extensions for video as well).  SoundMorpheus uses PureData for rendering MIDI, audio, and, possibly, video, across long-distance (as well as local) feeds.  We utilize various components to achieve this.  These include: • Various synthesizers (including an FM synthesis module) to render MIDI events (pitch and velocity) sent via OSC from the various MIDI instruments involved in a performance (via JythonMusic).   • Several Ambisonics renderers, for sound spatialization, including 2, 4, 5, and 8 channel decoders [5].  A wide range of different PureData patches (renders) may be easily driven by the SoundMorpheus system, as needed by specific performance requirements.  Additionally, as shown in Figure 5, the architecture supports distributed performances utilizing OSCgroups (for routing OSC messages behind different NAT routers), and JackTrip (for multi-machine, multi-channel audio streaming over the Internet). These performances may include more than one performance space, various audio and MIDI instruments in ad-hoc arrangements, and the SoundMorpheus Myo component, which can (from one or more location) act to control aspects of the whole performance distributed across the globe (see next section). 4. CURRENT PROJECTS This section describes several on-going projects that are utilizing and demonstrating the capabilities of SoundMorpheus.  These are Myo-instrument duet, combining a traditional instrument and Myo control of sound attributes; a transatlantic performance of Pierre Schaeffer and Pierre Henry’s “Symphonie pour un Homme Seul” (1950); and visual score                                                                     1 These APIs are available for perusal online at http://jythonmusic.org.  
generation, combining the Iannix-based score rendering system for sound spatialization and acousmatic music. 4.1 Myo-Instrument Duet SoundMorpheus allows combining Myo with a traditional instrument. The listening space utilizes an 8-speaker array, where listeners and performers are positioned within it, in a localized performance setting (as shown in Figure 4).   The instrument performer (e.g., guitarist) begins playing while the Myo performer joins in and manipulates timbral and spatial aspects of the instrument’s sound (see Figure 6) across the speaker array. A demonstration is provided here: http://bit.ly/soundmorpheus2.  In this example, the left arm Myo is set up as a positional Myo where only gyro data is being captured and sent to PureData, while the right Myo is set up as a gestural Myo which is combining inertial and gestural EMG data to allow for higher-level gestures. The higher-level gestures used in this example are as follows: • Grabbing The Sound In Its Entirety: Myo performer forms a fist while their arm is not fully extended. • Reaching Into And Grabbing A Piece Of The Sound: Myo performer fully extends their arm in the direction of the sound (by matching gyro data with the positional Myo) and forms a fist. • Pulling Out A Piece Of The Sound: After grabbing a piece of the sound, the Myo performer brings their fist back towards their body (unextending their arm). • Manipulate The Sound: After the Myo performer has grabbed a sound and still has a fist, they rotate their arm to change the timbre of the sound. The amount of rotation is tracked by the gyroscope of the Myo. • Releasing The Sound Back To Normal: The Myo performer unclenches their fist while their arm is not fully extended. This returns the sound back to how it was before being changed.  
 Figure 5.  Distributed performance arrangement (multiple venues, MIDI instruments, and audio instruments distributed across performance spaces).   
335
• Placing Manipulated Piece Of Sound Back Into The Overall Sound: While the Myo performer still has a fist, they fully extend their arm in the direction of the overall sound and unclench their fist. This keeps the changes made to the sound. When the Myo performer is manipulating only pieces of the overall sound, they can specify the channel they want to manipulate by waving in or out to cycle through the channels. This gives the Myo performer even more options and control over the sounds they are morphing.  4.2 Distributed Performance  This section discusses a transatlantic performance, which took place during Arts Festival 2016, at the American College of Greece (main site), in Athens, Greece, March 9, 2016 (see Figure 7).    The main site had the computer instrument (rendering the audio streams of computer-generated sound material) and, of course, the audience. The Myo performer was located at the College of Charleston, USA (second site).    The SoundMorpheus system (whose components were distributed between the two sites – see Figure 5) was an integral musical part of the performance, as it allowed the second site to be engaged in this transatlantic performance. The Myo performer wearing the two Myo armbands acted as something close to a ‘spatial conductor’, controlling the spatialization of the sound across the 8-speaker array surrounding the audience at the first site. The Myo performer was able to control also other musical attributes of the sound, such as volume, through wave-in and wave-out gestures.  The Myo performer was assisted by a graphical user interface displaying various sound objects and their attributes, as well as a two-way video feed, which allowed him to be linked to the performance space at the first site.   The performance included two movements (Intermezzo and Scherzo) from Pierre Schaeffer and Pierre Henry’s “Symphonie pour un Homme Seul” (Symphony for One Man Alone). During the 1950’s this piece was considered one of the most influential musique concrète compositions. This specific piece emulated an internal song produced from a lone man’s memories of the fears and struggles endured during World War 2 while trying to survive within the thick of all the chaos. This piece aimed to show, in the composers’ words, that “a single man possessed an internal symphony of notes and rhythms more vast and complex than any traditional instrument could get across”. An excerpt of this performance may be seen here: http://bit.ly/soundmorpheus2.   Central technical challenges that were planned for or that occurred during the creation of the performance were the maintenance of stable video communication (which was addressed by using multiple redundant feeds), the reliability of the transfer of the OSC messages (which was achieved using OSCGroups, a package specifically designed for this), as well as the management of the inherent latency in the system as a whole. The musical performance had been conceived in a manner that allowed for different degrees of latency, and showed that the capability of SoundMorpheus for spatialization control could transcend these issues.  4.3 Visual Score Generation This section describes an on-going project, which combines SoundMorpheus with Iannix (a 3D graphical sequencer for digital art), through the OSC protocol. This project facilitates the creation of a graphical representation (i.e., a visual score, or rendering) of the sound spatialization gestures captured during a performance.   SoundMorpheus sends positional data, as well as event data (i.e. sound on, sound off, etc), detailing the Myo performer’s movements for the duration of the performance, using OSC 
messages. These messages are received by Iannix, which renders a 3D visual representation.. This representation consists of curves, cursors, and triggers rendering the movement of the Myo arms, their progression through time, and gestures representing events in the performance, respectively (see Figure 8).   This project opens up the possibility for a performer to practice and replicate a past performance using SoundMorpheus. Iannix’s capabilities to set the speed of the progression through any graphical sequence allows a performer to practice a performance at different speeds. Additionally, a SoundMorpheus score may be played directly from Iannix, via OSC messages, thus allowing for the performance to be automated after its initial capture / rendering.  A natural extension of this project is to explore creating scores for electronic and acousmatic music providing a tangible 
  Figure 6.  A Myo-Instrument Duet performance.   
  Figure 7. Transatlantic performance of Pierre Schaeffer and Pierre Henry’s “Symphonie pour un Homme Seul”. 
336
record of events, movements and musical shapes that could be examined separately from the actual performance itself. With a visual and playable score, it is possible to automate aspects of a performance, so that anyone with the same listening space as the composer may listen to the actual piece in its entirety. This expands SoundMorpheus’s capabilities from just a live, distributed performance tool to a music production and analysis / teaching tool, which could be used to create musical scores across a wide range of musical activities, including, among other things, capturing orchestra conductor performances.  This is something we plan to explore in the near future. 5. CONCLUSION We have presented SoundMorpheus, a system combining MIDI and traditional instruments with one or more myoelectric sensors.  SoundMorpheus components may be physically collocated or distributed in various locales connected via the Internet.  A SoundMorpheus performer may affect sound placement of distributed instruments, as well alter their timbral characteristics, through arm movements, resembling those of a conductor.  Additionally, the system may facilitate visual recording of a gestural performance for later playback and further manipulation.  Running in parallel to the development of the system, our creative musical work using SoundMorpheus has allowed us to explore what the future might have in store for musical performance in a world that is connected almost instantaneously via audio and video. We can see how systems such as this (and others yet to be discovered) can expand the physical boundaries of musical performance, allowing musical collaborations to reach in real time across huge distances beyond the walls of the concert hall while still maintaining what audiences have always come to hear – coherent, integrated, engaging and hopefully inspiring musical performances.  As a closing thought we consider the evolving meaning of ‘live’ performance.  As many involved in making or listening to music made with electronic technology are acutely aware, such technology has radically changed our understanding of what it means to play ‘live’. From a benign and easily understood concept, suddenly it seems that ‘live’ has become a minefield of difficult definitions, where what it means to play ‘live’ means wildly different things to different musicians and listeners. This could be turned into a much simpler example question that might revolve in the minds of an audience watching a musician perform on their laptop – how exactly do you ‘play’ a laptop, and how do I understand and therefore enjoy 
your virtuosity as you do it? Many commentators (e.g. Emmerson [4]) call upon the historical role of the body and movement within music as key parts of a possible answer. It seems no coincidence therefore that SoundMorpheus is built fundamentally around movement.  It offers musical solutions to the problems of distance and separation in an ever more connected world, but at the same time, through movement, it offers us ideas as to how we might make coherent and therefore easily understood ‘live’ performances in technology-based music. 6. ACKNOWLEDGMENTS Funding for this project has been provided in part by the US National Science Foundation (DUE-1044861 and DUE-1323605), Google, and IBM. 7. REFERENCES [1] E. Bates. The Composition & Performance of Spatial Music. Ph.D. Thesis, Trinity College, Dublin, 2009. [2] B.D. Donato and J. Bullock. GSPAT: Live Sound Spatialisation Using Gestural Control. Proc. of the 21st International Conference on Auditory Display (ICAD 2015), Granz, Austria, 2015. [3] M. Donnarumma, B. Caramiaux, and A. Tanaka. Muscular Interactions: Combining EMG and MMG sensing for musical practice. Proc. of the 13th International Conference on New Interfaces for Musical Expression (NIME 2013), Daejeon, Korea, 2013, pp. 128-131. [4] S. Emmerson. Living Electronic Music. Ashgate, UK, 2007. [5] R. Graham and J. Harding. SEPTAR: Audio Breakout Circuit for Multichannel Guitar. Proc. of the 15th International Conference on New Interfaces for Musical Expression (NIME 2015), Baton Rouge, LA, USA, 2015, pp. 241-244. [6] B. Johnson, M. Norris, and A. Kapur. Diffusing Diffusion: A History of the Technological Advances in Spatial Performance, Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 126-132. [7] B. Johnson, M. Norris, and A. Kapur. tactile.motion: An iPad Based Performance Interface For Increased Expressivity in Diffusion Performance. Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 798-801. [8] D. Johnson, B. Manaris, Y. Vassilandonakis, and S. Stoudenmier. Kuatro: A Motion-Based Framework for Interactive Music Installations. Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 355-362. [9] B. Manaris, D. Johnson, and M. Rourk. Diving into Infinity: A Motion-Based, Immersive Interface for M.C. Escher’s Works. Proc. of the 21st International Symposium on Electronic Art (ISEA 2015), Vancouver, Canada, Aug. 2015. [10] K. Nymoen, M.R. Haugen, and A.R. Jensenius. MuMYO — Evaluating and Exploring the MYO Armband for Musical Interaction. Proc. of the 15th International Conference on New Interfaces for Musical Expression (NIME 2015), Baton Rouge, LA, USA, 2015, pp. 215-218. [11] S. Senturk, S.W. Lee, A. Sastry, A. Daruwalla, G. Weinberg. Crossole: A Gestural Interface for Composition, Improvisation and Performance using Kinect. Proc. of the 12th International Conference on New Interfaces for Musical Expression (NIME 2012), Ann Arbor, MI, 2012, pp. 449-502 [12] A. Tanaka and R.B. Knapp. Multimodal Interaction in Music Using the Electromyogram and Relative Position Sensing. Proc. of the International Conference on New Interfaces for Musical Expression (NIME 2002), Dublin, Ireland, 2002, pp. 171-176.  
  Figure 8. Iannix visual score demonstrating 3D curves, with time advancing from bottom-left corner to top-right.  White curve is associated with left arm, black with right.  These curves are captured in real-time. 
337