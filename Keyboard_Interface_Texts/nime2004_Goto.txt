The Case Study of Application of Advanced Gesture Interface and Mapping
Interface,
- Virtual Musical Instrument “ Le SuperPolm” and Gesture Controller “BodySuit”
Suguru Goto
(Composer)
82, rue Charles Nodier, 93500 Pantin, France
sugurugoto@csi.com
Takahiko Suzuki
(Waseda University)
1-104 Totsukamachi, Shinjuku-ku, Tokyo, 169-8050, Japan
suzuki@aoni.waseda.jp
ABSTRACT
We will discuss the case study of application of the Virtual
Musical Instrument and Sound Synthesis. Doing this
application, the main subject is advanced Mapping Interface in
order to connect these. For this experiment, our discussion
also refers to Neural Network, as well as a brief introduction of
the Virtual Musical Instrument “Le SuperPolm” and Gesture
Controller “BodySuit”.
Keywords
Virtual Musical Instrument, Gesture Controller, Mapping
Interface
INTRODUCTION
A composer, Suguru Goto has previously developed his
Virtual Musical Instrument “Le SuperPolm” and Gesture
Interface “Bodysuit” at IRCAM in 1996-1998. The main goal
in this experiment is to connect “Le SuperPolm” or “BodySuit
with Sound Synthesis and Interactive Video System in real
time. Doing so, the important point is the bridge between
these, which is called “Mapping Interface. In order to develop
this, an intelligent Mapping Interface is required. This may
allow us to work efficiently, as well as having more musical
expression with the Virtual Musical Instrument. There are
numerous researches in the field of the Gesture Controller, and
Sound Synthesis in real time. Yet, this relationship is rarely
discussed, and an example of advance application has not
sufficiently evolved by others yet. Gesture of a player is
transformed into analog signal with a sensor, and then is
converted to digital signal. As soon as these are captured in a
computer, these can be merely treated as numerical data.
However, gesture and musical expression have much meaning
for human being more than numbers. The contradiction may
rise, when these are tried to express these gesture and musical
expression with Sound Synthesis as output, although these are
flattened to be merely numerical data inside a computer. Our
interest is to raise a question, if the programming is further
developed, it would help to find the solution, while a
computer merely transforms the signals, and gesture and
sound results are subjective matter for a human being.
1. The Theoretical background of Mapping
Interface
Mapping Interface refers to a bridge between “Le SuperPolm”
or “BodySuit” and sound synthesis here. In other words, this
programming translates from input data to the parameters for
Sound Synthesis. Mapping Interface has functions as scaling
input data, switching in and off for each input, changing a
curve, matrix, etc. After Mapping Interface, the data can be
directly sent to inputs of Sound Synthesis. Otherwise, this
may go through Algorithm in order to generate automatic
musical events. This may be also selected depending on an
application in terms of compositional processes.
What we are interested in here, is advanced and intelligent
Mapping Interface. This may learn the input data by itself, then
decide some suitable output data. It is flexibly altered
automatically, depending on the situation of input data. These
are eventually meant to be a well-trained translator between an
expression of performer and an expression of musical sound.
A sensor may not always be able to send an expected data, with
a reason of technical side or a human player side. For example,
0 to 5 volt signal can be translated into either MIDI or Ethernet
protocol. Inside a program, this can be any number of bits. In
other case, depending of sound synthesis, a finger position
does not always correspond to same frequency. In other words,
there is no way to learn exact finger position in order to
control pitch precisely, unless one single sound present
constantly remain in entire composition. Due to this situation,
Mapping Interface can understand where a finger is on a
fingerboard and what this means for sound. Working with
Granular Synthesis, many parameters are necessary to be
controlled at the same time. In this sort of situation, a
performer may sometimes mistake, because of an excessive
complexity of control. Then, Mapping Interface can learn the
habit of player, and then can analyze a tendency of mistake.
Fuzzy logic can apply to flexible understanding for the input
data. Instead of demanding the precise input data, this can
tolerate certain amount of range, and then choose some
parameters, which are necessary.
Neural Network offers other possibilities for Mapping
Interface. This may learn the habit of player. This eliminates
unwanted data, such as noise. Then, it recognizes a pattern of
gesture, musical materials, violin technique, and human player
expression. These are categorized in a certain method. Some
expected input data are prepared beforehand, and then
recognized patterns are treated into required parameters for
sound.
In Granular Synthesis, some preset sounds are prepared
beforehand. Each preset consists of multiple parameters, and
this gradually changes one to another, according to the input
data. In order to do this, Morphing technique is applied to this
Mapping Interface
2. The Virtual Violin, “Le SuperPolm”
Virtual Musical Instruments refer to a system that a gesture of
performer is translated into electric signals. One may control
sound or video image of computer with movement of body in
real time. In “Le SuperPolm”, there is neither string nor hair of
bow. A gesture of performance with a violin is merely modeled.
“Le SuperPolm” was built in 1996 with a collaboration of an
engineer, Patrice Pierrot at IRCAM. “Le SuperPolm” is played
in a similar manner to the violin, except that the fingers touch
sensors on a fingerboard instead of pressing strings. Sounds
may also be modified by movements of the bow, which records
variations in resistance. An eight-button keyboard situated on
the body of the instrument can change the program and the
sounds as well as triggering different pitches, like a normal
keyboard.
“Le SuperPolm” was built in such a way as to respond to body
movements. However it can be assigned new functions by
programming, so as to take into account the compositional
needs of each piece: for instance a sensor can be used to trigger
sounds in one composition, whereas in another it can be used
to change the pitch.
3. Introducing  “BodySuit”
A performer wears a data suit, on which 12 sensors are attached
on each joint of the body. This data suit functions as an
interface of gesture. Depending on a movement, sound and
video images are changed in real time. This differs from a
traditional instrument and a controller. A player performs with
larger movements, such as stretching and bending joints,
twisting arms and so on. This gesture does not really function
like dance or theater. It contains, however, an element of
"performance" within the live musical context. The gesture is
not previously decided in a strict sense. An audience may
observe an obvious difference of intensity of movement
between a static section and a kinetic section in the
composition.
4. REFERENCES
[1] Suguru Goto: The Aesthetics and Technological Aspects of Virtual
Musical Instruments: The Case of the SuperPolm MIDI Violin, Leonardo
Music journal, Vol.9, 1999
[2] Suguru Goto: Virtual Musical Instruments: Technological Aspects
and Interactive Performance Issues, IRCAM. Trends in Gestural Control
of Music, 2000
[3] Masafumi Hagiwara: Neuro, Fuzzy, Genetic Algorithm, Sangyo
tosho, Tokyo, Japan
[4] Kazuo Yana, Yoshitake Suzuki: Neuro Jyoho Shori Gijyutsu,
Kaibundo, Tokyo, Japan
[5] Robert Row: Machine Musicianship, The MIT Press, 2001
[6] Michael Lee, Matt Wright: “mlp” external object of Max/MSP,
CNMAT, University of California, U.S.A