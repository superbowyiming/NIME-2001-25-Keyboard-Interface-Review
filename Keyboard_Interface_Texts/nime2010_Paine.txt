 
Towards a Taxonomy of Realtime Interfaces for 
Electronic Music Performance 
 
Dr. Garth Paine 
School of Communication Arts  
University of Western Sydney, Australia 
ga.paine@uws.edu.au
ABSTRACT 
This paper presents a discussion regarding organology 
classification and taxonomies for digital musical instruments 
(DMI), arising from the TIEM (Taxonomy of Interfaces for 
Electronic Music performance) survey (http://tiem.emf.org/), 
conducted as part of an Australian Research Council Linkage 
project titled “Performan ce Practice in New Interfaces for 
Realtime Electronic Music Performance”. This research is 
being carried out at the VIPRe Lab at , the University of 
Western Sydney in partnership with the Electronic Music 
Foundation (EMF), Infusion Systems 1 and The Input Devices 
and Music Interaction Laboratory (IDMIL) at McGill 
University. The project seeks to develop a sc hema of new 
interfaces for realtime electronic music performance.  
Keywords:  
Instrument, Interface, Organology, Taxonomy.  
1. INTRODUCTION 
Despite the continu ing and strong interest in the design and 
creation of new Digital  M u s i c a l  I n s t r u m e n t s  ( D M I )  [ 6 ] ,  the 
classical taxonomy of acoustic instruments focuses on the 
initial vibrating element in an instrument that produces its 
sound (air, skin, string). Developed by Mahillon [1] and later 
expanded by Hornbostel and Sachs [2] the taxonomy consists 
of four top -level classifications — Aerophones, Chordophones, 
Idiophones and Membranophones. Each of these top -level 
classifications is broken into numerous sub -categories creating 
over 300 basic categories in all. In 1940 Sachs expanded the 
classification system to include a fifth top -level group, 
electrophones for instruments involving electricity. In Sachs’ 
classification system the electrophones were separated into 
three sub-categories—  
1. instruments with an electronic action  
2. electro mechanical, acoustic sounds transformed into 
electric through amplification;  
3. radioelectric, instruments which are based on 
oscillating circuits.  
The classification system is of course woefully inadequate 
to capture the richness, diversity and trends of digital musical 
instrument design. By placing the focus on the initial sound 
making device, the differences, similarities and relationship s 
between instruments such as the eShofar [3], tooka [4] and T -
stick [5] are lost. 
More recent approaches to developing taxonomies of DMI 
have focused on the sensor types u sed, the nature of the 
interface, the way gestures are captured and the mappings 
between interface and sound generating functions [6]. Pringer 
[7] compared DMI with respect to expressivity, immersion and 
feedback. While Pressing [8] a n d  B i r n b a u m  e t  a l .  [9] h a v e  
proposed multi -dimensional spaces to represent DMI, 
incorporating their interactive potentials. Birnbaum et al.’s 
seven axes are—  
1. Role of Sound 
2. Required Expertise 
3. Music Control 
4. Degrees of Freedom 
5. Feedback Modalities 
6. Inter-actors 
7. Distribution in Space 
2. REVISING DEFINITIONS 
The TIEM survey received submissions across a wide range of 
innovative approaches to electronic music performance. 
Whether s e e n  a s  a n  i n s t r u m e n t  o r  i n t e r f a c e  ( a  m o r e  d e t a i l e d  
discussion about proposed definitions follows), it is clear that 
their principle focus is live music making , and as such have an 
underlying foundation concept of ‘Instrument’. It is useful to 
unpack that concept to illuminate the influence it has on design 
and development. 
Again, organologies [6] [11] fail us here too.  They present 
a method of categorising musical instruments, but they do not 
explicitly detail an underlying schema, a generic concept of 
musical instrument. 
In developing such a schema, a n examination of musi cal 
performance is very helpful as it establishes the constraints and 
requirements of a musical instrument. Godlovitch [12] 
carefully unpicks the construct of the musical performance. He 
lists, among other things, the following as essential:  
• a datable sound sequence (that is, sonic event),  
• immediately caused by some human( -like) being,  
• the immediate output of some musical instrument,  
• intended to be caused at a specified time and place, and 
in a specified manner,  
• the exercise of skilled activity,  
• an instance of some identifiable musical work,  
• intended for and presented before some third -party 
listener, exercising active concentrated attention.  
Three items emerge as thematic undercurrents of the model of 
performance offered.  
• First, the significance of performance is strongly 
emphasized. The model displays the major musical 
constituents - m u s i c i a n s ,  w o r k s ,  s o u n d ,  l i s t e n e r s  - a s  
converging upon performance, and deriving their 
musical purpose in and through it.  
• Second, performances can fail both by misrepresenting 
the work and by disaffecting the listener.  
• Third, performance is action- centred. 
                                                                                                 
1 see http://infusionsystems.com	  (viewed 02/02/10) 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provide d that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME2010, June 15-18, 2010, Sydney, Australia 
Copyright remains with the author(s).  
 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
436
Godlovitch’s model establishes certain primary causal 
conventions that may or may not apply when considering new 
instruments, which differ from acoustic instruments by the 
disconnection o f  t h e  e x c i t a t i o n  a n d  s o n i f i c a t i o n  m e c h a n i s m s . 
The laptop music pe rformance is a classic example of where 
the causality may be opaque.  
Endeavouring to address new instruments Godlovitch puts 
forward the concept of ‘remote control’, saying that “Computer 
assisted music, musical quasi -readymades, and experimental 
music cha llenge the centrality of immediate agency. … 
Primary causation involves direct control. Not all causation is 
primary. Causation is indirect when what the maker does 
skilfully is at a significant procedural remove from the final 
effect. Indirect causation i s standard in computer art and 
music. I will call the process remote control. ”  
The terms “computer assisted music, musical quasi -
readymades” refer to a performance which entails the replay of 
predetermined sequence s o f  m u s i c a l  m a t e r i a l  w i t h o u t  
intervention. Much has changed since Godlovitch coined these 
definitions in 1998. Computing power facilitates  r e a l t i m e  
software synthesis languages such as Supercollider 2, 
Max/MSP3, Pd4, Chuck5, Impromptu6, JSyn7, Audiomulch8 etc. 
wherein variables within these softw are synthesisers can be 
controlled in realtime. Such a paradigm is not considered by 
Godlovitch.  
3. METHOD 
The online TIEM Questionnaire 9 c o n s i s t e d  o f  7 2  q u e s t i o n s  
examining the practice and application of new interfaces for 
real-time electronic music perfor mance. The questions 
consisted of a mix of textural and numeric, qualitative and 
quantitative, arranged into six sections —  
1. General Description  
2. Design Objectives  
3. Physical Design  
4. Parameter Space  
5. Performance Practice  
6. Classification  
Participants were not  required to answer all questions and were 
able to revisit the questionnaire to complete their submission. 
The questionnaire was launched in Ju ne 2008 and attracting 
over  800 unique survey views with 80 completed responses.  
4. Analysis  
An online database of  t h e  i n t e r f a c e s / i n s t r u m e n t s  s u b m i t t e d  t o  
the survey (if they elected to be listed publicly) has been 
created at – h t t p : / / v i p r e . u w s . e d u . a u / t i e m .  S i n c e  l a u n c h i n g  t h e  
online database in September 2008 the web site has had over 
1900 unique visitors (500 per month) and 6400 page views 
(1800 per month). The TIEM database has also been 
referenced on (amongst others) WIRED 10, CNN 11 a n d  
Electroacoustic Resources 12. 
                                                             
2 see http://supercollider.sourceforge.net (viewed 02/02/10) 
3 see http://www.cycling74.com (viewed 02/02/10) 
4 see http://crca.ucsd.edu/~msp/s oftware.html (viewed 02/02/10) 
5 see http://chuck.cs.princeton.edu  (viewed 02/02/10) 
6 see http://impromptu.moso.com.au  (viewed 02/02/10) 
7 see http://www.softsynth.com/jsyn/   (viewed 02/02/10) 
8 see http://www.audiomulch.com (viewed 02/02/10) 
9 see http://tiem.emf.org/survey  (viewed 02/02/10) 
10 see http://www.wired.com (viewed 02/02/10) 
11 see http://edition.cnn.com (viewed 02/02/10) 
12 see http://ressources.electro. free.fr (viewed 02/02/10) 
Table 1 p r e s e n t s  a  s u m m a r y  o f  t h e  r e s p o n s e s  g i v e n  t o  
question six that asked for  a brief description of the submitted 
interface/instrument. 
Table 1. Summary of instrument families. 
 
Analysis for the brief descriptions, using NVivo 13, produced 
the parent nodes in Table 1; Collaborative, Computer Software, 
Controller and Instrument. It can also be noted that gesture 
forms a substantial child node.  
Further classification was sought in the final question of the 
survey (Q72) that asked,  “Do you consider the interface 
instrument to be part of a group of other 
interfaces/instruments.”  Answers included: 
 DJ equipment  
 Looping family of instruments...  
 Virtual musical instruments.  
 Tabletop interfaces/instruments  
 Collaborative instruments  
 Tangible interfaces  
 Multi-player controllers including ga me consoles.  
 Free-gesture interfaces  
 Synthesizers  
 MIDI controller  
 Hacked device used and available for other 
application (tablet, joystick etc..)  
 Hybrid controller 
 Information visualization interfaces  
 USB controllers 
                                                             
13 see http://www.qsrinternational.com/  (viewed 02/02/10) 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
437
Figure 1 Overlap in classifications  
Figure 1 represents an analysis of the crossover of the 
applications inherent in the classifications provided above. It 
also shows how technological descriptors (MIDI Controller, 
Virtual Instrument) are freely mixed with perfor mance 
methods (Collaborative, Free Gesture) and performer roles 
(DJ).  Such a classification then becomes somewhat 
meaningless as all these areas overlap at some level.  
A combination of the two approaches to classification as 
discussed above leads back to the principle classifiers 
illustrated in Figure 2. The instrument families in Figure 1 can 
be seen as examples of the top -level families defined in Figure 
2. 
Figure 2 Principle classes of instrument defined in Q6  
 
In order for the se classifications to be of use in practice, a 
taxonomic approach is needed. Taking the top -level classifiers 
of Figure 2, a taxonomy might look something like Figure 3. 
The taxonomy outlined in Figure 3 takes as its starting point 
the classifiers of Gesture, Digital Controller and Software, but 
excludes Instrument as it is a wide descriptor of all musical 
performance tools and therefore of no value here. Above these 
levels of classification, the discourse  s u r r o u n d i n g  c a u s a l i t y  i s  
introduced, as per Godlovitch’s [12] notion of remote control.  
Software only instruments remain separate in this taxonomy 
and should probably be integrated in further iterations.  
 
 
Figure 3 a Taxonomy - combining the above analysis  
 
The distinction between Create and Control is applied as 
higher order classifiers above Gestural and Digital Controller 
respectively. As in the previous classification discussions 
however, the distinction is not clear -cut.  For instance as we 
look to the lower order definitions in the taxonomy, where 
control is distinguished as moving from continuous to discrete, 
inter-relations proliferate. The range of continuous to discrete 
was also examined in th e TIEM survey, with 59.62% of 
respondents described their instrument/interface as p rocess 
based, meaning continuous control and variation of musical 
material, and 40.38% as event based, equating to the triggering 
of samples/loops or sequences. The next lay er down in Figure 
3 seeks to differentiate between tactile gestural interfaces, and 
digital controllers that contain discrete or continuous interface 
elements. For instance video tracking systems are generally not 
tactile and require gestural input. Some m usical controllers 
such as the WiiMote, and the Buchla Lightning 14 are driven by 
gesture, but held in the hand of the user (tactile). All digital 
controllers require direct manipulation to activate the interface 
elements such as sliders, joysticks or button s and switches and 
as such are tactile. The Wacom tablet 15 currently sits under the 
Digital Controller category, under the No Keys node, however 
it clearly requires gestural input, but unlike the low order 
gestural nodes, gestural input is highly constraine d, limiting the 
perform to addressing the interface directly rather than the 
broader performance space. 
 Tracing the lines from both the Create and Gestural 
categories also illustrates the issue of complexity. All of the 
interface descriptors can be utilis ed in music creation that 
shows both immediate agency and primary causation. 
Similarly, the digital controllers equate with all interface types 
except non-tactile. 
 
5. DISCUSSION 
 One of the characteristics of DMI that becomes clear when 
analyzing the data fr om the TIEM questionnaire, and 
attempting to develop a taxonomy (as in Figure 3 a b o v e ) ,  i s  
that most digital musical instruments simultaneously utilise 
elements of both the creation of music in the moment, and the 
control of the s ystem of which the interface/instrument is 
integral. 
It is useful to apply the above analysis to innovative and 
commercially available digital musical instruments.  
                                                             
14 see http://www.buchla.com/lightning3.html  (viewed 02/02/10) 
15 see http://www.wacom.com/index.html  (viewed 02/02/10) 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
438
6. CONCLUSION 
The taxonomic approach proposed in this paper is 
acknowledged to have weaknesses, and to be incomplete.  It is 
a first iteration of an attempt t o make meaningful categories 
that facilitate comparative studies of DMI, and a step towards a 
considered inclusion of DMI within the existing musical 
instrument classification frameworks.  
The classification, electrophones, should be of equal 
standing as th e existing top-level classifiers within the 
Hornbostel and Sachs [2] t a x o n o m y - Aerophones, 
Chordophones, Idiophones and Membranophones , but in order 
for that to be the case, they must encompass a framework of 
sub-classification t h a t  i s  a t  l e a s t  a s  r i c h  i n  n u a n c e  a s  t h a t  
existing in the other top level classifiers.  One might quickly 
see sub headings of Synthesisers, Wind -controllers, 
Percussion-controllers etc, however as discussed at the top of 
this paper, such classifications fail to address many evolving 
approaches to the design and use of DMI.  The classification of 
Gestural Controller, Digital Controller and Instrument as 
outlined in Figure 2 may be the first level of division, but as 
outlined in the commercial examples discussed above, new 
interfaces/instruments display a multi -faceted quality not 
previously seen in musical instruments. They do so by 
combining the roles of creation of realtime content and the 
control of the system.  In an area of music practice where the 
excitation-sonification mechanisms are not inherently 
connected, the analysis of design trends and the TIEM 
international survey suggest that the interplay of control a n d  
create, s eems  t o be one pos s i bl e pat h towards a taxonomy of 
realtime interfaces for elec tronic music performance .  
Perhaps a new taxonomy would seek to differentiate 
instruments where mechanically linked excitation -sonification 
(i.e the existing taxonomy) is inherent as subclasses in a new 
taxonomy that includes electrically linked excitation-
sonification with subclasses as defined by the electrophones, 
and an additional category of digitally linked excitation -
sonification, with subclasses based on the create and control 
paradigms. 
 
 
7. REFERENCES 
[1] V. C. Mahillon. Catalogue descriptif et analytiqu e de 
Musée Instrumental du Conservatoire Royal de Musique 
de Bruxelles, vol. I, second edition. Paris: Gand, [1880] 
1893. 
[2] E. M. Hornbostel and C. Sachs. Systematik der 
Musikinstrumente : Ein Versuch. Zeitschrift für 
Ethnologie Translated by A. Bains and K.  Wachsmann 
under the title “A Classification of Musical Instruments.” 
Galpin Society Journal, [1914] 1961.  
[3] R. Gluck, “eShofar as a Culturally Specific Live 
Electronic Performance System.” Journal SEAMUS, Vol. 
19, Society for Electroacoustic Music in the Un ited States, 
Fall, 2006. 
[4] S. Fels, L. Kaastra, S. Takahashi and G. McCaig. 
“Evolving Tooka: from Experiment to Instrument.” In 
Proceedings 4th International Conference on New 
Interfaces for Musical Expression (NIME04). pp.1 -6, 
2004. 
[5] J. Malloch and M. M. Wan derley, “The T-Stick: From 
Musical Interface to Musical Instrument.” In Proceedings 
of the 2007 International Conference on New interfaces 
for Musical Expression (NIME07), New York, USA, 
2007, pp. 66-69, 2007. 
[6] E. R. Miranda and M. M. Wanderley, New Digital  
Musical Instruments: Control and Interaction Beyond the 
Keyboard (The computer music and digital audio series; 
v.21). Middleton, Wis: A -R Editions, 2006. 
[7] J. Piringer. “Elektronische musik und interaktivität: 
Prinzipien, Konzepte, Anwendungen.” Master’s thesis, 
Technical University of Vienna, 2001.  
[8] J. Pressing. “Cybernetic Issues in Interactive Performance 
Systems.” Computer Music Journal, 14(2):12 -25, 1990. 
[9] D. Birnbaum, R. Fiebrink, J. Malloch, M. M. Wanderley, 
“Towards a Dimension Space for Musical Device s.” In 
Proceedings of the 2005 International Conference on New 
Interfaces for Musical Expression (NIME’05), Vancouver 
Canada, pp. 192-95, 2005. 
[10] D. J. Levitin, This is your brain on music: the science of a 
human obsession. New York, N.Y: Dutton, 2006. 
[11] M. J. Kartomi, On Concepts and Classifications of 
Musical Instruments, (Chicago Studies in 
Ethnomusicology). Chicago: University of Chicago Press, 
1990. 
[12] S. Godlovitch, S. Musical Performance: A Philosophical 
Study. London ; New York: Routledge, 1998.  
[13] D. Wessel, and M. Wright, “Problems and Prospects for 
Intimate Control of Computers.” Computer Music 
Journal, 26(3), 11-22, 2002. 
[14] P. Cook, “Principles for Designing Computer Music 
Controllers”. Paper presented at the 2001 International 
Conference on New Interfaces f or Musical Expression 
(NIME’01), in Proceedings of CHI 2001, Extended 
Abstracts, Seatle, 2001.  
[15] C. Roads, (Ed.). The Computer Music Tutorial. 2nd Ed. 
Massachusetts: The MIT Press, 1996. 
[16] R. Rowe, Interactive Music Systems. Massachusetts: The 
MIT Press, 1993. 
[17] T. Winkler, “Making Motion Musical: Gestural Mapping 
Strategies for Interactive Computer Music.” In 
Proceedings of the 1995 International Computer Music 
Conference, San Francisco, 1995.  
[18] J. Chadabe, “The Limitations of Mapping as a Structural 
Descriptive in Electronic Music.” In Proceedings of the 
2002 International Conference on New Interfaces for 
Musical Expression (NIME’02), Dublin, 2002. 
[19] I. Richardson, and C. Harper, “Corporeal Virtuality: The 
Impossibility of a Fleshless Ontology.” Body, Space and 
Technology Journal, 2(2), 2001.  
[20] D. Ihde, Technology and the Lifeworld: Fom Garden to 
Earth. (The Indiana series in the philosophy of 
technology). Bloomington: Indiana University Press, 
1990. 
[21] G. Paine, I. Stevenson, and A. Pearce, “The Thummer 
Mapping Project (ThuMP).” In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression (NIME’07), New York City, NY, 2007. 
[22] G. Weiss, Body Images: Embodiment as Intercorporeality. 
New York: Routledge, 1999. 
 
Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia
439