Synthesis and Control on
Large Scale Multi-Touch Sensing Displays
 Philip L. Davidson Jefferson Y. Han
Courant Institute of Mathematical Sciences
New York University
719 Broadway New York, NY 10003
{ philipd, jhan }@mrl.nyu.edu
ABSTRACT
In this paper, we describe our experience in musical interface 
design for a large scale, high-resolution, multi-touch display 
surface. We provide an overview of historical and present-
day context in multi-touch audio interaction, and describe our 
approach to analysis of tracked multi-ﬁnger, multi-hand data for 
controlling live audio synthesis.
Keywords
multi-touch, touch, tactile, bi-manual, multi-user, synthesis, 
dynamic patching
1 INTRODUCTION
The musician’s need to manipulate many simultaneous degrees 
of freedom in audio synthesis has long driven the development of 
novel interface devices. Touch sensors integrated with graphical 
display functionality can provide intuitively direct interactivity 
with richly dynamic context; however they are typically only able 
to respond to a single point of contact a time, making them quite 
limiting for musical input. Multi-touch sensors on the other hand 
permit the user fully bi-manual operation as well as chording 
gestures, offering the potential for great input expression. Such 
devices also inherently accommodate multiple users, which 
makes them especially useful for larger interaction scenarios 
such as interactive tables.
These devices have historically been dif ﬁcult to construct, but 
we have taken advantage of a new rear-projectable multi-touch 
sensing technology with unique advantages in scalability and 
resolution, to create novel musical interfaces for synthesis and 
control in a large format dynamic workspace.
2 PREVIOUS WORK
2.1 Multi-Touch Interfaces
Boards composed of a plurality of individual controls such as 
sliders, knobs, buttons, keys, and touchpads, can in a sense be 
considered multi-touch interfaces. Advanced devices of this 
class include large arrays of position-sensitive touch sensors 
such as Buchla’s Thunder [2], Eaton and Moog’s Multiple-Touch 
Keyboard [7] and the Continuum Fingerboard  [8]. However, 
we are more interested in homogeneous interaction surfaces that 
allow for dynamic contextualization.
Buxton experimented with continuous touch-sensing [22] as 
well as multi-touch sensing devices for music with the Fast 
Multiple-Touch-Sensitive Input Device [3][14]. This device was 
an active matrix of capacitive touch sensors, 6432 in resolution. 
Instead of integrating it with a display, Buxton utilized cardboard 
template overlays to partition the interaction surface to provide 
context, in addition to kinesthetic feedback.
Tactex more recently experimented in the marketplace with 
a product directly aimed at musicians called the MTC Express 
[23]. This device optically measured the compression of a 
translucent compressible foam, and though it only had a spatial 
resolution of 89, it has an impressive temporal sampling rate 
(200Hz) and dynamic range in pressure, making it mostly useful 
for percussive control.
The recent Lemur from JazzMutant [11] is a multi-touch sensor 
that is tightly integrated with an LCD display. The device is sized 
for , and functions as a software-conﬁgurable controller board. 
However, the device is low resolution (128100) and provides no 
pressure information, limiting the sophistication of the interface 
widgets that are provided. Furthermore, the system is not open 
enough to allow access to either the raw sensor data stream or to 
the raw display itself, limiting its usefulness for the exploration 
and development of novel interfaces.
All of the systems above have a complexity on the order of 
the number of tactels, which limits both resolution (though 
interpolation and other signal processing techniques can mitigate 
Figure 1: Rear-projected, multi-touch interaction session
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for proﬁt or commercial advantage and that 
copies bear this notice and the full citation on the ﬁrst page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior speciﬁc permission and/or a fee.
NIME 06, June 4-8, 2006, Paris, France.
Copyright remains with the author(s).
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
216
this for a sparse set of contacts) and physical scale, reducing 
their role in musical performance to a component within a larger 
system. Other more scalable multi-touch sensing technologies 
are starting to become available [6][21][26], but these are still 
difﬁcult/expensive to obtain, and we have not yet seen any 
reports of their usage in a musical context.
2.2 Tangible Interfaces
Larger scale musical interfaces have also developed around the 
concept of the manipulation of trackable tangible assets, such as 
blocks or pucks. These tangible interfaces [10] can accommodate 
more than one hand and/or more than one user, and take advantage 
of the user’s sense of kinesthesia and skills in three-dimensional 
spatialization.
The AudioPad [19], is a tabletop instrument which utilizes 
modiﬁed Wacom tablet systems to track the position and 
orientation of a limited number of pucks. This tabletop 
environment enabled the dynamic control of loops of other 
synthesis through marking menus, and also allowed the pucks to 
act as dials and other controllers to vary parameters. Pucks could 
also be equipped with a pushbutton, which could be regarded as 
1-bit pressure sensitivity.
d-touch [5] and the reacTable* [12] are more recent tabletop 
instruments based on vision-based tracking of optical ﬁducials. 
They track many more pucks without compromising the sensing 
update rate, and have developed several tangible musical 
interface paradigms.
We ﬁnd that these, and other tangible instruments [1][16][17][18] 
provide an intuitive and approachable environment for musical 
control, but face challenges as the complexity of the environment 
increases.
3 SYSTEM OVERVIEW
Through the usage of a scalable high-resolution multi-touch 
sensing technique, we build a system that encompasses the 
functionality of both the virtualized controllers possible on multi-
touch devices such as Lemur, and the space and scale of multi-
user patching systems such as AudioPad and reacTable*[13].
The technique is based on frustrated total internal reﬂection [9], 
implemented in the form factor of a 36”x27” drafting table, at a 
sensing resolution of ~2mm at 50Hz. It provides full touch image 
information without any projective ambiguity issues whatsoever. 
The touch information is true- it accurately discriminates 
touch from a very slight hover, while also providing pressure 
information. The sensor image sequence is analyzed and parsed 
into discrete stroke events and paths with a processing latency 
of about 3.5ms on a 3GHz Pentium 4. Measurements including 
position, velocity, pressure, and image moments are sent to client 
applications using the lightweight OSC protocol [27] over UDP. 
The system is notably graphically integrated via rear-projection, 
preventing undesirable occlusion issues.
For our experiments with audio control, we built a simple set 
of synthesis modules using STK [4], controlled by a modular 
patching interface.
4. DISCUSSION
4.1 Graphical Context
As Buxton ﬁrst demonstrated, context is a critical issue for 
touch interfaces. While we are a few steps beyond cardboard 
overlays, context for interaction on continuous control surfaces 
is a challenging problem. Although the pucks used in AudioPad 
and reactTable* are visually passive, information is projected 
on and around the puck to provide additional feedback to the 
user. As such, they are a convenient metaphor for control in 
contextualizing the surface.
4.2 Basic Gestures
Pucks emphasize our ability to precisely manipulate objects 
between our ﬁngers. True multi-touch surfaces should provide 
a similar capacity for manipulation, in contrast to a discrete set 
of continuous controls. We begin by extending the dextrous 
manipulation concept to the touch surface by creating regions 
of the surface that act as virtual puck-like widgets. Touch 
information captured by each widget is processed together as a 
single complex gesture. As with pucks, we use the space in and 
around these controllers for rich visual feedback.
4.3 Interpretation Model
Free from the limitation of the physical world, we can start to  
extend the metaphor of the basic puck- for instance, the control 
region associated with a widget can be dynamically resized or 
reshaped in the course of a performance.
We can also ﬂexibly divide inputs into separate control groups, 
and selectively constrain degrees of freedom while maintaining 
a robust handling of under- or overconstrained input cases. As 
an example, constraining the transformation to rotation and 
translation is equivalent to the degrees of freedom in a physical 
puck, while constraint to single-axis translation acts as a slider. 
We implemented the more traditional interface widgets such as 
sliders, knobs, and keys, which the performer can manipulate 
any set of simultaneously. Additionally, the availability of 
pressure information allows for more sophisticated revisions 
of these basic controls. We also use a ‘deadband’ model [15] to 
differentiate between tracking and control, permitting the precise 
acquisition of control elements by the user. Pressure data is also 
heavily used for more novel controls such as Zliders [20], as well 
as control pads which interpret relative pressure values as tilt 
measurements.
4.3 Complex Gestures
With the input captured from two or more hands, we can start to 
simulate physical manipulations such as strain, twist, or bending 
motions. Through this we can consider virtual instruments 
controlled by simpliﬁed physical systems - for example, we could 
monitor volume of a deformable object to determine the ﬂow 
rate for a wind controller, or use strain measurements to modify 
string tension or resonance modes. We are currently exploring 
Figure 2: AudioPad, reacTable*, and Lemur
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
217
the possibilities using a fretboard and plucked string model to 
produce an autoharp, or koto-like instrument.
4.3 Structural Flexibility
We ﬁnd that contextualizing manipulation through widgets 
allows similar precision in parametric control as a physical puck 
model, and that multi-touch gestures are a natural extension of 
the control space. Capturing the wide gestural range possible 
with the hand [24] requires that the sensor accurately track 
points in close proximity, and control gestures must recognize 
the limitations of hand geometry as described in [25], to prevent 
painful or impractical gestures. One advantage to virtualization 
is that each arrangement can conform to the size and shape 
of the user’s hands, preventing undue stress. As with any 
continuous control surface, widgets may be adjusted, expanded 
or repositioned without the synchronizing the location of their 
physical counterparts. In Figure 3, we show the use of a two-
dimensional view manipulator, actuated with a simple two-
ﬁngered gesture, allowing the user to pan, zoom, and rotate 
the workspace and inspect a modular element in detail with no 
loss of context, giving the performer the ability to manage large 
workspaces much more effectively.
5 FUTURE DIRECTIONS
There are some limitations in the core implementation that we 
would like to address that would further increase its usefuless 
for musical applications. For instance, our current sample rate 
of 50Hz is good but not great, particularly for percussive input, 
although this is mitigated by the fact that a large amount of 
simultaneous information can be updated for each frame. We 
will be immediately upgrading the system to achieve 120Hz or 
more. 
Also, our current setup provides context only through visual 
means, but we are de ﬁnitely looking to be able to provide some 
degree of haptic feedback as well.
We will continue to explore new and design of new widgets in this 
new domain. While the table has its advantages over traditional 
control surfaces, we are primarily interested in controls that take 
full advantage of the multi-touch data. A uniform control surface 
also raises the possibility of ﬂexible interfaces - for example, a 
piano keyboard interface that adjusts spacing based on a user 
playing a set of prompted chords. In provided a customized 
scaling of the interface we can adapt to different players to better 
ﬁt their stature, or to reduce RSI related conditions.
The versatility of the sensor allows for much more interesting 
form-factors than the console table we have shown here. In 
particular, for multi-user collaborative setups, we can envision a 
wider setup where two musicians perform on the same surface, 
while passing or linking sonic elements in a shared workspace.
Multi-touch sensing is currently an active ﬁeld in HCI research, 
Figure 3: Dynamic workspace- users easily pan/zoom/rotate with a bimanual gesture
so we stand to harness the fruits of much other work in advancing 
the intuitiveness, ef ﬁciency, and usability of this unique family 
of interfaces.
6 REFERENCES
[1] Berry, R., Makino, M. , Hikawa, N. and Suzuki, M. 2003. 
The Augmented Composer Project: The Music Table. 
In Proceedings of the 2003 International Symposium on 
Mixed and Augmented Reality, Tokyo, Japan, 2003.
[2] Buchla, D. 1990. Thunder. http://www.buchla.com/
historical/thunder/
[3] Buxton, W., Hill, R., and Rowley, P. 1985. Issues 
and Techniques in Touch-Sensitive Tablet Input. In 
Proceedings of the 12th Annual Conference on Computer 
Graphics and interactive Techniques SIGGRAPH ‘85. 
ACM Press, New York, NY , 215-224.
[4] Cook, P. R. and G. Scavone. 1999. The Synthesis Toolkit 
(STK). In Proceedings of the International Computer 
Music Conference. International Computer Music 
Association, pp. 164-166.
[5] Costanza, E, Shelley, S. B., and Robinson, J.. Introducing 
Audio d-touch: A Tangible User Interface for Music 
Composition and Performance. In Proceedings of the 
2003 International Conference on Digital Audio Effects, 
London, UK, September 8-11 2003b.
[6] Dietz, P. and Leigh, D. 2001. DiamondTouch: a Multi-
User Touch Technology. In Proceedings of the 14th 
Annual ACM Symposium on User interface Software and 
Technology (Orlando, Florida, November 11 - 14, 2001). 
UIST ‘01. ACM Press, New York, NY , 219-226.
[7] Eaton, J. and Moog, R. 2005. Multiple-Touch-Sensitive 
Keyboard. In Proceedings of the 2005 International 
Conference on New Interfaces for Musical Expression 
(NIME05), Vancouver, BC, Canada.
[8] Haken, L. 2005. Continuum Fingerboard. http://www.
cerlsoundgroup.org/Continuum/
[9] Han, J. Y . 2005. Low-Cost Multi-Touch Sensing through 
Frustrated Total Internal Reﬂection. In Proceedings of the 
18th Annual ACM Symposium on User interface Software 
and Technology (Seattle, WA, USA, October 23 - 26, 
2005). UIST ‘05. ACM Press, New York, NY , 115-118.
[10] Ishii, H. and Ullmer, B. 1997. Tangible Bits: Towards 
Seamless Interfaces between People, Bits and Atoms. 
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
218
In Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems (Atlanta, Georgia, United 
States, March 22 - 27, 1997). S. Pemberton, Ed. CHI ‘97. 
ACM Press, New York, NY , 234-241.
[11] JazzMutant. 2004. Lemur. http://www.jazzmutant.com/
[12] Jordà, S. & Kaltenbrunner, M. & Geiger, G. & Bencina, 
R. The reacTable*. In Proceedings of the International 
Computer Music Conference (ICMC2005), Barcelona 
(Spain)
[13] Kaltenbrunner, M. & Geiger, G. & Jordà, S. 2004. 
Dynamic Patches for Live Musical Performance. In 
Proceedings of the 2004 Conference on New Interfaces 
for Musical Expression (NIME04), Hamamatsu, Japan
[14] Lee, S. K., Buxton,W. and Smith, K. C. 1985. A Multi-
Touch Three Dimensional Touch-Sensitive Tablet. In 
Proceedings of CHI ’85 (April 1985), ACM/SIGCHI, NY , 
1985, pp. 21–25.
[15] Miller, T. and Zeleznik, R. 1999. The Design of 3D 
Haptic Widgets. In Proceedings of the 1999 Symposium 
on interactive 3D Graphics (Atlanta, Georgia, United 
States, April 26 - 29, 1999). SI3D ‘99. ACM Press, New 
York, NY , 97-102.
[16] Newton-Dunn, H, Nakao, H., and Gibson, J. 2003. 
Block Jam: A Tangible Interface for Interactive Music. 
In Proceedings of the 2003 International Conference 
on New Interfaces for Musical Expression, Montreal, 
Canada, May 22-24 2003.
[17] Paradiso, J. and Hsiao, K. 1999. A New Continuous 
Multimodal Musical Controller using Wireless Magnetic 
Tags. In Proceedings of the 1999 International Computer 
Music Conference, pages 24–27, Beijing, China, October 
22-28 1999.
[18] Paradiso, J. A. 2002. Several Sensor Approaches that 
Retroﬁt Large Surfaces for Interactivity. Presented at 
the UbiComp 2002 Workshop on Collaboration with 
Interactive Walls and Tables, Gothenburg, Sweden, 
September 29, 2002.
[19] Patten, J., Ishii, H., Hines, J., and Pangaro, G. 2001. 
SenseTable: A Wireless Object Tracking Platform for 
Tangible User Interfaces. In Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems 
(Seattle, Washington, United States). CHI ‘01. ACM 
Press, New York, NY , 253-260.
[20] Ramos, G. and Balakrishnan, R. 2005. Zliding: Fluid 
Zooming and Sliding for High Precision Parameter 
Manipulation. In Proceedings of the 18th Annual ACM 
Symposium on User interface Software and Technology 
(Seattle, WA, USA, October 23 - 26, 2005). UIST ‘05. 
ACM Press, New York, NY , 143-152.
[21] Rekimoto, J. 2002. SmartSkin: An Infrastructure for 
Freehand Manipulation on Interactive Surfaces. In 
Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems: Changing Our World, 
Changing Ourselves (Minneapolis, Minnesota, USA, 
April 20 - 25, 2002). CHI ‘02. ACM Press, New York, 
NY , 113-120.
[22] Sasaki, L., Fedorkow, G., Buxton, W., Retterath, C., 
Smith, K.C. 1981. A Touch Sensitive Input Device. In 
Proceedings of the 5th International Conference on 
Computer Music. North Texas State University, Denton 
Texas.
[23] Tactex. Smart Fabric Technology. http://www.tactex.com
[24] Waisvisz, M. 1984. The Hands. http://www.crackle.org/
TheHands.html
[25] Wessel, D., Wright, M., and Schott, J. 2002.Intimate 
Musical Control of Computers with a Variety of 
Controllers and Gesture Mapping Metaphors. In 
Proceedings of the 2002 Conference on New Instruments 
for Musical Expression (NIME-02), Dublin, Ireland, May 
24-26, 2002
[26] Westerman, W., Elias, J. G., and Hedge, A. 2001. Multi-
Touch: A New Tactile 2-D Gesture Interface for Human-
Computer Interaction. In Proceedings of the Human 
Factors and Ergonomics Society 45th Annual Meeting, 
V ol. 1, pp. 632-636. 2001.
[27] Wright, M. 2003. OpenSound Control: State of the Art 
2003. In Proceedings of the 2003 Conference on New 
Interfaces for Musical Expression, Montreal, Canada, 
2003.
Figure 4: Experiments in multi-touch interfaces
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
219