Stringesthesia: Dynamically Shifting Musical Agency
Between Audience and Performer Based on Trust in an
Interactive and Improvised Performance
Torin Hopkins
University of Colorado Boulder
Torin.Hopkins@colorado.edu
Emily Doherty
University of Colorado Boulder
Emily.Doherty@colorado.edu
Netta Ofer
University of Colorado Boulder
Netta.Ofer@colorado.edu
Suibi Che-Chuan Weng
University of Colorado Boulder
Suibi.Weng@colorado.edu
Peter Gyory
University of Colorado Boulder
Peter.Gyory@colorado.edu
Chad Tobin
University of Colorado Boulder
Chad.Tobin@colorado.edu
ABSTRACT
This paper introduces Stringesthesia, an interactive and
improvised performance paradigm. Stringesthesia was de-
signed to explore the connection between performer and
audience by using real-time neuroimaging technology that
gave the audience direct access to the performer’s internal
mental state and determined the extent of how the audience
could participate with the performer throughout the per-
formance. Functional near-infrared spectroscopy (fNIRS)
technology was used to assess metabolic activity in a net-
work of brain areas collectively associated with a metric we
call “trust”. The real-time measurement of the performer’s
level of trust was visualized behind the performer and used
to dynamically restrict or promote audience participation:
e.g., as the performer’s trust in the audience grew, more
participatory stations for selecting the performer’s chords
were activated. Throughout the paper we discuss prior work
that heavily influenced our design, conceptual and method-
ological issues with using fNIRS technology, and our sys-
tem architecture. We then describe an employment of this
paradigm with a solo guitar player.
Author Keywords
Performance paradigms, Neuroimaging, fNIRS, trust, mu-
sical agency, improvisation
CCS Concepts
•Applied computing → Sound and music computing; Per-
forming arts;
1. INTRODUCTION
Trust is a crucial component to promoting performer-audience
engagement, as both parties must express some level in con-
fidence to carry out a successful performance. While this
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’23, 31 May–2 June, 2023, Mexico City, Mexico.
interplay of trust has been explored in some interactive mu-
sical performances [24, 25], much of the trust literature ex-
plores the collaboration of human-agent teams (HATs) [6,
2]. In HAT studies, trust is often evaluated through the
manipulation of agent characteristics such as transparency
and reliability. In particular, when agents provide unreli-
able or incomplete information, humans are likely to ignore
the information and rely solely on their own experience, re-
sulting in a lack of trust in the agent. Conversely, when
agents provide information that is overly reliable, humans
may become complacent and over-rely on the agent, result-
ing in a lack of vigilance and degraded performance [12, 8,
6]. Communication or lack thereof between team members
can reveal the level of trust, which can greatly impact the
effectiveness of collaboration.
Figure 1: Shows a top-down view of the performance. The
trust level of the performer is represented as the size of the
orb located behind the performance area.
Similarly, in audience-interactive musical improvisation,
trust is built through communication between musicians
and audience members, as well as vulnerability and open-
ness to the creative process [5, 21]. However, establishing
trust can be a delicate balance, and unreliable (e.g. non-
interpretable rhythms) or overly reliable (e.g. static and
unchanging rhythms) musical performance can also nega-
tively impact the outcome of the musical collaboration [24,
25].
To explore trust and interactivity in a musical perfor-
mance setting, some musicians have included the audience
in interactive and improvised performance paradigms, where
the audience contributes music notation or can influence
the direction of the music with semiotic information [19,
18, 33, 34, 7, 20, 10, 31]. However, there is a trade-off be-
tween how much musical agency, defined as control over the
musical content, to give the audience vs. the performing
musician/s. Determining musical agency between audience
and performer significantly influences the outcome of the
musical performance in interactive and collaborative per-
formance paradigms [26, 3, 9].
Bio-sensing techniques have been used to enhance collab-
oration through transparency in both HAT studies and in-
teractive musical performance. For example, in HAT stud-
ies, bio-sensing has been used to display an autonomous
driving system’s level of certainty, which can inform the
driver to take over control before a collision event [17]. In
musical performance, bio-sensing has been used to approxi-
mate trust and inform performers of an AI-musician’s confi-
dence in their musical output [24]. Additionally, bio-sensing
techniques have been used to involve the audience in the
performance, creating a unique sense of intimacy and en-
gagement between the performer and the audience [7, 27,
5, 21].
One commonly used technique to analyze neural signals
and affective state information in performances is electroen-
cephalography (EEG) [7, 21, 27]. However, there is oppor-
tunity to contribute literature utilizing optical neuroimag-
ing techniques such as functional near-infrared spectroscopy
(fNIRS) in creative settings [35]. fNIRS is particularly well
suited for affective state feedback because it provides more
spatially resolute information about brain region activation
as opposed to EEG, serving to better represent affective
states such as trust.
In this paper, we present Stringesthesia, a collaborative
musical improvisation between audience and performer that
attempts to create an intimacy between the audience and
performer that is not as prevalent in traditional performance
paradigms. Stringesthesia measures the performer’s trust
level in real-time using fNIRS, which feeds back to affect
the aspects of the performance the audience can control.
This feedback is designed to dynamically tune the level of
agency the audience or performer have during the perfor-
mance, such that when trust is high audience members have
more agency, and when trust is low the performer has more
agency. The authors explain the development, implementa-
tion, design process, and performance of Stringesthesia, as
well as reflect on how to improve future performances (see
Figure 1).
2. RELATED WORKS
The following sections outline prior work that heavily influ-
enced the components of Stringesthesia.
2.1 Measuring Trust
Using fNIRS for the purpose of measuring trust was inspired
by numerous studies in HATs [6, 11] and came with several
limitations in the context of performance. In this section we
describe how fNIRS works, information present in the data
stream, and its uses for measuring trust in other fields. In
the limitations section we describe the implications of using
this technology during a live performance.
2.1.1 fNIRS Data
Functional Near-Infrared Spectroscopy (fNIRS) is a non-
invasive and lightweight neuroimaging modality that mea-
sures changes in hemoglobin concentration within the brain
using infrared light [1] (see Figure 2). It offers several ad-
vantages over similar modalities such as EEG and functional
magnetic resonance (fMRI). It is non-invasive which enables
examination in real-life, naturalistic settings. fNIRS is also
more safe and cost-effective compared to fMRI, and has
greater spatial resolution than EEG [11].
Following neuronal activation within a region of the brain,
oxygenated hemoglobin travels to that brain area. Infrared
light is used to detect changes in hemoglobin concentration
(oxygenated and deoxygenated), which thus can be used to
infer brain activity [22]. fNIRS has been shown to be suit-
able for measurement during complex cognitive and phys-
ical tasks such as yoga [4], dance [28], and classical mu-
sic performance [30], therefore we used fNIRS during the
Stringethesia performance for live neural recording.
Figure 2: fNIRS device being worn. https://NIRx.net
2.1.2 Brain Regions Associated with Trust
As fNIRS is a newer neuroimaging technique than compa-
rable technologies like fMRI and EEG, the literature using
fNIRS to measure trust and suspicion are limited [11, 6, 2].
However, because fNIRS and fMRI both measure the hemo-
dynamics of the brain, and have been proven to be corre-
lated [22], we can draw the wide swath of literature studying
trust using fMRI (as summarized by [11]). The main regions
of interest (ROIs) implicated in decision-making and trust
include the frontopolar area (FPA), medial prefrontal cortex
(MPFC), dorsolateral prefrontal cortex (DLPFC), and the
bilateral temporoparietal junction (TPJ) [11, 6] (see Figure
3). The regions have often been implicated in Theory of
Mind reasoning, a research paradigm evaluating how one
attributes thoughts, intentions and beliefs to others [29].
These areas are often activated during social interactions,
evaluation of self and others, decision-making and deter-
mining when to trust [29, 23, 6].
Trust was quantified by pilot data collected prior to the
performance in which the performer played both coopera-
tively and competitively with another musician. This data
was used to inform the weighting of the brain regions (TPJ,
FPA, MPFC, DLPFC) into their respective visual represen-
tations in our system architecture. TPJ was given a weight
of 2, meaning that these bilateral regions had greater weight
in our quantification of trust. FPA was given a weight of
0.5 because FPA is heavily associated with mental workload
and has been implicated in less transparent situations [6].
The other ROIs, DLPFC and MPFC, were given a weight
of 1 as we hypothesize they plan a similar role in trust.
Figure 3: Top-down view of the sensor and detector arrange-
ment for the measurement of trust (S = sensor, D = de-
tector). LPA shows left side of the head, RPA shows right
side of the head, Iz represents the back of the head, and Nz
represents the frong of the head. Colored lines show brain
regions associated with sensor-detector pairs.
2.2 Use of fNIRS in Music Technology
Instruments that adapt to the player based on affective state
have been exhibited by Yuksel, to inform music training sys-
tems and support creativity [36]. In this project, fNIRS de-
vices were used to measure affective state and adapt the
learning or creative environment accordingly. The team
proposes a training mode and a creativity mode, whereby
separate affective states and brain regions are recorded and
act as a proxy to affective state, in this case cognitive work-
load.
Using mental workload, which produces a reliable signal
in the frontopolar area of the brain, creates an easily de-
tectable signal captured by a single optical or electrical sen-
sor [36]. This makes for an easily deployable wearable device
for the purpose of detecting the amount of mental workload
present in the participant at any given point during the
activity.
2.3 Neural Signals in Interactive Musical
Performance
Biologically inspired performance paradigms include the use
of electroencephalogram (EEG), pulse oximetry measuring
heart rate, which were used to guide the tempo and bidirec-
tional communication between audience and performers [7].
The project Ringing Minds, is another example of an instal-
lation that explores concepts in multi-musician performance
in active and imaginative listening. This promotes the cre-
ation of a unique performance whereby collective brain re-
sponses of four audience members interact to inform a spon-
taneously generated musical piece [27].
Another example of musician-audience interaction using
EEG is the installation Spukhafte Fernwirkung (Spooky Ac-
tion at a Distance) [27]. This installation focuses on pro-
viding musical ideas for an improvising piano player by way
of EEG reading from an audience member. These examples
and more inform the way that players, audience members,
and installation participants can co-create music using the
information provided by the brain of either the performing
artist/s or audience members.
An example of a live collaborative improvisation using
neurofeedback is Eaton’s performance called,The Space Be-
tween Us [5]. In this performance EEG signals are read from
an audience member and a performer. Emotional proxies in
the EEG signal are converted into musical phrases played
by a computer and a musical score that is presented to a vo-
calist and piano performer. During the performance both
the performing vocalist and the audience member’s EEG
signals inform the selection of musical passages and seek to
illuminate the similarities and differences in their respective
emotional states during a live performance.
Stringesthesia draws from these paradigms by using the
fNIRS neuroimaging technique to further connect the au-
dience and performer. fNIRS is well suited to accurately
measure affective states such as trust, but to the best of
our knowledge there are no examples of fNIRS used for mu-
sical performance. Our use of fNIRS extends this concept by
conveying the internal state of performers’ trust to the au-
dience and tuning the musical agency given to the audience
based on the performers sense of trust–further connecting
the actions of the audience to the outcome of the perfor-
mance. Because the performer has no rehearsed material
the sense of trust between the performer and audience is
essential for a positive musical outcome.
3. SYSTEM ARCHITECTURE
In this section we describe the various components of the
system and their use in the performance. We then describe
how the system architecture supports real-time communi-
cation between the audience and performer. We begin the
section by describing a high-level connection diagram, then
describe the major components including: the performer
and trust levels, audience chord selection mechanism, and
the reactive visualization system.
Figure 4: System connectivity diagram demonstrating inputs
and outputs
3.1 Live-Stream Pipeline
The components in the Stringesthesia pipeline are config-
ured to live-stream data between the musician, audience
members, and a central processor that visualizes the infor-
mation using projection mapping (see Figure 4). All com-
ponents are connected via WiFi to a local network trans-
mitting messages using the Universal Datagram Protocol
(UDP).
3.2 Measuring and Reporting Trust Levels
Blood-oxygen levels measured by the fNIRS device (NIR-
Sport 2) were live-streamed via Lab Streaming Layer (LSL)
to MATLAB. The device had 50 channels spanning across
the four aforementioned ROIs (TPJ, FPA, DLPFC, and
MPFC). LSL parsed the data stream and calculated the
relative oxygenated blood density using the modified Beer-
Lambert law equation [16]. It was then sent to MATLAB
on a local device which ran further processing on the in-
coming data stream. For the performance we chose to em-
ploy a sliding window average with a window size of 200
data points and subtracted this figure from a baseline cal-
culated at the beginning of each jam session. The average
oxygenated blood density levels were produced at a rate of
about 0.5-2Hz. Jam sessions were variable in length (2-9
minutes) with a 1-3 minute resting period in between to
allow the signal to return to baseline.
3.3 Color/Chord Selection Controllers
Chord selection controllers were provided to the audience to
give the audience more agency over the performance. When
trust levels were high, more controllers to select chords that
the performer had to use in the jam became active for the
audience. The color selection controller was made with an
ESP8266 microcontroller. It sent a signal (color and device
ID) to the audio-visual system using a local network via a
User Datagram Protocol (UDP).
In order to bridge the gap in musical knowledge between
the audience and the performer, color was used to commu-
nicate musical information. We placed 6 controllers in the
performance space. Each had six different colors that the
audience members could select (see Figure 5, 1, and 8). The
performance consisted of 14 different “sets” ranging from 2-
9 minutes and had 1-3 minutes in between. Before each
set the audience was invited to select a color. Audience
members were free to select any color they wished. For the
audience member, this may have been an aesthetic choice,
an exploration of how the color affects the music, or any
other reason the audience member may have had. Selecting
a color caused the controller to change its LEDs to match
the selection, and sent the selected color to the visualization
system which reflected their choice on-screen.
Figure 5: Color/ Chord selection controller (glowing red in
image). Figure demonstrates a controller that already has a
chosen color. The six buttons show 6 different colors when
the controllers are ready to have a color selected. Rhyth-
mic instruments of various kinds are located near the con-
troller stations. Controllers are shown resting on top of mu-
sic stands.
Six colors were interpreted as chords by the performer
who was trained to establish a connection between the col-
ors and chords present in a major key. The colors were
presented in a spectral order (red, orange, yellow, green,
blue, purple) and represented major scale chords (I, ii, iii,
IV, V, vi) respectively to inform the improvisation. Color
choices were inspired by common mappings used in color
notations between colors and notes in a major scale [15, 14,
13].
3.4 Unity Engine-Controlled Projection
We used five projectors to create the visualizations during
the performance. To connect the projectors, we used the
hardware video wall controller DataPath (Fx4). After inte-
gration of the visual signals into the computer, MadMapper
software (v5.1.5) was used to control the precise location of
the projection.
The real-time audio-visual system was made using Unity
(2021.1.7f1). The visual was made with the unity-driven
plugin Visual Effect Graph (VFX Graph). The resulting
visuals reacted to the fNIRS and controller signals; both
sent via UDP.
3.4.1 Visualization of Trust and Musical Action
The level of trust corresponded to the diameter of the orb
behind the performer (when the orb was small trust was
low, and when the orb was large trust was high) (see Fig-
ure 6). Particles appeared to be flowing to and from the
audience members via a stream of diffuse particles and col-
ored lines, connecting the performer to the audience mem-
ber. The diffuse stream of colored particles converged on
the orb creating a multicolored representation of trust. We
chose an animation for the orb of particles that appeared
to be a complex motion resembling a hive or flocking dy-
namics. This choice was meant to represent the connection
between the audience and the performer as having a life of
its own–further drawing the audience toward the shared ex-
perience of the collaborative and improvisational nature of
the performance.
Figure 6: Demonstrates the orb located behind the per-
former. The larger the orb diameter the higher the trust
state
4. PERFORMANCE APPLICATION
The performance took place on the campus of the University
of Colorado, Boulder’s black box theater. Equipped with six
projectors, stage lights, scrum curtains, and a large rectan-
gular floor space (approx. 2700 square feet). The concert
lasted for 2 hours and 15 minutes divided by a 15 minute
intermission with a total of 115 audience members in atten-
dance.
The timeline of events was constructed so that audience
members had a chance to switch instruments, choose colors,
and interact in different ways throughout the performance.
Limitations of the fNIRS readings required us to divide the
performance into sets, allowing the oxygenated hemoglobin
signal from the fNIRS device to return to baseline and en-
abling a more effective reading of trust (see Figure 7).
Figure 7: Demonstrates the sequence of events during per-
formance. Cycle is repeated until performance is over. As
jam sessions become longer, blood-oxygen levels can satu-
rate. Thus, steps 7 and 8 must account for blood-oxygen
levels to normalize (1-3 minutes).
5. LIMITATIONS AND FUTURE WORK
When interpreting fNIRS data it is important to note that
hemodynamic signal lags behind neuronal activation. The
lag (approx. 3-5 sec [32]) imparted by the technology in-
fluenced the performance structure. Both real-time mea-
surements of trust (using a sliding window average) and an
overall average of trust at the end of every set were assessed
to account for both hemodynamic signal saturation and the
signal delay imparted by the technology. EEG has better
temporal resolution in this sense, but less spatial specificity
offered by fNIRS. A future study could take advantage of the
complimentary features of these modalities and use concur-
rent fNIRS-EEG recording during a performance to measure
performer-audience trust.
An additional limitation was imposed by only 6 colors to
represent chords in a Western music derived major key. In
future performances this can be extended to use a larger
range of chords, chord qualities, musical style, as well as
color palettes. Additional investigations as to how much
influence the audience has over the performance is also of
interest to us in the future.
6. CONCLUSION
In this paper we presented Stringesthesia, a performance
and underlying pipeline that presents a paradigm for inter-
action between the audience and a solo musical performer.
Figure 8: Shows a top-down view of the performance facing
the crowd.
Inspired by research using trust in human-agent teaming
studies, we presented the use of trust in an interactive musi-
cal performance using neuroimaging technique, fNIRS. Trust
was used to tune the amount of agency either the audience
or performer could exert during the performance. During
the performance audience members were given a rhythmic
instrument and were encouraged to collaborate improvisa-
tionally with the performer. When the audience and per-
former were engaged in cooperative exchange of musical
ideas, the visualization of trust visibly changed. When trust
levels were high more audience members were able to choose
chords that the performer must incorporate into their im-
provisation and more rhythmic instruments became avail-
able for audience members.
This feedback and promotion of trust in both the au-
dience and performer had a very positive impact on the
performance based on our observations. We found the vi-
sualization of trust using neuroimaging confusing for some,
therefore we plan to make the connection between trust and
the visualization more clear in future performances. Over-
all, the performance afforded a unique, engaging, and in-
timate experience by which both audience and performer
collaborated extemporaneously.
7. ACKNOWLEDGMENTS
The team would like to acknowledge the B2 at the Univer-
sity of Colorado, Boulder’s facility in the ATLAS Institute
for facilitating the performance and assisting in the produc-
tion of the show. We would also like to thank Sean Winters
for support with audio and Rishi Vanukuru for aiding with
the performance.
8. ETHICAL STANDARDS
The authors declare that the work presented was conducted
in the absence of any conflict of interest (related to either
commercial, financial or personal relationships) and in line
with the NIME Principles Code of Practice on Ethical Re-
search. An ethic certificate from our respective institutions
was not mandatory to conduct our work due to the artistic
nature of the project and the absence of scientific proce-
dures involving participants. The first and third author
collected data in the form of anonymous, voluntary, written
statements.
9. ADDITIONAL AUTHORS
Leanne Hirshfield
email: Leanne.Hirshfield@colorado.edu
Ellen Yi-Luen Do
email: Ellen.Do@colorado.edu.
10. REFERENCES
[1] H. Ayaz, W. B. Baker, G. Blaney, D. A. Boas,
H. Bortfeld, K. Brady, J. Brake, S. Brigadoi, E. M.
Buckley, S. A. Carp, et al. Optical imaging and
spectroscopy for the study of the human brain: status
report. Neurophotonics, 9(S2):S24001, 2022.
[2] P. Bobko, L. Hirshfield, L. Eloy, C. Spencer,
E. Doherty, J. Driscoll, and H. Obolsky. Human-agent
teaming and trust calibration: a theoretical
framework, configurable testbed, empirical
illustration, and implications for the development of
adaptive systems. Theoretical Issues in Ergonomics
Science, pages 1–25, 2022.
[3] C. Brown and G. Paine. A case study in collaborative
learning via participatory music interactive systems:
Interactive tango milonga. New Directions in Music
and Human-Computer Interaction, pages 285–306,
2019.
[4] H. Dybvik and M. Steinert. Real-world fnirs brain
activity measurements during ashtanga vinyasa yoga.
Brain Sciences, 11(6):742, 2021.
[5] J. Eaton, W. Jin, and E. Miranda. The space between
us. a live performance with musical score generated
via emotional levels measured in EEG of one
performer and an audience member. In Proceedings of
the International Conference on New Interfaces for
Musical Expression, pages 593–596, London, United
Kingdom, June 2014. Goldsmiths, University of
London.
[6] L. Eloy, E. J. Doherty, C. A. Spencer, P. Bobko, and
L. Hirshfield. Using fnirs to identify transparency-and
reliability-sensitive markers of trust across multiple
timescales in collaborative human-human-agent
triads. Frontiers in Neuroergonomics, page 10, 2022.
[7] Y.-Y. Fan and M. Sciotto. Biosync: An informed
participatory interface for audience dynamics and
audiovisual content co-creation using mobile ppg and
EEG. In Proceedings of the International Conference
on New Interfaces for Musical Expression , pages
248–251, Daejeon, Republic of Korea, May 2013.
Graduate School of Culture Technology, KAIST.
[8] E. Glikson and A. W. Woolley. Human trust in
artificial intelligence: Review of empirical research.
Academy of Management Annals , 14(2):627–660,
2020.
[9] E. Hein and S. Srinivasan. The groove pizza. New
directions in music and human-computer interaction ,
pages 71–94, 2019.
[10] A. Hindle. SWarmed: Captive portals, mobile devices,
and audience participation in multi-user music
performance. In Proceedings of the International
Conference on New Interfaces for Musical Expression ,
pages 174–179, Daejeon, Republic of Korea, May
2013. Graduate School of Culture Technology, KAIST.
[11] L. Hirshfield, P. Bobko, A. Barelka, N. Sommer, and
S. Velipasalar. Toward interfaces that help users
identify misinformation online: using fnirs to measure
suspicion. Augmented Human Research, 4(1):1–13,
2019.
[12] K. A. Hoff and M. Bashir. Trust in automation:
Integrating empirical evidence on factors that
influence trust. Human factors, 57(3):407–434, 2015.
[13] HookTheory. Create amazing music with hooktheory.
2022.
[14] T. Hopkins and E. Do. Jam tabs: A color based
notation system for novice improvisation. In
R. Gottfried, G. Hajdu, J. Sello, A. Anatrini, and
J. MacCallum, editors, Proceedings of the
International Conference on Technologies for Music
Notation and Representation – TENOR’20/21 , pages
56–62, Hamburg, Germany, 2020. Hamburg University
for Music and Theater.
[15] T. Hopkins, P. Pascente, W. Seltzer, K. Masterson,
and E. Y.-L. Do. The jam station: Gamifying
collaborative musical experiences through algorithmic
assessment. In Proceedings of the Fifteenth
International Conference on Tangible, Embedded, and
Embodied Interaction, TEI ’21, New York, NY, USA,
2021. Association for Computing Machinery.
[16] S. L. Jacques. Optical properties of biological tissues:
a review. Physics in Medicine & Biology , 58(11):R37,
2013.
[17] A. Kunze, S. J. Summerskill, R. Marshall, and A. J.
Filtness. Automation transparency: implications of
uncertainty communication for human-automation
interaction and interfaces. Ergonomics, 62(3):345–360,
2019.
[18] S. W. Lee, G. Essl, and Z. M. Mao. Distributing
mobile music applications for audience participation
using mobile ad-hoc network (MANET). In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 533–536,
London, United Kingdom, June 2014. Goldsmiths,
University of London.
[19] S. W. Lee and J. Freeman. echobo : Audience
participation using the mobile music instrument. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 450–455,
Daejeon, Republic of Korea, May 2013. Graduate
School of Culture Technology, KAIST.
[20] S. W. Lee and J. Freeman. echobo : Audience
participation using the mobile music instrument. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, pages 450–455,
Daejeon, Republic of Korea, May 2013. Graduate
School of Culture Technology, KAIST.
[21] G. Leslie, A. Ojeda, and S. Makeig. Measuring
musical engagement using expressive movement and
eeg brain dynamics. Psychomusicology: Music, Mind,
and Brain, 24(1):75, 2014.
[22] N. Liu, X. Cui, D. M. Bryant, G. H. Glover, and
A. L. Reiss. Inferring deep-brain activity from cortical
activity using functional near-infrared spectroscopy.
Biomedical optics express, 6(3):1074–1089, 2015.
[23] C. E. Mahy, L. J. Moses, and J. H. Pfeifer. How and
where: Theory-of-mind in the brain. Developmental
cognitive neuroscience, 9:68–81, 2014.
[24] J. McCormack, T. Gifford, P. Hutchings, M. T.
Llano Rodriguez, M. Yee-King, and M. d’Inverno. In
a silent way: Communication between ai and
improvising musicians beyond sound. In Proceedings
of the 2019 chi conference on human factors in
computing systems, pages 1–11, 2019.
[25] J. McCormack, P. Hutchings, T. Gifford,
M. Yee-King, M. T. Llano, and M. D’inverno. Design
considerations for real-time collaboration with
creative artificial intelligence. Organised Sound,
25(1):41–52, 2020.
[26] T. Mudd. Material-oriented musical interactions. New
Directions in Music and Human-Computer
Interaction, pages 123–133, 2019.
[27] T. Mullen, A. Khalil, T. Ward, J. Iversen, G. Leslie,
R. Warp, M. Whitman, V. Minces, A. McCoy,
A. Ojeda, et al. Mindmusic: playful and social
installations at the interface between music and the
brain. More Playful User Interfaces: Interfaces that
Invite Social and Physical Interaction , pages 197–229,
2015.
[28] J. A. Noah, Y. Ono, Y. Nomoto, S. Shimada,
A. Tachibana, X. Zhang, S. Bronner, and J. Hirsch.
fmri validation of fnirs measurements during a
naturalistic task. JoVE (Journal of Visualized
Experiments), (100):e52116, 2015.
[29] C. L. Sebastian, N. M. Fontaine, G. Bird, S.-J.
Blakemore, S. A. De Brito, E. J. McCrory, and
E. Viding. Neural processing associated with cognitive
and affective theory of mind in adolescents and
adults. Social cognitive and affective neuroscience,
7(1):53–63, 2012.
[30] P. Vanzella, J. B. Balardin, R. A. Furucho, G. A.
Zimeo Morais, T. Braun Janzen, D. Sammler, and
J. R. Sato. fnirs responses in professional violinists
while playing duets: evidence for distinct leader and
follower roles at the brain level. Frontiers in
psychology, 10:164, 2019.
[31] N. Weitzner, J. Freeman, S. Garrett, and Y.-L. Chen.
massmobile -an audience participation framework. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, Ann Arbor,
Michigan, 2012. University of Michigan.
[32] K. L. West, M. D. Zuppichini, M. P. Turner, D. K.
Sivakolundu, Y. Zhao, D. Abdelkarim, J. S. Spence,
and B. Rypma. Bold hemodynamic response function
changes significantly with healthy aging. NeuroImage,
188:198–207, 2019.
[33] A. Xamb´ o and V. Goudarzi. The mobile audience as a
digital musical persona in telematic performance. In
Proceedings of the International Conference on New
Interfaces for Musical Expression, The University of
Auckland, New Zealand, June 2022.
[34] A. Xamb´ o and G. Roma. Performing audiences:
Composition strategies for network music using
mobile phones. In R. Michon and F. Schroeder,
editors, Proceedings of the International Conference
on New Interfaces for Musical Expression , pages
55–60, Birmingham, UK, July 2020. Birmingham City
University.
[35] B. F. Yuksel, K. B. Oleson, R. Chang, and R. J. K.
Jacob. Detecting and Adapting to Users’ Cognitive
and Affective State to Develop Intelligent Musical
Interfaces, pages 163–177. Springer International
Publishing, Cham, 2019.
[36] B. F. Yuksel, K. B. Oleson, L. Harrison, E. M. Peck,
D. Afergan, R. Chang, and R. J. Jacob. Learn piano
with bach: An adaptive learning interface that
adjusts task difficulty based on brain state. In
Proceedings of the 2016 CHI conference on human
factors in computing systems , pages 5372–5384, 2016.