Sound-Based Sensors for NIMEs 
 
 
Sasha Leitman 
School of Engineering  
and Computer Science  
Victoria University of Wellington  
Wellington, New Zealand 
sleitman@gmail.com 
 
Dale A. Carnegie 
School of Engineering  
and Computer Science  
Victoria University of Wellington  
Wellington, New Zealand 
dale.carnegie@ vuw.ac.nz 
 
Jim Murphy 
New Zealand School of Music  
Victoria University of Wellington 
Wellington, New Zealand  
jim.murphy@vuw.ac.nz
ABSTRACT 
This paper examines the use of Sound Sensors and audio as input 
material for New Interfaces for Musical Expression (NIMEs),  
exploring the unique affordances and character of the interactions and 
instruments that leverage it. This paper first examines ten cases in 
which audio sensors, either microphone capsules or piezoelectric 
contact microphones, are used as a means of translating gesture into 
sound.  We present the results of a user study comparing sound-based 
sensors to other sensing modalities within the context of controlling 
parameters.  The study suggests that the use of Sound Sensors, and 
Dynamic Sensing Systems in general, can enhance gestural flexibility 
and nuance but that they also present challenges in accuracy and 
repeatability.   
 
Author Keywords 
User Study, Sensor, Contact Microphone, Audio Sensor 
 
CCS Concepts 
• Applied computing → Sound and music computing;  
• Hardware → Sensor applications and deployments;  
 
1. INTRODUCTION 
Designers of NIME related work and DMIs are almost universally 
aware of how microphones and transducers work.  They frequently 
have experience using contact microphones or condenser 
microphones as input into microcontrollers.  Members of this 
community are usually equally aware of the use of audio modulation 
techniques such as convolution and peak detection as techniques for 
creating musical interactions.  Audio is at the core of what we do in 
this field, so it might seem strange to write an academic paper 
examining the use of audio as a sensing technique.  And yet, when the 
sensors in NIME related work are analyzed [13] [14] and when this 
material is taught [11] [5] [2], audio as sensed material is not treated as 
a special category among the range of approaches. 
 Medeiros’ 2014 survey of sensor instrumentation methods in DMIs 
divides sensing methodologies into the broad categories of analog 
sensors, digital sensors, motion capture systems and consumer 
electronics[14].  Medeiros compares the results from Marshall’s 2009 
[13] dissertation and shows that the while some trends had changed 
between 2009 and 2014, the most common sensing techniques 
remained accelerometers and force sensitive resistors (FSRs).   
 Pigrem’s 2018 study showed that people with a range of experience 
using DMIs have developed a literacy with sensors that enables them 
to predict the behavior and function of instruments without actually 
operating them or being given any instruction or background 
information [22].  This sophistication level suggests the ability to form 
a mental model of how various sensor types operate and the idiomatic 
ways in which they are typically mapped. Fels et al. [8] make a strong 
case that a clear interaction metaphor strengthens the interaction of 
musical instruments.  Magnusson articulates that our tools strongly 
influence and prescribe our musical choices [12].     
 Sensing audio to control NIMEs is one possible method of creating 
an innate metaphor and intuitive mental model as providing a 
flexibility and transparency that is unique to Sound Sensors.  In 
looking at previous work, the authors make the case that there are 
unique advantages to using audio sensors that can increase the nuance, 
materiality, technical transparency and playfulness of DMIs. 
 The authors then present a user study designed to investigate the 
ways that the use of Sound Sensors create interactions notably 
different from existing interface paradigms.  Results from this user 
study are in keeping with the analysis of the prior work and point 
towards some of the elements of sound-based sensors that contribute 
to that uniqueness.  
 
2. AUDIO AS SENSOR DATA 
2.1 Materiality 
Armitage and McPherson [1] in Crafting digital musical 
instruments: an exploratory workshop study , describe a 
workshop where the AirHarp by Chris Heinrichs  is 
deconstructed and reimagined by workshop participants.  The 
workshop organizers provided a wide variety of wooden shapes, 
metal hardware and acrylic support pieces and prompte d the 
participants to re -assemble their own functioning DMI.  The 
accelerometer sensor that was in the original DMI was replaced 
with eight low-cost microphone capsules.  The authors write, 
“This offered a high -bandwidth connection between physical 
behavior and sonic response, necessary for facilitating gestural 
interaction using a wide variety of materials.” 
 In this case, the modification from an accelerometer to 
microphones increased the range of gestures and mater ials that 
could be sensed.  Eight different DMIs were created, each quite 
different from the other.  Instead of sensing the AirHarp’s 
rotation in air, kalimba like tines were plucked, tube s were 
struck, metal mesh was scraped, and rubber bands were plucked.  
This represents a much wider pallet of gestures and materials.  It 
also suggests a stronger connection to the physical and acoustical 
properties that constitute each instrument. 
2.2 Playfulness and Technical Transparency  
Using audio as data can also add to both the playfulness and the 
technical transparency of DMI s.  The Ocarina iPhone app, 
designed by Ge Wang [2 7], is an excellent example of these 
qualities in a physical interaction.  While the touchscreen is used 
to select the notes of the Ocarina, the iPhone’s built in 
microphone drives the onset and articulation of each note. 
 In Ocarina: Designing the iPhone's Magic Flute, Wang writes, 
“The design aimed to use only the existing features without 
hardware add-ons—and to use these capabilities to their 
maximum potential.”  This simplicity creates a physical  
interaction that is intuitive for even those without prior 
experience using DMIs or other music technology.  The analogy 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
182
between the sensor choices and the user experience is so direct 
that in describing the design, Wang notes, “the statement was not 
‘this simulates an ocarina,’ but rather ‘this is an ocarina.’”. 
 In Acoustruments: Passive, Acoustically -Driven, Interactive 
Controls for Handheld Devices, Laput et al. [10] describe a more 
ornate method that uses the speaker and microphone of an Apple 
IPhone as a se nsing system. An ultrasonic signal is sent out of 
the speaker, sensed by the microphone and analyzed by software. 
The signal travels through different types of ducts, created from 
rubber tubing or 3D printed parts.  The design of these physical 
ducts are c reated to mimic the behavior of passive electrical 
components such as switches, valves, tilt sensors, potentiometers 
and sliders. In the case of a valve, there is a portion of the d uct 
that can be turned 90 degrees, thus blocking the sound 
transmission through the duct. The slider is created by a piece 
that can constrict the flow of sound through the duct.  
 This overt and observable system is not necessarily practical, 
but it playfully makes visible the types of structures that make 
up more conventionally constructed sensors.  In a world of 
increasing obfuscation and “black box” technology, that sort of 
transparency is endearing.  The authors describe a variety  of 
whimsical applications for this technology such as a n alarm 
clock, an interactive doll and a toy car.  With the onboard 
microphone and speaker system, a wide variety of inte ractions 
and sensor types can be emulated in a unique and lucid manner.  
2.3 Playfulness and Materiality 
 Mogees [18] and the work of Ono et  al.[19][20] present two 
examples of using audio signals along with pattern recognition as a 
sensor methodology.  In both cases, the result is work that engages 
materiality and playfulness. 
 Mogees is a commercial product consisting of a contact microphone 
and iOS apps.  The apps use deep learning to recognize different types 
of gestures such as hits, scratches, and taps on the object to which the 
microphone is connected.  Those different gestures are then mapped 
to synthesis parameters within the app.  The system is marketed as a 
playful tool for musicians and dancers to engage with the physical 
world.  The website features examples of the inventor playing a bowl 
of wooden fruit and mimicking the sounds of a guitar player, break 
dancers that trigger different notes based on where they strike a piece 
of plywood with their feet, and a percussionist playing a large metal 
sculpture.  These examples leverage artists’ desire to engage with the 
material world and they also strive towards a playful aesthetic. 
 Touch & Activate: Adding Interactivity to Existing Objects Using 
Active Acoustic Sensing by Ono, et al. [19] describes a system that uses 
a contact mic similar to the Mogees but adds a vibration transducer to 
the object being sensed.  This allows static hand gestures and holding 
positions to be detected.  A sweep signal is sent to the vibration 
transducer, the microphone signal is spectrally analyzed and sent to a 
Support Vector Machine (SVM) learning algorithm. The system is 
designed to measure specific hand positions. In the case of a plastic toy 
shaped like a cat, the system can identify if it is being touched on the 
left side, right side, both sides or the top. In the case of a cellphone, the 
system identifies if the phone is held with the left hand, right hand, 
either hand using a thumb on the screen, a hand position with two 
thumbs on the screen or the type of grip characteristic of taking a photo 
with the phone.  
 In Sensing touch force using active acoustic sensing, by Ono et 
al.[20] a similar methodology is employed but the pressure with which 
the object is held or touched is detected as a continuous input 
parameter.  In this case, the spectrum is analyzed and plotted not as 
discrete events but as a continuous parameter that changes according 
to hand pressure. 
2.4 Nuance 
Romain Michon’s BladeAxe [16][17] is a musical instrument 
that is designed to control waveguide physical models of string 
instruments. The plucking mechanism consists of two piezo film 
sensors attached to thin pieces of plastic that are indi vidually 
plucked. One plucking surface controls individual notes and the 
other controls strummed notes. The piezo sensors are treated as 
audio signals and routed through a stereo audio interface into the 
iPad. The iPad software functions as a fretboard for the 
musician’s left hand allowing them to select and bend notes 
while simultaneously using the audio signals from the piezos as 
impulses to the physical modeling synthesis algorithms.  
 There are a number of interesting elements to the BladeAxe but 
the most intriguing is the use of audio signals as the impulse to 
the waveguide synthesis method. Waveguide synthesis is a 
powerful synthesis technique that is capable of great sonic 
nuance. It can be a challenge, however, for a user to manage and 
control the number of parameters required to create that nuance. 
The impulse to the synthesis model has a great effect on the sonic 
result and focusing on it can result in a wide variety of sonic 
variation. In the case of the BladeAxe, the audio signal will be  
different for each pluck of the piezo film sensor. Factors such as 
where along the thin piece of plastic the pluck sensor is struck, 
how hard it is struck, the use of fingers or plectrum (pick) to 
pluck the sensor, and the angle of the pluck will all vary the sonic 
results of the synthesis. Changes in technique will c reate 
dramatic changes in the sound produced and this is captured by 
the high resolution of the audio signal. This creates a level of 
expressive nuance that is similar to a traditional acoustic 
instrument.  Other examples of using audio signals as an input to 
waveguide synthesis show corresponding levels of nuan ce and 
musicality [9][23][24]. 
 This opportunity for nuance extends past waveguide  synthesis 
and into other compositional and sound design endeavors. 
 Paisa’s work, “ Enhancing the Expressivity of the Sensel 
Morph via Audio-rate Sensing” [21] uses contact microphones 
attached to the Sensel Morph to add an additional layer of data 
to the sound design mapping.  The audio data from the sensor 
provides a sonically rich base that is then modified by the user’s 
interaction with the touch pad.  This extra layer of complexity 
expands the Sensel into something that was more broad and 
surprising than a simple x-y grid. 
 Tomás’ Tangible Scores  [26] use contact microphones 
attached to textured scores as a dual-purpose musical score and 
instrument.  The contact microphones and variety of physical 
materials and textures used in his scores form a creative 
ecosystem that is tightly coupled both functionally and 
conceptually.   
 In M errill’s [1 5] Sound of Touch , a scraper with a contact 
microphone is passed over various semi -flat materials and 
recorded.  When the scraper is passed over other materials or the 
same material again, the new signal becomes the  basis for 
convolution-based sound playback.  This allows the nuance of 
materials to fold back on themselves and create a uni que, 
material-based approach to sound design. 
 These diverse examples point to  unique advantages and 
affordances of Sound Sensors in NIMES however, each of these 
examples exists within a complex musical system .  Our user 
study sought to isolate the use of Sound Sensors and look at the 
specific manner in which they differed from other sen sing 
systems. 
 
3. USER STUDY  
Despite our desire to isolate the impact of Sound Sensors, any 
study that we designed required incorporating the sensi ng 
systems into larger digital music systems.  We sought to create a 
system that was as transparent and immediately understandable 
as possible for our users.  To that end, we used simple mapping 
paradigms, relatively uncomplicated musical examples and 
simple physical input designs. 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
183
3.1 Methodology  
Participants were asked to control a single parameter within nine 
different musical examples in an Ableton Live session.  The first three 
examples controlled the volume of a given track (Piano, Kick Drum 
or Synth), the second three examples controlled the resonant frequency 
of an audio filter (Low-pass filter on Bass and Kick/Snare and a High-
pass filter on Strings), the final three examples controlled the wet/dry 
mix of reverb (Kick Drum, Guitar, or in one example the entire mix) 
 
 
Figure 1: Wooden Box Construction 
 
For each of the three types of audio parameters (volume, filter, reverb), 
the participants used one of three sensor systems (Section 3.4): 
- Slider: a single slider on a Korg nanoKONTROL 2. 
- Touch Sensor: a Sensel Morph touchpad with no silicon 
overlay applied.  
- Sound Sensor: a wooden box with a piezoelectric contact-
microphone attached to one of the two thin-plywood faces.  
3.2 Musical Examples 
The focus of the user study was the physical interaction with the three 
sensing systems. Simple mappings and musical examples were used, 
in order to maintain that focus  and ensure that users who were 
unfamiliar with computer-based music creation, were not distracted by 
unnecessary complexity. One-to-one mappings of sensor values to 
individual parameter values allowed users to quickly understand the 
effect that their physical choices had on the musical outcomes.  
 Each of the nine musical examples was between 30 seconds and one 
minute long.  The examples were kept relatively simple, with no more 
than five tracks making up a single example.  A variety of musical 
styles was employed in an effort to appeal to the widest range of 
musical tastes.  The users were given an opportunity to listen to the 
example and experiment with controlling the given parameter for as 
long as they liked with the individual track and with the entire mix 
playing.   When they were familiar with the tracks and controls and we 
had made any adjustments necessary, we recorded the automation data 
for one full loop of the musical example. 
 Within each parameter type (volume, filter or reverb), the order 
of the three tracks and which controller was used to control that 
parameter were randomized.  This was done to avoid 
inadvertently biasing users in case one of the musical examples 
was especially appealing to users.   The order of the controllers 
used was also randomized to again avoid biasing the users. 
   
3.3 Participants 
Participants from a wide  range of musical backgrounds were 
recruited.  Of the twenty-four participants, five had no musical 
training or limited musical training as children and did not 
consider themselves musical.  Six participants were 
accomplished acoustic musicians, three were accomplished 
electronic musicians and ten participants had significant 
experience in both electronic and acoustic music.  
 Musical experience was determined by asking a series  of 
questions about the length of time participants  had played 
acoustic instruments and/or created music using electronics or 
computers.  They were then asked to summarize their musical 
experience in 1 to 4 sentences. 
 All participants were adults and were recruited by a mixture of 
online publicity and extensive paper flyers throughout academic 
institutions and music venues in Wellington, New Zealand.  
3.4 Sensor Systems 
3.4.1 Sound Sensor 
In preliminary discussions of study design, several pote ntial 
participants expressed anxiety about being asked to make sound.  The 
act of making sounds with their voice or an object that had too many 
similarities to a musical instrument made them feel vulnerable and 
open to judgement.  Sensing sound requires making sound which can 
be intimidating for many people, including some trained musicians 
who might not feel comfortable being put on the spot and “tested” and 
even more so for people who do not consider themselves musicians. 
We wanted the gestures being sensed to be unassociat ed with 
traditional music making because we wanted to avoid any association 
between the gestures being sensed and musical skill.  However, we 
still needed there to be an audible sound being made that could be 
detected by the participant in order for the user to have a clear 
understanding of what was being sensed, even if they did not 
understand the technology behind it.  We were hopeful that this would 
give them the tools to form a mental model [8][22] of the sensing 
system. 
 We decided to use a hollow wooden box that would allow users to 
generate sound via actions such as scratching, tapping, or stroking.  We 
built the box to be roughly the size of an Apple iPad because that is a 
physical dimension with which contemporary technology users are 
familiar.  While tapping movements could be rhythmic, and indeed 
that was a feature that some users enjoyed, the control was not 
dependent on any type of fundamentally musical gesture and stroking 
or scratching worked just as well as more rhythmic actions.  The 
loudness of the contact microphone signal was analyzed over a 
window of samples in Max/MSP and a corresponding MIDI CC 
message was sent to Ableton Live where it was mapped to the 
appropriate automation parameters. 
3.4.2 Slider Sensor 
In order to compare the participants’ use of the Sound Sensor to their 
use of a more traditional physical control for an automation parameter, 
we chose to use a commercial controller with a vertical slider.  We 
chose a Korg nanoKONTROL 2 for ease of portability because the 
study was conducted at a number of locations.  We hoped that the 
slider would function as a standard base against which to compare 
other sensing protocols.   
3.4.3 Touch Sensor and Dynamic Input 
In initial mockups of the study, it became apparent that in addition to 
the difference in sensor technology between the Slider and Sound 
Sensor, there was a crucial difference in the interaction paradigm that, 
as far as we know, has not been articulated.  The Sound Sensor 
measured a fundamentally temporal signal which required constant 
input to register any value. 
 For the purposes of this paper, we will call this a Dynamic Sensing 
system.  Oxford University Press defines Dynamic as: “(of a process 
or system) characterized by constant change, activity, or progress” 
[27].  The word dynamic has distinct connotations in both the musical 
and engineering realms so it is not an ideal choice, however it will have 
to suffice until we find a more appropriate term. Identifying sensing 
systems as dynamic is distinct from categorizing them as continuous 
or discrete in that what is being looked at is not the range of data being 
sensed but whether the sensor needs constant input in order to maintain 
a value.   Dynamic sensing is closer to the disfluent design strategy 
described by Bin et al.  [3] however the “instability over time” 
described in the disfluent system is an intrinsic part of the dynamic 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
184
system and not an element that is intentionally mapped into the design 
strategy.  
 The need for constant physical involvement when interacting with 
the Sound Sensor was so different from the use of the slider that we 
decided an intermediary sensing system was needed – one that 
required constant input to register a value but was not built on the 
sensing of an audio signal. 
 We decided to use Sensel Morph because participants could use 
moving gestures that were similar to those made on the wooden box 
of the Sound Sensor.  In Max/MSP, we analyzed the distance that the 
users’ finger or fingers travelled over time on the Morph.  We ignored 
the multi-touch and the pressure sensitive aspects of the device.  
3.5 User Customization 
For all three parameters, the range of control could be adjusted.  
In the case of the Slider, we could adjust the range of the 
parameters to which the slider was mapped.  In the case of the 
Sound Sensor, we could control the pre-amplifier sensitivity on 
the audio interface that connected the contact microphone to the 
computer.  For both the Sound Sensor and the Touch Sensor, we 
could alter the window of time over which the signal was 
measured and averaged.  Shorter windows led to faster attack and 
decay times but also a need for a more constant input. 
 At the beginning of each user study, we explained the system 
to the users, allowed them to experiment with a visual 
representation of the data that was being produced in Max/MSP, 
and then showed them how they or we could change the 
sensitivity to suit their preferences.  Throughout the study, we 
checked in with the user to make sure that they were happy with 
the settings for the various controllers. 
3.6 Data and Questionnaires 
Results for the study were recorded in three ways: 
 
1) We recorded the automation data for each of the musical 
examples and that was eventually exported as text files via 
Max/MSP.  
2) We filmed the users’ interactions with the controllers. 
3) After the nine examples were completed, users were asked 
to fill out an eleven -point questionnaire.  For each 
controller, they were asked what they most and least liked 
and if they were to re-design the controller, what would 
they do.  Then they were asked if the re were musical 
possibilities they would like to explore with the controllers. 
 
4. USER RESULTS 
4.1 Individualized Physical Approaches 
One of the most interesting outcomes of the user study was the wide 
variation of physical techniques that users developed when interacting 
with the Sound Sensor system.  Users frequently developed an 
individualized ergonomic approach that was unique to their own 
experience of the physical world.    
 Four distinct examples highlight this physical difference: 
- A former Feldenkrais movement instructor asked that the 
contact microphones preamplifier be turned up to 75% of 
its maximum amplification.  Her movements were never 
more than 5 cm in diameter.   
- A classical violinist asked that the preamplifier be turned up 
to almost 100% and her movements rarely exceeded 3 cm 
in diameter. 
- A former rock musician who now makes Noise Music 
using analog gear controlled by Ableton Live 
enthusiastically stated that he appreciated knowing that he 
could kick the sound controller.  He preferred to hit and tap 
the wooden box and quickly abandoned any type of 
stroking or scratching movement.  The possibility of 
translating intense physical energy into the system without 
the fear of breaking a delicate music controller excited him. 
- A former puppeteer and percussionist who had struggled 
with severe, career-altering tendonitis, placed the wooden 
box on its edge and experimented with using different sides 
of the box and angles of approach.  
 
 Not all users were as individualized in their interactions with the 
Sound Sensor – some were content with the settings that they were 
given.  Likewise, there was variation in the musical and physical 
gesture choices of the users when operating the Sensel touch sensor 
and the Korg nanoKONTROL slider.  Users brought their own 
musical styles, experience and taste to bear on their choices.  However, 
their physical movements were mostly similar.  No user asked to 
physically reposition or place the nanoKONTROL or Sensel 
controllers in a different orientation.  Each tool remained flat on the 
desk and the user conformed their movements to the prescribed 
technology.  At the beginning of the study, the users were told that they 
could change the sensitivity and range of all three sensing systems.  As 
the test progressed, the users were repeatedly questioned to make sure 
that they were happy with the settings.  No user asked for an extremely 
different level of sensitivity or range in these sensing systems.   
 The uniqueness of each user’s interaction with the Sound Sensing 
device suggests an individualized idiomatic quality to the interactions, 
a personal style that is intrinsic to each user. 
4.2 User Forms 
There were a number of clear trends in the qualitative user 
feedback forms . A summary of common descriptors can be 
found in Figure 2 . Unsurprisingly, t he slider on the Korg 
nanoKONTROL was praised for its predictability and ease of use 
but it was also frequently described as boring.  The Sound Sensor 
was described as fun and varied but the background n oise of 
touching the box and the effort required to keep a steady v alue 
was criticized.  
 
 
Figure 2: Common Descriptors, Extracted From Qu 
 
4.2.1 Surprising , Unpredictable, Fun 
Surprising and fun but unpredictable was a common summary of 
the users’ feedback about the Sound Sensor.  Users that had 
experience with electronic music expressed a desire to explore 
the system in greater depth and with more freedom than a 
restrictive user study could provide.  One user wrote, “ Yes. I 
want to jam with it at home . Attach it to a bunch of plugins, 
filters, whatever. It's tactile and fun to interact with.” 
 This was in contrast to the Slider, which often was described 
as more predictable but also less fun.  “It took the excitement and 
sense of play out of the task. ”  The control that the slider 
provided, however, allowed some users to more accurately reach 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
185
their musical goals: “ I like the ability to have fine control and 
find the sweet spot.” 
4.2.2 Embodied 
The physicality of the Sound Sensor  was one of the most 
remarked upon advantages:  “The embodied interaction. It was 
very open in interaction in that I had to find multiple ways of 
imparting energy into the system to control it. It was more like 
an instrument in interaction.”     
 One user wrote about the possibilities that this embo diment 
offered for people of different abilities: “A much more embodied 
to the control of midi data. Elements of randomness due to lack 
of ability to control specifici ty. Allows for multiple 
interpretations of how to use the device. Could be used in 
different ways by people with different ability requirements.” 
 The materiality of the Sound Sensor  provided some users 
welcome tactile feedback: “The wood texture made it easy to feel 
how hard I was inputting sound and texturally it helped my 
senses to know what I was doing.” 
4.2.3 Noisy 
A large number of users remarked on the noise of the  Sound 
Sensor itself (the touching and tapping the wooden box) and the 
distraction that this provided. In the case of the examples that 
were discussed in Section 2, most of the devices were  not 
producing as much acoustic noise and if they were, it was 
fundamentally more musical.  In an effort to make sure that users 
were quickly developing an accurate mental model, we created a 
noisy, resonant box.  This level of noise would not occur in most 
DMIs. 
4.3 Effort and Dynamic Sensing 
The Touch Sensor and the Sound Sensor were both criticized for 
requiring too much effort.  One user wrote about the two sensors 
that they,  “required some more physical effort, which I may not 
feel like doing, only sometimes.(sic)”  
 This brings up interesting questions of user engagement within 
digital music.  It is only in the electronic age that we have 
become accustomed to sonic system s that do not require 
continual physical energy being placed into the system.  Pianos, 
flutes, drums etc. all require repeated effort.    
 There was also a frequent cognitive conflict between a user’s 
perception of effort and the actual action being performed.  In 
the case of the Touch Sensor, some users felt like they were 
moving very fast when they quickly switched directions of their 
finger back and forth.  An engineer said, “I thought that you were 
measuring speed, but you are just measuring distance travelled 
over time.”   This dissonance was sometimes perceived as the 
device not working properly. 
4.4 Slider Gestures Influenced 
Initially, we hoped to use the s lider as a control subject, a 
baseline for common control techniques.  Instead, we were 
surprised to find that users were often influenced by the Dynamic 
sensors to use the Slider in a more active manner. 
 One participant wrote about the Slider, “I would normally have 
moved it less, but the nature of the test somehow made me move 
around more.”  While he was the only person to make a note of 
this in his evaluation, this change in behavior was apparent with 
a significant majority of the participants.  Participants rarely left 
the slider in a static position but instead moved it up and down 
and tried to sync the changes to the music even when doing so 
was a slightly odd choice musically – for example the rapid and 
extreme changes to the wet/dry mix of the reverb.  Additionally, 
the small size of the nanoKONTROL slider meant that there was 
a tendency to move the slider to the two extremes of its range.  
4.5 Touch Sensor– Divided Opinion 
The Sensel Touch Sensor elicited the most divided responses.  
For both the Slider and the Sound Sensor, most users articulated 
positive and negative attributes about the interactions.  In the 
case of the touch sensor, user response was largely one sided.  
Sixteen participants were overwhelmingly positive, five were 
overwhelmingly negative and only three provided a less decisive 
response.  In contrast, the feedback given to both the  Sound 
Sensor and the Slider were more nuanced with people providing 
a mixture of both positive and negative remarks with 3 and 2 
users respectively giving only positive remarks. 
 
 
 
Figure 3: User Opinions of Sensors 
 Additionally, the people who appreciated the touch se nsor 
interaction used words such as fluid, smooth, intuitive, and 
natural.  They said that the interaction felt more artistic and made 
them feel like musicians.  This is particularly interesting because, 
as seen in Figure 2, the touch sensor also had the highest number 
of people say that they found using it to be tiring and require a 
significant amount of physical exertion.  
 It is possible that some of the negative reactions could be from 
the conflict between how a touchpad is typically used and how 
we were asking the users to use the Sensel.  Users of 
contemporary technology are accustomed to dealing with a two-
dimensional touch surface analyzed in an X -Y pattern.  There 
was a cognitive dissonance between that expectation a nd the 
analysis of dynamic movement that was felt by users, even when 
they enjoyed the interaction.  One user who particularly enjoyed 
the touch sensor suggested that an improvement to the device 
would be adding grid markings.  It is possible that he meant 
adding a layer of analysis that looked at movement location, but 
he verbally described it as a way for  a user to go back to the 
same section of the control space that they had enjoyed 
previously.  It was as though he somehow over -laid his 
expectations of a position -based touch sensor onto his 
experience with this dynamic controller and wanted those two 
interaction models to exist simultaneously.  
4.6 Summary and Future Work 
These user test results largely mirror what we might  have 
expected from the use of Sound Sensors – they are more flexible 
and open-ended and simultaneously more difficult to control and 
replicate. The range of responses to dynami c sensing and user 
effort was intriguing and points to one of the many ways t hat 
digital music making has departed from acoustic music. 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
186
 Several of the electronic musicians who participated in the user 
study expressed a great interest in using the Sound Sensor system 
in the own work.  The authors hope to work with these musicians 
to fine tune and further customize the system towards their 
particular creative practice. Many of these musicians  had 
excellent suggestions, such as adding textures and zones within 
the resonant box to illicit different results.  We look forward to 
working with some of these musicians and developing 
collaborations that are more musically mature and focused.   
5. CONCLUSION 
While most readers of this paper are familiar with s ound-based 
sensors, this paper has examined their unique advantages and 
limitations.  We have analyzed examples of NIMEs created with 
sound-based sensors and presented a user study that highlights 
and con firms the unique qualities of sound-based sensing 
systems.  
 We have shown that Sound Sensors and Dynamic Sensing 
systems are compelling ways of adding nuance, playfulness and 
embodiment into new musical devices.  Not every NIME would 
benefit from the addition of these paradigms.  But there are a 
great number of interactions and gestures that can be sensed 
through audio.  Sound Sensor s are capable of considerable 
nuance and complexity and there is often something clarifying 
and intuitive about the use of Sound Sensor  data within the 
design of DMIs and other musical interactions. Sound Sensors 
present unique capabilities that set them apart from other sensing 
modalities. 
 
 
6. REFERENCES 
[1] J. Armitage and A. P. McPherson. Crafting digital musical 
instruments: an exploratory workshop study. In Proceedings of 
the International Conference on New Interfaces for Musical 
Expression, Blacksburg, Virginia, USA, 2018. 
[2] E. Berdahl and W. Ju. Satellite CCRMA: A Musical Interaction 
and Sound Synthesis Platform. In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression, Oslo, Norway, 2011.    
[3] S. M. A. Bin, N Bryan-Kinns, and A. P. McPherson. Risky 
business: Disfluency as a design strategy. In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression, Blacksburg, Virginia, USA, 2018. 
[4] O. Brandtsegg, T. Engum, and B. I. Wærstad. Working 
methods and instrument design for cross-adaptive sessions. In 
Proceedings of the International Conference on New Interfaces 
for Musical Expression,, Blacksburg, Virginia, USA, 2018.  
[5] G. D’Arcangelo. Creating a context for musical innovation: a 
nime curriculum,” In Proceedings of the International 
Conference on New Interfaces for Musical Expression, Dublin, 
Ireland, 2009. 
[6] G. Essle, S. O'modhrain.  Scrubber: an interface for friction-
induced sounds.  In Proceedings of the International 
Conference on New Interfaces for Musical Expression, 
Vancouver, Canada, 2005. 
[7] S. Fasciani. Physical Audio Digital Filters. In Proceedings of 
the International Conference on New Interfaces for Musical 
Expression, Copenhagen, Denmark, 2017.  
[8] S. Fels, A. Gadd, and A. Mulder. Mapping transparency 
through metaphor: towards more expressive musical 
instruments. Organised Sound, 7(2):109–126, 2002. 
[9] https://www.korg.com/nz/products/drums/wavedrum_global_e
dition/ 
[10] G. Laput, E. Brockmeyer, S.E. Hudson, C. Harrison.  
Acoustruments: Passive, acoustically-driven, interactive 
controls for handheld devices.  In Proceedings of the 33rd 
Annual ACM Conference on Human Factors in Computing 
Systems, 2015. 
[11] S. Leitman.  Current Iteration of a Course on Physical 
Interaction Design for Music.  In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression, Copenhagen, Denmark, 2017.  
[12] Magnusson, Thor. “Of Epistemic Tools: Musical 
Instruments as Cognitive Extensions.” Organised Sound, 
vol. 14, no. 2, 2009. 
[13] M.T. Marshall. Physical Interface Design for Digital Musical 
Instruments. Ph.D. Thesis, McGill University, Montréal, QC, 
Canada, 2009.  
[14] C. B. Medeiros and M. M. Wanderley. A comprehensive 
review of sensors and instrumentation methods in devices for 
musical expression. Sensors, 14(8):13556–13591, 2014. 
[15] D. Merrill, H. Raffle, and Roberto Aimi. 2008. The sound 
of touch: physical manipulation of digital sound. In 
Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems (CHI ’08). Association for 
Computing Machinery, New York, NY, USA, 2018. 
[16] R. Michon, J. O. Smith, M. Wright, C. Chafe.  Augmenting the 
iPad: the BladeAxe. In Proceedings of the International 
Conference on New Interfaces for Musical Expression, 
Brisbane, Australia, 2016.  
[17] R. Michon, J. O. Smith.  A hybrid guitar physical model 
controller: The BladeAxe. In Proceedings of the International 
Computer Music Conference, Athens, Greece. 2014. 
[18] https://www.mogees.co.uk/ 
[19] M. Ono, B. Shizuki, J. Tanaka. Touch & activate: adding 
interactivity to existing objects using active acoustic sensing. In 
Proceedings of the 26th annual ACM symposium on User 
interface software and technology. St Andrews, United 
Kingdom. 2013. 
[20] M. Ono, B. Shizuki, and J. Tanaka. Sensing touch force using 
active acoustic sensing. In Proceedings of the Ninth 
International Conference on Tangible, Embedded, and 
Embodied Interaction, TEI ’15, New York, NY, USA, 2015.  
[21] R. Paisa and D. Overholt. 2019. Enhancing the Expressivity of 
the Sensel Morph via Audio-rate Sensing. In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression,  UFRGS, 2019. 
[22] J. Pigrem, A. Mcpherson. Do We Speak Sensor? Cultural 
Constraints of Embodied Interaction. In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression, Blacksburg, Virginia, USA, 2018. 
[23] B. L. Robertson, L. Dahl.  Harmonic wand: an instrument for 
microtonal control and gestural excitation. In Proceedings of the 
International Conference on New Interfaces for Musical 
Expression, Blacksburg, Virginia, USA, 2018. 
[24] D. Schlessinger, J. O. Smith.  The Kalichord : A Physically 
Modeled Electro-Acoustic Plucked String Instrument. In 
Proceedings of the International Conference on New Interfaces 
for Musical Expression, Pittsburgh, Pennsylvania, USA, 2009. 
[25] K. Tahiroglu, M. Gurevich, and R. B. Knapp. 2018. 
Contextualising Idiomatic Gestures in Musical Interactions 
with NIMEs. In Proceedings of the International Conference 
on New Interfaces for Musical Expression, Blacksburg, 
Virginia, USA, 2018. 
[26] E. Tomás and M. Kaltenbrunner. 2014. Tangible Scores: 
Shaping the Inherent Instrument Score. In Proceedings of 
the International Conference on New Interfaces for Musical 
Expression, Goldsmiths, University of London, 2014. 
[27] G. Wang. Ocarina: Designing the iPhone's Magic Flute. 
Computer Music Journal, Volume 38, Issue 2. Summer 2014. 
p.8-21 
[28] https://www.oxfordlearnersdictionaries.com/definition/english/
dynamic_1?q=Dynamic 
Proceedings of the International Conference on New Interfaces for Musical Expression (NIME-20), Birmingham, 2020
187