OtoKin: Mapping for Sound Space Exploration through
Dance Improvisation
Palle Dahlstedt
Dept. of Computer Science and Engineering
Chalmers Univ. of Techn. / Univ. of Gothenburg
palle@chalmers.se
Ami Skånberg Dahlstedt
Academy of Music and Drama
University of Gothenburg
ami.skanberg.dahlstedt@gu.se
ABSTRACT
We present a work where a space of realtime synthesized
sounds is explored through ear (Oto) and movement
(Kinesis) by one or two dancers. Movement is tracked
and mapped through extensive pre-processing to a
high-dimensional acoustic space, using a many-to-many
mapping, so that every small body movement matters.
Designed for improvised exploration, it works as both
performance and installation. Through this re-translation
of bodily action, position, and posture into inﬁnite-
dimensional sound texture and timbre, the performers are
invited to re-think and re-learn position and posture as
sound, eﬀort as gesture, and timbre as a bodily construc-
tion. The sound space can be shared by two people, with
added modes of presence, proximity and interaction. The
aesthetic background and technical implementation of the
system are described, and the system is evaluated based
on a number of performances, workshops and installation
exhibits. Finally, the aesthetic and choreographic motiva-
tions behind the performance narrative are explained, and
discussed in the light of the design of the soniﬁcation.
Author Keywords
movement soniﬁcation, interactive dance, mapping
CCS Concepts
•Human-centered computing → Gestural input;
•Applied computing→ Performing arts; Sound and
music computing;
1. INTRODUCTION
Oto means sound in Japanese, and ear in Latin, while
Kin is short for kinesis, meaning motion in Greek. Mo-
tion through the ear, music and movement, or music and
dance are intimately connected as embodied human expres-
sions. The addition of technology makes the combination
of the two both easier and more diﬃcult. The path from
electronic music to dance is easy to see, while the other di-
rection is harder to tame. Electronic music is sometimes
far removed from embodied performance, maybe because
the path from the body to the actual sound sources is long
and winding, where the directness of traditional instruments
may be lost.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’19, June 3-6, 2019, Federal University of Rio Grande do Sul,
Porto Alegre, Brazil.
In this paper, we describe a project where an electronic
musician (PD) with a strong focus on embodied musician-
ship collaborates with a choreographer and dancer (ASD)
with a thorough background within both Western and
Japanese dance traditions. Together, we have created a
performance space extended by a layer of realtime synthe-
sis, where each subtle movement is soniﬁed, and where we
can perform as musician-dancers. In focus is a discussion
about the aesthetics behind the mapping from movement
to sound, and how the sound engines together form a rich
landscape for improvised exploration, including mediated
interaction between the performers. The work has been
performed several times by the authors, and also been
shown as participatory interactive installation/workshop
in several places (including NIME 2018 in Blacksburg, VA,
USA), with visitors moving and dancing in the space, and
very positive response.1
The paper also describes the chosen narrative framework
and performance constraints, where our shared background
in classical Japanese performing arts is combined with the
special constraints of moving in an experimental soniﬁed
space, together shaping a performance narrative.
2. AESTHETIC DEPARTURE POINTS
The departure point of OtoKin was a commission from the
art and science AHA Festival in Gothenburg, Sweden.
Our main idea is to augment a performance space with
sound, adding new dimensions and hidden layers, which
would enrich the performance from both the performers’
and the spectators’ perspective. The addition of a new layer
transforms the space into something else, and in an impro-
visatory context, it changes the way you move. Each move-
ment acquires sonic properties, and movement and musical
gestures inform each other – none can take place without
the other, and it will be impossible to tell if you make a
movement to cause a sound, or if you make a sound that re-
quires you to move. They become entangled, and the space
is experienced as a multi-modal space, not as sound in a
3-dimensional physical space.
Dahlstedt has previously developed mapping algorithms
based on spatial metaphors (e.g., [3, 6]), primarily based
on vectorization of the output from high-dimensional con-
trollers to navigate and explore similarly high-dimensional
synthesis parameter spaces. The idea to apply this to
precise tracking in a physical space has been a recurring
thought, and this is the ﬁrst step in that direction.
The idea is to perform in in a physical space of potential
sounds, with a strong correspondence between movement
and sound gestures. This requires that the mapping is suf-
ﬁciently good, retaining subtle dynamics and a rich ﬂow
1A video of the premiere: https://youtu.be/HB0AX4NSs38
Videos of workshops: https://youtu.be/HB0AX4NSs38,
https://youtu.be/cJ50sBA9NvY
156
of information from motion tracking to synthesis. From
a perspective of number of dimensions and amount of in-
formation, it is a good match. Sound synthesis usually
have a large number of parameters, but space is only three-
dimensional. However, a dancer’s body has many more de-
grees of freedom than its position in space. With existing,
aﬀordable and accessible technology, we can measure the co-
ordinates of all major points/joints of the body over time,
enabling the capturing of:
Position Location on the ﬂoor in absolute coordinate space
Posture The characteristic conﬁguration of body parts
Movement of the whole body, and of each joint as mea-
sured relative to some central point on the body
In this space, each movement causes a sound, and they to-
gether can be used as a medium for expression combining
choreography and musicianship. From a performer’s point
of view, the sonic dimensions should feel so rich that it
would almost be possible to dance blind-folded. This served
as a good design criterion during the implementation.
We were also interested in exploring how to allow for me-
diated interactions in this space, with performers and per-
haps agents. Mediated interaction between dancers did in-
deed become an integral part of the work, but the virtual
agents in the direct sense in not yet implemented. Still,
some of the sound engines have complex, unpredictable be-
havior, which can make it feel like you are trying to control
an unruly puppet.
The space is designed for improvisation, which for us
means that it is not made for one single piece. No ex-
act choreography should be needed, and there should be
no score for the music. One may create a predetermined
framework for the improvisation, but it must be possible to
improvise details, and most importantly, to respond to what
is happening in the moment, like on a musical instrument.
The space should not be based on any presets or stored
states, but the dancers should interact with a system that
is always on, and that makes them a part of the system.
If the humans do nothing, the machines do nothing. When
humans act, the machine response shapes further human be-
havior, etc.. This is a very important feedback loop, which
requires high resolution in time and space, and the trans-
lation between the two worlds is crucial. Transformations
must occur, but a signiﬁcant portion of the information is
lost, problems appear.
The space should be designed based on rich potentials to
be probed, explored and harnessed for artistic expression.
Navigational strategies for the exploration of such complex
(non-physical) sound spaces has previously been discussed
by Dahlstedt [4].
2.1 Interaction design, aesthetics of mapping
The interaction design of the project involves choices for
how the space is set up, what can be done in it, which
actions should be detected and have sonic consequences,
aesthetic ideas about movement repertoire, how computer-
mediated interactions between the dancers happen, the de-
sired qualities of the interaction, and maybe most impor-
tantly, how the mapping from movement to sound is to be
implemented.
Soniﬁcation of movements can be approached in diﬀer-
ent ways. We want to avoid triggering notes and events,
and instead prefer the ﬂuidity and continuity of body move-
ments to translate into the sounds. This can be done either
by mapping to parameters of continuously ongoing synthe-
sis, or by mapping to continuous parameters for generative
sound structures, which can be varied and gesturally con-
trolled based on realtime feedback [4]. Both approaches are
used in OtoKin, for diﬀerent sound engines.
2.2 Effort-based mappings
A design ideal of ours is that mappings for physical em-
bodied playing should be based on eﬀort. Moving in the
space should feel like playing an instrument – a rich space
of possibilities, with repeatability, learnability and intimate
control over the sound, to borrow a term from David Wessel
[15].The output sound energy should be in some way pro-
portional to the amount of movement, momentum, or accel-
eration. This also implies that the system should be silent
when you are still. This principle was applied to a large ex-
tent in OtoKin, by letting the amount of acceleration map
to volume and core ﬁlters in some of the sound engines.
However, an approach focusing only on motion misses the
idea of potential energy, which is also eﬀort. And it does
not allow for high-energy still states. For this reason, we
also included possibilities where either lifting a leg (a tem-
porary eﬀort that takes some energy and cannot be done for
ever) or raising your arm either is generates sound or has a
profound impact on the sound contents.
There are many reasons that we use physical metaphors
for the eﬀort-based mapping. We think this is extra impor-
tant when translating bodily movement to sound, because
the body lives in a Newtonian physics space, and our pat-
terns of movement are so much shaped, both evolutionarily
and on a more short term scale, by these constraints. For
example, a constant movement can be considered a state
because it does not involve a change of energy. Neither
does a free fall – it is in fact a very still state. The ac-
tive use of energy to change motion path is perceived by a
spectator as an event. A well-known example is conduct-
ing: it is the very bottom point of the beating hand pattern
that is perceived as the beat, exactly when the conductor’s
hand changes direction and starts upwards. This move-
ment is both prepared and have an extended decay, so that
its timing and impact can be predicted and experienced
by a spectator, as part of a partly known dynamic system.
For us, it was important to think in these terms also when
designing the mapping, to keep a correspondence between
movements gestures and sound gestures, and a correspon-
dence between the feel of both of them. This discussion on
empathetic experience of movement in electronic music is
developed further in [5].
2.3 Interactions
An overview of the interactions possible in OtoKin:
Musical alarm sounds are heard when you are too close
to the border of the bounding box. It fades in the last 0.3m
(adjustable), when any skeleton coordinate is too close.
This can be exploited musically by the dancers, but is also
there for pragmatic reasons.
The head (x,z) coordinates are used as the center point
position, and allow for choice of sound engine to play. There
are three main sound engines, and there are crossfades be-
tween them, controlled by the ﬂoor coordinates.
Height matters. The ﬂoor is silent, so when sitting or
lying down, you stop making sounds. There has to be a
way to rest. When the highest point goes above a certain
threshold, it fades into a special sound engine, which we call
The bells in the sky.
Coordinates of the joints of the upper body and arms
are used to control synthesis parameters, according to a
complex mapping engine. Primarily, the coordinates of the
shoulders, arms and hand joints are used, because they are
our most expressive of the larger limbs.
157
OtoKin is made for two dancers. Each one generates
music based on his/her position, posture and movement.
Here, it is perfectly possible to interact, in a similar man-
ner to two musicians who play similar instruments. But
there is a special mode entered when dancers touch each
other. Then they instead make a shared sound, completely
diﬀerent, stronger and shimmering. It is a separate sound
engine, and the sound is shaped by both dancers’ posture
and movement. The possibility of such mediated interac-
tions was an important part of the initial concept.
3. RELATED WORK
Music and dance have symbiotically coexisted throughout
history, and probably long before that. A philosophical per-
spective on their relationship is given by Hodgins [9], based
on the analysis of 20th century dance works. An early exam-
ple of technological mediation between movement and sound
is Leon Theremin’s invention of the Theremin instrument
around 1920, and its dance-based cousin, the Terpsitone
(1932). They allowed for pure gesture, with no physical con-
tact with the instrument, to produce sound. The same tech-
nology was used in John Cage’s and Merce Cunningham’s
collaboration Variation V (1965), where large theremin an-
tennae (built by Bob Moog) were placed on the dance ﬂoor,
sonifying the dancers’ movements.
From a NIME perspective, the sound-movement relation-
ship has been analyzed thoroughly. In a broad overview,
Winkler [16] provides an analysis of the physical conditions
of motion in diﬀerent contexts, including dance, and its im-
plications for the design of good mappings from motion
to sound. He emphasizes the importance of understand-
ing the physical parameters and constraints of movement
to see how they can be translated to sound. Other signif-
icant reviews have been given by Siegel and Jacobsen [13],
again stressing the importance of the physical constraints,
eﬀort, and resistance in musical instruments, which in dance
are primarily contributed by gravity and inertia. In a later
overview, Siegel [12] discusses the distinction between nat-
ural and unnatural mappings, where the former ”complies
with our common experience of the real world”. He also
mentions the lack of helpful audience expectations in the
ﬁeld of art and technology, since there are no general con-
ventions about how to do things. We agree that the audi-
ence do not need to understand how the soniﬁcation works,
but we think there needs to be cues that help them empa-
thetically perceive eﬀort (physical and motorical-cognitive),
direction and intention. This has been further discussed by
Dahlstedt in [5].
Both Winkler and Siegel emphasize Ryan’s [10] thoughts
on the importance of eﬀort for musical expression, and to
include this aspect when designing musical interfaces. Ryan
have also been a major inspiration for us, and has inﬂuenced
the design of our previous instruments, as well as OtoKin.
Mapping is a key problem in this ﬁeld, and rich map-
pings are hard to design. Skogstad et al [14] writes: ”We
soon discovered that it was challenging to design a single
instrument, or one synthesizer state, that would be interest-
ing enough to listen to and watch for a whole performance.
The performer needed to be able to change between diﬀer-
ent mappings.”
An broad discussion of the problem of mapping is pro-
vided by Schacher [11], even though he arrives at diﬀerent
conclusions than we do. He argues that there is an insur-
mountable diﬀerence between how we perceive musical ges-
tures and dance movements. He quotes interactive artist
Marc Coniglio, stating that in order to make music and for
the audience to understand that the dancers produce the
Figure 1: A map of the skeleton joints available
from the Kinect 2 sensor. Most of these were used
in OtoKin. Image from vvvv.org.
music, they need to move like musicians. This is very dif-
ferent from how dance movements are perceived. ”We really
see energy.” Schacher concludes that ”a straight parametric
linking between the phenomena of each domain seems im-
possible”, and that we may have to translate movement to
”meta-gesture”, perhaps by a detour to emotion and aﬀect,
”instead of focusing on physiology and physics.” This meta-
layer is then translated into music.
We do not agree. Rather, a thorough understanding and
acceptance of the physics and physiology behind music-
making (with traditional instruments, and electronic instru-
ments based on similar principles) allows for bridging this
gap. It is certainly possible to design mappings where the
movement are not in service of the sound, but still inte-
grated, stemming from the same underlying (mental, im-
material) gesture and intention. What we see when ob-
serving a musician is also to a large degree energy and ef-
fort, perceived empathetically through the mirror neuron
system. Could a solution to this divide be the understand-
ing that musical sound is also based on real or perceived
change of energy states? Perhaps a simpler kind of meta-
gestures is hidden in the transformations we introduce in
our mappings, which preserves the contour and shape but
maps the minute speciﬁcs of gestures in complex ways to
synthesis, in a many-to-many mapping inspired by acoustic
instruments. A detour via meta-layers of aﬀect and emotion
will inevitably lose the crucial synchrony between movement
and sound gestures, so fundamental for perceived agency
and for merging them into a combined expression.
Finally, an interesting perspective on mapping is pre-
sented by Bergsland and Wechsler [2]. They state that the
important thing is not if the mapping is simple, complex,
one-to-one or many-to-may, but that ”[t]he critical issue in
terms of user engagement is how the environment evolves
over time, i.e. how the user is guided back and forth between
’causal-ordered-predictable’ and ’intuitive-improvised’ pro-
cesses. This is to say that either, at exclusion of the other,
can quickly lose interest.”
4. DESIGN AND IMPLEMENTATION
OtoKin uses a 2nd-generation Microsoft Kinect sensor, pro-
duced 2013-2017. It has good precision, and tracks 25 skele-
ton joint coordinates each for several people (see Fig.1). The
frame rate is 30fps, and the sensing area is a 84 ° cone, with
a maximum depth of 4.5m.
The sensor is placed at stage front, ca 80cm above the
ﬂoor, turned inwards towards the dancers. A laptop screen
is discretely placed under it to provide visual feedback for
the dancers. The system can be calibrated for diﬀerent
sensor height, dancer body height and active sensing area.
158
Figure 2: Map showing the zones for the sound en-
gines, viewed from above and from the side. The
audience is on the right side.
Tracking and mapping preprocessing implemented in
vvvv2, a data-ﬂow programming environment specialized in
interactive realtime graphics. The patch creates a simple
visualization for debugging and stage monitoring, and per-
forms extensive preprocessing as part of the mapping, such
as coordinate scaling, coordinate comparisons, proximity
calculations, smoothing, and sends the translated data oﬀ
to two hardware Nord Modular G2 DSP farms for ﬁnal
mapping and synthesis, via MIDI.
4.1 Mapping
The mapping in OtoKin consists of many diﬀerent parts
and mechanisms. The main idea, as already stated, is to
do an eﬀort-based translation from movement gestures to
sound gestures, free from triggering, focusing on continuous
modulation of ongoing synthesis systems.
The stage is divided into diﬀerent sub-spaces in all dimen-
sions, which correspond to diﬀerent sound engines, crossfad-
ing into each other. Each has a somewhat diﬀerent map-
ping mechanism. For example, only two of them use the
concept of chromatic pitch, while others are based on non-
linear synthesis techniques where pitch is not a parameter,
but an emergent feature. The main implementation is cen-
tered around the three core properties of the dancer:
Position – of the dancer, in three dimensions, determine
the crossfading between sound engines. Extreme positions
of any joint near the edges of the bounding box, in all di-
rections, cause special sonic responses. The ”walls” have a
bell-like warning sound. There is a silent zone near the ﬂoor
to provide for rest, and the ”ceiling” features a special sound
engine, reached by raising any joint (usually a hand) very
high. Position is also mapped to sound panning, so that the
sound is co-located with the dancer.
Posture – the conﬁguration of the limbs, primarily the
arms, determine synthesis parameters, through a many-to-
many translation engine based on linear transformations.
Arm coordinates are measured relative to the shoulder. Lift-
2https://vvvv.org/
ing one leg is a special posture – this represent an eﬀort, and
is mapped to volume/intensity, just like movement.
Movement– the diﬀerence of a dancer’s posture between
two measured frames (1/30s) is used to calculate a rough
measure of the total magnitude of movement and acceler-
ation. This is in turn used to control volume and overall
contour of the sonic gesture, when applicable. (A couple
of sound engines have other dynamic control methods, to
provide for variation in movement and sonic expression.)
This is not unrelated to Schacher’s [11] approach of
”multi-tiered” sensing, where camera-based global, ”to-
pographic” sensing gives information about position, and
sensors on the dancers’ bodies give data with ”limited scope
but expressive scale”. However, with today’s technology,
we are able to do both with the same sensor.
In addition, the mapping looks at the proximity between
the dancers, measured as the minimum of all possible pair-
wise distances of skeleton points. This means that we with
surprisingly high reliability and precision can tell when they
touch each other, with any body part against any other,
which enables the Together sound engine.
4.1.1 Pitch
Pitch is a parameter diﬀerent from all others. If mapped
continuously, we end up in an undesirable ”glissando hell”.
For the pitch-based engines, we had to design a mechanism
that derives discrete pitch values. Simple quantization does
not make sense. Instead, we perform six coordinate com-
parisons of arm joint positions, such as ”is the x coordinate
of the hand larger than x of the shoulder?” If true, this en-
ables a speciﬁc musical interval, and all enabled intervals
are added. This sum gives an oﬀset from a ﬁxed fundamen-
tal pitch, for that speciﬁc arm. The intervals are chosen
to create a ﬂexible yet characteristic melodic contour. In
this way, the arm posture at any moment corresponds to a
speciﬁc pitch. The two pitch values for the two arms are
continuously sent to the sound engines. The actual pitch
is only updated when the acceleration goes above a certain
threshold, e.g., at the start of an arm movement or a change
of direction. This additive pitch approach was inspired by
earlier mechanisms Dahlstedt has designed for various mu-
sical interfaces with similar complications and no obvious
pitch control [7, 6]. See the example video at time 4’30”-
5’10” for an example of this mechanism in action.
4.1.2 Timbre
The word timbre is here used in the sense of all other syn-
thesis parameters, and is controlled through a translation
mechanism from essential arm coordinates, relative to the
shoulder. The ( x, y, z) coordinates of the hand and the
elbow of both arms, relative to the shoulder, are sent to the
sound engines, where they are subjected to a matrix-based
linear transformation. In essence, each synthesis param-
eter is modulated by a weighted sum of all input values.
The matrix coeﬃcients were randomly generated at design
time, within a range of (-1.0,1.0). This many-to-all map-
ping approach keeps the gestural contour and magnitude,
but modulates all parameters in a coupled way – all pa-
rameters are aﬀected by every movement. It is in no way
random, but repeatable, and interesting directions of move-
ment or postures can be further explored as they are found.
Further discussions on playing through this kind of mapping
transformations can be found in [3].
4.2 Sound engines
OtoKin contains a number of diﬀerent sound engines, all
running simultaneously. They are all implemented in the
Nord Modular G2 signal processing development environ-
159
ment3, based around a number of hardware DSP processors
using sample-by-sample processing, which means that there
is minimal latency, and feedback can be used as a core sound
design element, which is more diﬃcult in buﬀer-based envi-
ronments. Two of the OtoKin sound engines are based on
feedback of diﬀerent kinds, so this is crucial.
For a speciﬁc dancer, only one sound engine is gener-
ally heard at any one time, except in the cross-fades be-
tween them. Which sound engine is heard is dependent on
the dancer’s position, and on his/her maximum height over
ground (see Fig.2):
VowelThe main melodic sound engine, covering the rear
half of the stage. Using two wave shaping oscillators and
formant ﬁlters, it uses the additive pitch mechanism, and
sounds a bit like a pair of synthetic voices, one for each arm.
Cross-modulation Front right of the stage. More com-
plex and intense sounds, but still gesturally controlled. The
core is two cross-modulating oscillators, using frequency
modulation. There is potential for chaotic behavior.
Feedback droneAn oscillating feedback circuit based
around a delay, a ﬁlter, a leveling waveshaper and a multi-
stage phaser. It is not based on movement energy, but
sounds continuously. This sound engine can be very intense.
Bells in the skyFor when a dancer reaches up above the
head level with any part of the body, usually the hand. It
sounds continuously, and consists of a generative dual bell
rhythmic structure, where the pitch, rhythm pattern and
timbre parameters are modulated from dancer posture
Together Takes over as soon as the two dancers touch
each other with any part of their bodies. It consists of
two parts: a shimmering, pulsating tone cluster made from
white noise through a tuned resonant ﬁlter bank, and a
pitched granular scraping sound from pulse trains directly
generated from dancers gestures (like bowing in the air),
sent through a resonant multi-pole phaser ﬁlter. It uses the
additive pitch mechanism, and has a vocal character.
The walls A warning sound which intentionally stands
out, to signal that the dancer are too close to the borders.
Similar to an alarm bell ringing.
Silence Near the ﬂoor is always silence.
5. PERFORMANCE
Given the aesthetic design choices behind the soniﬁcation
system, the choreographer in parallel developed a narrative
framework for a semi-improvised performance within the
OtoKin sound space, where both authors act as dancers.
We both have a long practice of collaboration and of im-
provisation, but also of traditional Japanese theater and
dance. In a recent collaboration, commissioned by the Noh
Reimagined festival in London, 2016, we created a decon-
struction of Zeami’s Noh drama Atsumori. In the piece,
ASD wore the mask of Chujo, and performed relating to
traditional body postures and movements from Noh and
Nihon Buyo (the traditional Japanese dance, [8]), ﬁltered
through a contemporary dance practice. Also, traditional
bodily constructions of gender and cross-gender were ex-
ploited to transform the classical narrative.
In the performance for OtoKin, we wanted to continue
this work, and again worked with the masks. ASD wore the
mask of Chujo once more, a young sensitive nobleman, while
PD wore Uba, the mask of an aging melancholic woman.
This became a constraint for our movements, by continuing
a practice that we both value and respect and by attempt-
ing to ﬁnd an expression for it inside the OtoKin space. We
did not use explicit choreographed movements, but decided
3https://www.nordkeyboards.com/products/nord-
modular-g2
Figure 3: From the premiere of OtoKin.
to move according to lived and performed knowledge, and
allow for exploration of various movement patterns within
the sound space. The Noh also adds the constraint of re-
duced vision – it is diﬃcult to see where you are, where your
performing partner is, and even where your own limbs are.
This enforced a diﬀerent spatial awareness, where move-
ments needed to be precautious and precise. They needed
to be felt in a diﬀerent way, almost as if we performed with
a blindfold. It also induced a particular sense of ma, a
Japanese term denoting the sense of spacing, in any dimen-
sion (space or time).
We also decided to work with the classic formal dra-
maturgical concept of jo-ha-kyu, meaning “beginning,
break, rapid”. It is traditionally used on all levels to shape
the ﬂow of time, and can perhaps be described as slow
prelude, steadily increase and breaking apart, with a swift
ending, going back into silence.
So, we relate to Japanese and Chinese concepts such as
ma and jo-ha-kyu, but we also go against this. The form
helped us create a rhythmic constraint for playing the space.
We put special emphasis on the jo. We wanted to meet the
sounds slowly, not to over-represent the space. We also
wanted to reach the powerful ha through a long-term build
up, not using the more intense sonic possibilities of OtoKin
until the second half. Having our faces covered also created
a possibility of staying in an inner world, where sound (yet
spatial) was the only interface between us.
Slow suriashi (walking with sliding feet) is used when
actors and dancers enter and leave the Noh stage on the
hashigakari, the bridge leading from the left side up to the
stage. This represents the connection between the stage
and the spiritual world, between the human and the kami
(god/desses). The audience has plenty of time to experi-
ence the entrance. We walked in suriashi from upstage left
and right, forming our individual hashigakari, and then sat
down in seiza (kneeling position), which is quiet in OtoKin,
because it is low.
The aﬀective responses from the sound created an intensi-
ﬁed space for us, where we were working with the limitations
of codiﬁed movements, the masks, and also the fact that we
needed to move as if in separate rooms. In a way it was as
if we were continuing seiza but in movement. There is an
intense build up after that slow start, and since the sound
space is designed with greater sonic intensity and complex-
ity towards the stage front, the progression and build up
happens in a very organic way.
A considerable amount of energy is generated through
consciously resisting speed, while performing high intensity
on the inside, behind the mask. Actor and director Barba
[1] describes how the performers in Japanese Noh theater
160
use the energy needed to move through space, but that they
actually do not move much through space. He explains that
the energy they would have expended in space is instead
kept in their bodies, creating a certain intense atmosphere
not easily discerned by the untrained eye, but certainly felt
as a particularity. According to Japanese theater scholar
Gunji [8], the smallness of the stage and the restricted move-
ments of the dancers has a symbolic value, and it also re-
veals a speciﬁc interpretation of space. In some dances, the
ideal is ‘to dance without actually moving’. He also points
out the characteristic restriction of the traditional Japanese
dancer’s movements in space, which diﬀers from Western
dance in that it is oriented towards the Earth, with little of
the raised arms, lifted bodies and leaps of Western dance.
The sound space we built challenges presumptions of space.
In the ceiling there was indeed a kind of heavenly sound that
we both loved to touch now and then, it represented another
sound world with a more seductive tonality than the bound-
ing walls. The diﬀerent sub-spaces lent themselves very well
to giving shape to the jo-ha-kyu development. In fact, they
directly provided a spatial and sonic representation of this
progression, including a resting position to depart from and
return to.
As a part of the performance, thanks to the strong aﬀor-
dance for interaction through touch, which transforms the
space completely, a parallel narrative developed around the
themes of solitude, searching and belonging.
6. DISCUSSION
How is kinesis related to oto, or sound? In many dance
traditions, kinesis is no more than a representation of oto,
i. e. movements are created as exclamation marks next
to a music piece. Even if the movements are composed
as a dialogue with the existing music, the result is more a
music visualization than an independent artistic piece. Ki-
nesis is responsive to oto, and historically this has created
a hierarchical imbalance. In Noh theater, even though the
movements underline and amplify the narrative in abstract
ways, movement and sound coexist independently, and the
space in between creates the suspension and intensity we
experience when watching Noh. During the 20 th century,
there has been a shift also in the West, towards letting ki-
nesis and oto operate independently, mainly because art has
become less Eurocentric.
In ASD’s training as a dancer, she has both the expe-
rience of movement as a representation of or response to
sound, where movement is composed as music visualiza-
tion. In her own work, choreography is an independent
artistic piece where movement and sound operate interdis-
ciplinarily. What was diﬀerent in working with OtoKin was
the strong link between them in both directions, merging
them into a responsive system we had to learn to exist in.
A choreographic impulse immediately got a sonic response,
sensitive to diﬀerent intensities in the movement. As she ex-
pressed it: “The room around us became ﬁlled with strings
and objects to pick up, to explore, of walls to touch and lean
against. The sounds ﬁlled me with images, and created a
transformed space to dance in.”
7. CONCLUSIONS
We have presented the soniﬁed performance space of
OtoKin, based on a particular design ideal of embodied
performance and eﬀort-based mapping, true to physical
gesture in both domains. It lends itself well for both im-
provisatory exploration, and for semi-prepared narratives.
To be in it feels like playing an instrument and dancing
at the same time, where movement and sound are equally
guiding the development of the performance.
The aesthetic design decisions of the sound space, com-
bined with the choreographer’s background and interest
in traditional Japanese performance, together shaped an
emerging narrative. In this process, it was evident that
the OtoKin sound space provided sonic richness, detail,
and expressive potential, allowing for an extensive duet
performance, and that the movement-to-sound mapping
in a meaningful way merged sonic and choreographic
expression.
8. REFERENCES
[1] E. Barba and N. Savarese. A dictionary of theatre
anthropology: The secret art of the performer.
Routledge, 2003.
[2] A. Bergsland and R. Wechsler. Composing interactive
dance pieces for the motioncomposer, a device for
persons with disabilities. In NIME15, pages 20–23,
2015.
[3] P. Dahlstedt. Dynamic mapping strategies for
expressive synthesis performance and improvisation.
In LNCS 5493, pages 227–242, 2009.
[4] P. Dahlstedt. Circle squared and circle keys -
performing on and with an unstable live algorithm for
the disklavier. In NIME14, pages 114–117, London,
United Kingdom, 2014.
[5] P. Dahlstedt. Action and perception: Embodying
algorithms and the extended mind. In R. T. Dean and
A. McLean, editors, OUP Handbook of Algorithmic
Music, chapter 3, pages 41–65. Oxford University
Press, 2018.
[6] P. Dahlstedt, P. Karlsson, K. Widell, and
T. Blomdahl. Youhero - making an expressive concert
instrument from the guitarhero controller. In
NIME14, pages 403–406, 2014.
[7] P. Dahlstedt and P. A. Nilsson. Free ﬂight in
parameter space: A dynamic mapping strategy for
expressive free impro. In LNCS 4974, pages 479–484.
2008.
[8] M. Gunji. Buyo the Classical Dance.
Weatherhill/Tankosha, 1970.
[9] P. Hodgins. Relationships Between Score and
Choreography in Twentieth-Century Dance Music,
Movement, and Metaphor. Mellen, 1992.
[10] J. Ryan. Eﬀort and expression. In ICMC, pages
414–416, 1992.
[11] J. C. Schacher. Motion to gesture to sound: Mapping
for interactive dance. In NIME10, pages 250–254,
2010.
[12] W. Siegel. Dancing the music: Interactive dance and
music. In Oxford Handbook of Computer Music.
Oxford University Press, Oxford, 2011.
[13] W. Siegel and J. Jacobsen. The challenges of
interactive dance: An overview and case study.
Computer Music Journal, 22(4):29–43, 1998.
[14] S. A. Skogstad, K. Nymoen, Y. de Quay, and A. R.
Jensenius. Developing the dance jockey system for
musical interaction with the xsens MVn suit. In
NIME12, 2012.
[15] D. Wessel, M. Wright, and J. Schott. Intimate musical
control of computers with a variety of controllers and
gesture mapping metaphors. In NIME02, Dublin,
Ireland, 2002, 2002.
[16] T. Winkler. Making motion musical: Gesture
mapping strategies for interactive computer music. In
ICMC, page 26, 1995.
161