Introducing Locus: a NIME for Immersive Exocentric Aural
Environments
Disha Sardana
Virginia Tech
Human Centered Design
Blacksburg, VA, USA
dishas9@vt.edu
Woohun Joo
Virginia Tech
Human Centered Design
Blacksburg, VA, USA
joowh@vt.edu
Ivica Ico Bukvic
Virginia Tech
SOPA, DISIS, ICAT
Blacksburg, VA, USA
ico@vt.edu
Gregory Earle
Virginia Tech
Electrical and Computer
Engineering
Blacksburg, VA, USA
earle@vt.edu
ABSTRACT
Locus is a NIME designed speciﬁcally for an interactive, im-
mersive high density loudspeaker array environment. The
system is based on a pointing mechanism to interact with
a sound scene comprising 128 speakers. Users can point
anywhere to interact with the system, and the spatial in-
teraction utilizes motion capture, so it does not require a
screen. Instead it is completely controlled via hand ges-
tures using a glove that is populated with motion-tracking
markers.
The main purpose of this system is to oﬀer intuitive physi-
cal interaction with the perimeter-based spatial sound sources.
Further, its goal is to minimize user-worn technology and
thereby enhance freedom of motion by utilizing environ-
mental sensing devices, such as motion capture cameras
or infrared sensors. The ensuing creativity enabling tech-
nology is applicable to a broad array of possible scenarios,
from researching limits of human spatial hearing perception
to facilitating learning and artistic performances, including
dance. Below we describe our NIME design and implemen-
tation, its preliminary assessment, and oﬀer a Unity-based
toolkit to facilitate its broader deployment and adoption.
Author Keywords
NIME, gestural control, motion capture, spatial sound, ex-
ocentric, interaction, immersive aural environment, mobile,
wearable, screen-less
CCS Concepts
•Human-centered computing→Gestural input; User
studies; Auditory feedback;
1. INTRODUCTION
“Screen-less wearable devices allow for the smallest form
factor and thus the maximum mobility” [12]. Gustafson
et al. state that current screen-less devices only support
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’19, June 3-6, 2019, Federal University of Rio Grande do Sul,
Porto Alegre, Brazil.
buttons and gestures; pointing is not supported because
users have nothing at which to point.
In this paper we introduce Locus, a NIME created as part
of the SADIE (Spatial Audio Data Immersive Experience)
research project funded by the National Science Foundation.
Its primary focus is to facilitate user interaction with spatial
aural content in an immersive exocentric aural environment
[7]. This is achieved via a pair of gloves with retroreﬂective
markers, which leverage the intuitive way we interact with
the world, particularly using ﬁnger pointing and other sim-
ple hand gestures. The environmental hardware and soft-
ware infrastructure tracks these markers to monitor user
input and generate immersive spatial aural content. In this
paper we use the term “exocentric environment” to refer to
a virtual reality or other immersive environment that com-
pletely encompasses the user [21] and allows exploration of
the environment from multiple vantage points. Speciﬁcally,
head-motions, echoes, and phase diﬀerences based on prox-
imity and the listener’s vantage point are fully experienced
in an exocentric environment, but are not evident with the
headphone-only approach to aural immersion.
In everyday experience most if not all human interac-
tions and auditory sensations occur in exocentric environ-
ments, so these are both natural and intuitive. Further, we
commonly combine sensory inputs from multiple sources to
understand the stimuli in the environment around us. By
doing so we increase our cognitive bandwidth [1]. For in-
stance, we can walk around to better pinpoint a particular
sound source, and by doing so we combine our senses of lo-
cation, motion, hearing, and sight to form a more accurate
spatial image of our surroundings. Locus is built upon this
speciﬁc meaning of the term “exocentric environment”.
1.1 New Interface for Musical Expression
In Figure 1, a user is wearing gloves ﬁtted with reﬂective
markers, and is making hand gestures to manipulate sounds.
Using both hands, he is manipulating two sounds. He can
turn around and use hand gestures to direct the sound to
emanate from any direction. In addition to location, the
user is also able to manipulate loudness and modulation.
The ensuing aural fabric is centered on the manipulation
of the spatial dimension, thus providing a foundation for
the exploration of form and structure through the spatial
dimension of sound.
The beneﬁt of such a system is that it enhances the artis-
tic expression in a performance, and does so using natural
250
Figure 1: User manipulating the sound ﬁeld using
gloves ﬁtted with retroreﬂective markers
and intuitive gestures (e.g., pointing) [14]. The user is not
entangled in wires, nor do they need to use buttons or a
mouse. Instead they are immersed in the physical space
while manipulating the sound ﬁeld, and the audience ex-
periences a desired spatial soundscape that is further rein-
forced by the user/performer’s natural and easy to compre-
hend gestures. The intuitive nature of the interface yields
a system that is easy to learn with minimal training, while
the related body language provides opportunities for easy
communication of the user’s actions and their aural mani-
festations.
Locus is a NIME that consists of several components: a
pair of gloves with retroreﬂective markers, a motion tracking
system, a software-driven sound engine realized in Max/MSP
[17], a high density loudspeaker array (HDLA), and a Unity-
based software toolkit that links all of these elements to-
gether. This ecosystem creates an opportunity for devel-
oping an intuitive interface for controlling and spatializing
multiple immersive audio streams. This new NIME en-
compasses ideas and techniques common to wearable com-
puting, gesture and spatial interaction, and other types
of NIMEs associated with screen-minimum infra-maximum
mobility [3] [11]. Several types of control interfaces in these
areas have been studied to enhance the experience and in-
teraction between a human, a computing system and an
aural environment. These include a physical sound inter-
face for real time performance [5] and a glove-type control
device [8] [18] [13].
2. PRIOR WORK
Sound spatialization control interfaces are diverse. They
typically rely on a custom visual widget that may be in-
spired by the physical controllers, and they frequently use
one or more hardware controllers, such as faders, poten-
tiometers, multidimensional and/or multi-touch surfaces, or
simply a mouse and a keyboard (e.g., TouchOSC (https://
hexler.net/) and Lemur (https://liine.net/en/)). Such
controllers are often programmable and able to perform
many diﬀerent tasks, but they are not necessarily designed
with sound spatialization in mind. An example of research
that does focus on sound spatialization in this area is the
work of Salvati et al., who developed a system [19] that al-
lows a performer to localize the position of sounds that em-
anate from a musical instrument during a live performance.
In an earlier study, Marshall et al. examined multiple ap-
proaches for gestural control of sound spatialization by com-
paring and analyzing existing software models to pinpoint
the primary roles of gesture control for spatialization [15].
There is a growing interest in virtual, augmented, and
mixed reality devices like Samsung’s HMD Odyssey, which
is designed to lower the cognitive load and make interaction
with immersive sound more natural and intuitive. Deacon
et al. created Objects VR [9], in which accessibility to their
interactive music system is controlled by a 3D object-based
control interface; their system can provide joyful interaction
experiences at levels ranging from novice to expert. Locus’
fundamental approach is to signiﬁcantly diminish our cogni-
tive load while providing an intuitive approach to interact-
ing with sound source(s). For instance, one can simply and
intuitively point with a ﬁnger to identify and then move a
sound source to a desired location.
In this paper we describe the key elements of Locus and
the early user studies performed with the system, in order
to highlight the ﬁndings of the qualitative and quantitative
data obtained. Much work remains to be done, but our
initial studies conﬁrm the promise of utilizing Locus as the
primary control interface in an interactive design and per-
formance studio environment that can create a new dimen-
sion for artistic expression in the performance-based arts.
3. IMPLEMENTATION
Locus was originally designed with two goals in mind: to
study how humans perceive sound in an exocentric envi-
ronment, and to expand the potential for artistic expres-
sion using immersive audio, with particular focus on the
high-density loudspeaker arrays (HDLAs). It utilizes two
oﬀ-the-shelf gloves ﬁtted with six retroreﬂective markers
per hand, and is designed for use with the environmental
or perimeter-based 24-camera Qualisys motion capture sys-
tem. The spatial marker data are extracted using the Unity
gaming engine-based toolkit that includes raycasting and
vector-based transformation capabilities. It is designed to
facilitate data interpretation, as well as gesture and motion
recognition. The system can detect simple continuous ges-
tures, such as pinch, thumb trigger, and hand roll, which
allows users to interact with the environment in a natural
and familiar way. With its vector processing capabilities it
is able to detect the location of a spot on the wall to which
the user points with sub-millimeter accuracy, while simulta-
neously monitoring and responding to other hand gestures.
Furthermore, it provides a convenient visualization designed
to assist with system setup and troubleshooting.
Once the data are processed they are sent to a sound en-
gine, in this case Max/MSP, that responds to the recognized
gestures. Examples of simple interactions include changing
the loudness of the sound, moving the sound from one lo-
cation to another, or modulating the sound source. Below
we discuss each component in greater detail.
3.1 Glove Controller
We chose to use optical tracking for our study so as to allow
users to interact with the system as naturally as possible.
The occlusion issue can be mitigated for optical tracking by
using a large array of cameras that monitor the space from
various angles. Spherical displaced retro-reﬂective markers
integrated into gloves are preferred over reﬂective tape be-
cause they are less susceptible to occlusion [10]. We place
these markers on the relevant ﬁngers and joints to allow ac-
curate interpretation of hand positions and motions. Joints
in the human skeleton naturally possess diﬀerent degrees of
freedom (DoF) [16]. The distal phalange of index ﬁnger is
stable, as compared to that of middle ﬁnger or pinky ﬁnger
[20]. We therefore put one marker on the upper joint of
the index ﬁnger, another on top of the thumb, two on the
251
leftmost and rightmost knuckles, and two on left and right
wrist joints. This conﬁguration of passive markers allows
the relevant hand gestures to be perceived accurately, while
minimizing the total number of markers required.
Another beneﬁt of the optical tracking system is that it
does not require any user-speciﬁc ﬁnger model. Since the
markers are ﬁxed to a glove, which when coupled with ac-
ceptable deviations of marker positions (e.g. due to diﬀer-
ences in hand size), provides a relatively unrestrained and
easy-to-use environment [10].
For our initial implementation we have opted to use sim-
ple static gestures, like pointing with the index ﬁnger, ana-
log and digital interpretation of thumb motion (pressing the
thumb against the hand), and pinch (touching the thumb
and index ﬁnger), as well as other analog gestures like roll.
When coupled with arm motion and/or each other (e.g.
pinch and roll together) even static gestures can become dy-
namic. The ensuing combination of variables and conditions
can therefore generate a rich, intuitive, and easy-to-detect
gesture vocabulary.
3.2 Hand Gesture Interpretation
Figure 2: Hand gestures are used to manipulate the
sound
Using the Qualysis Unity SDK, real time data represent-
ing the user’s motion are sent to Unity with minimal latency
(http://www.qualisys.com/software/unity/). In Unity,
a virtual space is created with four walls and a ceiling that
represent the true room dimensions. In the examples be-
low we use marker abbreviations as provided in Table 1 for
the right hand only. When applying the same values to
the left hand, its markers are mirrored accordingly. The
index ﬁnger knuckle (RKL) and index ﬁnger (RI) markers
are used to calculate a vector, and the intersection between
this vector and the walls in the room indicate where a user
is pointing. To ensure that the user is indeed pointing we
compare the vector of the index ﬁnger knuckle (RKL) and
the thumb joint (RT) marker. When the angle between the
two vectors is less than 20 degrees the system assumes that
the user is pointing. This condition ensures that a user hold-
ing his index ﬁnger in a relaxed state that is typically not
aligned with the palm will not erroneously be interpreted
as pointing. In addition to carrying out the vector point-
ing calculations, the Unity environment allows us to easily
visualize the observed markers, pointing direction, and in-
tersection in a natural coordinate system (see Figure 3).
Pinch motions for musical expressions are detected using
a proximity threshold between the thumb (RT) and index
ﬁnger (RI) markers. A trigger motion is expressed using
a proximity threshold between the thumb (RT) and pinky
Table 1: Identiﬁable markers used
Marker Tag Marker Tag
Left Index Finger LI Right Index Finger RI
Left Thumb LT Right Thumb RT
Left Left Knuckle LKL Right Left Knuckle RKL
Left Right Knuckle LKR Right Right Knuckle RKR
Left Left Wrist LWL Right Left Wrist RWL
Left Right Wrist LWR Right Right Wrist RWR
Figure 3: The image shows virtual room as seen on
the Unity platform. The red ray shows where the
user is pointing with his right hand, and the blue
ray shows the same for the user’s left hand. The
blue and red spheres are the points where these rays
intersect with the room’s wall/ceiling. The cyan
and purple spheres show the projection of blue and
red spheres respectively, on the x-z plane (ﬂoor)
knuckle (RKR) markers, as previously described. This is
more accurate than trying to detect proximity between the
thumb and index ﬁnger knuckle, where overreaching with a
thumb could result in a double trigger interpretation; mark-
ers are shown in Figure 2. Lastly, rolling motions of the
hand are measured using a perpendicular vector from the
index ﬁnger knuckle (RKL) to the pinky knuckle (RKR) and
the thumb joint (RWL) to the index ﬁnger knuckle vector
(RKL). The reason for the relative rotational nature of the
roll is due to the human ability to place the hand in a way
that, akin to a gimbal lock, may swap roles of the diﬀerent
axes (e.g. when pointing directly upwards), thereby mak-
ing it extremely diﬃcult to accurately detect the absolute
roll without resorting to having another reference marker
placed on a relatively static point on the user’s torso. This
choice allows the user to place their hand in any position, to
possibly extend the roll range prior to initiating the actual
roll (e.g. by using a trigger or some other binary action).
User motions of interest are detected and the resulting
data are sent to Max/MSP over a network socket using
an OSC-like format ( http://opensoundcontrol.org/) via
User Datagram Protocol (UDP) packets designed to be eas-
ily parsed using Max/MSP’s built-in [route] object.
The commands sent from Unity are formatted hierarchi-
cally, as follows: <left, right > <pinch, thumb, roll, han-
dazel, pointazel> <value 1, value 2,...,value n>
The ﬁrst argument deﬁnes which hand the data packet
refers to, and the second argument deﬁnes the kind of data
being sent. It is followed by one or more ﬂoating point val-
ues, depending on the type of data. Table 2 oﬀers additional
details for each of the commands for a speciﬁc example of
sound manipulation.
4. USER STUDIES
252
Table 2: Recognized Commands
Command Arguments Description
handazel Absolute angles in degrees expressed
as ﬂoating-point values with respect
to the spatial origin
Oﬀers azimuth and elevation angle at the collision point on the wall
extended by vector (RKL - RWL). Determines whether hand is above
ground or not.
pointazel Absolute angles in degrees expressed
as ﬂoating-point values with respect
to that spatial origin
Oﬀers azimuth and elevation angle at the collision point on the wall
extended by vector (RI - RKL). When compared to the handazel value,
it can be used to determine whether a user is pointing or not.
pinch Distance in meters expressed as a
ﬂoating-point value
Oﬀers distance between RI and RT. It can be used to construct a
threshold-based on/oﬀ value for detecting pinch, or as a dynamic range
value between the two markers.
thumb Distance in meters expressed as a
ﬂoating-point value
Oﬀers distance between position vectors RT and RKR. It can be used
to construct a threshold-based on/oﬀ value for detecting trigger, or as
a dynamic range value between the two markers.
roll Relative angle in degrees expressed
as a ﬂoating-point value
Oﬀers relative roll deﬁned by the RKL-RKR vector in respect to the
RWL-RKL vector. Due to lack of additional markers, it is relative and
is typically associated with a triggering command, such as threshold-
driven pinch and thumb gestures.
We conducted user-studies to investigate users’ ability to
interact intuitively with the glove interface and the spatial
aural content. The goal of the study was to test the func-
tionality of the Locus system, including how accurately the
design and implementation are working, and to analyze the
ease and expressivity of human subjects’ interaction with
the spatial sound. The experiments required people to be
in a room containing a HDLA and a motion capture system
that would minimize any spatial ﬁdelity issues commonly as-
sociated with sparser multichannel loudspeaker arrays. The
wearable controller (gloves) with its retroreﬂective markers
conveys hand position and motion data to the Qualisys mo-
tion capture system that is used for tracking movements.
The hand-position data are extracted using a custom toolkit
built in Unity, which creates a virtual environment in which
hand gestures can be recognized and interpreted. Once this
information is processed, it is sent to another software ele-
ment (Max/MSP) that responds to the gestures detected.
The whole system requires a careful calibration. The
Qualysis Streaming follows a left-handed Cartesian coor-
dinate system, Unity software interprets data in a right-
handed coordinate system (requiring a simple vector trans-
formation), and the D4 library is implemented so as to fol-
low the left hand rule. Calibration is done with respect
to world stabilized axes, not body or head stabilized sys-
tems[4]. The active spatial region in the room is calibrated
so that the center of the room is the origin of a ﬁxed-axis,
Cartesian, three-dimensional space. The speakers within
the room populate the walls and ceiling area at or above
the horizon, and the origin for the vertical (Y) axis is de-
ﬁned to be at ﬂoor level in the geometric center of the room.
4.1 Apparatus
Motion Capture
For this study, we used 24 Qualisys Oqus 500+ motion
tracking cameras and an AIM (Automatic Identiﬁcation of
Markers) model oﬀered by the Qualisys QTM. The advan-
tage of using an AIM model over a rigid body is that it
allows for ﬁnger motion without compromising recognition
of the body, as long as the system is properly trained prior
to its use. Once trained, the AIM model is capable of iden-
tifying the trained object regardless of hand size, ﬁnger or
hand position, or orientation.
Immersive Sound
Unity toolkit’s OSC-like output formatting allows it to in-
terface with a wide variety of network-enabled digital signal
processing software. In our case, we used Max/MSP for all
our sound-related processing. This is in part to leverage the
functionality of the D4 system [6], which is an audio spatial-
ization library designed speciﬁcally for HDLA low-latency
scenarios. This setup allows us to distribute sound across
the 128 loudspeakers with a high degree of control.
Figure 4: D4 monitor
4.2 Tasks and Procedures
Subjects were asked to interact with the glove controller to
manipulate two sounds. We selected two easily distinguish-
able sounds that were perceived as pleasing, and therefore
easy to listen to over extended periods of time. One was
the noisy sound of cicadas and another a spectrally-distinct
collage of pitched wind chime sounds. While these are not
interpretable as a musical composition, they are designed
to promote musicality. The primary goal of this scenario is
to allow users to focus on natural spatialization, including
sound source location, trajectory, loudness, and modula-
tion. Before the users begin interacting with the glove, clear
instructions are given and one of the investigators demon-
strates available gestures and their perceived outcomes. The
ﬁrst and most intuitive gesture is ‘Pointing’. Pointing is to
be done such that the markers on the index ﬁnger and hand
are in a straight line (see Figure 2). Lowering the pointing
ﬁnger while keeping the hand up will stop pointing. Users
can use this to instantaneously relocate the sound to a dif-
253
ferent location by moving the hand without pointing and
then resuming the pointing stance. They can also move the
sound in a continuous manner from one point to another by
moving their index ﬁnger in pointing form to the desired
location. In the thumb-trigger gesture users bring their
thumb closer to the index ﬁnger. This is used to change the
loudness of sound. Moving the thumb closer to the ﬁnger
decreases the loudness, and moving it away from the hand
increases it. When used in its extreme positions, one can
also use this as a toggle switch, since minimum loudness is
zero, and the user can turn on and oﬀ sound at any point.
Rapid triggering motions create low frequency amplitude
modulation, which proved as a useful derivative gesture in
musical expression scenarios. Another allowable interaction
includes two gestures implemented together. For example,
pinch and roll produces periodic swells or sound modula-
tion; these work regardless of where the hand is pointing
(above or below the horizon). Akin to an inﬁnite knob, it
is relative in nature.
After trying out the glove controller for about 5 minutes,
users were asked to give both numerical ratings and quali-
tative feedback on their experiences. We ask the following
speciﬁc questions and record the users’ responses. The ﬁrst
section of the questions is based on the Likert scale [2] from
1 to 5, and the second section is devoted to short answer
questions.
Section 1
1. Interacting with the interface was comfortable.
2. Interaction with the system was unnatural.
3. I felt the system was easy to use.
4. The experience for me was not as engaging as other
approaches to interactively spatializing sound.
5. I experienced discomfort during the experiment.
6. It was a physically tiring experience.
7. I found the interaction intuitive.
8. I found the experience frustrating.
9. This approach to interactive spatialization is not as
natural than other approaches to interactively spatial-
izing sound.
10. I felt the training was helpful.
11. The experience was fun and interesting.
12. This approach to interactive spatialization is more in-
tuitive than other approaches to interactively spatial-
izing sound.
13. I found the interface to have expressive potential
14. I experienced no issues with the process and the in-
frastructure (e.g. controller interface, acoustics in the
room, preliminary brieﬁng material, etc).
15. This approach to interactive spatialization is overall
better than other approaches to interactively spatial-
izing sound.
Section 2
1. What aspect of the experience did you ﬁnd most chal-
lenging, and why?
2. Please brieﬂy state any comments pertaining to fa-
tigue, usability, the potential of the system, expres-
siveness, and how it may compare with existing tech-
niques.
3. Please list other approaches to interactively spatialize
sound you have previously used and brieﬂy describe
them.
4. Please share any other feedback you may have for im-
proving the interface below.
4.3 Participants
As of writing this paper a total of seven test subjects partic-
ipated in the user study. The study was open to adults who
did not have any hearing or mobility issues. Participants
were recruited from students, faculty, and staﬀ. Three of
the test subjects were female and the rest were male, rang-
ing in age from 26-39. All the participants came voluntarily,
and no ﬁnancial or other rewards were given to encourage
participation. Most of the subjects that came were right-
handed except one. Six of them had previous experience
with a gesture-based device. Only one of them wasn’t sure
if they had been previously exposed to some form of spatial
sound environment, but the rest of them had prior exposure.
All subjects were asked to conﬁrm that they knew how to
abort the test in case of any discomfort before the study be-
gan. The study was granted an Institutional Review Board
approval.
5. PRELIMINARY OBSERV ATIONS
The most notable observations during our preliminary eval-
uation of the system are linked to its intuitive nature. In
preliminary tests the system was observed to have a minimal
learning curve, making it possible for any user to immedi-
ately engage in the system, regardless of prior experience
with immersive sound and/or music-making. The simple
and natural motions, such as ﬁnger pointing, thumb trig-
ger, pinch, and hand rolling motions are skills that normal
humans attain and master at an early age. The ability to
concurrently move two sounds around in three dimensions
by simply pointing to a wall while simultaneously modifying
each sound’s parameters generated a sense of excitement,
immersion, and a seamless connection with the spatial au-
ral material. Further, in their exploration of the interface’s
vocabulary and mapping of gestures to sound, the users
were able to quickly discover new and unanticipated ways
of interacting.
One of the users mentioned it as “ a really cool experi-
ence”. The same user gave us feedback that the interface
“feels magical because I’m pointing at something and the in-
teraction components are invisible.”
One of the challenges as described by a user: “Volume
control was challenging, but I believe that is due to limited
exposure. Were I “playing” the gloves as an instrument reg-
ularly that would no longer be an issue (I would gain muscle
memory).”
An issue that users reported in the survey was fatigue.
Users had to keep their arms in the air for performing spe-
ciﬁc gestures. This may be in part because only some ges-
tures could be performed while keeping the hands down.
For long performances fatigue could be a factor, as men-
tioned in the feedback. One subject mentioned that “ There
is potential for fatigue if you choose to spatialize a often and
over longer distances (which I did), but as with any other
instrument, that can be ﬁxed with practice. The potential
for fatigue and expressivity is on par with all other analog
instruments I currently play, some of which are fatiguing by
their sheer physicality (cello). ” Another person mentioned,
“I did notice slight arm strain during use. this might limit
adoption but I don’t believe it should be considered an issue,
more a feature”.
One suggestion that we received was “Keep going in this
direction. It’s already shaping up to be a highly intuitive
instrument. I would strongly recommend bringing in more
performers to “play” the gloves, from as wide a variety of
backgrounds as possible to make it truly human-centered” .
Based on the questionnaire’s results, 100% of the subjects
agreed that they found the interface to have expressive po-
tential.
6. CONCLUSION
254
In this paper we present the design and implementation of
Locus, a NIME designed speciﬁcally for an immersive ex-
ocentric aural environment. It is driven by a simple glove
interface in combination with an environmental motion cap-
ture technology and a 128-loudspeaker HDLA. Locus’ abil-
ity to eﬃciently leverage a HDLA makes it also more broadly
applicable, including sparser and more readily available loud-
speaker arrays whose computational overhead is signiﬁcantly
lower. The ensuing ecosystem of tools and toolkits allows for
near seamless transition between various HDLA conﬁgura-
tions. The observed interaction suggests that Locus and the
supporting toolkit has the potential to signiﬁcantly lower
the cognitive load in both artistic and research scenarios
utilizing immersive exocentric aural environments. Further,
its intuitive design pushes the boundaries of interactive spa-
tialization, allowing for one hand to control multiple dimen-
sions of a single sound, while also oﬀering clear and natural
interaction and its projection onto the audience. Encom-
passing the spatial component is arguably the last frontier
of sound that remains to be exploited as a primary driver
of musical structure and expression. 21st century advances
in computing power and reduced latency have set the stage
for this transformation. Locus may serve as an important
catalyst in this process. The supporting toolkit designed in
Unity oﬀers a rich platform for future augmented and vir-
tual reality implementation that may further enhance the
multisensory experience of the spatial component of sound,
and its under-explored potential as the structural driver of
musical expression. Further, Locus has the potential to en-
hance learning in educational scenarios, with particular fo-
cus on immersive and inherently spatial data. Our goal is to
build upon these preliminary ﬁndings with more in-depth
studies.
Locus was designed for use with the Cube’s environment.
However, its toolkit is designed to easily adapt to other
spaces. We aim to make the toolkit publicly available by
the summer of 2019.
7. ACKNOWLEDGMENTS
This material is based upon work supported by the Na-
tional Science Foundation under Grant No.(1748667). The
authors would also like to acknowledge Virginia Tech’s In-
stitute for Creativity, Arts, and Technology (ICAT) for the
project support.
8. REFERENCES
[1] K. P. A. Cognitive load theory: implications of
cognitive load theory on the design of learning.
Learning and Instruction, 12(1):1–10, Feb 2002.
[2] I. E. Allen and C. A. Seaman. Likert scales and data
analyses. Quality progress, 40(7):64–65, 2007.
[3] T. Baudel and M. Beaudouin-Lafon. Charade: remote
control of objects using free-hand gestures.
Communications of the ACM , 36(7):28–35, 1993.
[4] M. Billinghurst, J. Bowskill, N. Dyer, and
J. Morphett. Spatial information displays on a
wearable computer. IEEE Computer Graphics and
Applications, 18(6):24–31, 1998.
[5] B. Bongers. Physical interfaces in the electronic arts.
interaction theory and interfacing techniques for
real-time performance. In M. Wanderley and M.
Battier, eds. Trends in Gestural Control of Music.
Ircam - Centre Pompidou , 2000.
[6] I. Bukvic. 3d time-based aural data representation
using d4 library’s layer based amplitude panning
algorithm. International Conference on Auditory
Displays. Canberra, Australia., 2016.
[7] I. I. Bukvic and G. D. Earle. Reimagining human
capacity for location-aware aural pattern recognition:
A case for immersive exocentric soniﬁcation. Georgia
Institute of Technology, 2018.
[8] G. Costantini, M. Todisco, and G. Saggio. A wireless
glove to perform music in real time. In 8th WSEAS
International Conference on APPLIED
ELECTROMAGNETICS, WIRELESS and
OPTICAL COMMUNICATIONS, Malaysia, 2010.
[9] T. Deacon, T. Stockman, and M. Barthet. User
experience in an interactive music virtual reality
system: an exploratory study. In International
Symposium on Computer Music Multidisciplinary
Research, pages 192–216. Springer, 2016.
[10] K. Dorfmuller-Ulhaas and D. Schmalstieg. Finger
tracking for interaction in augmented environments.
In Proceedings IEEE and ACM International
Symposium on Augmented Reality, pages 55–64, 2001.
[11] M. Fukumoto and Y. Tonomura. “body coupled
ﬁngerring”: wireless wearable keyboard. In
Proceedings of the ACM SIGCHI Conference on
Human factors in computing systems , pages 147–154.
ACM, 1997.
[12] S. Gustafson, D. Bierwirth, and P. Baudisch.
Imaginary interfaces: spatial interaction with empty
hands and without visual feedback. In Proceedings of
the 23nd annual ACM symposium on User interface
software and technology, pages 3–12. ACM, 2010.
[13] C. Lai and K. Tahiroglu. A design approach to engage
with audience with wearable musical instruments:
sound gloves. in Proceedings of the international
conference on new interfaces for musical expression,
Ann Arbor, Michigan, 2012.
[14] P. Majdak, M. J. Goupell, and B. Laback. 3-D
localization of virtual sound sources: Eﬀects of visual
environment, pointing method, and training.
Attention, Perception, & Psychophysics ,
72(2):454–469, Feb. 2010.
[15] M. T. Marshall, J. Malloch, and M. M. Wanderley.
Gesture control of sound spatialization for live
musical performance. In International Gesture
Workshop, pages 227–238. Springer, 2007.
[16] V. I. Pavlovic, R. Sharma, and T. S. Huang. Visual
interpretation of hand gestures for human-computer
interaction: a review. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 19(7):677–695, Jul
1997.
[17] M. Puckette and D. Zicarelli. Max/msp. Cycling 74
(1990): 1990-2006 .
[18] F. G. H. P.-W. S. M. S. Seraﬁn, S. Trento and
T. Mitchell. Controlling physically based virtual
musical instruments using the gloves. in Proceedings
of the international conference on new interfaces for
musical expression, London, United Kingdom ,
25(2):521–524, 2014.
[19] D. Salvati, S. Canazza, and A. Roda. Sound
spatialization control by means of acoustic source
localization system. In Proceedings of the 8th Sound
and Music Computing Conference , pages 284–289,
2011.
[20] L. Shao. Hand movement and gesture recognition
using leap motion controller. Virtual Reality, Course
Report, 2016.
[21] A. L. Shelton and N. Yamamoto. Visual memory,
spatial representation, and navigation. pages 140–177,
2009.
255