Expressive potentials of motion capture in the
Vis Insitamusical performance
Nicolas Bazoge
Université de Rennes 2
Rennes, France
Nicolas.Bazoge@univ−rennes2.fr
Ronan Gaugne
Univ Rennes, Inria, CNRS,
IRISA, France
Ronan.Gaugne@irisa.fr
Florian Nouviale
Univ Rennes, INSA Rennes,
Inria, CNRS, IRISA, France
Florian.Nouviale@irisa.fr
Valérie Gouranton
Univ Rennes, INSA Rennes,
Inria, CNRS, IRISA, France
Valerie.Gouranton@irisa.fr
Bruno Bossis
Université de Rennes 2
Rennes, France
Bruno.Bossis@univ−rennes2.fr
ABSTRACT
The paper presents the electronic music performance project
Vis Insita implementing the design of experimental instru-
mental interfaces based on optical motion capture technol-
ogy with passive infrared markers (MoCap), and the anal-
ysis of their use in a real scenic presentation context. Be-
cause of MoCap’s predisposition to capture the movements
of the body, a lot of research and musical applications in the
performing arts concern dance or the soniﬁcation of gesture.
For our research, we wanted to move away from the capture
of the human body to analyse the possibilities of a kinetic
object handled by a performer, both in terms of musical
expression, but also in the broader context of a multimodal
scenic interpretation.
Author Keywords
Motion capture, control interface, musical performance
CCS Concepts
• Human centered computing →Sound based in-
put/output; Gestural input; • Applied computing
→Sound and music computing;
1. INTRODUCTION
Since the early 2000s and the industrial development of op-
tical motion capture systems, several studies [4, 5, 9, 10]
have been carried out on the use of such systems in musi-
cal applications, particularly within the NIME community.
However, partly due to the signiﬁcant ﬁnancial investment
that a professional motion capture (MoCap) system repre-
sents, few musical performances based on this technology
have yet been created [5, 13].
Moreover, because of MoCap’s predisposition to capture
the movements of the human body, we can see that a large
majority of the work that has been done concerns the ﬁeld
of dance or the soniﬁcation of gesture.
This research-creation project was therefore mainly inter-
ested in the problems of performing electronic music live,
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’19, June 3-6, 2019, Federal University of Rio Grande do Sul,
Porto Alegre, Brazil.
and the potential of MoCap’s technologies to stage this mu-
sic and make it heard through plastic elements.
In order to approach our subject as exhaustively as pos-
sible, we included it in a bibliographical research on how
art historians, artists and researchers question the way in
which live electronic music is perceived by the audience and
interpreted by the performer-musicians [2, 3, 8].
However, the study described in this article focused on
practical and experimental research through the creation of
a musical performance that attempts to implement expres-
sive and poetic forms of musical interpretation. The objec-
tive was multiple: to analyse the possibilities and limits of
this instrumental interface under real conditions of presen-
tation, to make a video recording of the performance 1 in
order to keep an analyzable trace, and to collect the feed-
back of spectators.
We relied on the NIME community, in particular on the
research of the University of Oslo about the results of the
experiment with the SoundSaber [15], an instrument de-
signed on a similar principle of motion capture. Unlike the
Soundsaber, which requires continuous manipulation to be
kinetically active (except in the case where it is launched
by the performer), the tangible interfaces we imagined had
to be able to behave autonomously after being manipulated
because of their own energy (kinetic energy) and bring a
contingent dimension to the performance.
From an artistic point of view, we wanted the aesthet-
ics of these kinetic objects to visually arouse the viewer’s
curiosity and attention. We had thus imagined very early
giving them a very strong plastic dimension, as much in
their shape, their colour, their material, their lines, as in
their own lighting, with as reference and inspiration, the
kinetic art of the 50s and 60s [1, 6, 7].
The term ”motion capture instrument”(MCI) used in this
article will refer to the instrumental interface as a whole,
including both hardware and software. Proposed by K. Ny-
moen et al. [15], it well reﬂects both the idea of a musical
instrument and the technology from which it is designed.
This article will propose a reﬂection on these two aspects:
the hardware - the motion capture system and the objects
captured - as well as the software that had to be developed
for digital audio use, MoCap systems being generally de-
signed for applications in the ﬁeld of video games, cinema
or biomechanical research.
Finally, we will attempt to report on the points that
seemed important and original to us at the end of a complete
creative process that goes as far as the public presentation
of the performance.
1https://vimeo.com/328923793
266
2. METHODOLOGY
The project was carried out in two stages at the University
of Rennes: (1) a research, experimentation and development
phase of approximately six months inImmersia, a virtual re-
ality platform equipped with motion capture systems; (2) a
two-week artistic residency in the Pˆ ole Num´ erique Rennes-
Villejean (PNRV), a university building equipped with dig-
ital tools and dedicated to the creation of shows and inter-
active installations.
The performance was shown four times to the public dur-
ing the JACES2 in the University of Rennes2, a one-week
group exhibition dedicated to digital art productions, as
well as once for the Journ´ ee Science et Musique3, a general
public event organized by the IRISA computer science re-
search laboratory. These performances gave rise to valuable
exchanges with the audience, contributing to the analysis
presented here.
2.1 In the virtual reality platform
The preliminary research and experiments in the Immersia
platform were carried out in a space of approximately 20m2
equipped with an ARTTrack2 4 motion capture system. A
ﬁrst phase focused on the detailed operation of the MoCap
system made it possible to consider the applications that
would have to be developed in the light of the project’s ob-
jectives. These applications ( ARTtoOSC and µZYX ) will
be presented in detail later in this paper.
The second phase concerned the design of kinetic objects
in order to prepare their subsequent manufacture in the
fablab of the University of Rennes2 equipped with the nec-
essary tools (laser cutting machine, power tools, 3D printer,
etc.). During this phase, work was made on both the func-
tional and technical aspects of these objects (grip, strength,
etc.), while keeping good aesthetics.
2.2 Intermediate period
An intermediate period of a few weeks before the artistic
residency made it possible to ﬁnalize the prototyping of six
infrared targets, one kinetic object (a light pendulum), and
to create an Ableton Live session in preparation for the
mapping of the traditional instruments used on stage. This
included the import of diﬀerent virtual synthesizers and au-
dio processing plugins that we wanted to be able to control
during the performance. Finally, we established a lightplot
with the projectors that we intended to control with the
MCI.
2.3 Artistic residency
Following this preliminary research and validation of the
technical viability of the system, a two-week residency pe-
riod was held at the PNRV, during which the performer
worked independently to compose the music, develop the
staging and dramatic construction of the performance and
create the lighting.
3. THE KINETIC OBJECTS
This section will describe the design of the pendulum used
for the Vis Insitaperformance, as well as the infrared targets
necessary for a kinetic object to be detected.
3.1 The luminous pendulum
In order to make it easy to read for the audience and im-
plement it into our study, we imagined from the outset an
2https://https://journees-arts-culture-sup.fr/
3https://jsm.irisa.fr/
4from the German manufacturer A.R. Tracking
object that could be handed in several ways and capable of
occupying the entire stage space.
A simple solution was to suspend an infrared target from
the ceiling. However, the target was not heavy enough to
swing for a long enough time, so we had to weigh it down.
From there came the idea of making a kinetic object inspired
by a pendulum.
As the object was intended to be observed by the public,
it was important to also think about its aesthetic aspect.
Poetically inspired by the inner force of matter ( Vis Insita)
deﬁned by Isaac Newton [14], we started with the aesthetic
idea of an ”energy ball” (Figure 1).
Figure 1: The light pendulum equipped with its
passive infrared target.
The pendulum consists of thin white PVC plates assem-
bled in the shape of a sphere using threaded rods, inside
which we have placed a 24V-50W incandescent lamp that
can be dimmed using a lighting application installed on a
laptop computer, and a dimmer controlled via a USB/DMX
interface.
The target for tracking the pendulum was attached to
the cable at the top with a metal clamp, where the MoCap
system has less occlusions during the performance.
3.2 Passive infrared targets
Unlike an isolated marker, a passive infrared target (IrT)
composed of several markers is a robust way to track the
objects in space: the multi-marker geometry allows, after a
recognition phase by the MoCap system, an indexing and
thus a re-detection in case of partial or total momentary
occlusion. It also allows a capture of rotation, which is
impossible with a single spherical marker.
However, in order for the system to discriminate diﬀer-
ent targets present on stage, it was necessary to give them
diﬀerentiating enough geometries to avoid any ambiguity.
During the ﬁrst tests with the targets supplied with the
MoCap system, we quickly realized that we would have to
manufacture our own ones.
First, from a functional point of view, the manufacturer’s
targets were only designed to be attached to the human
body, including wrist and ankle joints, with an adjustable
elastic band. This system prevented us from attaching them
to the moving objects we imagined. On the other hand,
they did not correspond to the general aesthetics we wanted
to give to our kinetic objects, and it was not technically
possible to modify them.
Considering these targets are the basic element of our
MCI, we approached their design by taking inspiration from
the atom, the latter being made up of a nucleus around
which elementary particles gravitate.
267
However, these handmade IrT had to be able to with-
stand potential shocks and be ﬁrmly attached to kinetic
objects. The IrT nucleus was carved from a piece of raw
wood (beech) dense enough to add wooden inserts in or-
der to securely screw threaded rods, to the opposite end of
which the infrared markers were attached.
In order to standardize and stiﬀen the entire IrT, we cov-
ered it with a thermosetting polymer paste and then covered
it with a white acrylic paint.
The markers were made from commercially available beech
logs to which we applied retro-reﬂective tape. This tape was
laser cut to optimize the quantity and cleanliness of the con-
tours.
4. SOFTWARE IMPLEMENTATION
The software implementation is based on two main com-
ponents, as shown in Figure 2: (1) the ARTtoOSC com-
ponent is dedicated to converting raw MoCap system data
into OSC data via a VRPN interface; (2) the µZYX com-
ponent is dedicated to managing these OSC data for use in
music and lighting applications.
Figure 2: Software implementation diagram.
4.1 The ARTtoOSC application
Natively, the ARTTrack2 system sends angular and spa-
tial data from the optical capture to its embedded DTrack2
application in a proprietary format. It was therefore neces-
sary, as a ﬁrst step, to convert these data into usable data
for our project. The choice was quickly made on the Open
Sound Control (OSC) [11] protocol, which is fast, open and
versatile. For this purpose, we coded in C# an application
called ARTtoOSC.
In order to allow portability to MoCap systems from dif-
ferent manufacturers, we chose to code this application us-
ing the set of VRPN5 (Virtual Reality Peripheral Network)
classes and servers. This open-source system, designed to
implement a transparent interface between software appli-
cations and the many physical devices used in a virtual re-
ality system, was perfectly suited to the devices of the ART
system.
The ARTtoOSC application returns two types of OSC
messages:
- The target identiﬁer (integer) and its position in the
three-dimensional reference frame (Cartesian coordinates x,
y and z expressed in metres with respect to the reference
point determined during the calibration of the MoCap sys-
tem), in format body/identifier/pos [x][y][z].
- The target identiﬁer (integer) and its three rotation an-
gles (Euler angles rx, ry and rz expressed in degrees on a
scale from -180 ◦ to 180◦) in format body/identifier/rot
5https://github.com/vrpn/vrpn/wiki
[rx][ry][rz]. The OSC data from ARTtoOSC is then sent
to µZYX via UDP in order to speed up the communication
(to the detriment of lost packets).
4.2 The µZYX interface
The main functions of the µZYX interface are to format
the spatial and rotational data from IrTs into MIDI data
compatible with digital audio space, as well as to calibrate
and route them.
As our MCI must potentially be able to control other het-
erogeneous parameters such as light parameters for exam-
ple, we have opted for a second possibility of conversion to
OSC, a communication protocol that is also very widespread
among applications encountered in the performing arts.
The µZYX application was developed in Max 8 6, on the
one hand because this programming environment is com-
pletely prescribed for the realization of patches requiring
OSC and MIDI data management, and on the other hand
because it allowed to quickly realize a graphical interface es-
sential to an eﬃcient data management in scenic conditions,
both in repetition and presentation.
This graphical interface consists of a main module and as
many target modules as necessary (Figure 3).
Figure 3: Graphical interface ofµZYX: main mod-
ule (top) and one target module (bottom).
The main module allows selecting the OSC data input
port, saving the conﬁguration of all target modules as pre-
sets, disabling the general MIDI output, choosing the MIDI
input port for remote patch control and pre-conﬁguring the
dimensions of the playing area.
The target module allows (according to a reading from
left to right) the calibration to the digital audio space of the
physical data of the IrT speciﬁed in the upper left corner,
the calibration, routing, visualization and smoothing of this
data to a MIDI or OSC output port.
Additional options enhance these features, such as the
ability to specify an oﬀset for each of the six input param-
eters and initialize them in a position in the physical space
diﬀerent from the zero point of the MoCap system.
The central window of the target module also oﬀers vari-
ous options for setting MIDI signals, such as disabling them
or inverting the range. It also has an arpeggiator that dis-
cretizes incoming continuous signals into MIDI notes. The
range, mode and tone of this arpeggiator are adjustable.
The velocity and duration of the notes can be associated
with a gestural parameter within this window.
5. THE VIS INSITAPERFORMANCE
5.1 Creation
The creation of the performance followed our preliminary
research and took place over a period of two weeks in a
PNRV plateau of about 60m2 in which we installed the de-
vice. This stage could accommodate approximately thirty
6https://cycling74.com/
268
people per performance, for one hundred and twenty people
out of the four performances given.
The device consisted of several standard instruments (elec-
tric bass guitar, MIDI keyboard, electronic percussion kit)
connected to a laptop computer via a sound card. The
solo performer sampled live these instruments in the Able-
ton Live software using a MIDI pedalboard to trigger both
recording and playback of the samples. This management
of live sampling was supported in Live by MIDI remote
scripts.
The pendulum was suspended in the centre of the playing
area, as presented in Figure 4. It was equipped with an IrT
whose movements were captured by the eight cameras of
the MoCap system, four hung on the technical grid, and
four placed on the ground.
Figure 4: Overview of theVis Insitaperformance.
The displacements and rotations of this IrT acted dur-
ing the ﬁrst quarter of the performance on the tone, height
and distortion of a synth pad. In the ﬁnal part, it modu-
lated dozens of sound processing parameters (glitch, delay,
reverb, phasing, pan, etc.) on the main audio output bus,
and controlled the horizontal and vertical movements of the
automated LED projector located above.
This switch of the parameters controlled by the same tar-
get was made possible thanks to the preset system of the
µZYX interface. These presets, accessible in the main win-
dow of the interface (Figure 3, at the top) and remotely
selectable in MIDI, instantly recalled a set of diﬀerent con-
ﬁgurations for the target concerned, but also for all the
targets used on stage.
For a greater variety of movements and instrumental con-
trols, we decided to implement two other IrTs. The ﬁrst
was ﬁxed on the head of the electric bass guitar (Figure 5,
left), modulating in real time the sound of the latter ac-
cording to its movement on stage, as well as the luminous
intensity of the pendulum. The guitar became therefore
an augmented instrument. The second Irt appeared in the
last third of the performance, when the performer played
the electronic percussion kit (Figure 5, right). It was ﬁxed
on his left wrist and then modulated diﬀerent sound treat-
ments that altered the sounds of these percussions accord-
ing to the three-dimensional movements of the wrist. These
percussions were themselves connected directly in MIDI to
the lighting software in order to turn on some projectors in
a synchronized way with the drum strikes.
5.2 Presentation
The public presentation for the Journ´ ee Science et Musique
event was performed in front of an audience of about one
hundred and ﬁfty people, and lasted about twenty minutes.
Figure 5: Left: target on bass guitar; Right: target
on performer’s wrist.
For this event, the Company ART provided us with the
latest generation equipment (ARTTrack5) equipped with
POE (Power Over Ethernet) technology, reducing the sys-
tem’s assembly time, and providing a higher refresh rate
(150Hz instead of 60Hz with the ARTTRACK2 system),
improving the quality of tracking and reducing overall la-
tency by about 10ms.
6. DISCUSSIONS
6.1 MCIs in scenic context
One of the main technical unknowns before the ﬁeld tests
was to know to what extent an MoCap system designed
for the ﬁelds of virtual reality, animation cinema or biome-
chanical research, would be adaptable to an environment
for which it was not planned at all: the stage.
First, since the system was de facto sensitive to infrared
radiation, the ﬁrst question was whether it would be aﬀected
by incandescent light sources installed on stage, which, in
addition to the visible spectrum, also emit in infrared wave-
lengths (850nm). After tests and adjustments, it turned
out that this did not aﬀect the quality of the tracking, the
system being robust enough to recognize the geometry of
the targets despite the presence of many parasitic detection
points.
If MCIs in live conditions oﬀer a number of expressive
and artistic perspectives that we will see later, they raise
spatial questions for the performer that do not exist for a
standard instrument.
In the case of an MCI, it is necessary to clearly deﬁne the
two universes to which we have to deal: the real universe -
the room, the stage, the physical place of the presentation -
and the computer universe, which has its own size ranges ac-
cording to the transfer protocol considered (from 0 to 127
in MIDI and from 0 to 1 in OSC in the case of µZYX ).
These two universes are not isomorphic since diﬀerent cal-
ibrations for each dimension as well as non-linear transfer
curves (exponential, logarithmic, inverted, etc.) can occur.
One of the main tasks in designing the µZYX software
interface was therefore to allow an easy transposition from
the physical to the digital universe. We used six horizontal
sliders (Figure 3, bottom), three for positions and three for
rotations, allowing both to visualize the modulations of the
input signal with the black cursor and to set an output range
(in cyan blue).
These essential spatial data calibration operations can be
tedious and daunting because they require the performer
to be present on stage to position and move the IrTs while
269
having to work on the µZYX interface. In order to make
the MCIs viable in stage conditions, a lot of work had to be
done on the creation of a clear, light and intuitive graphical
interface, as well as on the possibilities of controlling this
interface wirelessly and remotely via a Bluetooth alphanu-
meric keyboard.
Beyond this problem, MCIs seem to oﬀer interesting po-
tential in terms of space. Indeed, the evolution zone of
musicians is often constrained by the ﬁxed nature of the
electronic instruments and control interfaces they use, espe-
cially in electronic music [8]. Most of the time, they remain
standing behind them, and have very little places to move.
Although the MCIs are spatially limited by the detection
zone of infrared cameras, we can see that the evolution -
and therefore expression - zone can become much larger
and act on macroscopic levels [13] order of several meters.
6.2 Autonomous movements
As we saw in the introduction, the most widespread ex-
amples of musical applications of MoCap are particularly
relevant to dance, with technological and aesthetic issues in
this ﬁeld becoming well known and analysed.
Of the three targets used during the performance, the
target of the wrist is certainly the one that most closely
resembles a case of body capture (in this case, a part of the
body) as found in dance. On the other hand, the pendulum
represents a diﬀerent scenario since the capture takes place
on an object that is both manipulated by the performer
(and therefore dependent on his movements) and also has
autonomous movements.
In both cases, the activation energy source remains the
performer. But in the case of the pendulum, the movements
are also related to physical forces during the autonomous
phases (for example when the pendulum makes large circu-
lar trajectories throughout the stage space) such as gravity,
rebound, elasticity, etc., giving it very speciﬁc movement
qualities that the performer (and even a dancer) would most
certainly have great diﬃculty reproducing.
What seems important to us here is therefore the propen-
sity of the pendulum - and kinetic objects in general - to gen-
erate natures of movement (and therefore parameter con-
trol) that are diﬃcult to obtain in any other way, whether
in their regularity, scale, complexity, velocity, or regular
damping, for example.
This opens up interesting interpretation perspectives for
a performer: the inﬂuence on sound parameters is no longer
directly linked to his/her gestures; it is mediated by an in-
terface with its own physical and kinetic characteristics that
he/she can creatively exploit. This type of relationship in-
volves a soniﬁcation of movement and an intentionality on
the part of the performer in the trajectories he/she imprints
on the object. In return, the object’s behaviour inﬂuences
the performer’s actions.
The work of learning beforehand becomes essential: like
a juggler with clubs or a puppeteer with articulated charac-
ters, by working at length with the object and taking over
its kinematics, the performer ends up knowing the slight-
est reactions and can play with them in a very subtle way.
There is then interaction between him/her, the object and
the music. If it is a dance, then it is neither a solo nor a duo,
but a dynamic and creative trio, using both improvisation
and an important kinesthetic mastery.
In addition, the dissociation of the sound generator (vir-
tual synthesizers and sound engine) from the action device
(e.g. the pendulum) oﬀers the possibility of adapting the
latter to both the morphology and the proprioception of
the musician. The interface is conﬁgurable and customiz-
able regardless of the nature of the sound to be generated.
6.3 A relational interface
We have seen that the choice of manufacture of the pendu-
lum, with its ﬁns entangled in each other, was to allow easy
gripping in diﬀerent ways.
If this was the case during the performance, it turned out
that the aﬀordance of the object was not satisfactory, the
number of plates and the rotation of the pendulum on its
vertical axis sometimes making it diﬃcult to grasp and an-
ticipate. This diﬃculty was felt by some spectators: even
without having had the opportunity to manipulate the pen-
dulum, they talked, for example, about their feeling of ”fear”
that the object would fall out of the performer’s hands, or
that he/she would not be able to catch it.
This phenomenon, called ”motor resonance” by percep-
tion neurophysiologists [16], describes the empathic rela-
tionship that occurs in an individual’s mind through his/her
mirror neurons, when he/she watches another person per-
forms a movement and mentally simulates the same move-
ment within his/her own body.
Thus, in the case of the pendulum, the unstable and dy-
namic relationship between the object and the performer
created a palpable tension on stage. The emotion trans-
mitted was not only through musical expression but also
through anticipation and physical appropriation of the artist’s
gestures.
On the other hand, one of the problems frequently en-
countered during a concert in the ﬁeld of live electronic
music is the doubt that the spectator may sometimes have
between what he/she sees of the performer’s actions and the
real consequences on the sound [8]. Feedback regarding our
performance has shown that if the movements are partly
autonomous, even random, and the sounds heard are per-
ceived as being related to their movements, then the viewer
has oﬀered conﬁdence in this relationship, since he/she con-
siders that it cannot be pre-programmed, or planned in ad-
vance. Just like the string of a guitar that can break at
any moment, creating the random aspect of the interface
on stage would therefore be a way to restore a relationship
of trust between the performer’s actions and what is shown
and heard by the spectator.
6.4 Plasticity of interfaces
Among the various existing motion capture technologies,
optical MoCap is distinguished by the non-invasive aspect
of electronics. The captured part of the MCI then becomes
constitutable of any material and can take on almost any
shape or appearance, the only constraint being that it must
be equipped with an IrT to be detected.
This has the eﬀect of promoting a design based not only
on a technical, functional or technological approach, but
also on plastic and aesthetic considerations.
To think of an instrumental interface with regard to its
own movements is above all to think of an object in the
space it occupies, in its materiality, in its relationship to
the one who manipulates it, but also to the one who looks
at it. It means considering this object in its spatial quali-
ties, its geometry, its lines, its balance, its dynamics, and in
the astonishment and questioning it arouses in the viewer.
The history of visual arts sheds light on a trend that has
insatiably sought an aesthetic of movement in the object:
kinetic sculpture.
Particularly active in Europe and the United States in
the 1950s and 1960s, this movement, led by artists such as
Jean Tinguely, Nicolas Sch¨oﬀer and Alexander Calder, has
constantly challenged the spectator’s perceptive faculties.
Could the instrumental interface not thus tend towards this
fully-ﬂedged art object, capable of combining movement,
sound and visual?
270
In the case of the Vis Insita performance, we have seen
earlier that the pendulum has been designed to be easily
gripped, to be readable by the public, and to have certain
luminous aspects. However, it has also been based on artis-
tic intent: creating a poetry of movement and musical in-
terpretation on stage and shaking up the spectator’s usual
perception in order to encourage multi-sensory listening to
live music.
Through technologies and new interfaces, the role of mu-
sicians is shifting: not only are they becoming luthiers of
their own instruments, but also their ﬁeld of relational po-
tentialities with the spectator is expanding. The instrumen-
tal interface is no longer only used to generate or control a
sound; it becomes an artistic element in itself, an integral
part of an overall creative process.
7. CONCLUSION AND FUTURE WORK
Our initial intention was mainly to evaluate the expressive
and instrumental potential of a MCI designed from a kinetic
object that can be manipulated by a performer, beyond the
capture of his/her own body itself.
There are many tracks and they will deserve to be fur-
ther developed technologically, especially with regard to the
µZYX interface (improvement of the graphic user interface,
preset management, implementation of remote control via
smartphone, etc.).
We would like to conclude this article by focusing on how
the creation of performance has evolved the conceptual ap-
proach of our MCI throughout our research.
Faced with the constraints imposed by the challenge of
a real presentation in public, we have indeed gradually be-
come aware of the possibilities of MCIs in terms of multi-
modal interaction and simultaneous management of hetero-
geneous parameters, with synthesis and sound processing
parameters, but also light parameters.
The prospects for applications of this type of research
can therefore extend well beyond the Vis Insitaproject and
are addressed not only to digital musicians, but also to any
artist (visual artists, dancers, choreographers, etc.) work-
ing on interactive installations requiring tangible, plastic
and original means of human-machine interaction, and even
human-human interaction.
The consideration of the place of presentation and the
proximity to the public led us to have a global reﬂection on
the design of the interface with an ecological approach [12].
We believe that this type of practical approach by con-
fronting concrete situations of meeting the public in less and
less conventional places will deserve to be developed in the
future in order to cross-fertilize experiences and to bring
new theoretical elements to the design of NIMEs.
8. ACKNOWLEDGMENTS
We would like to thank the Company ART for the ART-
TRACK5 system loans, the organizers of the Journ´ ee Sci-
ence et Musique 2018 , and Marin Esnault for his support.
9. REFERENCES
[1] S. D. Arnauld Pierre, P. Mari, A. Pierre, and
D. Trudel. Nicolas Sch¨oﬀer Space, Light, Time. Yale
University Press, 2018.
[2] P. Auslander. Liveness: Performance in a Mediatized
Culture. Routeledge, London, 1999.
[3] M. A. Baytas, T. G ¨oksun, and O. ¨Ozcan. The
perception of live-sequenced electronic music via
hearing and sight. In Proceedings of NIME, 2016.
[4] F. Bevilacqua. 3d motion capture data: Motion
analysis and mapping to music. In In Quentin Stout
and Michael Wolfe, editors, The Sixth Distributed
Memory Computing Conference, pages 562–569.
IEEE, IEEE Computer Society Press, 2002.
[5] F. Bevilacqua, L. Naugle, and I. Valverde. Virtual
dance and music environment using motion capture.
In In Proceeding of IEEE Multimedia Technology And
Applications Conference, 2001.
[6] A. Borchardt-Hume. Alexander Calder Performing
Sculpture. Yale University Press, 2016.
[7] E. Bourdages. Les Sculptures Cin´ etiques de Jean
Tinguely. Editions universitaires europeennes EUE,
2010.
[8] M. J. Butler. Playing with something that runs:
technology, improvisation, and composition in DJ and
laptop performance. Oxford University Press, New
York, 2014.
[9] A. Camurri, B. Mazzarino, M. Ricchetti, R. Timmers,
and G. Volpe. Multimodal analysis of expressive
gesture in music and dance performances. In
A. Camurri and G. Volpe, editors, Gesture Workshop,
volume 2915 of Lecture Notes in Computer Science ,
pages 20–39. Springer, 2003.
[10] C. Dobrian and F. Bevilacqua. Gestural control of
music: Using the vicon 8 motion capture system. In
Proceedings of the 2003 Conference on New Interfaces
for Musical Expression, NIME ’03, pages 161–163,
Singapore, Singapore, 2003. National University of
Singapore.
[11] A. Freed and A. Schmeder. Features and future of
open sound control version 1.1 for nime. In In
Proceedings of the 9th Conference on New Interfaces
for Musical Expression, pages 116–120, 2009.
[12] M. Gurevich and J. Trevi˜ no. Expression and its
discontents: Toward an ecology of musical creation. In
Proceedings of the 7th International Conference on
New Interfaces for Musical Expression , NIME ’07,
pages 106–111, New York, NY, USA, 2007. ACM.
[13] A. R. Jensenius. Microinteraction in Music/Dance
Performance. In Proceedings of the International
Conference on New Interfaces For Musical
Expression, Baton Rouge, LA, 2015.
[14] I. Newton and G. de Breteuil Du Chˆ atelet.Principes
math´ ematiques de la philosophie naturelle. Number
vol. 1 in Principes math´ ematiques de la philosophie
naturelle. Desaint & Saillant, 1759.
[15] K. Nymoen, S. A. Skogstad, and A. R. Jensenius.
Soundsaber — a motion capture instrument. In
Proceedings of the International Conference on New
Interfaces for Musical Expression , pages 312–315,
Oslo, 2011.
[16] G. Rizzolatti and C. Sinigaglia. Mirrors in the Brain:
How Our Minds Share Actions and Emotions . Oxford
University Press, New York, 2008.
271