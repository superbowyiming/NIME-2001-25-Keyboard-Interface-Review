Frontiers: Expanding Musical Imagination With Audience Participation  Marcelo Gimenes, Pierre-Emmanuel Largeron, Eduardo R Miranda Interdisciplinary Centre for Computer Music Research (ICCMR) Plymouth University {marcelo.gimenes, pierre-emmanuel.largeron, eduardo.miranda}@plymouth.ac.uk   ABSTRACT This paper introduces Performance Without Borders and Embodied iSound, two sound installations performed at the 2016 Peninsula Arts Contemporary Music Festival at Plymouth University. Sharing in common the use of smartphones to afford real-time audience participation, two bespoke distributed computer systems (Sherwell and Levinsky Music, respectively). Whilst the first one implements a cloud-based voting system, the second implements movement tracking and iBeacon-based indoor-positioning to control the choice of soundtracks, audio synthesis, and surround sound positioning, among other parameters. The general concepts of the installations, in particular design and interactive possibilities afforded by the computer systems are presented.  Author Keywords Sound installation, audience participation, smartphone  ACM Classification [Human-centered computing] Gestural input, [Applied computing] Sound and music computing.  1. INTRODUCTION Performance Without Borders and Embodied iSound are two sound installations (‘the installations’) performed at the last edition of the Peninsula Arts Contemporary Music Festival at Plymouth University, which in 2016 celebrated 11 years of musical innovation. The general theme of the festival (“Frontiers: expanding musical imagination”), provided the axis around which a number of approaches to music creation were presented, including unconventional computing [1], music opportunities for motor-disabled individuals with brain-computer music interfaces [2] and audience participation.  This paper introduces the general concepts of the installations, in particular the design and interactive possibilities afforded by two bespoke computer systems that were specially implemented to support them: Sherwell and Levinsky Music. 1.1 Background Among the myriad of technological possibilities currently available for developing new digital musical instruments [3], mobile technologies are an area that is increasingly receiving attention. It is not difficult to see why: particularly since the introduction of the iPhone by Apple in 2007, including generic processors and memory, smartphones can easily be defined today as ‘computers that make phone calls’. What makes (current models of) smartphones especially interesting, however, is a combination of their technical capabilities (mainly sensors) with the central (personal, portable and unobtrusive) role they currently play in people’s lives. Smartphone sensors enable the communication with the world in various ways, offering "huge 
potential to gather precise, objective, sustained, and ecologically valid data on the real-world behaviours and experiences of millions of people” wherever they are [4, p. 221].  Virtually everything done with smartphones (including time, location, etc.) can be detected and traced, from communication data (phone calls and instant messaging) to the use of each and every installed application, including map navigation, web browsing, and audio/video streaming. On the one hand, this fact can potentially give rise to difficult privacy issues but, on the other, to great opportunities for sound production and control. Smartphones’ audio (microphone/speakers) and photo/video capabilities, for instance, can be used to collect sound and image from the environment with direct application in many musicological investigations. Examples include research in (sound/timbre qualities of) instrument performance or the perception of (algorithms and synthesis of) the output in music generative applications. It is also possible to know about the general conditions of the environment [5], looking for particular sound patterns, noise level, or light conditions that can help to determine where the user is at a particular moment (context of activity) [6] or the impact these conditions have on other measurements, such as physical and emotional states.  In addition to the above mentioned possibilities, smartphones1 also offer a convenient and rich set of tools for music performance with audience participation and a number of interactive and sound production techniques can already be found in the literature. To name just a few, for example Hindle’s research [7], explores audience participation via online tools (web portal) and local networking. In this case, data is sent from mobile devices as digital music material. With a different approach, Oh and Wang [8] use text-based data streams taken from social networks to create a musical discourse. Wang, Oh et al. [9], exploring mobile devices’ technical capabilities and data crowdsourcing for musical applications, reported the implementation of a mobile-based system that reaches audiences via popular Internet messaging services. Hödl, Kayali and Fitspatrick [10] introduced an interactive system for audience participation based on smartphones to control the stereo panorama of an electric guitar. Essl and Rohs [11, 12] proposed a formal analysis of mobile phones’ sensor capabilities (including microphones, accelerometers, multi-touch screens and magnetometers), to evaluate how they can facilitate interactive performances. As the technologies and processing power embedded in smartphones continue to expand, we believe these devices continue to be an excellent tool to explore new possibilities in live interactive performances, especially with audience participation. 2. THE INSTALLATIONS The contribution of Plymouth University’s installations to the field reported in this paper consists of using mobile devices as part of distributed systems devised to support specific aesthetical ideas. In both installations smartphones are operated by members of the                                                                     1 The terms ‘smartphones’ and ‘mobile devices’ (including tablets and wearable devices, such as smartwatches) are used interchangeably in this paper. 
350
public, who control in real time some parameters of the audio generative engines.   During the performance, the smartphone app communicates with a server running on a desktop computer which, in turn, processes the data received from the mobile devices and, following its own internal logics defined algorithmically by the composers, control the sound produced. In summary, these systems follow the paradigm defined in many of today’s performance systems, by decoupling control and sound generation but leaving part of the control to the public.  The installations share a common general design but address different compositional ideas. For this reason, specific pairs of smartphone/desktop systems had to be implemented for each one of them. Performance Without Borders is supported by a system called Sherwell and Embodied iSound by a system called Levinsky Music2. The particularities of each one of these systems and how they afford their individual composition concepts, and audience participation are better explained in the sections below. 2.1 Performance Without Borders The main research question behind Performance Without Borders is the relationship between social and personal identifications to music performance in live audience participation conditions, i.e., whether one could determine particular emerging behaviour (in the use of the control system) depending on the type of music on one side and, on the other, the participants’ musical background. Data generated during the installation is currently being used to establish thresholds as how these two poles operate. The analysis of this data, however, will be the object of future publications, as it goes beyond the scope of this paper. 2.1.1 The composition Departing from the general theme proposed by the Peninsula Arts Contemporary Music Festival, “Frontiers”3, the composers devised a musical storyline to allow a dialogue between the audience and two live performers playing piano and violin, conveying the idea of a rupture of the divide that (more often than not) exists between audience and performers on stage.  The initial musical concept was to use John Cage’s minimalistic atmosphere to shape a basic sonic landscape in pre-recorded soundtracks, on which the composers/performers added their own musical improvisation. These soundtracks served as a basis for the audience participation, which then decided which track to play, when, and the effect played within a certain time lapse in the performance.  John Cage’s composition Dream for piano (1948) was used to create a soundtrack (‘participative file’) that could fit the piece at any point in time. An extra instrumentalist’s improvisation, on violin, was added before the audience participation took place, as a transition between solo piano and trio (audience, violin and piano) performance. The structure of the performance was set as a continuation between a first part, played by on-stage musicians and a second part dedicated to audience participation with live musicians’ participation. The audience was guided through the performance by an on-screen light signal which indicated the appropriate time for participation through the mobile devices’ interface. Figure 1 shows a general overview of the structure of the composition. 
                                                                     2 The iOS implementation of these systems is freely available for download from the iTunes App Store and a link to them can be found at http://computermusiclab.com/ 3 More information about the Festival can be found on the Interdisciplinary Centre for Computer Music Research website: http://cmr.soc.plymouth.ac.uk/event.htm 
Figure 1: Performance Without Borders’ composition structure (p: piano, v: violin, a: audience) 2.1.2 The system As mentioned above, a distributed computer system (Sherwell) was implemented to support the interaction during the Performance Without Borders installation. The main components of this system are a smartphone app (for iOS devices), a server and a Max/MSP patch. Figure 2 shows the main interface of the Sherwell app:  
 Figure 2: Interactive view of the Sherwell app  Participants chose from one of five buttons on the interface and, for each one of them, cast (tap) their ‘votes’ (a point with corresponding ‘x’ and ‘y’ coordinates) on the central square. This process continues throughout the performance as participants cast their votes according to their preferences (like/dislike) in response to the effects their participation had on the production of the sound generated in real time. Every time participants tap on the central square, the Sherwell app sends the information pair parameter/vote to a database on the Internet (the ‘cloud’). The server, running on a desktop computer, continuously (every number of seconds, configurable on the server’s user interface) gather the votes of the participants and calculates, based on the last ‘n’ number of votes (also configurable on the server’s UI), the ‘final vote’. The final vote for each one of the parameters is then used by the Max/MSP patch to control the behaviour of the sound synthesis in real time. Figure 3 shows a general overview of the Sherwell system. 
351
 Figure 3: The Sherwell system  2.1.3 Audience participation Before the performance, participants were informed about the use of the app and the role of each one of the parameters that were being controlled. Participants were also informed that data would be anonymously collected and used by the second author in his academic research at Plymouth University on the sociological and psychological mechanisms involved in audience participation.  A crucial component of the system, obviously, is the algorithm that calculates the final votes of each one of the five parameters mentioned above as this plays a major role in defining the sense of control the audience perceives during the performance. This aspect is also one of the subjects of research of the second author.  Finally, it is relevant to notice that, given the fact that the Sherwell system uses online cloud technology for further development of live music parameter control by an audience, potentially anyone in the world would be able to participate at the interactive experience. Of course, in this case, additional resources (e.g., video conference) would have to be implemented in order to allow participants to virtually attend the installation. 2.2 Embodied iSound The general theme of the 2016 Peninsula Arts Contemporary Music Festival, “Frontiers”, is also at the heart of the Embodied iSound installation. In addition to audience participation, some of its major concepts are the principles derived from Embodied Music Cognition [13]. Music is “organised sound” [14, p. 192], of course, but it is also a medium through which we all communicate. It is therefore important to understand how this communication happens, why and what we communicate through music. In this context, new technological tools can be used to enhance our natural ability to communicate musically [3]. The general proposition is that we can only achieve a better understanding about the way we communicate through music by mentally, physically and emotionally experiencing what happens when we cross (stylistic, geographical, and historical) musical frontiers. Technology is what allows this participation, mediating what happens between the physical world (the sound), our minds and personal enjoyment. 
 Another source of inspiration comes from the work done by Musicians Without Borders4, an international organisation based in Amsterdam that promotes programs and activities with local musicians and groups in communities that suffer from the effects of conflict. In MwB’s own words: “using the power of music to bridge divides, connect communities, and heal the wounds of war” [15]. 2.2.1 The composition The ‘soundtrack’ (composition) of the installation consists of an algorithmic element (a Pd patch) and an electroacoustic element (running on Logic Pro). These components were conceived to convey the idea of stylistically-defined regions that would be explored by the audience during the performance. According to the choices made, some of these regions would be reinforced while others be softened. The expected perception of this dynamics would metaphorically correspond to the experience of crossing stylistic frontiers.  To achieve this effect, the compositional challenge was to define a common musical thread that would support (as opposed to having disconnected ideas that would eventually compromise) the work’s unity. This difficulty (or risk) was increased by the fact that the actual result of the audience’s participation would only be known during the performance itself. 2.2.2 The system To achieve the objectives of the composition, the installation requires that members of the audience become active participants (as opposed to just passive listeners) of the musical experience, controlling the sound that is produced from the space where it is projected. Embodiment is not just about the body, it is about action and participation [16].  As with the Performance Without Borders installation, the Embodied iSound installation is also supported by a distributed computer system, in this case called Levinsky Music, and which includes a smartphone app running on participants’ iOS devices (the ‘app’) and a server running on a desktop computer (the ‘server’).   To better clarify how the system is organised, Figure 5 at the end of this paper shows a sketch of the space on the ground floor of the Roland Levinsky Building known as the Crosspoint. Participants holding their smartphones are invited to walk around this space while they listen to the composition. Whilst they do so, data generated by the smartphones is continuously sent to the server. This includes the participants’ proximity relative to the six columns where six beacons are positioned (one in each column), gyroscope information and button taps. The volume (and therefore the relevance) of each one of the tracks of the composition is defined by the percentage of participants that are next to each one of the beacons in the space. Information from the gyroscope (e.g., “yaw”), control the position (surround angle) of the sound source (the track) in the quadraphonic speaker system. Figure 4 below shows the interactive view of the smartphone app. Taps on the only button on the main interface triggers different sound events on the Pd patch.  The protocols used for communication between the various components of the Levinsky Music system are the ones commonly used in other music applications: MIDI (between the server and Logic Pro) and OSC (between the smartphone apps and the server and between the server and Pd). 
                                                                    4 Musicians Without Borders kindly authorised the use of audio taken from some of its projects as part of the Embodied iSound composition. 
352
 Figure 4: Levinsky Music app interactive view 2.2.3 Audience participation As with the Performance Without Borders installation, participants were informed before the performance about the effects their movements would produce on the sound controlled in real time. They were also informed that data would be anonymously collected and used by the first author in his academic research at Plymouth University on the mechanisms involved with audience participation. The outcome of the analysis of this data will be reported in future publications.  Given the characteristics of the installation, in particular regarding the unexpected results that would potentially be produced by the dynamics defined by the audience movements during the performance, the Levinsky Music system implements a number of algorithms that specifically define whether control is attributed to individual participants or shared between all of them at any specific moment. This discussion around shared control is well know in the community of system designers. For this particular installation, Levinsky Music implements two different approaches, depending on specific performance parameters. At any given moment, for instance, data from all the participants contribute to determining some of these parameters (e.g., track relevance) but only data from specific participants determine another set of parameters (e.g., surround angle of sound sources).   In addition to that, what most particularly distinguishes the Embodied iSound installation (and the Levinsky Music system) from other interactive experiences with audience participation is the fact that, in addition to transmitting information from their smartphones, participants also receive instructions from the server in real time in the form of vibrations, flashes and changes of colour on the background and the button of their devices. Of course, a number of different scenarios with different levels of control can be devised, depending on the aesthetical values and technological needs of any particular case.  In summary, the Levinsky Music system works as a hub where performance data is transmitted, received and processed. As ‘real instruments’, in one end of the chain, smartphones become an extension of the performers’ bodies. The server, in the other end of the chain, processes the control data collectively produced by the performers and sends back instructions when appropriate, allowing an interactive experience supported by a two-way communication between the music engine and the performers 3. CONCLUSION Plymouth University’s sound installations demonstrate two cases in which music results from the interaction between sound producing 
machines and audience control. This paper mainly focused on the general concepts of the installations, the design of the computer systems Sherwell and Levinsky Music, as well as the interactive possibilities afforded by them. The technology that allowed this experience was carefully designed so that specific compositional needs were addressed. Given a combination of their technical capabilities with the central role they play in people’s lives, smartphones are particularly suitable in cases like these, where members of the audience become active participants of the musical experience as opposed to just passive listeners. 4. REFERENCES [1] Braund, E. and E. Miranda, Music with Unconventional Computing: Towards a Step Sequencer from Plasmodium of Physarum Polycephalum, in Evolutionary and Biologically Inspired Music, Sound, Art and Design. 2015, Springer. p. 15-26. [2] Miranda, E.R. and J. Castet, Guide to Brain-Computer Music Interfacing. 2014: Springer. [3] Miranda, E.R. and M.M. Wanderley, New Digital Musical Instruments: Control and Interaction Beyond the Keyboard. 2006, Middleton, Wisconsin: A-R Editions, Inc. [4] Miller, G., The smartphone psychology manifesto. Perspectives on Psychological Science, 2012. 7(3): p. 221-237. [5] Feitshans, T. and R. Williams. Smartphones for distributed multimode sensing: biological and environmental sensing and analysis. in SPIE Defense, Security, and Sensing. 2013. International Society for Optics and Photonics. [6] Brajdic, A. and R. Harle. Walk detection and step counting on unconstrained smartphones. in Proceedings of the 2013 ACM International Joint Conference on Pervasive and ubiquitous computing. 2013. ACM. [7] Hindle, A. Swarmed: Captive portals, mobile devices, and audience participation in multi-user music performance. in 13th International Conference on New Interfaces for Musical Expression. 2013. [8] Oh, J. and G. Wang, Audience-participation techniques based on social mobile computing. 2011: Ann Arbor, MI: MPublishing, University of Michigan Library. [9] Wang, G., et al. World Stage: A crowdsourcing paradigm for social/mobile music. in International Computer Music Conference. 2011. [10] Hödl, O., F. Kayali and G. Fitzpatrick (2012). Designing interactive audience participation using smart phones in a musical performance. International Computer Music Conference. Ljubljana, Slovenia. [11] Essl, G. and M. Rohs. ShaMus: A Sensor-Based Integrated Mobile Phone Instrument. in International Computer Music Conference. 2007. [12] Essl, G. and M. Rohs, Interactivity for Mobile Music-Making. Organised Sound, 2009. 14(02): p. 197-207. [13] Leman, M., Embodied music cognition and mediation technology. 2008: The MIT Press. [14] Varèse, E., Edgard Varèse on Music and Art: A Conversation between Varèse and Alcopley. Leonardo, 1968. 1(2): p. 187-195. [15] MWB. Musicians Without Borders. 2016  21/01/2016]; Available from: https://www.musicianswithoutborders.org. [16] Dourish, P., Where the action is: the foundations of embodied interaction. 2004: MIT press.  
353
 
 Figure 5: The Crosspoint    
354