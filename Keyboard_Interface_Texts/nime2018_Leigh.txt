Guitar Machine: Robotic Fretting Augmentation for Hybrid Human-Machine Guitar Play   Sang-won Leigh MIT Media Lab  75 Amherst St, Cambridge MA 02139, USA sangwon@media.mit.edu 
 Pattie Maes MIT Media Lab  75 Amherst St, Cambridge MA 02139, USA pattie@media.mit.edu 
 ABSTRACT Playing musical instruments involves producing gradually more challenging body movements and transitions, where the kinematic constraints of the body play a crucial role in structuring the resulting music. We seek to make a bridge between currently accessible motor patterns, and musical possibilities beyond those – afforded through the use of a robotic augmentation. Guitar Machine is a robotic device that presses on guitar strings and assists a musician by fretting alongside her on the same guitar. This paper discusses the design of the system, strategies for using the system to create novel musical patterns, and a user study that looks at the effects of the temporary acquisition of enhanced physical ability. Our results indicate that the proposed human-robot interaction would equip users to explore new musical avenues on the guitar, as well as provide an enhanced understanding of the task at hand on the basis of the robotically acquired ability.   Author Keywords Human Augmentation; Robotic Musicianship; Human-Robot Interaction  CCS Concepts • Human-centered computing →  Human computer interaction (HCI) →  Interaction devices  1. INTRODUCTION The interrelationship between human movement and musical structure has long been known and studied in ethnomusicology [13][15]. The ergonomic constraints of an instrument carry with them certain intrinsic modes of operation, shaping the structure of the music and channeling creativity in predictable directions. This also has been investigated through case studies on novel instruments such as the dutâr and rubâb [14]. The avant-garde instrument, Pikasso guitar (Linda Manzer, 1984), had four necks, two sound holes, and 42 strings [3]. It was used by Pat Metheney in several recordings and performances, where he integrated the inventive instrument into 
innovative musical pieces [21]. His performance embodies the idea that musical instruments can be viewed as “aids for musical thinking and planning” in the cognitive loop of a musician’s brain [12].  The prior research studies the embodiment of musical tools and the way it governs how our hands interact upon them. However, instead of changing the affordances [29] of an instrument, augmenting what a person can do on the instrument poses another opportunity in expanding the musical palette. Guitar Machine is a robot working in synchrony with a musician’s hand, that aims to enable novel movement patterns on the guitar. The robot sits on the neck of a guitar and performs fretting actions (figure 1), where the musician combines the movements of both his and the robotic hands to generate music.  The design of the robot taking a similar role and kinematics to the hand aims to let both human and machine work in the same physical dimensions of the instrument. Therefore, a musician or a learner focuses on the physicality of the instrument (in contrast to other sonic augmentation systems [8][23][25]) and maintains control of the overall musical process. Prior works including Robotar [7] and other guitar-playing automatons [11] explored motorized fretting, however, they are preprogrammed or triggered by modalities outside the playing hands. In contrast, our system aims to combine information about human actions on the fretboard. Therefore, the proposed system would respond accordingly and promptly, by executing note patterns upon certain human fretting actions or by choosing fretting sequences based on the intended chord.  This paper presents the implementation of the Guitar Machine, and illustrates different strategies on how to utilize the system. We also discuss a user study on how such a robotic augmentation working in parallel with the human hands can affect possible actions on the instrument as well as musical cognition. In the user study, we observed how beginner guitar players use our system, and analyzed how the robotic augmentation induces changes in the way they behave and perceive the musical task. The goal of this study is to have a preliminary observation on whether (and how) a technologically acquired motor ability can help users achieve understanding of musical patterns beyond their current ability. Our results support the hypothesis and suggest that the system increases user confidence resulting in a more satisfying learning experience, as well as enabling spontaneous and exploratory activities on the guitar. 
Figure 1: a) Guitar Machine plays alongside a human musician allowing the user to create note combinations that are not accessible, b) consists of robotic fingers of configurable length, c) and press-fits onto the neck of a regular electric guitar. 
NIME Proceedings Template for LaTeX
Ben Trovato⇤
Institute for Clarity in
Documentation
1932 Wallamaloo Lane
Wallamaloo, New Zealand
trovato@corporation.com
G.K.M. Tobin†
Institute for Clarity in
Documentation
P .O. Box 1212
Dublin, Ohio 43017-6221
webmaster@marysville-
ohio.com
Lars Thørväld‡
The Thørväld Group
1 Thørväld Circle
Hekla, Iceland
larst@afﬁliation.org
Lawrence P . Leipuner
Brookhaven Laboratories
Brookhaven National Lab
P .O. Box 5000
lleipuner@researchlabs.org
Sean Fogarty
NASA Ames Research Center
Moffett Field
California 94035
fogartys@amesres.org
Anon Nymous
Redacted
8600 Datapoint Drive
San Antonio, Texas 78229
cpalmer@prl.com
ABSTRACT
This paper provides a sample of a LATEX document for the
NIME conference series. It conforms, somewhat loosely, to
the formatting guidelines for ACM SIG Proceedings. It is an
alternate style which produces atighter-lookingpaper and
was designed in response to concerns expressed, by authors,
over page-budgets. It complements the documentAuthor’s
(Alternate) Guide to Preparing ACM SIG Proceedings Us-
ing LATEX2✏ and BibTEX. This source ﬁle has been written
with the intention of being compiled under LATEX2✏ and
BibTeX.
To make best use of this sample document, run it through
LATEX and BibTeX, and compare this source code with your
compiled PDF ﬁle. A compiled PDF version is available to
help you with the ‘look and feel.’The paper submit-
ted to the NIME conference must be stored in an
A4-sized PDF ﬁle, so North Americans should take
care not to inadvertently generate letterpaper-sized
PDF ﬁles.This paper template should prevent that from
happening if thepdflatex program is used to generate the
PDF ﬁle.
The abstract should preferably be between 100 and 200
words.
Author Keywords
NIME, proceedings, LATEX, template
CCS Concepts
•Applied computing! Sound and music comput-
ing; Performing arts; •Information systems! Music
retrieval;
⇤Dr. Trovato insisted his name be ﬁrst.†The secretary disavows any knowledge of this author’s ac-
tions.‡This author is the one who did all the really hard work.
Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’18,June 3-6, 2018, Blacksburg, Virginia, USA.
Please read the comments in the nime-template.tex
ﬁle to see how to create the CCS Concept Classiﬁ-
cations!
1. INTRODUCTION
The proceedingsare the records of a conference. ACM seeks
to give these conference by-products a uniform, high-quality
appearance. To do this, ACM has some rigid requirements
for the format of the proceedings documents: there is a
speciﬁed format (balanced double columns), a speciﬁed set
of fonts (Arial or Helvetica and Times Roman) in certain
speciﬁed sizes (for instance, 9 point for body copy).
The good news is, with only a handful of manual set-
tings,1 the LATEX document class ﬁle handles all of this for
you.
The remainder of this document is concerned with show-
ing, in the context of an “actual” document, the LATEXc o m -
mands speciﬁcally available for denoting the structure of a
proceedings paper, rather than with giving rigorous descrip-
tions or explanations of such commands.
2. THE BODYOF THE PAPER
Typically, the body of a paper is organized into a hierar-
chical structure, with numbered or unnumbered headings
for sections, subsections, sub-subsections, and even smaller
sections. The command\section that precedes this para-
graph is part of such a hierarchy.2 LATEX handles the num-
bering and placement of these headings for you, when you
use the appropriate heading commands around the titles of
the headings. If you want a sub-subsection or smaller part
to be unnumbered in your output, simply append an aster-
isk to the command name. Examples of both numbered and
unnumbered headings will appear throughout the balance
of this sample document.
Because the entire article is contained in thedocument
environment, you can indicate the start of a new paragraph
with a blank line in your input ﬁle; that is why this sentence
forms a separate paragraph.
1Two of these, the \numberofauthors and \alignau-
thor commands, you have already used; another,\bal-
ancecolumns, will be used in your very last run of LATEX
to ensure balanced column heights on the last page.2This is the second footnote. It starts a series of three
footnotes that add nothing informational, but just give an
idea of how footnotes work and look. It is a wordy one, just
so you see how a longish one plays out.
403
2. RELATED WORK In order to contextualize our research, we discuss prior works in this section. We categorized the works into two categories, discussing robotic and digital means to enhance acoustic instruments. 2.1 Robotically Augmented Instruments Research in robotic musicianship [16][22] explores systems that automate musical instruments, sometimes utilizing them in musical practices or performances, e.g. the Orchestrion [6] or AUTOMATICA [9]. Robots for guitar include LEMUR [2], Z-Machines [11], and instruments by Maywa Denki [5].  These systems use mechanical actuations to create novel musical expressions that are different from or impossible by regular means. However, the robots are designed to be fully autonomous and replace human musicians.  In contrast, little effort has been invested in exploring human and machine actions on the same musical instrument, at the same time – the majority of existing works focused on piano [10][30][32][33].  The prosthetic robotic arm for drumming [17] has two drum sticks attached – one is controlled by the human drummer and the other one by a machine. For the guitar, Ogata and colleagues [28] created a hammering device that substitutes the plucking hand on the guitar. The robot and a musician divide the task into left- and right- hand actions. Robotar [7] presents the closest implementation to our system, whereas its applications focused on prescript automation of fretting. This reduces the agency of users, and most critically our system attempts to amend this by a closed-loop interaction between humans and the robot via real-time detection of fretting on the guitar. 2.2 Digital Enhancement to Performances Hyperinstruments describe a range of sensor and software systems that expand the musical expressivity of acoustic instruments [24]. One of those instruments, Hypercello, detects musicians’ finger movements, wrist angle, and bow position, and uses these to drive its sonic output. Its design is centered around the way an instrument is used naturally, while providing an expanded palette of sound.  These days such systems are widely used or developed to increase the expressivity of musical instruments. Matthew Bellamy uses his custom guitar, Manson MB-1 [4], that has a MIDI screen controller. Gong et al. [20] integrated capacitive touch sensing onto the top of a ukulele for sonic control, Freed developed a similar system for finger style guitar players [1] and Newton and his colleagues explored how musicians would augment their instruments using electronics [26][27]. Apart from string instruments, keyboards have been designed with extra control modalities through the use of soft material [8][23] or fabric with embedded sensors [31]. Grace Leslie utilizes EEG to generate sounds that blend into her flute performances [25]. From a signal processing perspective, these systems afford means to “post-process” the output of musical instruments, or add sound elements outside the sonic dimension of those instruments.   Piano systems created by Yuksel et al. assist in a musician’s performance within the sonic dimension of the piano [34][35]. Their BRAAHMS system [34] assists a musician’s improvisation with a harmonic addition or removal as a “pleasing enhancement” without compromising the general direction of the music played. These systems rather take the role of a cognitive assistant, only minimally aiding the task in terms of their audio output. 3. IMPLEMENTATION The prototype of Guitar Machine consists of a microcontroller and five robotic finger units. The fingers are servomotors with a motor horn designed to push on guitar strings. A 3D printed platform press-fits onto the guitar letting the mounted motors reach the first two or three frets. The placement and dimension of each finger are fitted to press the strings at the right angle and strength. The tips of the 3D printed motor horns include an extrusion towards the guitar strings so that they make vertical contact with the string. 
 The current arrangement of robotic fingers covering the first two frets is intended to assist in accessing 2nd and 6th intervals, that are normally difficult due to the need of finger stretch. In the C major configuration, five diatonic notes (F, B, E, A, C) can be selected. This allows for playing C, G, Am, F chords by adding a few notes on the third fret (figure 2 left). The E blues scale is supported by setting the robot to play E7, A7add9, B7 chords by itself (figure 2 right). The configuration is not limited to these scales, and a configuration for a specific scale can be created by choosing robotic fingers with different lengths.  A MIDI guitar is used to acquire information about a user’s activity on the guitar. We connected the microcontroller to a guitar neck with integrated resistive sensors on the fretboard (Fender by Squier Rock Band controller) using an SPI interface, and the data gets streamed back to our control software over serial. The overall latency between a fretting event and data reception is within 5 ms, that is critical in enabling real-time interaction with the robot. Upon reception, our software interprets incoming MIDI signals from the guitar and recognizes what notes are pressed by the user. Then, the software collects information such as the notes currently pressed and the history of notes.   The software matches these input events to a list of predefined events, and drives the robot accordingly in order to complement what the user is doing. An event can be simply a combination of notes, and also can be a sequence of notes that signifies specific musical phrases. In the current implementation, we maintain a list of note combinations and/or sequences and their corresponding robotic fretting patterns. The software compares the latest fretting events to the list, and determine the robot’s following action. It can also utilize the information about the current play head position in a backing track and so on, to further diversify the robotic fretting patterns based on the musical context.  4. INTERACTION STRATEGIES Interactions with the robot include automating chord positions with the robot while a human musician adds a melody on top, or moving the robotic fingers in synchronization with the musician – in order to collectively play complex phrases/chords that are normally unachievable. The examples here include ones that a beginner can utilize to practice relatively simple guitar phrases, as well as ones that expand upon and beyond existing styles used in music.  
Figure 3: Chords can be made by combining notes played by the robot and a musician: left) an arpeggio pattern with a chain of 2nd and 3rd intervals can be made easily that is not reachable with bare hands, middle/right) convoluted or unreachable chord shapes and variations can be made easily together with the robot. The dots and arrows in orange illustrate variations of each chord.  
Figure 2: The robot can reach the first two frets on the guitar, where the location of each robotic finger can be reconfigured. The resulting configuration can play chords collaboratively with a musician or by itself. 
404
 The possible maneuvers through such a human-robot interaction (or interplay) are not limited to the presented ones, and are to showcase different strategies when it comes to designing a musical pattern using the additional robotic assistance. Also, please note that the examples shown are particular to the configurations we have and are designed for discussion, while more complex sequences will be possible with an enhanced version of the system with increased number of actuators. 4.1 Complementary Chord Construction Figure 3 illustrates a few extended chord patterns devised through our system, where widespread notes that are not easily accessible can be played together with the help of the robot. The boxes represent the fretboard of the guitar from the head side, where the greyed areas are covered by the robot. Chords can be made at a distinct note configuration, or arpeggio patterns with stacks of 2nd and 3rd interval increment can be played with ease.   Once a few notes are pressed by a user, our software will find what the intended chord would be, and complement the action by playing the rest of the notes. The same strategy can be used for constructing simple chords (figure 2 left) for users of relatively low skill level, where only one or two notes from C, G, or F chords are required to be played by the user. This setup is used in a user study later introduced in this paper. This responsive action by the robot is a critical part in our design, as it allows users to follow their own pace as well as to experiment freely with different sequences – instead of following a preset progression. We used simple one-to-one mappings between user and robotic fretting in the user study and in the demonstration, but the software can also take the planned chord progression into account to adaptively decide resulting actions by the robot.  The benefit of hybrid chord construction is not only the possibility to create complex note combinations, but also that one can easily switch individual notes in an already held chord. For example, the Fadd9(b5) chord shown is technically accessible if a musician has a large hand span. However, switching between the flat 5th and major 3rd notes (arrows in the figure), or between major 7th and 9th notes (arrows in the figure), still requires shuffling multiple fingers at the same time. Our system can greatly simplify these transitions, in other words, it makes utilizing or exploring such variations of chords more accessible and natural. We also observed in our user study that beginner-level participants tried adding notes outside the chord progression they were instructed to play.  4.2 Progression Guidance Figure 4 illustrates a funk guitar example, where the robot keeps the base chord progression. The musician can mix bass notes with lead fills at the same time, and switch between robot- and human- played 
notes in quick successions (video figure). This way the musician can continuously have access to the overall progression of the music, while focusing on the lead portion on the higher frets. This results in music with two guitar lines closely coupled – the musician is empowered to have decisions over multiple parts of the music without needing to coordinate with another player.  This strategy differs from other ones in that the robot takes a relatively lead role in the musical progression. However, it is important to note that the particular setup of the guitar, in which fretting actions are only embodied through the sound made by plucking actions, allows for the musician to override or nullify the robot’s actions. This way, the proposed human-robot interaction creates a room for the musician to make creative decisions. 4.3 Complementary Melody Lines Figure 5 shows an example of changing root notes in a four-notes tapping pattern – using both left and right hands for pulling/hammering on the guitar neck. This example is designed to demonstrate how notes played by the robot can interleave with a sequence of notes played by a human musician to enable (potentially) more dynamic melodies.  Tapping patterns with open-string notes usually creates a more dynamic composition, however, regular guitars have limited choice of six for the open-string notes. Using our system, it is possible to use roots outside the six notes, enabling the pattern to be more diverse and dynamically changing. While a musician plays higher notes (figure 5, black dots), root notes can constantly shift to create progressions in the bass (figure 5, red dots). For example, if a segment is supposed be played on A minor chord and on the third string, it is hard to achieve that since the fourth open-string note is G. If we were not to use the robot, either using a different string or omitting the G open-string note is necessary in order to fit into the progression. However, with the A root note played by the robot, we could keep the overall flow of the phrase without needing to find other unwanted alternatives.  The application of this strategy is not limited to tapping sequences, and can potentially be used for creating five notes per string (5 NPS) scale patterns, or making some other 4 NPS patterns to be greatly playable. In the case of ascending notes on a string, however, the control of the robots may need extra attention as the software needs to know which string the musician is skipping to after a maneuver on one string. 4.4 Open String Shift Another way to incorporate our system in a novel composition is to offset open-string notes for shifting standing wave harmonics. Guitarists employ open-string harmonics for unique sonic addition to their music, where normally the notes accessible by 
Figure 4: The robot guides the chord progression while the musician can (a) make quick successions between lead and backing portions, or (b) combine the portions, to sound as if two guitars are playing at the same time.  
Figure 5: Dynamic root note change in our tapping example. The lowest note (marked in red) on a single tapping sequence changes according to the background chord progression. This allows tapping patterns to have more freedom and dynamics. The annotations on the notes represent which finger is used for each note.  
405
such technique are limited to octaves (harmonics at the 5th and 12th frets) or 5th intervals (harmonics at the 7th fret) from the open string notes. These only cover A, B, D, E, and G notes from the 2nd octave, while it is possible to cover all the other notes (#A, C, #C, #D, F, #F, and #G) with the help of the robotic fingers. 5. PRELIMINARY USER STUDY We conducted a user study to examine our hypothesis that a temporary acquisition of motor ability (through a robotic assistance) could lead to an enhanced understanding or thinking around a task, and may as well result in practical improvements. That way, we could formulate future research connecting the hand’s ability to the cognitive unfolding throughout musical (or potentially other) processes – that lie beyond a person’s intrinsic or present limitations in skills or physical aptitude.  The user study consisted of evaluating entry level guitar players for two main reasons: to ensure we have enough participants to have a meaningful number of observation, and to reduce variance in their skill level and styles of playing. A total of 18 participants (nine male and female each with age ranging from 20 to 31) were recruited who had little to no experience with the guitar. The participants were given the task to learn and play parts of one of three songs on a guitar: in order of increasing difficulty, Orange Sky by Alexi Murdoch (G-C-G-C, key shifted from the original song), She will be loved by Maroon 5 (C-G-Am-G), or Hey Jude by the Beatles (C-G- F-C-F-C-G-C). The song for each participant is chosen based on their prior experience with the guitar, so that the task is challenging to the extent the participants are able to play somewhat decently but not perfectly within 30 minutes of practice.  In order to see how the engagement with the robot changes the course of learning, we designed three consecutive phases of practice sessions: 10 minutes with a regular electric guitar (phase 1), 10 minutes with the robot (phase 2 or robotic phase), and 6 minutes without the robot again (phase 3). In the robotic phase, the robot helped the participants playing chords by reducing the number of notes 
they have to press (as described in the previous section). The participants were able to keep the overall coordination of music by controlling the robot’s action by actually fretting on the guitar, with less complexity on the fretting hand. They only needed to press a single note for C and G chords, none for Am, and two for F. This way they can make chord transitions and progressions more easily while maintaining a strong agency.   We interviewed the participants with regards to their experiences, challenges and their perceived competency (10-point scale) after each phase, and also videotaped the entire test sessions to observe their behaviors over the course of the study. We particularly focused on their guitar playing with respect to rhythm, sound control, stroke speed and intensity of their guitar playing, as well as any other noticeable behaviors. 5.1 Results and Discussion 5.1.1 Competency in Musical Skill and Cognition Overall, participants had different rates of learning and initial skill levels, however, more than half (56%) of the participants reported a significant improvement (more than 4-points difference) in their skill between phase 1 and 3. One participant even reported feeling more competent in phase 3 compared to phase 2, and three reported to have similar performances between phase 2 and 3.  Table 1 summarizes our post-experiment survey on the helpfulness of the robot. 7 participants (39%) commented that the robotic phase helped with understanding the sound and chord progression. They explain that the robotic phase offered a reference for the correct sound as well as for the overall flow of the music – “the chord shift sequence became more familiar, and the memory of smooth transition may have helped”. A more important insight is that the augmentation phase did not only provide the physical capability to produce the intended musical patterns, but also provided an enhanced musical cognition as 
Table 1: Comments on how the robot has helped the participants’ practice from interviews and open commentaries. 
Table 2: Summary of reported challenges from the participants after phase 1 and the robotic phase.  
406
commented by p2: “listening ability has increased”. This also supports the initial hypothesis of our research that an augmented ability can lead to better understanding of a (musical) task.  Table 2 summarizes challenges reported by the participants after the first two phases. After the first phase, a majority of participants reported difficulties regarding left-hand movements (67%), chord transition (50%), and sound control (44%). After the robotic phase, 11 participants (61%) reported the rhythm as their main challenge – as opposed to only 3 responses in phase 1. The result implies that the offloading of left-hand movements to the robot help them decompose [19] the learning task into smaller manageable components (such as fretting, rhythm, and chord sound). Participants commented that “the only challenges (left) are my musical capability and rhythm”, and that “the robot gives freedom to try different things”.   However, one result of our study that was unexpected is that a decent portion of participants (6, 33%) showed practical improvements in chord transitions, which also required refinement in left-hand movements. This has also been shown in post-test video analysis, where six participants clearly performed better in chord transitions. We have two hypotheses that could explain this: 1) the robotic phase helped the understanding of the overall flow of the songs, or 2) the robotic phase allowed for practicing the timing of the chord transitions in between strokes – with the reduced number of notes to press. If the former is correct, this result would imply that the improved perception of a task can lead to practical improvements in skill. Studies in the future would be helpful to verify the hypotheses and understand the further effects of robotic assistance in physical tasks. 5.1.2 Engagement and Musical Experiment We also observed participants showing signs of strong engagement to the music – such as humming or moving their body along with the music. This relates to the other participants reporting in their surveys that the robotic phase gives confidence (13, 72%) and enjoyment (4, 22%). One participant (p3) articulated the experience as “the robot takes (me) off conscious learning state, and I started to enjoy”.   Another interesting tendency we observed was increased frequency of explorative maneuvers demonstrated by several participants – trying new techniques or extra notes in combination with the chords used for the test. In the robotic phase, one participant began to experiment with new techniques, such as slapping or plucking on the strings. He would not have been able to do so without the robot, because of his limited experience in playing the guitar. We also observed a few participants trying to utilize the robot’s hammering on the strings for novel effects, in other words, the characteristics of the system facilitated different ways the guitar can be played. These observations preface our research around fast-forward musical explorations not only through making new abilities accessible with the Guitar Machine, but also by observing how “playing with abled-hands” can affect and enhance a user’s overall engagement with the instrument. Our study showed positive initial signs – the participants internalizing the supposed sound and the musical flow, and thinking beyond afterwards. The relationship between afforded mechanisms to produce sound and how users, advanced musicians in particular, shape the course of creative exploration or composition would deserve more in-depth attention in future research. 5.1.3 Confidence and Motivation Many children begin to learn musical instruments each year, however, few continue to play over time [18]. Many factors are known to play a role in this, but motivation is of central importance for those who persist with their training. Our subjects included several participants who gave up playing guitar at some 
point in the past (figure 6), often because of the hard finger movements required, frustration, and the resulting lack of motivation. We have found that the augmented guitar play provided by Guitar Machine helps participants with (re-)vitalizing their motivation and improving their confidence for learning the instrument. Comments from the participants include “I was able to remember a time in my life that I had the feeling of the progression”, “I learned that it is not crazy difficult”, and it “motivated to learn more”. Contrary to the common belief that an augmentation system demotivates users from improving their own ability, the comments support an alternative view that they could instead motivate and ease the users to stick with a challenging learning task. The participants also made suggestions for how to further design a learning process with our system – by “gradually removing the robot”, “tune the robot so a person can pick up the right habit”, or “train particular muscle memory”.   We also observed that all participants immediately began to play rhythmically in the robotic phase, and found the rhythm as a new focus in their practice. The effect may be similar to the use of training wheels on a bicycle, that prevent a rider from falling and help them focus on other skillsets needed for biking (pedaling, steering). Some participants also started to create the correct chord sounds (no buzzing, no stringed accidentally muted) after the robotic phase. As mentioned above, we hypothesize the “reference sound” afforded by the robot functioned as a demonstration to the user, and it could have provided a clear motivation towards and security in attempting to produce the music. 5.1.4 Perception of Technologically Acquired Skills In open commentary, participants described how they perceived the robot in various ways: as a teacher, as a partner, and something that has a strong bond with them. A participant commented that he felt “handicapped” (p5) once phase 3 started. He also mentioned “I was anticipating the robot would switch before me, somehow subconsciously…  (although) obviously I knew that I have to switch before the robot (in phase 2)”. Could this imply that the participants were able to establish a strong connection with the robot, and subconsciously extend their planning-action loop to include the actions by the robot? Ten of the other participants perceived the robot as a partner or teacher – describing that they learned from it or had to consciously coordinate with it through close observation. These somewhat opposing views of the robot-as-an-extension and robot-as-a-partner could be a duality idiosyncratic to robotic systems that act in synchrony with and in response to a user. A robotic augmentation system can take varying degrees of initiative – from following the user’s intention, to providing feedback to the user. 
Figure 6: A ten-year old participant engaging with the system, (left) head-banging and (right) commenting “this is fun, papa” to his father. The study session was conducted under approved COUHES procedures with the supervision of the parents.  
407
6. CONCLUSION In this paper we presented Guitar Machine, a robotic augmentation system that collaboratively plays the guitar with a human musician. The system allows a musician or a learner to perform movement patterns that are not accessible otherwise, and therefore enables musical performances beyond their physiological or skill limits. We showcased use strategies and musical phrases that can only be realized with our system, but are not limited to the ones presented.  The user study examines the effect of our system on the experience of beginners. The results suggest that the system helped the participants decompose and focus separately on multiple aspects of the task, as well as achieve a better musical understanding in terms of rhythm and sound. This work is presented as a case study for a larger topic around robotic augmentation for creative processes. In addition to facilitating improvement and better understanding of a musical task, an augmentation system can enable explorative activities by offloading physical or mental load, and by affording new ways to go about the task with novel characteristics of the system. A future study with more advanced players would be greatly helpful in examining the exploratory aspect of the presented type of systems. And towards that, the Guitar Machine system would need to undergo improvements in terms of enabling more number of actors, other complex sound generating mechanisms, and ways to provide more nuanced control by and feedback to users. 7. REFERENCES [1] A. Freed, Augmented Electric Guitar. http://www.adrianfreed.com/content/augmented-electric-guitar [2] LEMUR. http://www.singerbots.com [3] Linda Manzer. Pikasso Guitar. http://www.manzer.com [4] Manson MB-1 Guitar. https://www.mansons.co.uk [5] Maywa Denki. Ultra Folk ver 2. http://www.maywadenki.com/products/tsukuba/ultra-folk-ver-2 [6] Orchestrion. http://www.patmetheny.com/orchestrioninfo [7] Robotar. http://www.robo-tar.com [8] Seaboard. https://roli.com/products/seaboard/grand-stage [9] N. Stanford. AUTOMATICA. https://nigelstanford.com [10] Yamaha Disklavier. http://www.disklavier.com [11] Z-machines. http://yurisuzuki.com/design-studio/z-machines [12] M. Aho. The Tangible in Music: The Tactile Learning of a Musical Instrument. Routledge. 2016. [13] J. Baily. Music and the Body. In the World of Music; Florian Noetzel GmbH Verlag, VWB - Verlag für Wissenschaft und Bildung, Schott Music GmbH & Co. KG, Bärenreiter. 1995. [14] J. Baily, J.A.R. Blacking.  Research on the Herati Dutar. Current Anthropology. 19(3). pp.610-611. 1978. [15] J. Baily, P. Driver. Spatio-motor thinking in playing folk blues guitar. The world of music. 1992 Jan 1;34(3):57-71. [16] M. Bretan, G. Weinberg. A survey of robotic musicianship. Communications of the ACM. 59(5):100–109, 2016. [17] M. Bretan, D. Gopinath, P. Mullins, G. Weinberg. A Robotic Prosthesis for an Amputee Drummer. 2016. [18] J. Davidson, J. Sloboda, M. Howe. The Role of Parents and Teachers in the Success and Failure of Instrumental Learners. Bulletin of the Council for Research in Music Education, (127), pp 40-44. 1995. 
[19] J.R. Frederiksen, B.Y. White. An approach to training based upon principled task decomposition. Acta Psychologica. 1989 Aug 31;71(1):89-146. [20] N.-W. Gong, N. Zhao, J.A. Paradiso. A Customizable Sensate Surface for Music Control. In Proc. NIME '12. 2012. pp. 417-420. [21] B. Heile, P. Elsdon, J. Doctor. editors. Watching Jazz: Encounters with Jazz Performance on Screen. Oxford University Press. 2016. [22] G. Hoffman, G. Weinberg. Synchronization in Human-Robot Musicianship. In 19th International Symposium in Robot and Human Interactive Communication, IEEE. 2010. pp 718–724. [23] R. Lamb, A.N. Robertson. Seaboard: a new piano keyboard-related interface combining discrete and continuous control. In Proc. NIME '11. 2011. [24] T. Machover, J. Chung. Hyperinstruments: Musically intelligent and interactive performance and creativity systems. In Proc. Intl. Computer Music Conference. 1989. [25] S. Makeig, G. Leslie, T. Mullen, D. Sarma, N. Bigdely-Shamlo, C. Kothe. First Demonstration of a Musical Emotion BCI. In Affective Computing and Intelligent Interaction: Fourth International Conference, ACII 2011. 2011. pp 487–496. [26] D. Newton, M.T. Marshall. Examining How Musicians Create Augmented Musical Instruments. In Proc. NIME’11. 2011. [27] D. Newton, M.T. Marshall. The augmentalist: enabling musicians to develop augmented musical instruments. In Proc. TEI '11. 2011. [28] T. Ogata, G. Weinberg. Robotically Augmented Electric Guitar for Shared Control.  In Proc. NIME '17. 2017.  [29] M. Reybrouck. Musical sense-making and the concept of affordance: an ecosemiotic and experiential approach. Biosemiotics. 2012 Dec 1;5(3):391-409. [30] Y. Tomita, G. Barber. New technology and piano study in higher education: getting the most out of Computer-Controlled Player Pianos. British Journal of Music Education 13, no. 2 (1996): 135-141. [31] I. Wicaksono, J.A. Paradiso. FabricKeyboard: Multimodal Textile Sensate Media as an Expressive and Deformable Musical Interface. In Proc. NIME '17. 2017. [32] X. Xiao, B. Tome, H. Ishii. Andante: Walking Figures on the Piano Keyboard to Visualize Musical Motion. In Proc. NIME’14.  2014. [33] X. Xiao, H. Ishii. 2010. MirrorFugue: communicating hand gesture in remote piano collaboration. In Proc. TEI '11. 2011. [34] B.F. Yuksel, D. Afergan, E. Peck, G. Griffin, L. Harrison, N. Chen, R. Chang, R. Jacob. BRAAHMS: A Novel Adaptive Musical Interface Based on Users' Cognitive State. In Proc. NIME'15. 2015. pp 136-139. [35] B.F. Yuksel, K.B. Oleson, L. Harrison, E.M. Peck, D. Afergan, R. Chang, R. Jacob. Learn Piano with BACh: An Adaptive Learning Interface That Adjusts Task Difficulty Based on Brain State. In Proc. CHI'16. 2016. pp 5372–5384.  8. Appendix Please refer to our supplementary data that includes recordings of samples illustrated in the paper   
408