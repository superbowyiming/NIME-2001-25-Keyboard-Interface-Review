1city1001vibrations : development of a interactive 
sound installation with robotic instrument performance
 
Sinan Bökesoy 
composer &multimedia artist 
Büyükada, Istanbul 
sinan@sonic-disorder.com 
  
 Patrick Adler 
Robot Engineer 
Augsburg, Germany 
paadler@gmx.net 
 
 
ABSTRACT 
 
 “1city1001vibrations” is a sound installation project  of Sinan 
Bökesoy. It does continuous analysis of live  sounds with t h e  
microphones installed on top of significant places  at Bosphorus  
- I s t a n b u l. The transmitted sounds are accompanied by an 
algorithmic composition derived from this content  analysis for 
controlling two Kuka industrial robot arms p erforming the 
percussions installed around them while creating a metaphor 
through an intelligent composition/performance system. This 
paper aims to focus on the programming strategies taken for 
developing a musical instrument out of an industrial robot .  
 
 
Figure 1. The installation venue in Istanbul, July2010  
 
Keywords 
Sound installation, robotic music, interactive systems  
1. INTRODUCTION 
 
Robots capture great interest while creating an existence by 
providing physical and visual cues to the audience. (Figure 1) 
Our aim was creating a metaphor by translating the Bosphorus 
sounds to the vibrations of percussion surface of various ethnic 
percussions. As example applications using robots, there are 
mechanical systems offering one degrees of freedom by 
applying the hit stroke to a fixed p oint of percussion surface 
within the dynamics of a hammer action [3 ][6][7]. We used an 
industrial robot with 6 de grees of freedom offering the 
movement potential close to the human arm. A communication 
protocol with semantic description of the robot control 
commands has been developed  w i t h  a n  i n t e r f a c e  b e t w e e n  t h e  
robot software and the controlling musical applicatio n.  
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
NIME’11, 30 May–1 June 2011, Oslo, Norway. 
Copyright remains with the author(s).  
 
 
 
1.1 Listening to the “Bosphorus” sounds  
 
The microphones, as the ‘ears’ of the robots do transmit the 
signals as internet audio stream via an AudioIP unit. This 
audio-stream is being interpreted by an analysis application 
(Figure 2) with the computer at the installation v enue.  
 
 
Figure 2. Transmitting the Bosporus sounds to the analysis 
application and tracking of the sound sources on Max/MSP  
 
The analysis software is a Max/MSP[2] application and based 
on the interpretation of the spectral analysis of the audio stream 
with the help of the powerful Zsa.Descriptors [5]. The feature 
extraction process from the spectral data aims to recognize 5 
specific sound sources found on everyday routine of the 
Bosphorus. The analysis software is reporting the onset times 
of the sound events we are interested in. Each tracking of the 
relevant sound source is used to create a percussion score 
according to the rules defined for the translation of Bosphorus 
sonic space to percussion gestures.   
 
The input of the interactive sound system is th e content analysis 
of the audio- stream coming directly from Bosphorus. The 
output of this patch is the percussion score to be performed by 
the robots and also some of signal processing tools to create a 
dispositive sound distribution at the installation venue. A 10sec. 
long audio analysis frame will be captured live and     the 
analysis will be sent after each 10sec. frame to the algorithmic 
composition patch. Therefore the performed percussion score 
belongs to 10sec. in the past. Each robot has access to 2 types 
of Istanbul cymbals, 1 gong, one glockenspiel, 1 davul (a ethnic 
drum), 1 bendir (a ethnic hand drum) and 2 tibetian bowls. The 
generated rhythmic pattern for the relevant instrument is 
influenced by the length of the tracked source, and also on 
some statistical distribution of spectral parameters. The 
translation of percussion score is achieved by dedicated XML 
strings, which will be interpreted by the robot software. Each of 
the five tracks corresponds to a sound source in the analysis.  
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
52
The appli cation selects the tracks belonging to the sound 
sources occurring with highest density in each 10sec. There are 
39 different gestures programmed on the robot software, and 
these can be called with appropriate parameters to modify the 
gesture according to the performance needs.  
 
2. THE DEVELOPMENT PHASE 
  
We use 2 Kuka KR16 industrial robots having 6axis motion 
capability, 1.5m reach capacity and up to 15kg load capacity. 
First, we intended to investigate the dynamics of drumming 
with sticks, since the percussion timbre is highly dependent on 
the factors including contact area, hit damping duration and 
pressure on the drum skin. By being able to control these 
parameters and simulate them with robot gestures we can 
achieve the timbre richness and sonic variety for each 
instrument. 
 
2.1 The robot gripper part 
 
The gripper of the robot is attached at the flange of the robot, 
positioned next to the 6 th a x i s  m o t o r  A 6 .  T h e  d r u m  s t i c k s  
consist of parts named as the butt, shaft, shoulder and the tip. In 
our gripper design;  t h e  b u t t  p a r t ,  w h i c h  w o u l d  b e  g r a s p e d  
normally by a human hand, is positioned inside a spring for 
holding the stick tightly. (Figure 3) Our gripper can hold 3 
different sticks; wooden, soft and plastic stick. The robot can 
turn the gripper with the A6 motor, so that the chosen stick can 
hit on the vertical axis to the percussion surface. On Figure 8, 
there is the list of the programmed gestures along with the 
selected stick type for each instrument and the impact points.  
 
 
 
Figure 3. The gripper design (by GD Engineering, 
Germany) and its application  
The mechanism of the “hit gesture” is inspired by a piano 
hammer action and it is quite simple for quick adjustments. For 
each stick we have chosen an appropriate spring to realize the 
required elastic hit motion applied by the gripper. (Figure 3)  
 
2.2 The robot programming part 
 
Kuka Kr16 has 6 degrees of freedom, and the movement on 
each axis is driven by an independent motor labeled from A1 to 
A6. To control the motion of the robot each given value is 
compared t o  t h e  a c t u a l  a n g l e  o f  e a c h  a x i s  a n d  r e g u l a t e d  b y  
motion controllers for each axis. Amon g the two basic motion 
types; [4]  
 
PTP (point to point) movement: Each axis uses the shortest way 
from the actual position to the destination position. PTP can be 
programmed directly with values for each axis (eg "A1 30 " in 
degrees) or with cartesian values referring to a given base (eg 
"X 100" in mm). (Figure 4)  
 
LIN (linear) movement:  e a c h  a x i s  i s  controlled for a joined 
movement; they start and stop  its movement at the same time 
and the slowest axis will cause the others to slow down. The 
movement is a direct line from the actual position to the 
destination point. 
 
The cartesian movements have to be related to a base origin, 
which a r e  r e f e r r e d  t o  t h e  " r o bot r o o t ". T he gripper hot point 
(the sticks end) has to be defined and are always  r e f e r r i n g  t o  
one base except from the PTP movements with the axis values 
in degrees. T h e  r o t a t i o n ,  t h e  speed and acceleration for each 
movement has to be defined with in t h e  m o t i o n  c o m m and. 
KUKA offers spline motion path since software version 5.5, but 
with the software version we had, our robot -percussion gesture 
ended up with linear movement path.  
 
 
Figure 4. PTP movement: Robot axes are rotational; curved 
paths can be executed faster than straight paths.  
2.2.1 Communication interface  
We have chosen the KUKA Ethernet KRL XML  p l a t f o r m  t o  
develop our communication interface. Both the MaxMSP and 
the KUKA Ethernet Interface are working as TCP clients, and it 
was necessary to develop a bridge application, which receives 
the commands from MaxMSP and sends them to the robot. The 
maximum speed of ToolCentralPoint transfer is 2meters/sec. 
After establishing the basic command transfer, we defined the 
parameter space for each command according to the dynamics 
of the “hit gesture” and the common XML structure. (Figure 5) 
 
 
Figure 5. The Max patch building the XML messages and 
sending to the robot via TCP/IP protocol.   
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
53
2.2.2 The dynamics of the “hit gesture”  
 
The programming of the robot was divided into two parts. First 
the received commands had to be read, stored to variables and 
then verified. The stored values were used to recognize the 
selected instrument and to decide which movement had to be 
taken in order to move to the instrument without collisions. 
Then, the stroke parameters had to be applied to a common 
stroke movement routine. According to the robot coordinate 
system the impact point of the stick is the end point of the arm. 
The robot utilizes the 3 transition points to move the arm 
between instrument groups: for the cymbals Pt1, for the drums 
Pt2, and for the bowls and the glockenspiel Pt3. From each 
transition point, it moves to the relevant Hr p o i n t s  f o r  t h e  
instrument it will hit at the Hh point (Figure 6). The Hr point is 
the state of the robot, w h e r e  i t  w a i t s  r e a d y  t o  a p p l y  t h e  “ h i t  
gesture” Hr - Hh - Hr. 
 
 
Figure 6. Robot engineer P. Adler testing the software. The 
robot moves itself between various taught points.  
 
The kinetic hitting force caused by this movement is stored on 
the spring holding the relevant stick, and then the robot stops 
necessarily at the Hh p o i n t  b u t  t h e  s p r i n g  r e l e a s e s  t h e  s t o r e d  
energy and the stick continues and hits the surface. This is a 
basic spring oscillation mechanism applied on the stick axis. 
(Figure 7) 
 
 
   
Figure 7. According a command, the robot passes through a 
transition point and then a Hr point to hit at Hh point 
 
We proceeded by characterizing the percussion gestures, which 
will be applied by the robot arm. (Figure 8) Each instrument 
got its own base (coo rdinate system) and a common base for 
the mov ements between the instruments. Each gesture or 
transition movement was referred to these bases.  
 
We have developed a special subroutine for the gestures , which 
uses the quicker/faster axis motor s on the arm  ( A 4 -  A 6 ,  
especially A5) for the basic stroke movement and added motion 
in the kinematics through (A1 - A 3 )  t o  i n c r e a s e  the stroke 
hardness. This strategy does use the advantages of a robot arm 
with 6 degrees of freedom against the mechanical arms with 
one axis movement capability, and the richness of the 
movement does translate into sound quality. 1 
 
 
Figure 8. A catalogue of percussion gestures  
Our expectations from the robot drummer were reformed with 
meaningful optimizations of our model according to the 
dynamic motion and kinematic boundaries of the robot. The 
deceleration of the robot was in this case (small and therefore 
fast robot) adequate. Thanks to the spring mechanism there was 
no negative effect on the  speed of the stick. 
2.2.3 The parameter space of th e “hit gesture”  
 
To achieve the natural dynamics of percussion performance, the 
robot should be able to adjust the sound quality of its gestures 
in several manners. The analogous method is by striking the 
percussion surface at different locations with different sticks 
and contact points. The loudness variety is achieved by hitting 
harder or softer, which is the amount of pressure on the surface. 
The angle of the stick on the impact point is also relevant. We 
have implemented some controllable parameters, which can 
adjust the position and behavior of the robot arm to modify the 
impact quality. The following parameters can be adjusted 
directly within our musical application.  
 
- Stick type : 1- Woodenstick 2- Softstick  3- PlasticStick 
- Destination address : (Pt) (Hr) All transition points and hit 
gesture start/return points available for each instrument are the 
possible positions of the robot. Instrumen t number i is specified 
like 1 for Davul  2 - Bendir  3 - Gong etc.. Hit positions for each 
instrument are sub categorized as P1, P2, P3 etc. (Figure 8) 
- Speed : is a percentage of the maximum speed of the motors 
driving the axis movement specified between 0 a n d  100. The 
robot specs give the speed in terms of angular speed.  
- Hr position  off: offset value to mo ve the Hr position towards 
the percussion surface or away. 0 is the original taught point.  
- Hh position off: offset value to move the Hr position towards 
the percussion surface or away. 0 is the original taught point.  
Repositioning the Hr/Hh p o i n t s  a r e  of crucial importance in 
order to modify the sound quality achieved. For instance, when 
both the Hr and Hh points are modified with the same amount 
of offset value, the result is hardening or softening the impact 
while maintaining the speed of the robot movement.  
 
                                                                    
1 www.c-av-e.com/Kukarobot-bendir.mov 
 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
54
- Gesture Type  : 0 –  Basic Shot, 1 – Full Loop ; With “basic 
shot”, the robot hits the surface and goes back to the Hr point 
and maintains its position until a next command. When “full 
loop” is selected, the robot repeats the hit sequence with the 
specified amount. 
- Loop amount  :  T h e  r o b o t s  d o e s  r e p e a t  t h e  h i t  s e q u e n c e  t h i s  
number of times and the Hr, Hh and speed parameters can be 
adjusted inside this loop to achieve performance techniques 
such as accelerando, crescendo with percussion tremolos. 
 
We have also implemented on the robot software controlled 
randomness for the Hh point in terms of adjusting its coordinate 
on the horizontal plane of the percussion surface, which would 
humanize the impact since otherwise a robot would always hit 
to a pre cise point. During the evaluation process our first hand 
guide is always our ears, hence it was easy to observe whether 
the representation of the robot performer model was sufficient 
to finely represent the subtility of required gesture dynamics or 
not. We have created a database for each gesture for the easy 
access of the gesture types with specific dynamics; soft hit, 
hard hit, slow tremolo – soft hit, fast tremolo fast hit etc.  
 
 
 
2.3 Event scheduling 
 
In a musical performance knowing precisely each proces s time 
is necessary. When the robot receives a command, it sends back 
an acknowledgement to the MaxMSP application. (Figure 9) It 
does also send task finished messaged when the robot arm 
returns back to the specific Hr position. We define the time of 
command receive as 
€ 
Tcommand(x−1)for the hit event (x -1). Then 
the total task period, which lasts for the “hit gesture” is;  
 
€ 
Treturn(x −1) −Tcommand (x −1) = ttask(x −1)  
 
We do calculate this time interval easily by starting a clock 
with the send of the command and stopping it with the task end 
message received from the robot. This is a full cycle as shown 
on Figure 8. We need to know, the time interval between the 
command receive and the surface contact of the robot precisely. 
We name the moment of impact as 
€ 
Thit(x −1) , and the hit 
process interval can be calculated with;  
 
€ 
Thit(x −1) −Tcommand(x −1) = thitproc(x −1) 
 
We use a microphone to capture precisely the act of surface 
impact on the percussion. For instance with the help of the 
maxmsp bonk~ object[8] we can detect the hit event to stop the 
clock. Between
€ 
Tcommand( x)and 
€ 
Treturn(x −1)  t h e  r o b o t  w i l l  b e  
busy. The musical application has to consider this fact. 
Therefore we had to catalogue all possible 
€ 
thitproc(x ) a n d  
€ 
ttask( x ) values, which means 
€ 
C(37,2) = 39!
37!2!= 741 values to 
be stored for both. For this purpose, another max patch has 
been used 2 in to automate this process. The patch measures all 
741 
€ 
thitproc(x ) and 
€ 
ttask( x ) values and prepares a matr ix. 
                                                                    
2 www.c-av-e.com/gest-walk.mov  
Therefore we define much time in advance we need to send the 
command to the robot at its last position to get a hit on a certain 
time 
€ 
t( x ) and what would be the minimum task duration.  
 
Figure 9. The chronological view to the event schedule and 
message exchange between 2 computers.  
 
A high level musical representation of this structure has been 
required for direct score –to- robot gesture translatio n. We have 
chosen Ableton Live[1 ] with the Max4L addon, since it can 
directly integrat e a MaxMSP patch, which does translate the 
incoming sequencer messages to XML messages interpreted by 
the robot software. The track midi information is translated to 
relevant XML strings with proper time stamping of events as 
explained on section 2.3 by checking the 
€ 
thitproc(x ). A midi 
note represents a percussion gesture. Continuous midi 
controller messages assigned to it are translated to real -time 
parameters such as gesture speed, Hr a n d  Hh p o s i t i o n  o f  t h e  
stick and the loop amount.  
 
3. ACKNOWLEDGEMENTS 
My thanks to Kuka Robotics, Germany, which has supported 
the project by providing us 2 KR16 robots and also has assisted 
to the organization of robot motion programming and logistics. 
This project has been realized with the financial support of 
Istanbul2010 European Capital of Culture Agency . 
  
4. REFERENCES 
[1] Ableton – Ableton Live Suite, www.ableton.com 
[2] Cycling74 – MaxMSP, http:www.cycling74.com  
[3] Kapur A., Darling M. A Pedagogical Paradigm for 
Musical Robotics. In Proceedings of the Conference on New 
Interfaces for Musical Expression , (Sydney, Australia, 2010)  
[4] Kuka Robot Group, Kuka System 5.4 Software  Reference 
Manual. Issued on 24.04.2008. 
[5] Malt, M. and Jourdan, E. Real -Time Uses of Low Level  
Sound Descriptors as Event Detection functions Using 
Max/MSP Zsa.Descriptors. In  Proceedings of the SMCM9  
conference. (Recife, Brasil, 2009) 
[6] Singer, E., Feddersen, J., Redmon, C. and B owen B. 
LEMUR’s Musical Robots. In Proceedings of the NIME 
Conference (Hamamatsu, Japan, 2004)  
[7] Weinberg, G., Driscoll S., Parry M. Musical Interactions with 
a Perceptual Robotic Percussionist. Proceedings of IEEE 
International Workshop on Robot and Human Interactive  
Communication. (Nashville, TN 2005). 
Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway
55