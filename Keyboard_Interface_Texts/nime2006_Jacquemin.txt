Transmodal Feedback as a New Perspective for
Audio-visual Effects
Christian Jacquemin
LIMSI & Universit´e Paris 11
BP 133
91403 ORSAY , France
FirstName.FamilyName@limsi.fr
Serge de Laubier
Puce Muse
2 rue des Pyr´en´ees, Silic 520
Wissous 91320 RUNGIS Cedex, France
sdl@pucemuse.com
ABSTRACT
A new type of feedback is presented that involves both
the auditory and visual modalities. It co mbines an au-
dio resonant bandpass ﬁlter, a geo metrically constructed
mass-spring system and its graphical skin. The syste m
shows a resonant behavior that is detailed in various pa-
rameter setups. Complex mass-spring topologies result in
a coherent self-sustained audio-visual system that mimics
gusts of wind blowing a veil and associated sound eﬀects.
Keywords
Audio-visual composition, Transmodality, Feedback
1. TRANSMODAL FEEDBACK
The twentieth century has seen a very large body of
work concerning the connection between the visual and
acousticmodalities and, more speciﬁcally, between sound,
music, light, and image. Most of these works can be classi-
ﬁed as transmodal: either using images to generate sound,
or analyzing sound and music to generate graphics that
c a ni nt u r nb eu s e dt omodify sound and music [1].
Another line of artistic exploration concerns the con-
nection of one modality with itself: the notion of feedback.
First considered as a undesirable eﬀect, audio feedback
has been appropriated by popmusicians such as The Who
and Jimy Hendrix as an interesting ornamentation of their
music in which their instrume n t( ag u i t a r )w a su s e da sa
control ﬁlter. Audio feedback can be considered as an
intra-modalsystem that uses sound to generate sound.
Our purpose in this work is to explore the potentialities
of the combination of trans- and intramodal communica-
tions in what we ter m transmodal feedback.H o w c a n a
system for audio↔graphic feedback be designed, in which
sonic output is used as input for graphical synthesis, that
is in turn fed into the sound generator?
We ﬁrst analyze somet r a n smodal applications which of-
fer interesting insights into the correspondences that can
be established between the audio and graphic modalities.
Then, a transmodal feedback system that combines phys-
ical modeling, graphical rendering, and a sound resonator
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on theﬁrst page. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior
speciﬁc permission and/or a fee.
NIME 06,June 4-8, 2006, Paris, France
Copyright remains with the author(s).
is presented. Last, several variations are proposed in order
to illustrate diﬀerent parametrizations and renderings.
2. TRANSMODAL CORRESPONDENCES
The correspondences between twomodalities tend to be
metaphorical when they are used for artistic and creative
purposes, and tend to be more literal when they are used
for control purposes.
In the metaphorical category, and connecting sound to
graphics, is the work of Golan Levin. The sound (the
voice) is transformed into illustrative graphical eﬀects in-
spired from the cartoon world [9]. Similarly, rich graphical
environments such as urban models can be easily associ-
ated with sonic interpretations [19]. Metaphorical repre-
sentations introduce a distance between the source sti m-
ulus (image or sound) and its perceived eﬀect. For this
reason they are not appropriate for feedback eﬀects which
require better coherence between input and output.
Literal transmodal correspondences, that are better suited
to feedback, are encountered in systemsw h e r eamodality
is used to control another one. One of the motivations
behind these works is that human perceptual capabilities
depend on the modality. For instance, vision is very good
at distinguishing visual patters in large sets of visual data,
while audition is good at perceiving very brief sound vari-
ations.
Visual representation ofmusic is a literal correspondence
between graphics and audio that has its origin in the no-
tation ofmusic through scores. Digital media have oﬀered
new perspectives to interactive co mposition through the
graphical representation ofmusical composition. It can be
based on sophisticated musical theories such as Xenaki’s
theory for Iannix [5] ormore abstract representations such
as Sonos [16] or Metasynth [11]. Si milarly virtual instru-
ments are visual interfaces for music synthesis that focus
on playability, directmanipulation, and real-timei n t e r a c -
tion [8].
Duality of sonic and visual representation is also well il-
lustrated by visual representations of sound databases [15]
that can help the user to build amental map of the sound-
scape of the sample collection. The reverse combination of
sound and graphics is abstract data soniﬁcation: the pro-
cess of representing generic data bymeans of audio signals
[2].
Since our purpose is to close the loop and allow recip-
rocal transmodal information exchange, we return to the
notion of feedback in a resonating system before introduc-
ing our model of audio↔graphic feedback.
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
156
3. RESONANCE AND FEEDBACK
3.1 Audio and Video Feedback
Unimodal feedback is the process of capturing the signal
produced by an e mitter in a modality (typically a loud-
speaker for sound) and rea mplifying it. It is illustrated
by Figure 1. It generally involves the contribution of an
external trigger source that plays amore important role
in video feedback than in audio feedback.
Amplifier
External sound source
Oscillator for Audio Feedback Self−amplifier for Video Feedback
Loudspeaker
Amplifier
MonitorMicrophone Camera
External light source
Figure 1: Unimodal Feedback.
The “classic” audio feedback (also known as Larson ef-
fect) occurs when an a mpliﬁer receives as input its own
output. The loop results in an increasingly loud signal un-
til the limits of the ampliﬁer are reached. Audio feedback
can be seen as an echo with very short delay deﬁned by
the characteristics of the system (distance between loud-
speaker and microphone, ampliﬁer, characteristics of I/O
devices, room...), and transforms it into an oscillator. The
selected frequencies correspond to the Barkhausen eﬀect
and are such that the input and output signals are in phase
(with additive intensities) and the gain is slightly above 1.
Since the ampliﬁed signal ismainly controlled by the char-
acteristics of the system, the external sound source plays
the role of a trigger and the output pitch is dominated by
resonance frequencies.
What is known as video feedback is by nature very dif-
ferent from audio feedback since it relies only on gain and
not on oscillation. For this reason, all colors are equally
subject to ampliﬁcation, contrary to audio feedback that
ampliﬁes a very narrow band of frequencies. Periodicity
in video feedback occurs in space and not in ti me, and
results in tiling or kaleidoscopic eﬀects whose base graphi-
cal components are deﬁned by the external signal. (Visual
perception occurs in ti me and in space, but only spatial
perception involves resonance and periodicity.)
3.2 Transmodal Feedback
In order to design the architecture of a transmodal feed-
back system,w e must establish a reciprocal co mmunica-
tion between a graphical and an audio application so that
the signal emitted by one co mponent is accepted by the
other one. For this purpose we use networked applications
and encapsulate transmitted data via network messages.
The overall architecture is given in Figure 2 in which emit-
ters have been preserved for human access to the system
output, but sensors ( microphone and camera) have been
removed because they are not necessary any more (even
though unimodal feedback could be comb i n e dw i t ht r a n s -
modal feedback).
The design of an audio ↔graphic oscillator is not as
straightforward as it is for a pure audio syste m. First
there is a temporal inconsistency between the processing
Light source
Sound source
Sonic data transmission
Graphic Synthesis
Graphical data transmission
Sound Synthesis
Transducer
Loudspeaker Transducer
Monitor
Figure 2: Transmodal Feedback.
delays of the audio, graphic, and communication systems.
The processing delays in a graphic system are higher than
or equal to the frame refresh rate (typically 40ms). They
cumulate with the communication delays between the au-
dio and graphic system (around 1ms). In an audio system,
the delays are close to the period of the sound signal (a
few μs). The processing delays of an audio ↔graphic sys-
tem are controlled by the frame rate, and therefore greater
than 40ms.
A second temporal inconsistency concerns the e mitted
signals. The phase of the visual signal is several orders
ofmagnitude higher than the phase of the audio signal,
which is in turnmuch higher than the delays involved in a
looping audio↔graphic system. The syste m cannot work
as an oscillator as discussed for an ampliﬁer in pure audio
feedback.
When comparing unimodal audio and video feedbacks,
it appears that audio feedback oﬀers a richer do main of
experimentation because of its double nature: phase co-
incidence (the signal is tuned to the characteristics of the
system) and self-reinforcement. It see ms therefore desir-
able to build a system which will act as a resonator. Since
we cannot work on the signal directly (because of the
second temporal inconsistency), oscillations will concern
higher level audio para meters such as envelope or pitch.
Because of the ﬁrst temporal insconsistency, the resonator
frequency must be lower than 25hz, possibly much lower.
The architecture proposed in Figure 2 has no reason to
be an oscillator if the trans mitted data are not periodic.
In order to equip the application with a generator of pe-
riodic signals, the graphical co mponent is complemented
with amass-spring system (MSS) that directly controls the
graphical output, and indirectly the sound generation. We
now turn to the implementation of this architecture and its
two major building blocks: a skinned MSS on the graphi-
cal part, and a resonator related to the MSS dynamics on
the audio part. The application is named GraphSon.
4. AUDIO ↔GRAPHICS FEEDBACK
The architecture ofGraphSon is made of two networked
applications: an audio patch under Max/MSP [10] that
implements a resonant bandpass ﬁlter externally controlled
by the speed and acceleration of the graphical ele ments,
and a virtual 3D scene under Virtual Choreographer (Vir-
Chor) [18] that ismade of a skinned MSS parametrized by
the sound envelope derived from the audio patch. Data ex-
change between these components is made through OSC.
Figure 3 show the instantiation of Figure 2 in the case of
GraphSon.
4.1 Graphical and Physical Components
Mapping is considered as an important issue in the de-
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
157
Monitor
Loudspeaker
Transmission of sound enveloppe Massspring system
Transmission of mass location
+ color under cursor
Bandpass filter
with variable
peak frequency
+ skin
Skin and background
textures
Sound source
(pink noise or sample)
Virtual ChoreographerMax/MSP
Graphic SynthesisSound Synthesis
Figure 3: GraphSon Architecture.
sign of virtual instruments and concerns the “intelligent”
and sensitive association between a musician’s gestures
and the control of his/her instru ment. Mapping tends
to be considered not just as an interface, but as an au-
tonomous component in virtual instru ments. Because of
their intuitive and rich behavior, physical models can be
used as mapping devices that produce co mplex and vari-
able responses to stimuli: for instance, obstructions in par-
ticle ﬂows and resulting collisions (FlowField [4]), or MSSs
and their complex dynamics (GENESIS [3] orPMPD [12]).
Our interest for such systemsi nt h i sw o r ki sn o tf o rt h e
purpose of mapping human stimuli to musical synthesis,
but for the introduction of a resonator in our audio↔graphic
feedback loop parameter (Figure 3). The MSS associates
input sound envelope values to a graphical output through
an indirectmechanism.
In a MSS, the equation that controls the dynamics of a
mass Mi that is linked to ni masses Mi,j ,i s
mi.x′′= −d.x′+mi.gx +
niX
j=1
ki,j (d(Mi,M i,j )−linii,j )( 1 )
in which mi is the mass of Mi, d the viscous damping coef-
ﬁcient, g the gravity, ki,j the spring constants, and linii,j
the lengths of the unstretched springs. Sound envelope e
is used to modify dynamically two of the MSS character-
istics: its damping factor and the spring elasticity
d = kdamp.e and ∀i, j ki,j = kelast.e (2)
The audio-visual eﬀect is that high sounds result in a stiﬀ
and constrained MSS (mild and sustained wind in a non
extensible veil), while low sounds result in a weak and free
MSS (strong gusts of wind in a light and extensible veil).
In the second case, the potential energy accumulated in the
veil can be released suddenly and transfor med in kinetic
energy. Such a correlation produces perceptually plausible
correspondences between audio and graphics [7].
The graphical scene is i mplemented in VirChor.T h e
<graph> element describes a MSS, and the <patch> ele-
ment a Bezier patch. At each fra me, a script is executed
that reconnects the control points of the skin to themasses
of the MSS. Twomodels are designed according to table 1
and illustrated by Figure 4. A quad is used as skin in the
simplest model GraphSon2.
The target application is GraphSon4×4 because it oﬀers
richer behaviors, and better graphical renderings and an-
imations. It co mbines a 4 × 4 MSS with a grid topology
and a bicubic Bezier patch deﬁned by 16 control points
(masses at nodes, springs for inter-connectivity). The sim-
plest application is used for analyzing the para meter ef-
fects and resonating behaviors in section 5 under si mpler
experimental conditions and fewer parameters.
Table 1: Parameters ofGraphSon Instances.
Name Masses (ﬁxed) Springs Skinning
GraphSon2 2( 1 ) 1 Q u a d
GraphSon4×4 16 (2) 24 Patch 4 × 4
GraphSon
GraphSon
Spring
Masses
Handlebar for mouse control of fixed masses
Masses
Springs
Translucent
veil (Bezier
patch controlled
by the MSS)
Handlebar for mouse control of fixed masses
4x4
2
Figure 4: Two instances of GraphSon:G e s t u r e s
are transmitted to the upper masses of a MSS that
controls an animated translucent veil.
4.2 Audio Component
For audio-visual coherence purposes, the sound gener-
ated from the graphical output is intended to reproduce
the noise of a veil in the wind. The eﬀect is obtained by
using a pink noise source (the wind) ﬁltered by a digital
bandpass ﬁlter that produces high pitch noise for strong
gusts of wind.
The ﬁlter is controlled by its quality Q, its gain G,
and its center frequency fres . The higher the quality, the
shorter the bandwith, and the higher the output at the
resonance frequency. The second order equation used for
the ﬁlter is
yn = G(xn − r.xn−2)+ c1.yn−1 + c2.yn−2 (3)
r, c1,a n dc2 are parameters calculated from fres and Q.
In order to produce a satisfactory audio eﬀect, the reso-
nance frequency is controlled by the acceleration ofmasses
in the bottom line. Strong accelerations of these masses
correspond to high pitch output, giving the impression of
a strong wind blowing the veil.
The resonance ﬁlter is implemented in Max/MSP with
the reson~ object that has 4 inputs: an audio signal and
3 digital values G, fres ,a n d Q. Equation (3) is taken
from [10]. The frequency fres is a linear function of the
acceleration of one of the masses in the MSS. It is co m-
puted in the audio patch from the values of themass loca-
tion received from the graphical component. The output
of reson~ is the ﬁltered audio input, pink noise produced
by the object pink~. The envelope of the output audio
signal, sent to the graphical component, controls damping
and elasticity.
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
158
5. FEEDBACK CONTROL AND ANALY-
SIS
We now turn to the study of the resonating audio↔gra-
phic feedback loop under various para meter values. The
behavior of the feedback loop depends on several factors:
the topology of the MSS, the parameters of the audio sys-
tem including the nature of the base sound (noise or sam-
ple), the transmission delays through the network, and the
motion of the controlled mass by the user. This section is
intended to provide better insight of the basic echo reso-
nance in the system in its simplest form: pink noise and
a2 -mass 1-spring syste m. More detail is also provided
on the parametrization of the system and its eﬀect on the
animation of the graphical scene and the audio output.
5.1 Basic System
If the si mplest MSS ( GraphSon2 presented in 4.1) is
connected to the resonance ﬁlter fed with a pink noise a
periodic behavior is observed, illustrated by Figure 5. In
this ﬁgure two values are plotted that trace the dynamics
of the audio and graphic systems:
• the height of the lower mass, the free mass since the
other one is ﬁxed to the handle (dotted line),
• the sound level which is used to control damping and
spring coeﬃcient (solid line).
-8
-6
-4
-2
 0
 2
 4
 6
 8
 20  25  30  35  40  45  50  55  60
enveloppe and free mass height
time (sec)
Sound enveloppe e and free mass location y (log_GS2_pink)
e(t)
y(t)
Figure 5: GraphSon2 Basic Resonating System (see
upper part of Figure 4): 1 Fixed Mass, 1 Vertically
Moving Mass, 1 Spring, and Pink Noise.
The basic behavior can be described as follows. When
the mass reaches its lowest position ( maximal extension
of the spring), it slows down, decreases the pitch of the
resonance frequency, and increases theQ of the ﬁlter. This
results in a weaker sound that in turn decreases damping
and spring coe ﬃcients. Because of low da mping values,
the MSS beco mes more reactive to s mall movements of
the lower mass and the spring then retracts very quickly.
The use of various sound samples does not modify sig-
niﬁcantly the behavior of the resonator, even though it has
as t r o n gimpact on the audio output. Several tests were
made with various kinds of music: piano romantic music,
techno/world music, natural sound eﬀects... but none had
as t r o n gimpact on the system behavior. Such observations
are coherent with resonating audio feedback, in which res-
onance is controlled by the system characteristics and the
trigger sound plays a secondary role.
5.2 Color Parametrization
In order to provide the user with easy access to the pa-
rametrization of the audio system (and also indirectly on
the graphical system), the red, green, blue components of
the color under the mouse cursor are trans mitted to the
audio patch and associated with para meters of the audio
resonator. The associations are made as follows:
• T h eg r e e nv a l u ec o n t r o l samultiplicative factor of
acceleration that deﬁnes fres and also controls G,
• t h er e dv a l u ec o n t r o l sQ (the height and width of its
bandwith),
• the red and blue values bring an additional additive
factor to fres .
The color can be used in two ways. It can either be
used as a control device for the user. If she/he moves the
mouse cursor on the background image, various responses
are obtained from the system. Color can also be used in a
more passive way by placing themouse cursor on the ani-
mated veil. Then the variation of colors under the mouse
cursor results in dyna mic modiﬁcation of audio para me-
ters that reciprocally modify the animation and rendering
of the audio scene.
Various types of veil colorings are used to produce dif-
ferent color variations and thus diﬀerent behaviors of the
feedback loop. In Figure 4 above, two types of veils are
shown. In the upper snapshot, a blended semi-translucent
veil is used: from white opaque at the top to translucent at
the bottom. The bottom snapshot shows a more complex
rendering of the veil that is implemented through shaders:
the veil color is the composite of several semi-transparent
textures combined with masks. The transparency param-
eters of textures and masks are computed from dynamic
geometrical characteristics of the veil and vary according
to its dynamics.
The combined eﬀects of color and veilmotion are shown
in Figure 6. The color under themouse cursor is the blend-
ing of a red background color and the se mi-transparent
white color of the veil. Because of the high value of the red
channel, the audio resonator is a sharp ﬁlter with a narrow
bandwith. Because of the veilmotion, when the veil drops
the color under the mouse cursor becomes whiter, which
makes blue and green values higher, and thus tend to re-
activate the audio system.T h e c ombination of these two
eﬀects gives the resonator of the feedback loop a s maller
period than it had without the veil (compare Figure 5 for
ﬁxed pink color andmouse cursor outside the veil and Fig-
ure 6 with mouse cursor over the veil).
5.3 Complex System Behavior
We now turn to GraphSon4×4,aM S S made of a 4 × 4
grid of masses that controls a bicubic Bezier patch. As for
simpler systems, we plot on the sa meg r a p ht h el o c a t i o n
of the lowest left mass and the audio level.
Because of the more complex internal dyna mics of its
MSS, the loop resonance is not as clear as it is in the case
of a 2-mass 1-spring system. The veil has its own internal
short term dynamics that combine with the longer ter m
loop dynamics. The loop resonance is easier to detect for
low color values associated with a soft ﬁlter (Figure 7).
If the audio syste m receives a bright color associated
with high parameter values for gain and quality, the out-
put level is higher and higher values are sent for damping
and spring coeﬃcients. The veil has short amplitude move-
ments with very short periods. Periodicity is much more
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
159
-8
-6
-4
-2
 0
 2
 4
 6
 8
 20  25  30  35  40  45  50  55  60
enveloppe and free mass height
time (sec)
Sound enveloppe e and free mass location y (log_GS2_redOnVeil)
e(t)
y(t)
Figure 6: MSS Topology of Figure 5 and Pink
Noise, with an Additional Control through the
Color under the Mouse Cursor. The Cursor is
Located over a Semi-transparent White Veil and
a Red Background Color.
-60
-50
-40
-30
-20
-10
 0
 10  12  14  16  18  20  22  24  26  28  30
enveloppe and free mass height
time (sec)
Sound enveloppe e and free mass location y (log_GS4x4_mountain)
e(t)
y(t)
Figure 7: GraphSon4×4 A MSS made of a Grid of
16 Masses (see lower part of Figure 4): 2 Fixed
Masses, 14 Vertically Moving Masses, 24 Springs,
and Pink Noise. Mouse Cursor on Dark Back-
ground Color (0,0.02,0.07).
diﬃcult to detect in the resultingmotion and audio signal
(Figure 8).
As for the previous simpler MSSs, the audio signal does
not play an i mportant role in the dyna mics of the sys-
tem.O t h e rp a r ametrizations of veil and sound should be
considered if the purpose is to inﬂuencemore strongly the
loop resonance by the audio signal.
5.4 Combination with Gesture
The simplest system (GraphSon2 and the mouse cursor
on a static pink color) has an autonomous resonance that
is shown Figure 5. If this syste m is manipulated by an
operator who controls the location of the ﬁxed mass (the
upper mass), the system behaves as follows (see Figure 9):
1. during gesture control, the output follows the con-
strained motion of the upper mass (the values be-
tween the two vertical dotted lines),
2. when the manipulation is completed, the system has
a transient chaotic behavior (5 to 10 seconds),
3. ﬁnally the periodic resonance restarts and begins by
-15
-14
-13
-12
-11
-10
-9
-8
-7
-6
 10  12  14  16  18  20  22  24  26  28  30
enveloppe and free mass height
time (sec)
Sound enveloppe e and free mass location y (log_GS4x4_white)
e(t)
y(t)
Figure 8: MSS Topology of Figure 7 and Pink
Noise. Mouse Cursor on White Background Color
(1,1,1).
a decreasing slope followed by a short peak.
These results show that strong gestures can control the
system while they are executed and for a short ti mea f -
terward, but the syste m quickly returns to its periodic
behavior when the excited state is over.
-25
-20
-15
-10
-5
 0
 5
 10
 15
 0  20  40  60  80  100  120  140  160  180
enveloppe and free mass height
time (sec)
Sound enveloppe e and free mass location y (log_GS2_pink_gesture)
e(t)
y(t)
Small circles Vertical motion Horizontal motion Large circles
Figure 9: Gesture Mapping withGraphSon,C o n -
ditions of Figure 5.
6. SYNTHESIS AND PERSPECTIVES
In this study, we have presented a model and an appli-
cation that build an audio ↔graphic feedback loop made
of a MSS and its visual skinning, and a resonant bandpass
ﬁlter. Audio level is used to control the physical system
dynamics, while mass acceleration controls the ﬁlter char-
acteristics. In addition, color under mouse cursor directly
parametrizes the ﬁlter and indirectlymodiﬁes the MSS re-
activity. The loop actually behaves like a resonant system
with a period between 2 and 5 seconds. Periodicity is bet-
t e ro b s e r v e do nas imple MSS or in quiet situations (soft
ﬁlter and dark color).
Further studies could be carried out:
• The syste m dynamics can be studied for mally in
the simple case by taking into account the internal
characteristics of the audio and graphic systemsa n d
the information propagation delays between the two
components. The output of the formal study should
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
160
be then compared with the dynamics observation in
the computer model.
• The artistic or industrial applications of such an audio-
visual environment for the realistic or non-realistic
rendering of natural phenomena such as wind can be
further investigated. Current works tend to study
separately graphical and sonicmodeling [13], but we
are convinced that deeper investigations of the per-
ceptual correlations between sound and image in the
modeling of such natural pheno mena are promising
directions of research [7, 6]. It is therefore neces-
sary to design new generations of audio-visual envi-
ronments such as the one presented in this study to
oﬀer a fra mework for such studies on multi-modal
modeling and perception.
• For sound creation purposes, richer para meter sets
and richer topologies could be taken into consider-
ation: other MSS topologies such as the ones ex-
plored by PMPD for audio-visual composition [12],
other audio patches with physical modeling of wind
phenomena such as the ones used formusical instru-
ments [17], other color parameters such as hue, sat-
uration, and value, and more complex visual render-
ings through physical cloth modeling or shaders and
BTF textures.
• If the purpose is to design a virtual instrument that
uses the feedback resonance for graphical and au-
dio synthesis, gesture-based control should be in-
vestigated more deeply, possibly with haptic feed-
back [14]. High speed in graphical rendering through
bitmap animation or decoupling of mass-spring an-
imation and associated skinning would yield higher
resonance frequencies in the audio↔graphic loop and
produce interesting audio-visual patterns.
7. ACKNOWLEDGMENTS
This study has beneﬁted fro m a research collaboration
on virtual instru ments between the authors and Hugues
Genevois (LAM), Brian Katz (LIMSI), and Norbert Schnell
(IRCAM). Many thanks to Brian Katz, Jean-Baptiste Thie-
baut (Queen Mary Univ.), and the three anonymous re-
viewers for their comments on a preliminary version of the
paper.
8. REFERENCES
[1] Audiosculpt. http://forumnet.ircam.fr/349.html.
[2] S. Barrass and G. Kramer. Using soniﬁcation.
Multimedia Systems, 7(1):23–31, 1999.
[3] C. Cadoz, A. Luciani, J.-L. Florens, and
N. Castagn´e. ACROE-ICA: Artistic creation and
computer interactive multisensory simulation force
feedback gesture transducers. In Proceedings of New
Interfaces for Musical Expression (NIME’03), pages
235–246, 2003.
[4] T. Chen, S. Fels, and T. Schiphorst. Flowﬁeld:
Investigating the semantics of caress. In ACM
Special Interest Group on Graphics and Interaction
(SIGGRAPH’02), page 185, 2002.
[5] T. Coduys and G. Ferry. IanniX
aesthetical/symbolic visualisations for hypermedia
composition. In Proceedings International
Conference Sound and Music Computing (SMC ’04),
2004.
[6] T. Funkhouser, N. Tsingos, I. Carlbom,G .E l k o ,
M. Sondhi, J. West, G. Pingali, P. Min, and
A. Ngan. A beam tracing method for interactive
architectural acoustics. The Journal of the Acoustical
Society of America (JASA), 115(2):739–756, 2004.
[7] J. K. Hahn, J. Geigel, J. W. Lee, L. Gritz,
T. Takala, and S. Mishra. An integrated approach to
motion and sound. The Journal of Visualization and
Computer Animation, 6(2):109–124, 1995.
[8] S. Jord`a. Sonigraphical instruments: From FMOL to
the ReacTable. In Proceedings of New Interfaces for
Musical Expression (NIME’03), pages 70–76, 2003.
[9] G. Levin and Z. Lieber man. In-situ speech
visualization in real-time interactive installation and
performance. In Proceedings of NPAR’04, pages
7–14, 2004.
[10] MaxMSP 4.5 reference manual.
http://www.synthesisters.com/download.
[11] MetaSynth.
http://www.uisoftware.com/MetaSynth/.
[12] A. Momeni and C. Henry. Dynamic independent
mapping layers for concurrent control of audio and
video synthesis. Computer Music Journal,
30(1):49–66, 2006.
[13] S. Ota, T. Fujimoto, M. Tamura, K. Muraoka,
K. Fujita, and N. Chiba. ”1/fβ noise-based real-time
animation of trees swaying in wind ﬁelds. In
Proceedings of Computer Graphics International
(CGI’03), 2003.
[14] X. Rodet, J.-P. Lamb e r t ,R .C a h e n ,T .G a u d y ,
F. Gosselin, and F. Gu´edy. Sound and music control
using haptic and visual feedback in the PHASE
installation. InProceedings of New Interfaces for
Musical Expression (NIME’05), pages 109–114, 2005.
[15] D. Schwarz. Recent Advances in Musical
Concatenative Sound Synthesis at Ircam.I n
Workshop Audio Mosaicing: Feature-Driven Audio
Editing/Synthesis. (ICMC’05), Barcelona, 2005.
[16] A. Sedes, B. Courribet, and J.-B. Thiebaut.
Visualization of sound as a control interface. In
Proceedings of the 7th International Conference on
Digital Audio Eﬀects (DAFX’04), 2004.
[17] J. Smith. Virtual acoustic musical instruments:
Review and update. Journal of New Music Research,
33(3):283–304, – 2004.
[18] Virtual Choreographer 1.2 reference manual.
http://virchor.sourceforge.net/html/.
[19] P. Waters and A. Rowe. Alt-space: Audio-visual
interactive software for developing narrative
environments. In Proceedings CADE2004, 2004.
Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06), Paris, France
161